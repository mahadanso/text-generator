{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahadanso/text-generator/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LouEQKWTkbhY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdc7941e-75c7-4f73-9008-c6a64ab02344"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\"\"\"\n",
        "    Shape(row, column)\n",
        "\"\"\"\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "\"\"\"\n",
        "    Load data and get all the chars in text.\n",
        "\"\"\"\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "chars = sorted(list(set(text)))\n",
        "char_size = len(chars)\n",
        "print('char_size : ' + str(char_size))\n",
        "\"\"\"\n",
        "    create dictionary to link each char to an id, and vice versa\n",
        "\"\"\"\n",
        "char2id = dict((c, i) for i, c in enumerate(chars))\n",
        "id2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Text data have to be arranged into set of sections of {len_per_section} characters text \n",
        "    with the next character following the section as the output of the section.\n",
        "    \n",
        "    Then from the starting of the previous section, {skip} characters are skipped to start the next\n",
        "    section that will form the input of the following input.\n",
        "    \n",
        "    Considering {len_per_section} to be 50 and skip = 2.\n",
        "    section_1 = text[n:n+50]        =>      next_char_1 = text[n+50]\n",
        "    section_2 = text[n+2:n+2+50]    =>      next_char_2 = text[n+2+50]\n",
        "    ......\n",
        "    ....\n",
        "\"\"\"\n",
        "len_per_section = 5\n",
        "skip = 2\n",
        "sections = []\n",
        "next_chars = []\n",
        "\n",
        "\n",
        "for i in range(0, len(text) - len_per_section, skip):\n",
        "    sections.append(text[i: i + len_per_section])\n",
        "    next_chars.append(text[i + len_per_section])\n",
        "\n",
        "\"\"\"\n",
        "    Create two vectors of zeros to\n",
        "    \n",
        "    X   => to store sections, 3 dimension.\n",
        "            1-D to store a char,\n",
        "            2-D to store a specific section of characters\n",
        "            3-D to store all the sections\n",
        "    \n",
        "    y   =>  to store the next chars, 2 dimension.\n",
        "            1-D to store a char,\n",
        "            2-D to store all the next chars each for a section\n",
        "            \n",
        "    dtype   =>  int\n",
        "\"\"\"\n",
        "X = np.zeros((len(sections), len_per_section, char_size), dtype=int)\n",
        "y = np.zeros((len(sections), char_size), dtype=int)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Go through the sections, grab the characters and one-hot encode them.\n",
        "\"\"\"\n",
        "for i, section in enumerate(sections):\n",
        "    for j, char in enumerate(section):\n",
        "        X[i, j, char2id[char]] = 1\n",
        "    y[i, char2id[next_chars[i]]] = 1\n",
        "    \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "\"\"\"\n",
        "Var\n",
        "\"\"\"\n",
        "batch_size = 500\n",
        "max_steps = 10\n",
        "log_every = 1\n",
        "save_every = 5\n",
        "hidden_nodes = 100\n",
        "\"\"\"\n",
        "    Directory to store a trained model\n",
        "\"\"\"\n",
        "checkpoint_directory = 'ckpt/model'  # + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H:%M:%S')\n",
        "\n",
        "if tf.gfile.Exists(checkpoint_directory):\n",
        "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
        "tf.gfile.MakeDirs(checkpoint_directory)\n",
        "\n",
        "\"\"\"\n",
        "    Directory to store tensorboard summaries\n",
        "\"\"\"\n",
        "tensorboard_directory = 'temp/tensorboard'\n",
        "\n",
        "if tf.gfile.Exists(tensorboard_directory):\n",
        "    tf.gfile.DeleteRecursively(tensorboard_directory)\n",
        "tf.gfile.MakeDirs(tensorboard_directory)\n",
        "\n",
        "\"\"\"\n",
        "    Define variable needed for the operations and outline the flow of data through this operations of computations.\n",
        "    Basically outlining a computation graph to be later used.  \n",
        "\"\"\"\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "    global_step = tf.Variable(0)\n",
        "\n",
        "    with tf.name_scope('train_input'):\n",
        "        # placeholders (no data in it during graph const), but will hold data during a session\n",
        "        \"\"\"\n",
        "        1D  :   Store batch\n",
        "        2D  :   Store input chars section\n",
        "        3D  :   Store char\n",
        "        \"\"\"\n",
        "        data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size], name=\"X\")    # input data\n",
        "\n",
        "        \"\"\"\n",
        "            1D  :   Store batch\n",
        "            3D  :   Store output char\n",
        "            \"\"\"\n",
        "        labels = tf.placeholder(tf.float32, [batch_size, char_size], name=\"Y\")    # output data\n",
        "\n",
        "    with tf.name_scope(\"WeightsLayer1\"):\n",
        "        \"\"\"\n",
        "            Initialise weights and biases.\n",
        "            Weights initialised with random values from a truncated normal distribution.\n",
        "            Biases initialised to zero.\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"update\"):\n",
        "            w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wui')\n",
        "            tf.summary.histogram('weights_i', w_ii)\n",
        "            w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wio')\n",
        "            tf.summary.histogram('weights_o', w_io)\n",
        "            b_i = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bi')\n",
        "            tf.summary.histogram('bias', b_i)\n",
        "        with tf.name_scope(\"reset\"):\n",
        "            w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wri')\n",
        "            tf.summary.histogram('weights_i', w_fi)\n",
        "            w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wfo')\n",
        "            tf.summary.histogram('weights_o', w_fo)\n",
        "            b_f = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bf')\n",
        "            tf.summary.histogram('bias', b_f)\n",
        "        with tf.name_scope(\"output\"):\n",
        "            w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='woi')\n",
        "            tf.summary.histogram('weights_i', w_oi)\n",
        "            w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='woo')\n",
        "            tf.summary.histogram('weights_o', w_oo)\n",
        "            b_o = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bo')\n",
        "            tf.summary.histogram('bias', b_o)\n",
        "        with tf.name_scope(\"Cell\"):\n",
        "            w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wci')\n",
        "            tf.summary.histogram('weights_i', w_ci)\n",
        "            w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wco')\n",
        "            tf.summary.histogram('weights_o', w_co)\n",
        "            b_c = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bc')\n",
        "            tf.summary.histogram('bias', b_c)\n",
        "\n",
        "    \"\"\"\n",
        "        lstm(i, o, s):\n",
        "        Take in Inputs i and Outpus o, and Previous State and compute a new\n",
        "        state and output.\n",
        "        _____________________________________________________        \n",
        "            Input   i   :   shape=(batch_size, char_size)\n",
        "            Output  o   :   shape=(batch_size, hidden_layers)\n",
        "            State   s   :   shape=(batch_size, hidden_layers)\n",
        "        _____________________________________________________\n",
        "    \"\"\"\n",
        "    def lstm(i, o, s, name):\n",
        "        with tf.name_scope(str(name) + \"LSTMLayer1\"):\n",
        "            # the scalars are all between zero and one inclusive.\n",
        "            with tf.name_scope(\"Gates\"):\n",
        "                # compute a scalar that decides what to remember about the previously seen characters\n",
        "                # and what to include about the new character.\n",
        "                with tf.name_scope(\"update_gate\"):\n",
        "                    update_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
        "                    # tf.summary.scalar('input_gate', input_gate)\n",
        "\n",
        "                # compute a scalar that decides what to discard about the previous output.\n",
        "                with tf.name_scope(\"reset_gate\"):\n",
        "                    reset_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
        "                    # tf.summary.scalar('forget_gate', forget_gate)\n",
        "\n",
        "                    # compute a scalar that decides what to include in the new output.\n",
        "                with tf.name_scope(\"output_gate\"):\n",
        "                    output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
        "                    # tf.summary.scalar('output_gate', output_gate)\n",
        "            with tf.name_scope(\"compute_cell\"):\n",
        "                memory_cell = tf.matmul(i, w_ci) + reset_gate * tf.matmul(o, w_co) + b_c\n",
        "\n",
        "            with tf.name_scope(\"Update_state\"):\n",
        "                s = update_gate * s + (1 - update_gate) * memory_cell\n",
        "\n",
        "            o = output_gate * tf.tanh(s)\n",
        "            # tf.summary.scalar('state', s)\n",
        "\n",
        "            return o, s\n",
        "\n",
        "    with tf.name_scope(\"WeightsLayer2\"):\n",
        "        \"\"\"\n",
        "            Initialise weights and biases.\n",
        "            Weights initialised with random values from a truncated normal distribution.\n",
        "            Biases initialised to zero.\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"update\"):\n",
        "            w_ii1 = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wui')\n",
        "            tf.summary.histogram('weights_i', w_ii1)\n",
        "            w_io1 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wio')\n",
        "            tf.summary.histogram('weights_o', w_io1)\n",
        "            b_i1 = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bi')\n",
        "            tf.summary.histogram('bias', b_i1)\n",
        "        with tf.name_scope(\"reset\"):\n",
        "            w_fi1 = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wri')\n",
        "            tf.summary.histogram('weights_i', w_fi1)\n",
        "            w_fo1 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wfo')\n",
        "            tf.summary.histogram('weights_o', w_fo1)\n",
        "            b_f1 = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bf')\n",
        "            tf.summary.histogram('bias', b_f1)\n",
        "        with tf.name_scope(\"output\"):\n",
        "            w_oi1 = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='woi')\n",
        "            tf.summary.histogram('weights_i', w_oi1)\n",
        "            w_oo1 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='woo')\n",
        "            tf.summary.histogram('weights_o', w_oo1)\n",
        "            b_o1 = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bo')\n",
        "            tf.summary.histogram('bias', b_o1)\n",
        "        with tf.name_scope(\"Cell\"):\n",
        "            w_ci1 = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32), name='wci')\n",
        "            tf.summary.histogram('weights_i', w_ci1)\n",
        "            w_co1 = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32), name='wco')\n",
        "            tf.summary.histogram('weights_o', w_co1)\n",
        "            b_c1 = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32), name='bc')\n",
        "            tf.summary.histogram('bias', b_c1)    \n",
        "  \n",
        "    \"\"\"\n",
        "        lstm(i, o, s):\n",
        "        Take in Inputs i and Outpus o, and Previous State and compute a new\n",
        "        state and output.\n",
        "        _____________________________________________________        \n",
        "            Input   i   :   shape=(batch_size, char_size)\n",
        "            Output  o   :   shape=(batch_size, hidden_layers)\n",
        "            State   s   :   shape=(batch_size, hidden_layers)\n",
        "        _____________________________________________________\n",
        "    \"\"\"\n",
        "    def lstm_1(i, o, s, name):\n",
        "        with tf.name_scope(str(name) + \"LSTMLayer2\"):\n",
        "            # the scalars are all between zero and one inclusive.\n",
        "            with tf.name_scope(\"Gates\"):\n",
        "                # compute a scalar that decides what to remember about the previously seen characters\n",
        "                # and what to include about the new character.\n",
        "                with tf.name_scope(\"update_gate\"):\n",
        "                    update_gate = tf.sigmoid(tf.matmul(i, w_ii1) + tf.matmul(o, w_io1) + b_i1)\n",
        "                    # tf.summary.scalar('input_gate', input_gate)\n",
        "\n",
        "                # compute a scalar that decides what to discard about the previous output.\n",
        "                with tf.name_scope(\"reset_gate\"):\n",
        "                    reset_gate = tf.sigmoid(tf.matmul(i, w_fi1) + tf.matmul(o, w_fo1) + b_f1)\n",
        "                    # tf.summary.scalar('forget_gate', forget_gate)\n",
        "\n",
        "                    # compute a scalar that decides what to include in the new output.\n",
        "                with tf.name_scope(\"output_gate\"):\n",
        "                    output_gate = tf.sigmoid(tf.matmul(i, w_oi1) + tf.matmul(o, w_oo1) + b_o1)\n",
        "                    # tf.summary.scalar('output_gate', output_gate)\n",
        "            with tf.name_scope(\"compute_cell\"):\n",
        "                memory_cell = tf.matmul(i, w_ci1) + reset_gate * tf.matmul(o, w_co1) + b_c1\n",
        "\n",
        "            with tf.name_scope(\"Update_state\"):\n",
        "                s = update_gate * s + (1 - update_gate) * memory_cell\n",
        "\n",
        "            o = output_gate * tf.tanh(s)\n",
        "            # tf.summary.scalar('state', s)\n",
        "\n",
        "            return o, s      \n",
        "          \n",
        "    # initial output and state to zero\n",
        "    output = tf.zeros([batch_size, hidden_nodes])\n",
        "    state = tf.zeros([batch_size, hidden_nodes])\n",
        "\n",
        "    # loop through all the sections.\n",
        "    for i in range(len_per_section):\n",
        "\n",
        "        \"\"\"\n",
        "            data[:, i, :]   :   i(th) section from the given batch\n",
        "        \"\"\"\n",
        "        output, state = lstm(data[:, i, :], output, state, str(i))\n",
        "\n",
        "        \"\"\"\n",
        "            outputs_all_i   :   stores the outputs from the lstm\n",
        "            labels_all_i    :   stores the characters that follow, Which are the correct labels\n",
        "        \"\"\"\n",
        "\n",
        "        if i == 0:  # if first section\n",
        "            outputs_all_i = output  # make current output the start\n",
        "            labels_all_i = data[:, i + 1, :]    # make next input as the start\n",
        "\n",
        "        elif i != len_per_section - 1:  # not first or last section\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)   # append the current output\n",
        "            labels_all_i = tf.concat([labels_all_i, data[:, i + 1, :]], 0)  # append the next input\n",
        "\n",
        "        else:   # the last section\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)   # append the current output\n",
        "            labels_all_i = tf.concat([labels_all_i, labels], 0)     # append the final labels\n",
        "\n",
        "    for i in range(len_per_section):\n",
        "\n",
        "        \"\"\"\n",
        "            data[:, i, :]   :   i(th) section from the given batch\n",
        "        \"\"\"\n",
        "        output, state = lstm_1(data[:, i, :], output, state, str(i))\n",
        "\n",
        "        \"\"\"\n",
        "            outputs_all_i   :   stores the outputs from the lstm\n",
        "            labels_all_i    :   stores the characters that follow, Which are the correct labels\n",
        "        \"\"\"\n",
        "\n",
        "        if i == 0:  # if first section\n",
        "            outputs_all_i = output  # make current output the start\n",
        "            labels_all_i = data[:, i + 1, :]    # make next input as the start\n",
        "\n",
        "        elif i != len_per_section - 1:  # not first or last section\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)   # append the current output\n",
        "            labels_all_i = tf.concat([labels_all_i, data[:, i + 1, :]], 0)  # append the next input\n",
        "\n",
        "        else:   # the last section\n",
        "            outputs_all_i = tf.concat([outputs_all_i, output], 0)   # append the current output\n",
        "            labels_all_i = tf.concat([labels_all_i, labels], 0)     # append the final labels\n",
        "\n",
        "    with tf.name_scope('ouput_weight_bias'):\n",
        "        w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1), name='w')\n",
        "        tf.summary.histogram('W', w)\n",
        "        b = tf.Variable(tf.zeros([char_size]), name='b')\n",
        "        tf.summary.histogram('B', w_co)\n",
        "\n",
        "    with tf.name_scope('logits'):\n",
        "        logits = tf.matmul(outputs_all_i, w) + b\n",
        "    with tf.name_scope('cross_entropy'):\n",
        "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_all_i))\n",
        "    tf.summary.scalar('cross_entropy', loss)\n",
        "    with tf.name_scope('train'):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
        "    with tf.name_scope('accuracy'):\n",
        "        with tf.name_scope('correct_predictions'):\n",
        "            correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_all_i, 1))\n",
        "        with tf.name_scope('accuracy'):\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    merged = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter(tensorboard_directory, graph)\n",
        "\"\"\"\n",
        "    create a session and train the model on the data\n",
        "\"\"\"\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    # initialise all variables\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    offset = 0\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    X_length = len(X_train)\n",
        "\n",
        "    # retrain on the data every epoch\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        # pass the data to the model in batches\n",
        "        for batch in range(X_length//batch_size):\n",
        "\n",
        "            offset = offset % X_length\n",
        "\n",
        "            if offset <= (X_length - batch_size):\n",
        "                batch_data = X_train[offset: offset + batch_size]\n",
        "                batch_labels = y_train[offset: offset + batch_size]\n",
        "                offset += batch_size\n",
        "            else:\n",
        "                to_add = batch_size - (X_length - offset)\n",
        "                batch_data = np.concatenate((X_train[offset:X_length], X_train[0: to_add]))\n",
        "                batch_labels = np.concatenate((y_train[offset:X_length], y_train[0: to_add]))\n",
        "                offset = to_add\n",
        "\n",
        "            _, acc, summary, training_loss = sess.run([optimizer, accuracy, merged, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
        "\n",
        "            if step % log_every == 0:\n",
        "                print('training loss at step %d - batch %d: %.2f (%s)' % (step, batch, training_loss, datetime.datetime.now()))\n",
        "                print('Accuracy at step %d - batch %s: %s' % (step, batch, acc))\n",
        "\n",
        "                if batch % save_every == 0:\n",
        "                    saver.save(sess, checkpoint_directory + '/model', global_step=step)\n",
        "\n",
        "        train_writer.add_summary(summary, step)\n",
        "        \n",
        "    X_test_length = len(X_test)\n",
        "      \n",
        "    # pass the data to the model in batches\n",
        "     \n",
        "    for batch in range(X_test_length//batch_size):\n",
        "\n",
        "        offset = offset % X_test_length\n",
        "\n",
        "        if offset <= (X_test_length - batch_size):\n",
        "          batch_data = X_test[offset: offset + batch_size]\n",
        "          batch_labels = y_test[offset: offset + batch_size]\n",
        "          offset += batch_size\n",
        "        else:\n",
        "            to_add = batch_size - (X_test_length - offset)\n",
        "            batch_data = np.concatenate((X_test[offset:X_test_length], X_test[0: to_add]))\n",
        "            batch_labels = np.concatenate((y_test[offset:X_test_length], y_test[0: to_add]))\n",
        "            offset = to_add\n",
        "\n",
        "        acc = sess.run([accuracy], feed_dict={data: batch_data, labels: batch_labels})\n",
        "\n",
        "        if step % log_every == 0:\n",
        "          print('Accuracy at step %d - batch %s: %s' % (step, batch, acc))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "char_size : 65\n",
            "training loss at step 0 - batch 0: 4.18 (2019-08-04 13:33:49.743027)\n",
            "Accuracy at step 0 - batch 0: 0.012\n",
            "training loss at step 0 - batch 1: 3.75 (2019-08-04 13:33:49.903178)\n",
            "Accuracy at step 0 - batch 1: 0.1604\n",
            "training loss at step 0 - batch 2: 3.56 (2019-08-04 13:33:49.916193)\n",
            "Accuracy at step 0 - batch 2: 0.1504\n",
            "training loss at step 0 - batch 3: 3.48 (2019-08-04 13:33:49.930927)\n",
            "Accuracy at step 0 - batch 3: 0.1476\n",
            "training loss at step 0 - batch 4: 3.42 (2019-08-04 13:33:49.943366)\n",
            "Accuracy at step 0 - batch 4: 0.1504\n",
            "training loss at step 0 - batch 5: 3.40 (2019-08-04 13:33:49.963696)\n",
            "Accuracy at step 0 - batch 5: 0.07\n",
            "training loss at step 0 - batch 6: 3.44 (2019-08-04 13:33:50.076217)\n",
            "Accuracy at step 0 - batch 6: 0.156\n",
            "training loss at step 0 - batch 7: 3.69 (2019-08-04 13:33:50.094020)\n",
            "Accuracy at step 0 - batch 7: 0.0848\n",
            "training loss at step 0 - batch 8: 3.41 (2019-08-04 13:33:50.107014)\n",
            "Accuracy at step 0 - batch 8: 0.146\n",
            "training loss at step 0 - batch 9: 3.34 (2019-08-04 13:33:50.120311)\n",
            "Accuracy at step 0 - batch 9: 0.1432\n",
            "training loss at step 0 - batch 10: 3.31 (2019-08-04 13:33:50.133868)\n",
            "Accuracy at step 0 - batch 10: 0.1652\n",
            "training loss at step 0 - batch 11: 3.30 (2019-08-04 13:33:50.248971)\n",
            "Accuracy at step 0 - batch 11: 0.1576\n",
            "training loss at step 0 - batch 12: 3.31 (2019-08-04 13:33:50.266360)\n",
            "Accuracy at step 0 - batch 12: 0.1488\n",
            "training loss at step 0 - batch 13: 3.28 (2019-08-04 13:33:50.280082)\n",
            "Accuracy at step 0 - batch 13: 0.1544\n",
            "training loss at step 0 - batch 14: 3.25 (2019-08-04 13:33:50.295234)\n",
            "Accuracy at step 0 - batch 14: 0.2104\n",
            "training loss at step 0 - batch 15: 3.36 (2019-08-04 13:33:50.308046)\n",
            "Accuracy at step 0 - batch 15: 0.152\n",
            "training loss at step 0 - batch 16: 3.53 (2019-08-04 13:33:50.426839)\n",
            "Accuracy at step 0 - batch 16: 0.1176\n",
            "training loss at step 0 - batch 17: 3.30 (2019-08-04 13:33:50.439959)\n",
            "Accuracy at step 0 - batch 17: 0.1476\n",
            "training loss at step 0 - batch 18: 3.24 (2019-08-04 13:33:50.455413)\n",
            "Accuracy at step 0 - batch 18: 0.1744\n",
            "training loss at step 0 - batch 19: 3.17 (2019-08-04 13:33:50.470007)\n",
            "Accuracy at step 0 - batch 19: 0.1536\n",
            "training loss at step 0 - batch 20: 3.19 (2019-08-04 13:33:50.483123)\n",
            "Accuracy at step 0 - batch 20: 0.202\n",
            "training loss at step 0 - batch 21: 3.19 (2019-08-04 13:33:50.599043)\n",
            "Accuracy at step 0 - batch 21: 0.148\n",
            "training loss at step 0 - batch 22: 3.27 (2019-08-04 13:33:50.612477)\n",
            "Accuracy at step 0 - batch 22: 0.0996\n",
            "training loss at step 0 - batch 23: 3.29 (2019-08-04 13:33:50.624596)\n",
            "Accuracy at step 0 - batch 23: 0.15\n",
            "training loss at step 0 - batch 24: 3.47 (2019-08-04 13:33:50.636433)\n",
            "Accuracy at step 0 - batch 24: 0.0968\n",
            "training loss at step 0 - batch 25: 3.21 (2019-08-04 13:33:50.648727)\n",
            "Accuracy at step 0 - batch 25: 0.1616\n",
            "training loss at step 0 - batch 26: 3.11 (2019-08-04 13:33:50.765746)\n",
            "Accuracy at step 0 - batch 26: 0.184\n",
            "training loss at step 0 - batch 27: 3.08 (2019-08-04 13:33:50.784287)\n",
            "Accuracy at step 0 - batch 27: 0.1896\n",
            "training loss at step 0 - batch 28: 3.02 (2019-08-04 13:33:50.796729)\n",
            "Accuracy at step 0 - batch 28: 0.2132\n",
            "training loss at step 0 - batch 29: 2.99 (2019-08-04 13:33:50.809123)\n",
            "Accuracy at step 0 - batch 29: 0.2064\n",
            "training loss at step 0 - batch 30: 2.96 (2019-08-04 13:33:50.822233)\n",
            "Accuracy at step 0 - batch 30: 0.2348\n",
            "training loss at step 0 - batch 31: 2.95 (2019-08-04 13:33:50.932785)\n",
            "Accuracy at step 0 - batch 31: 0.1984\n",
            "training loss at step 0 - batch 32: 2.98 (2019-08-04 13:33:50.946092)\n",
            "Accuracy at step 0 - batch 32: 0.2296\n",
            "training loss at step 0 - batch 33: 3.25 (2019-08-04 13:33:50.960133)\n",
            "Accuracy at step 0 - batch 33: 0.1872\n",
            "training loss at step 0 - batch 34: 3.44 (2019-08-04 13:33:50.976913)\n",
            "Accuracy at step 0 - batch 34: 0.1296\n",
            "training loss at step 0 - batch 35: 3.21 (2019-08-04 13:33:50.989167)\n",
            "Accuracy at step 0 - batch 35: 0.1556\n",
            "training loss at step 0 - batch 36: 3.36 (2019-08-04 13:33:51.104626)\n",
            "Accuracy at step 0 - batch 36: 0.1704\n",
            "training loss at step 0 - batch 37: 3.04 (2019-08-04 13:33:51.117085)\n",
            "Accuracy at step 0 - batch 37: 0.1992\n",
            "training loss at step 0 - batch 38: 3.11 (2019-08-04 13:33:51.129867)\n",
            "Accuracy at step 0 - batch 38: 0.1468\n",
            "training loss at step 0 - batch 39: 3.14 (2019-08-04 13:33:51.141999)\n",
            "Accuracy at step 0 - batch 39: 0.1992\n",
            "training loss at step 0 - batch 40: 2.97 (2019-08-04 13:33:51.154468)\n",
            "Accuracy at step 0 - batch 40: 0.2312\n",
            "training loss at step 0 - batch 41: 2.87 (2019-08-04 13:33:51.277033)\n",
            "Accuracy at step 0 - batch 41: 0.236\n",
            "training loss at step 0 - batch 42: 2.81 (2019-08-04 13:33:51.293178)\n",
            "Accuracy at step 0 - batch 42: 0.274\n",
            "training loss at step 0 - batch 43: 2.78 (2019-08-04 13:33:51.307068)\n",
            "Accuracy at step 0 - batch 43: 0.2572\n",
            "training loss at step 0 - batch 44: 2.90 (2019-08-04 13:33:51.319273)\n",
            "Accuracy at step 0 - batch 44: 0.2368\n",
            "training loss at step 0 - batch 45: 2.80 (2019-08-04 13:33:51.332921)\n",
            "Accuracy at step 0 - batch 45: 0.2516\n",
            "training loss at step 0 - batch 46: 2.79 (2019-08-04 13:33:51.456913)\n",
            "Accuracy at step 0 - batch 46: 0.2692\n",
            "training loss at step 0 - batch 47: 2.76 (2019-08-04 13:33:51.469871)\n",
            "Accuracy at step 0 - batch 47: 0.244\n",
            "training loss at step 0 - batch 48: 2.92 (2019-08-04 13:33:51.484655)\n",
            "Accuracy at step 0 - batch 48: 0.2124\n",
            "training loss at step 0 - batch 49: 2.79 (2019-08-04 13:33:51.496839)\n",
            "Accuracy at step 0 - batch 49: 0.252\n",
            "training loss at step 0 - batch 50: 2.88 (2019-08-04 13:33:51.509948)\n",
            "Accuracy at step 0 - batch 50: 0.2344\n",
            "training loss at step 0 - batch 51: 2.82 (2019-08-04 13:33:51.627629)\n",
            "Accuracy at step 0 - batch 51: 0.2284\n",
            "training loss at step 0 - batch 52: 2.74 (2019-08-04 13:33:51.641597)\n",
            "Accuracy at step 0 - batch 52: 0.2732\n",
            "training loss at step 0 - batch 53: 2.72 (2019-08-04 13:33:51.653854)\n",
            "Accuracy at step 0 - batch 53: 0.2632\n",
            "training loss at step 0 - batch 54: 2.86 (2019-08-04 13:33:51.665918)\n",
            "Accuracy at step 0 - batch 54: 0.2272\n",
            "training loss at step 0 - batch 55: 2.63 (2019-08-04 13:33:51.678989)\n",
            "Accuracy at step 0 - batch 55: 0.2808\n",
            "training loss at step 0 - batch 56: 2.63 (2019-08-04 13:33:51.797105)\n",
            "Accuracy at step 0 - batch 56: 0.2916\n",
            "training loss at step 0 - batch 57: 2.66 (2019-08-04 13:33:51.809513)\n",
            "Accuracy at step 0 - batch 57: 0.282\n",
            "training loss at step 0 - batch 58: 2.64 (2019-08-04 13:33:51.822196)\n",
            "Accuracy at step 0 - batch 58: 0.286\n",
            "training loss at step 0 - batch 59: 2.60 (2019-08-04 13:33:51.835082)\n",
            "Accuracy at step 0 - batch 59: 0.27\n",
            "training loss at step 0 - batch 60: 2.73 (2019-08-04 13:33:51.849122)\n",
            "Accuracy at step 0 - batch 60: 0.2608\n",
            "training loss at step 0 - batch 61: 2.56 (2019-08-04 13:33:51.964508)\n",
            "Accuracy at step 0 - batch 61: 0.2952\n",
            "training loss at step 0 - batch 62: 2.56 (2019-08-04 13:33:51.979001)\n",
            "Accuracy at step 0 - batch 62: 0.2912\n",
            "training loss at step 0 - batch 63: 2.54 (2019-08-04 13:33:51.991338)\n",
            "Accuracy at step 0 - batch 63: 0.2936\n",
            "training loss at step 0 - batch 64: 2.67 (2019-08-04 13:33:52.005600)\n",
            "Accuracy at step 0 - batch 64: 0.2708\n",
            "training loss at step 0 - batch 65: 2.53 (2019-08-04 13:33:52.019872)\n",
            "Accuracy at step 0 - batch 65: 0.2892\n",
            "training loss at step 0 - batch 66: 2.67 (2019-08-04 13:33:52.142609)\n",
            "Accuracy at step 0 - batch 66: 0.2716\n",
            "training loss at step 0 - batch 67: 2.74 (2019-08-04 13:33:52.156168)\n",
            "Accuracy at step 0 - batch 67: 0.2688\n",
            "training loss at step 0 - batch 68: 2.64 (2019-08-04 13:33:52.167927)\n",
            "Accuracy at step 0 - batch 68: 0.3028\n",
            "training loss at step 0 - batch 69: 2.62 (2019-08-04 13:33:52.181030)\n",
            "Accuracy at step 0 - batch 69: 0.2676\n",
            "training loss at step 0 - batch 70: 2.65 (2019-08-04 13:33:52.193499)\n",
            "Accuracy at step 0 - batch 70: 0.2596\n",
            "training loss at step 0 - batch 71: 2.55 (2019-08-04 13:33:52.315008)\n",
            "Accuracy at step 0 - batch 71: 0.2844\n",
            "training loss at step 0 - batch 72: 2.80 (2019-08-04 13:33:52.332046)\n",
            "Accuracy at step 0 - batch 72: 0.2332\n",
            "training loss at step 0 - batch 73: 2.72 (2019-08-04 13:33:52.345246)\n",
            "Accuracy at step 0 - batch 73: 0.2564\n",
            "training loss at step 0 - batch 74: 2.64 (2019-08-04 13:33:52.358165)\n",
            "Accuracy at step 0 - batch 74: 0.2596\n",
            "training loss at step 0 - batch 75: 2.60 (2019-08-04 13:33:52.371230)\n",
            "Accuracy at step 0 - batch 75: 0.2868\n",
            "training loss at step 0 - batch 76: 2.73 (2019-08-04 13:33:52.500247)\n",
            "Accuracy at step 0 - batch 76: 0.2356\n",
            "training loss at step 0 - batch 77: 2.53 (2019-08-04 13:33:52.513700)\n",
            "Accuracy at step 0 - batch 77: 0.2936\n",
            "training loss at step 0 - batch 78: 2.53 (2019-08-04 13:33:52.533083)\n",
            "Accuracy at step 0 - batch 78: 0.3164\n",
            "training loss at step 0 - batch 79: 2.38 (2019-08-04 13:33:52.545928)\n",
            "Accuracy at step 0 - batch 79: 0.3348\n",
            "training loss at step 0 - batch 80: 2.48 (2019-08-04 13:33:52.558903)\n",
            "Accuracy at step 0 - batch 80: 0.3112\n",
            "training loss at step 0 - batch 81: 2.52 (2019-08-04 13:33:52.679398)\n",
            "Accuracy at step 0 - batch 81: 0.3032\n",
            "training loss at step 0 - batch 82: 2.58 (2019-08-04 13:33:52.695523)\n",
            "Accuracy at step 0 - batch 82: 0.2744\n",
            "training loss at step 0 - batch 83: 2.56 (2019-08-04 13:33:52.708271)\n",
            "Accuracy at step 0 - batch 83: 0.296\n",
            "training loss at step 0 - batch 84: 2.48 (2019-08-04 13:33:52.720500)\n",
            "Accuracy at step 0 - batch 84: 0.2924\n",
            "training loss at step 0 - batch 85: 2.50 (2019-08-04 13:33:52.732687)\n",
            "Accuracy at step 0 - batch 85: 0.3112\n",
            "training loss at step 0 - batch 86: 2.55 (2019-08-04 13:33:52.853703)\n",
            "Accuracy at step 0 - batch 86: 0.3052\n",
            "training loss at step 0 - batch 87: 2.41 (2019-08-04 13:33:52.871418)\n",
            "Accuracy at step 0 - batch 87: 0.3384\n",
            "training loss at step 0 - batch 88: 2.49 (2019-08-04 13:33:52.883591)\n",
            "Accuracy at step 0 - batch 88: 0.3104\n",
            "training loss at step 0 - batch 89: 2.61 (2019-08-04 13:33:52.895860)\n",
            "Accuracy at step 0 - batch 89: 0.2956\n",
            "training loss at step 0 - batch 90: 2.79 (2019-08-04 13:33:52.908005)\n",
            "Accuracy at step 0 - batch 90: 0.2384\n",
            "training loss at step 0 - batch 91: 2.66 (2019-08-04 13:33:53.024979)\n",
            "Accuracy at step 0 - batch 91: 0.28\n",
            "training loss at step 0 - batch 92: 2.68 (2019-08-04 13:33:53.042588)\n",
            "Accuracy at step 0 - batch 92: 0.224\n",
            "training loss at step 0 - batch 93: 2.57 (2019-08-04 13:33:53.058546)\n",
            "Accuracy at step 0 - batch 93: 0.2808\n",
            "training loss at step 0 - batch 94: 2.62 (2019-08-04 13:33:53.073952)\n",
            "Accuracy at step 0 - batch 94: 0.2516\n",
            "training loss at step 0 - batch 95: 2.48 (2019-08-04 13:33:53.085947)\n",
            "Accuracy at step 0 - batch 95: 0.298\n",
            "training loss at step 0 - batch 96: 2.42 (2019-08-04 13:33:53.206068)\n",
            "Accuracy at step 0 - batch 96: 0.3232\n",
            "training loss at step 0 - batch 97: 2.48 (2019-08-04 13:33:53.220327)\n",
            "Accuracy at step 0 - batch 97: 0.3096\n",
            "training loss at step 0 - batch 98: 2.56 (2019-08-04 13:33:53.233059)\n",
            "Accuracy at step 0 - batch 98: 0.2888\n",
            "training loss at step 0 - batch 99: 2.81 (2019-08-04 13:33:53.246108)\n",
            "Accuracy at step 0 - batch 99: 0.208\n",
            "training loss at step 0 - batch 100: 2.50 (2019-08-04 13:33:53.258919)\n",
            "Accuracy at step 0 - batch 100: 0.292\n",
            "training loss at step 0 - batch 101: 2.49 (2019-08-04 13:33:53.379091)\n",
            "Accuracy at step 0 - batch 101: 0.3216\n",
            "training loss at step 0 - batch 102: 2.45 (2019-08-04 13:33:53.392385)\n",
            "Accuracy at step 0 - batch 102: 0.3188\n",
            "training loss at step 0 - batch 103: 2.40 (2019-08-04 13:33:53.405560)\n",
            "Accuracy at step 0 - batch 103: 0.3376\n",
            "training loss at step 0 - batch 104: 2.36 (2019-08-04 13:33:53.418836)\n",
            "Accuracy at step 0 - batch 104: 0.3336\n",
            "training loss at step 0 - batch 105: 2.32 (2019-08-04 13:33:53.433599)\n",
            "Accuracy at step 0 - batch 105: 0.3524\n",
            "training loss at step 0 - batch 106: 2.25 (2019-08-04 13:33:53.560350)\n",
            "Accuracy at step 0 - batch 106: 0.352\n",
            "training loss at step 0 - batch 107: 2.39 (2019-08-04 13:33:53.574794)\n",
            "Accuracy at step 0 - batch 107: 0.3304\n",
            "training loss at step 0 - batch 108: 2.71 (2019-08-04 13:33:53.587932)\n",
            "Accuracy at step 0 - batch 108: 0.264\n",
            "training loss at step 0 - batch 109: 2.67 (2019-08-04 13:33:53.600956)\n",
            "Accuracy at step 0 - batch 109: 0.2668\n",
            "training loss at step 0 - batch 110: 2.60 (2019-08-04 13:33:53.613675)\n",
            "Accuracy at step 0 - batch 110: 0.2596\n",
            "training loss at step 0 - batch 111: 2.36 (2019-08-04 13:33:53.725716)\n",
            "Accuracy at step 0 - batch 111: 0.3364\n",
            "training loss at step 0 - batch 112: 2.42 (2019-08-04 13:33:53.737772)\n",
            "Accuracy at step 0 - batch 112: 0.322\n",
            "training loss at step 0 - batch 113: 2.51 (2019-08-04 13:33:53.751442)\n",
            "Accuracy at step 0 - batch 113: 0.3028\n",
            "training loss at step 0 - batch 114: 2.53 (2019-08-04 13:33:53.765369)\n",
            "Accuracy at step 0 - batch 114: 0.2996\n",
            "training loss at step 0 - batch 115: 2.56 (2019-08-04 13:33:53.780340)\n",
            "Accuracy at step 0 - batch 115: 0.3024\n",
            "training loss at step 0 - batch 116: 2.66 (2019-08-04 13:33:53.898727)\n",
            "Accuracy at step 0 - batch 116: 0.2528\n",
            "training loss at step 0 - batch 117: 2.56 (2019-08-04 13:33:53.914225)\n",
            "Accuracy at step 0 - batch 117: 0.3064\n",
            "training loss at step 0 - batch 118: 2.35 (2019-08-04 13:33:53.927596)\n",
            "Accuracy at step 0 - batch 118: 0.3424\n",
            "training loss at step 0 - batch 119: 2.34 (2019-08-04 13:33:53.939977)\n",
            "Accuracy at step 0 - batch 119: 0.34\n",
            "training loss at step 0 - batch 120: 2.27 (2019-08-04 13:33:53.952002)\n",
            "Accuracy at step 0 - batch 120: 0.3672\n",
            "training loss at step 0 - batch 121: 2.36 (2019-08-04 13:33:54.071749)\n",
            "Accuracy at step 0 - batch 121: 0.3232\n",
            "training loss at step 0 - batch 122: 2.40 (2019-08-04 13:33:54.085235)\n",
            "Accuracy at step 0 - batch 122: 0.3376\n",
            "training loss at step 0 - batch 123: 2.47 (2019-08-04 13:33:54.097753)\n",
            "Accuracy at step 0 - batch 123: 0.3196\n",
            "training loss at step 0 - batch 124: 2.48 (2019-08-04 13:33:54.110025)\n",
            "Accuracy at step 0 - batch 124: 0.3112\n",
            "training loss at step 0 - batch 125: 2.40 (2019-08-04 13:33:54.124273)\n",
            "Accuracy at step 0 - batch 125: 0.322\n",
            "training loss at step 0 - batch 126: 2.55 (2019-08-04 13:33:54.241755)\n",
            "Accuracy at step 0 - batch 126: 0.2628\n",
            "training loss at step 0 - batch 127: 2.40 (2019-08-04 13:33:54.255023)\n",
            "Accuracy at step 0 - batch 127: 0.3292\n",
            "training loss at step 0 - batch 128: 2.22 (2019-08-04 13:33:54.267967)\n",
            "Accuracy at step 0 - batch 128: 0.3624\n",
            "training loss at step 0 - batch 129: 2.21 (2019-08-04 13:33:54.283125)\n",
            "Accuracy at step 0 - batch 129: 0.3864\n",
            "training loss at step 0 - batch 130: 2.17 (2019-08-04 13:33:54.295459)\n",
            "Accuracy at step 0 - batch 130: 0.398\n",
            "training loss at step 0 - batch 131: 2.17 (2019-08-04 13:33:54.411276)\n",
            "Accuracy at step 0 - batch 131: 0.3916\n",
            "training loss at step 0 - batch 132: 2.09 (2019-08-04 13:33:54.429426)\n",
            "Accuracy at step 0 - batch 132: 0.4156\n",
            "training loss at step 0 - batch 133: 2.22 (2019-08-04 13:33:54.441759)\n",
            "Accuracy at step 0 - batch 133: 0.3736\n",
            "training loss at step 0 - batch 134: 2.37 (2019-08-04 13:33:54.454790)\n",
            "Accuracy at step 0 - batch 134: 0.3512\n",
            "training loss at step 0 - batch 135: 2.58 (2019-08-04 13:33:54.466703)\n",
            "Accuracy at step 0 - batch 135: 0.2732\n",
            "training loss at step 0 - batch 136: 2.57 (2019-08-04 13:33:54.595149)\n",
            "Accuracy at step 0 - batch 136: 0.32\n",
            "training loss at step 0 - batch 137: 2.31 (2019-08-04 13:33:54.608988)\n",
            "Accuracy at step 0 - batch 137: 0.3556\n",
            "training loss at step 0 - batch 138: 2.26 (2019-08-04 13:33:54.622126)\n",
            "Accuracy at step 0 - batch 138: 0.3584\n",
            "training loss at step 0 - batch 139: 2.55 (2019-08-04 13:33:54.634869)\n",
            "Accuracy at step 0 - batch 139: 0.2708\n",
            "training loss at step 0 - batch 140: 2.56 (2019-08-04 13:33:54.646902)\n",
            "Accuracy at step 0 - batch 140: 0.3284\n",
            "training loss at step 0 - batch 141: 2.60 (2019-08-04 13:33:54.772478)\n",
            "Accuracy at step 0 - batch 141: 0.322\n",
            "training loss at step 0 - batch 142: 2.31 (2019-08-04 13:33:54.786011)\n",
            "Accuracy at step 0 - batch 142: 0.3508\n",
            "training loss at step 0 - batch 143: 2.37 (2019-08-04 13:33:54.799946)\n",
            "Accuracy at step 0 - batch 143: 0.3496\n",
            "training loss at step 0 - batch 144: 2.29 (2019-08-04 13:33:54.813059)\n",
            "Accuracy at step 0 - batch 144: 0.3612\n",
            "training loss at step 0 - batch 145: 2.21 (2019-08-04 13:33:54.825889)\n",
            "Accuracy at step 0 - batch 145: 0.3692\n",
            "training loss at step 0 - batch 146: 2.26 (2019-08-04 13:33:54.959181)\n",
            "Accuracy at step 0 - batch 146: 0.352\n",
            "training loss at step 0 - batch 147: 2.24 (2019-08-04 13:33:54.971472)\n",
            "Accuracy at step 0 - batch 147: 0.362\n",
            "training loss at step 0 - batch 148: 2.60 (2019-08-04 13:33:54.984003)\n",
            "Accuracy at step 0 - batch 148: 0.2936\n",
            "training loss at step 0 - batch 149: 2.37 (2019-08-04 13:33:54.996684)\n",
            "Accuracy at step 0 - batch 149: 0.3652\n",
            "training loss at step 0 - batch 150: 2.18 (2019-08-04 13:33:55.011635)\n",
            "Accuracy at step 0 - batch 150: 0.3948\n",
            "training loss at step 0 - batch 151: 2.09 (2019-08-04 13:33:55.132894)\n",
            "Accuracy at step 0 - batch 151: 0.3916\n",
            "training loss at step 0 - batch 152: 2.25 (2019-08-04 13:33:55.144924)\n",
            "Accuracy at step 0 - batch 152: 0.3608\n",
            "training loss at step 0 - batch 153: 2.16 (2019-08-04 13:33:55.157169)\n",
            "Accuracy at step 0 - batch 153: 0.3728\n",
            "training loss at step 0 - batch 154: 2.17 (2019-08-04 13:33:55.170498)\n",
            "Accuracy at step 0 - batch 154: 0.38\n",
            "training loss at step 0 - batch 155: 2.01 (2019-08-04 13:33:55.183765)\n",
            "Accuracy at step 0 - batch 155: 0.4304\n",
            "training loss at step 0 - batch 156: 2.09 (2019-08-04 13:33:55.306670)\n",
            "Accuracy at step 0 - batch 156: 0.3948\n",
            "training loss at step 0 - batch 157: 2.08 (2019-08-04 13:33:55.319206)\n",
            "Accuracy at step 0 - batch 157: 0.4064\n",
            "training loss at step 0 - batch 158: 2.06 (2019-08-04 13:33:55.331823)\n",
            "Accuracy at step 0 - batch 158: 0.4116\n",
            "training loss at step 0 - batch 159: 2.04 (2019-08-04 13:33:55.345653)\n",
            "Accuracy at step 0 - batch 159: 0.416\n",
            "training loss at step 0 - batch 160: 2.05 (2019-08-04 13:33:55.360923)\n",
            "Accuracy at step 0 - batch 160: 0.428\n",
            "training loss at step 0 - batch 161: 2.00 (2019-08-04 13:33:55.490134)\n",
            "Accuracy at step 0 - batch 161: 0.4492\n",
            "training loss at step 0 - batch 162: 1.99 (2019-08-04 13:33:55.506585)\n",
            "Accuracy at step 0 - batch 162: 0.4388\n",
            "training loss at step 0 - batch 163: 2.32 (2019-08-04 13:33:55.527694)\n",
            "Accuracy at step 0 - batch 163: 0.3812\n",
            "training loss at step 0 - batch 164: 2.41 (2019-08-04 13:33:55.539895)\n",
            "Accuracy at step 0 - batch 164: 0.332\n",
            "training loss at step 0 - batch 165: 2.25 (2019-08-04 13:33:55.554095)\n",
            "Accuracy at step 0 - batch 165: 0.3948\n",
            "training loss at step 0 - batch 166: 2.18 (2019-08-04 13:33:55.676859)\n",
            "Accuracy at step 0 - batch 166: 0.4024\n",
            "training loss at step 0 - batch 167: 2.23 (2019-08-04 13:33:55.692212)\n",
            "Accuracy at step 0 - batch 167: 0.39\n",
            "training loss at step 0 - batch 168: 2.16 (2019-08-04 13:33:55.704538)\n",
            "Accuracy at step 0 - batch 168: 0.3848\n",
            "training loss at step 0 - batch 169: 2.34 (2019-08-04 13:33:55.718937)\n",
            "Accuracy at step 0 - batch 169: 0.3716\n",
            "training loss at step 0 - batch 170: 2.35 (2019-08-04 13:33:55.733042)\n",
            "Accuracy at step 0 - batch 170: 0.3456\n",
            "training loss at step 0 - batch 171: 2.19 (2019-08-04 13:33:55.849030)\n",
            "Accuracy at step 0 - batch 171: 0.382\n",
            "training loss at step 0 - batch 172: 2.19 (2019-08-04 13:33:55.866526)\n",
            "Accuracy at step 0 - batch 172: 0.392\n",
            "training loss at step 0 - batch 173: 1.97 (2019-08-04 13:33:55.878796)\n",
            "Accuracy at step 0 - batch 173: 0.4544\n",
            "training loss at step 0 - batch 174: 1.99 (2019-08-04 13:33:55.891104)\n",
            "Accuracy at step 0 - batch 174: 0.4496\n",
            "training loss at step 0 - batch 175: 1.85 (2019-08-04 13:33:55.904063)\n",
            "Accuracy at step 0 - batch 175: 0.4852\n",
            "training loss at step 0 - batch 176: 1.81 (2019-08-04 13:33:56.026840)\n",
            "Accuracy at step 0 - batch 176: 0.4924\n",
            "training loss at step 0 - batch 177: 1.79 (2019-08-04 13:33:56.040070)\n",
            "Accuracy at step 0 - batch 177: 0.4976\n",
            "training loss at step 0 - batch 178: 1.88 (2019-08-04 13:33:56.052915)\n",
            "Accuracy at step 0 - batch 178: 0.47\n",
            "training loss at step 0 - batch 179: 1.88 (2019-08-04 13:33:56.065592)\n",
            "Accuracy at step 0 - batch 179: 0.4724\n",
            "training loss at step 0 - batch 180: 1.96 (2019-08-04 13:33:56.077821)\n",
            "Accuracy at step 0 - batch 180: 0.4504\n",
            "training loss at step 0 - batch 181: 2.11 (2019-08-04 13:33:56.195645)\n",
            "Accuracy at step 0 - batch 181: 0.4116\n",
            "training loss at step 0 - batch 182: 1.96 (2019-08-04 13:33:56.209005)\n",
            "Accuracy at step 0 - batch 182: 0.4504\n",
            "training loss at step 0 - batch 183: 2.08 (2019-08-04 13:33:56.221428)\n",
            "Accuracy at step 0 - batch 183: 0.4092\n",
            "training loss at step 0 - batch 184: 1.98 (2019-08-04 13:33:56.236301)\n",
            "Accuracy at step 0 - batch 184: 0.46\n",
            "training loss at step 0 - batch 185: 1.83 (2019-08-04 13:33:56.249010)\n",
            "Accuracy at step 0 - batch 185: 0.4836\n",
            "training loss at step 0 - batch 186: 1.76 (2019-08-04 13:33:56.368762)\n",
            "Accuracy at step 0 - batch 186: 0.5072\n",
            "training loss at step 0 - batch 187: 1.70 (2019-08-04 13:33:56.385430)\n",
            "Accuracy at step 0 - batch 187: 0.5132\n",
            "training loss at step 0 - batch 188: 1.78 (2019-08-04 13:33:56.398776)\n",
            "Accuracy at step 0 - batch 188: 0.5152\n",
            "training loss at step 0 - batch 189: 1.79 (2019-08-04 13:33:56.410745)\n",
            "Accuracy at step 0 - batch 189: 0.4836\n",
            "training loss at step 0 - batch 190: 2.04 (2019-08-04 13:33:56.422870)\n",
            "Accuracy at step 0 - batch 190: 0.4468\n",
            "training loss at step 0 - batch 191: 1.98 (2019-08-04 13:33:56.547240)\n",
            "Accuracy at step 0 - batch 191: 0.4512\n",
            "training loss at step 0 - batch 192: 1.92 (2019-08-04 13:33:56.561174)\n",
            "Accuracy at step 0 - batch 192: 0.448\n",
            "training loss at step 0 - batch 193: 1.77 (2019-08-04 13:33:56.574041)\n",
            "Accuracy at step 0 - batch 193: 0.49\n",
            "training loss at step 0 - batch 194: 1.82 (2019-08-04 13:33:56.587072)\n",
            "Accuracy at step 0 - batch 194: 0.484\n",
            "training loss at step 0 - batch 195: 1.81 (2019-08-04 13:33:56.599695)\n",
            "Accuracy at step 0 - batch 195: 0.4916\n",
            "training loss at step 0 - batch 196: 1.79 (2019-08-04 13:33:56.723419)\n",
            "Accuracy at step 0 - batch 196: 0.4908\n",
            "training loss at step 0 - batch 197: 1.88 (2019-08-04 13:33:56.741440)\n",
            "Accuracy at step 0 - batch 197: 0.4968\n",
            "training loss at step 0 - batch 198: 1.82 (2019-08-04 13:33:56.756916)\n",
            "Accuracy at step 0 - batch 198: 0.5052\n",
            "training loss at step 0 - batch 199: 1.68 (2019-08-04 13:33:56.769568)\n",
            "Accuracy at step 0 - batch 199: 0.5316\n",
            "training loss at step 0 - batch 200: 1.70 (2019-08-04 13:33:56.781601)\n",
            "Accuracy at step 0 - batch 200: 0.538\n",
            "training loss at step 0 - batch 201: 1.85 (2019-08-04 13:33:56.901773)\n",
            "Accuracy at step 0 - batch 201: 0.4956\n",
            "training loss at step 0 - batch 202: 1.75 (2019-08-04 13:33:56.915762)\n",
            "Accuracy at step 0 - batch 202: 0.4972\n",
            "training loss at step 0 - batch 203: 1.63 (2019-08-04 13:33:56.928127)\n",
            "Accuracy at step 0 - batch 203: 0.55\n",
            "training loss at step 0 - batch 204: 1.54 (2019-08-04 13:33:56.940853)\n",
            "Accuracy at step 0 - batch 204: 0.5664\n",
            "training loss at step 0 - batch 205: 1.58 (2019-08-04 13:33:56.952749)\n",
            "Accuracy at step 0 - batch 205: 0.5596\n",
            "training loss at step 0 - batch 206: 1.73 (2019-08-04 13:33:57.082323)\n",
            "Accuracy at step 0 - batch 206: 0.54\n",
            "training loss at step 0 - batch 207: 1.67 (2019-08-04 13:33:57.099711)\n",
            "Accuracy at step 0 - batch 207: 0.5196\n",
            "training loss at step 0 - batch 208: 1.60 (2019-08-04 13:33:57.112078)\n",
            "Accuracy at step 0 - batch 208: 0.5388\n",
            "training loss at step 0 - batch 209: 1.76 (2019-08-04 13:33:57.124573)\n",
            "Accuracy at step 0 - batch 209: 0.5268\n",
            "training loss at step 0 - batch 210: 1.75 (2019-08-04 13:33:57.136635)\n",
            "Accuracy at step 0 - batch 210: 0.5328\n",
            "training loss at step 0 - batch 211: 2.23 (2019-08-04 13:33:57.258856)\n",
            "Accuracy at step 0 - batch 211: 0.3764\n",
            "training loss at step 0 - batch 212: 2.01 (2019-08-04 13:33:57.274124)\n",
            "Accuracy at step 0 - batch 212: 0.46\n",
            "training loss at step 0 - batch 213: 1.81 (2019-08-04 13:33:57.289074)\n",
            "Accuracy at step 0 - batch 213: 0.5112\n",
            "training loss at step 0 - batch 214: 1.65 (2019-08-04 13:33:57.301970)\n",
            "Accuracy at step 0 - batch 214: 0.548\n",
            "training loss at step 0 - batch 215: 1.64 (2019-08-04 13:33:57.315078)\n",
            "Accuracy at step 0 - batch 215: 0.5372\n",
            "training loss at step 0 - batch 216: 1.65 (2019-08-04 13:33:57.433425)\n",
            "Accuracy at step 0 - batch 216: 0.538\n",
            "training loss at step 0 - batch 217: 1.67 (2019-08-04 13:33:57.448421)\n",
            "Accuracy at step 0 - batch 217: 0.5356\n",
            "training loss at step 0 - batch 218: 1.63 (2019-08-04 13:33:57.460530)\n",
            "Accuracy at step 0 - batch 218: 0.5448\n",
            "training loss at step 0 - batch 219: 1.88 (2019-08-04 13:33:57.473550)\n",
            "Accuracy at step 0 - batch 219: 0.4988\n",
            "training loss at step 0 - batch 220: 1.74 (2019-08-04 13:33:57.485678)\n",
            "Accuracy at step 0 - batch 220: 0.5252\n",
            "training loss at step 0 - batch 221: 1.77 (2019-08-04 13:33:57.620207)\n",
            "Accuracy at step 0 - batch 221: 0.5032\n",
            "training loss at step 0 - batch 222: 1.65 (2019-08-04 13:33:57.632679)\n",
            "Accuracy at step 0 - batch 222: 0.5444\n",
            "training loss at step 0 - batch 223: 1.59 (2019-08-04 13:33:57.645190)\n",
            "Accuracy at step 0 - batch 223: 0.5636\n",
            "training loss at step 0 - batch 224: 1.75 (2019-08-04 13:33:57.658136)\n",
            "Accuracy at step 0 - batch 224: 0.5432\n",
            "training loss at step 0 - batch 225: 1.53 (2019-08-04 13:33:57.670736)\n",
            "Accuracy at step 0 - batch 225: 0.5888\n",
            "training loss at step 0 - batch 226: 1.55 (2019-08-04 13:33:57.788129)\n",
            "Accuracy at step 0 - batch 226: 0.5716\n",
            "training loss at step 0 - batch 227: 1.75 (2019-08-04 13:33:57.802090)\n",
            "Accuracy at step 0 - batch 227: 0.5148\n",
            "training loss at step 0 - batch 228: 1.85 (2019-08-04 13:33:57.814300)\n",
            "Accuracy at step 0 - batch 228: 0.5156\n",
            "training loss at step 0 - batch 229: 1.69 (2019-08-04 13:33:57.828988)\n",
            "Accuracy at step 0 - batch 229: 0.5384\n",
            "training loss at step 0 - batch 230: 1.50 (2019-08-04 13:33:57.841898)\n",
            "Accuracy at step 0 - batch 230: 0.5896\n",
            "training loss at step 0 - batch 231: 1.50 (2019-08-04 13:33:57.960055)\n",
            "Accuracy at step 0 - batch 231: 0.586\n",
            "training loss at step 0 - batch 232: 1.51 (2019-08-04 13:33:57.974667)\n",
            "Accuracy at step 0 - batch 232: 0.5932\n",
            "training loss at step 0 - batch 233: 1.48 (2019-08-04 13:33:57.987550)\n",
            "Accuracy at step 0 - batch 233: 0.586\n",
            "training loss at step 0 - batch 234: 1.43 (2019-08-04 13:33:58.000479)\n",
            "Accuracy at step 0 - batch 234: 0.604\n",
            "training loss at step 0 - batch 235: 1.41 (2019-08-04 13:33:58.014361)\n",
            "Accuracy at step 0 - batch 235: 0.6124\n",
            "training loss at step 0 - batch 236: 1.37 (2019-08-04 13:33:58.137032)\n",
            "Accuracy at step 0 - batch 236: 0.6124\n",
            "training loss at step 0 - batch 237: 1.39 (2019-08-04 13:33:58.150596)\n",
            "Accuracy at step 0 - batch 237: 0.6168\n",
            "training loss at step 0 - batch 238: 1.39 (2019-08-04 13:33:58.162734)\n",
            "Accuracy at step 0 - batch 238: 0.6196\n",
            "training loss at step 0 - batch 239: 1.50 (2019-08-04 13:33:58.175044)\n",
            "Accuracy at step 0 - batch 239: 0.5712\n",
            "training loss at step 0 - batch 240: 1.41 (2019-08-04 13:33:58.188287)\n",
            "Accuracy at step 0 - batch 240: 0.6068\n",
            "training loss at step 0 - batch 241: 1.35 (2019-08-04 13:33:58.306157)\n",
            "Accuracy at step 0 - batch 241: 0.6256\n",
            "training loss at step 0 - batch 242: 1.36 (2019-08-04 13:33:58.322103)\n",
            "Accuracy at step 0 - batch 242: 0.6028\n",
            "training loss at step 0 - batch 243: 1.47 (2019-08-04 13:33:58.335083)\n",
            "Accuracy at step 0 - batch 243: 0.592\n",
            "training loss at step 0 - batch 244: 1.50 (2019-08-04 13:33:58.352000)\n",
            "Accuracy at step 0 - batch 244: 0.5884\n",
            "training loss at step 0 - batch 245: 1.39 (2019-08-04 13:33:58.365237)\n",
            "Accuracy at step 0 - batch 245: 0.6216\n",
            "training loss at step 0 - batch 246: 1.33 (2019-08-04 13:33:58.486496)\n",
            "Accuracy at step 0 - batch 246: 0.6464\n",
            "training loss at step 0 - batch 247: 1.44 (2019-08-04 13:33:58.504259)\n",
            "Accuracy at step 0 - batch 247: 0.5948\n",
            "training loss at step 0 - batch 248: 1.48 (2019-08-04 13:33:58.517191)\n",
            "Accuracy at step 0 - batch 248: 0.5964\n",
            "training loss at step 0 - batch 249: 1.42 (2019-08-04 13:33:58.530161)\n",
            "Accuracy at step 0 - batch 249: 0.5928\n",
            "training loss at step 0 - batch 250: 1.25 (2019-08-04 13:33:58.543151)\n",
            "Accuracy at step 0 - batch 250: 0.6644\n",
            "training loss at step 0 - batch 251: 1.33 (2019-08-04 13:33:58.674443)\n",
            "Accuracy at step 0 - batch 251: 0.6316\n",
            "training loss at step 0 - batch 252: 1.42 (2019-08-04 13:33:58.688693)\n",
            "Accuracy at step 0 - batch 252: 0.6156\n",
            "training loss at step 0 - batch 253: 1.36 (2019-08-04 13:33:58.701099)\n",
            "Accuracy at step 0 - batch 253: 0.6012\n",
            "training loss at step 0 - batch 254: 1.28 (2019-08-04 13:33:58.714067)\n",
            "Accuracy at step 0 - batch 254: 0.64\n",
            "training loss at step 0 - batch 255: 1.32 (2019-08-04 13:33:58.726986)\n",
            "Accuracy at step 0 - batch 255: 0.6288\n",
            "training loss at step 0 - batch 256: 1.25 (2019-08-04 13:33:58.847950)\n",
            "Accuracy at step 0 - batch 256: 0.6472\n",
            "training loss at step 0 - batch 257: 1.23 (2019-08-04 13:33:58.866433)\n",
            "Accuracy at step 0 - batch 257: 0.6584\n",
            "training loss at step 0 - batch 258: 1.16 (2019-08-04 13:33:58.880184)\n",
            "Accuracy at step 0 - batch 258: 0.6756\n",
            "training loss at step 0 - batch 259: 1.19 (2019-08-04 13:33:58.894965)\n",
            "Accuracy at step 0 - batch 259: 0.672\n",
            "training loss at step 0 - batch 260: 1.16 (2019-08-04 13:33:58.907967)\n",
            "Accuracy at step 0 - batch 260: 0.68\n",
            "training loss at step 0 - batch 261: 1.22 (2019-08-04 13:33:59.029701)\n",
            "Accuracy at step 0 - batch 261: 0.654\n",
            "training loss at step 0 - batch 262: 1.22 (2019-08-04 13:33:59.044767)\n",
            "Accuracy at step 0 - batch 262: 0.6572\n",
            "training loss at step 0 - batch 263: 1.19 (2019-08-04 13:33:59.056898)\n",
            "Accuracy at step 0 - batch 263: 0.6708\n",
            "training loss at step 0 - batch 264: 1.11 (2019-08-04 13:33:59.068539)\n",
            "Accuracy at step 0 - batch 264: 0.6972\n",
            "training loss at step 0 - batch 265: 1.15 (2019-08-04 13:33:59.081936)\n",
            "Accuracy at step 0 - batch 265: 0.6836\n",
            "training loss at step 0 - batch 266: 1.09 (2019-08-04 13:33:59.203419)\n",
            "Accuracy at step 0 - batch 266: 0.7036\n",
            "training loss at step 0 - batch 267: 1.09 (2019-08-04 13:33:59.218367)\n",
            "Accuracy at step 0 - batch 267: 0.6952\n",
            "training loss at step 0 - batch 268: 1.13 (2019-08-04 13:33:59.230935)\n",
            "Accuracy at step 0 - batch 268: 0.6992\n",
            "training loss at step 0 - batch 269: 1.26 (2019-08-04 13:33:59.243976)\n",
            "Accuracy at step 0 - batch 269: 0.6512\n",
            "training loss at step 0 - batch 270: 1.34 (2019-08-04 13:33:59.256312)\n",
            "Accuracy at step 0 - batch 270: 0.6332\n",
            "training loss at step 0 - batch 271: 1.32 (2019-08-04 13:33:59.380049)\n",
            "Accuracy at step 0 - batch 271: 0.6328\n",
            "training loss at step 0 - batch 272: 1.35 (2019-08-04 13:33:59.394169)\n",
            "Accuracy at step 0 - batch 272: 0.6416\n",
            "training loss at step 0 - batch 273: 1.23 (2019-08-04 13:33:59.406821)\n",
            "Accuracy at step 0 - batch 273: 0.666\n",
            "training loss at step 0 - batch 274: 1.41 (2019-08-04 13:33:59.420175)\n",
            "Accuracy at step 0 - batch 274: 0.6064\n",
            "training loss at step 0 - batch 275: 1.24 (2019-08-04 13:33:59.434447)\n",
            "Accuracy at step 0 - batch 275: 0.65\n",
            "training loss at step 0 - batch 276: 1.18 (2019-08-04 13:33:59.553650)\n",
            "Accuracy at step 0 - batch 276: 0.6608\n",
            "training loss at step 0 - batch 277: 1.15 (2019-08-04 13:33:59.571205)\n",
            "Accuracy at step 0 - batch 277: 0.6712\n",
            "training loss at step 0 - batch 278: 1.15 (2019-08-04 13:33:59.587616)\n",
            "Accuracy at step 0 - batch 278: 0.6912\n",
            "training loss at step 0 - batch 279: 1.15 (2019-08-04 13:33:59.604245)\n",
            "Accuracy at step 0 - batch 279: 0.688\n",
            "training loss at step 0 - batch 280: 1.00 (2019-08-04 13:33:59.619956)\n",
            "Accuracy at step 0 - batch 280: 0.72\n",
            "training loss at step 0 - batch 281: 1.15 (2019-08-04 13:33:59.740848)\n",
            "Accuracy at step 0 - batch 281: 0.6748\n",
            "training loss at step 0 - batch 282: 1.02 (2019-08-04 13:33:59.758504)\n",
            "Accuracy at step 0 - batch 282: 0.7252\n",
            "training loss at step 0 - batch 283: 1.07 (2019-08-04 13:33:59.770703)\n",
            "Accuracy at step 0 - batch 283: 0.7064\n",
            "training loss at step 0 - batch 284: 1.08 (2019-08-04 13:33:59.784437)\n",
            "Accuracy at step 0 - batch 284: 0.6932\n",
            "training loss at step 0 - batch 285: 1.07 (2019-08-04 13:33:59.798738)\n",
            "Accuracy at step 0 - batch 285: 0.69\n",
            "training loss at step 0 - batch 286: 1.20 (2019-08-04 13:33:59.927212)\n",
            "Accuracy at step 0 - batch 286: 0.67\n",
            "training loss at step 0 - batch 287: 1.06 (2019-08-04 13:33:59.939770)\n",
            "Accuracy at step 0 - batch 287: 0.6844\n",
            "training loss at step 0 - batch 288: 1.12 (2019-08-04 13:33:59.952307)\n",
            "Accuracy at step 0 - batch 288: 0.6868\n",
            "training loss at step 0 - batch 289: 1.06 (2019-08-04 13:33:59.965021)\n",
            "Accuracy at step 0 - batch 289: 0.7056\n",
            "training loss at step 0 - batch 290: 1.12 (2019-08-04 13:33:59.977154)\n",
            "Accuracy at step 0 - batch 290: 0.6992\n",
            "training loss at step 0 - batch 291: 1.10 (2019-08-04 13:34:00.114710)\n",
            "Accuracy at step 0 - batch 291: 0.6944\n",
            "training loss at step 0 - batch 292: 1.15 (2019-08-04 13:34:00.129165)\n",
            "Accuracy at step 0 - batch 292: 0.6712\n",
            "training loss at step 0 - batch 293: 1.20 (2019-08-04 13:34:00.143781)\n",
            "Accuracy at step 0 - batch 293: 0.6636\n",
            "training loss at step 0 - batch 294: 1.00 (2019-08-04 13:34:00.155694)\n",
            "Accuracy at step 0 - batch 294: 0.7176\n",
            "training loss at step 0 - batch 295: 0.97 (2019-08-04 13:34:00.168024)\n",
            "Accuracy at step 0 - batch 295: 0.7288\n",
            "training loss at step 0 - batch 296: 0.99 (2019-08-04 13:34:00.285360)\n",
            "Accuracy at step 0 - batch 296: 0.724\n",
            "training loss at step 0 - batch 297: 1.01 (2019-08-04 13:34:00.301512)\n",
            "Accuracy at step 0 - batch 297: 0.7208\n",
            "training loss at step 0 - batch 298: 1.24 (2019-08-04 13:34:00.313352)\n",
            "Accuracy at step 0 - batch 298: 0.6512\n",
            "training loss at step 0 - batch 299: 1.00 (2019-08-04 13:34:00.328745)\n",
            "Accuracy at step 0 - batch 299: 0.7216\n",
            "training loss at step 0 - batch 300: 1.01 (2019-08-04 13:34:00.341748)\n",
            "Accuracy at step 0 - batch 300: 0.7212\n",
            "training loss at step 0 - batch 301: 0.99 (2019-08-04 13:34:00.482543)\n",
            "Accuracy at step 0 - batch 301: 0.7328\n",
            "training loss at step 0 - batch 302: 1.03 (2019-08-04 13:34:00.499920)\n",
            "Accuracy at step 0 - batch 302: 0.7224\n",
            "training loss at step 0 - batch 303: 0.96 (2019-08-04 13:34:00.520639)\n",
            "Accuracy at step 0 - batch 303: 0.732\n",
            "training loss at step 0 - batch 304: 0.98 (2019-08-04 13:34:00.536962)\n",
            "Accuracy at step 0 - batch 304: 0.7252\n",
            "training loss at step 0 - batch 305: 0.93 (2019-08-04 13:34:00.549784)\n",
            "Accuracy at step 0 - batch 305: 0.7508\n",
            "training loss at step 0 - batch 306: 0.94 (2019-08-04 13:34:00.675766)\n",
            "Accuracy at step 0 - batch 306: 0.7336\n",
            "training loss at step 0 - batch 307: 0.97 (2019-08-04 13:34:00.688135)\n",
            "Accuracy at step 0 - batch 307: 0.7324\n",
            "training loss at step 0 - batch 308: 0.92 (2019-08-04 13:34:00.701695)\n",
            "Accuracy at step 0 - batch 308: 0.7388\n",
            "training loss at step 0 - batch 309: 0.93 (2019-08-04 13:34:00.715502)\n",
            "Accuracy at step 0 - batch 309: 0.7376\n",
            "training loss at step 0 - batch 310: 0.92 (2019-08-04 13:34:00.727591)\n",
            "Accuracy at step 0 - batch 310: 0.7456\n",
            "training loss at step 0 - batch 311: 0.95 (2019-08-04 13:34:00.846234)\n",
            "Accuracy at step 0 - batch 311: 0.7392\n",
            "training loss at step 0 - batch 312: 1.19 (2019-08-04 13:34:00.860049)\n",
            "Accuracy at step 0 - batch 312: 0.666\n",
            "training loss at step 0 - batch 313: 0.98 (2019-08-04 13:34:00.874258)\n",
            "Accuracy at step 0 - batch 313: 0.73\n",
            "training loss at step 0 - batch 314: 1.01 (2019-08-04 13:34:00.887157)\n",
            "Accuracy at step 0 - batch 314: 0.7128\n",
            "training loss at step 0 - batch 315: 1.03 (2019-08-04 13:34:00.899126)\n",
            "Accuracy at step 0 - batch 315: 0.7128\n",
            "training loss at step 0 - batch 316: 0.99 (2019-08-04 13:34:01.012255)\n",
            "Accuracy at step 0 - batch 316: 0.7256\n",
            "training loss at step 0 - batch 317: 0.96 (2019-08-04 13:34:01.025111)\n",
            "Accuracy at step 0 - batch 317: 0.7296\n",
            "training loss at step 0 - batch 318: 0.93 (2019-08-04 13:34:01.038147)\n",
            "Accuracy at step 0 - batch 318: 0.7456\n",
            "training loss at step 0 - batch 319: 0.96 (2019-08-04 13:34:01.052296)\n",
            "Accuracy at step 0 - batch 319: 0.7396\n",
            "training loss at step 0 - batch 320: 0.86 (2019-08-04 13:34:01.065247)\n",
            "Accuracy at step 0 - batch 320: 0.7672\n",
            "training loss at step 0 - batch 321: 0.94 (2019-08-04 13:34:01.179671)\n",
            "Accuracy at step 0 - batch 321: 0.7436\n",
            "training loss at step 0 - batch 322: 0.93 (2019-08-04 13:34:01.193271)\n",
            "Accuracy at step 0 - batch 322: 0.7416\n",
            "training loss at step 0 - batch 323: 0.91 (2019-08-04 13:34:01.205729)\n",
            "Accuracy at step 0 - batch 323: 0.7444\n",
            "training loss at step 0 - batch 324: 0.97 (2019-08-04 13:34:01.218141)\n",
            "Accuracy at step 0 - batch 324: 0.7172\n",
            "training loss at step 0 - batch 325: 0.91 (2019-08-04 13:34:01.231051)\n",
            "Accuracy at step 0 - batch 325: 0.746\n",
            "training loss at step 0 - batch 326: 0.94 (2019-08-04 13:34:01.351500)\n",
            "Accuracy at step 0 - batch 326: 0.7464\n",
            "training loss at step 0 - batch 327: 0.88 (2019-08-04 13:34:01.364150)\n",
            "Accuracy at step 0 - batch 327: 0.75\n",
            "training loss at step 0 - batch 328: 0.98 (2019-08-04 13:34:01.376871)\n",
            "Accuracy at step 0 - batch 328: 0.7228\n",
            "training loss at step 0 - batch 329: 1.08 (2019-08-04 13:34:01.389463)\n",
            "Accuracy at step 0 - batch 329: 0.6984\n",
            "training loss at step 0 - batch 330: 0.86 (2019-08-04 13:34:01.402277)\n",
            "Accuracy at step 0 - batch 330: 0.768\n",
            "training loss at step 0 - batch 331: 0.86 (2019-08-04 13:34:01.526363)\n",
            "Accuracy at step 0 - batch 331: 0.756\n",
            "training loss at step 0 - batch 332: 0.88 (2019-08-04 13:34:01.538575)\n",
            "Accuracy at step 0 - batch 332: 0.7596\n",
            "training loss at step 0 - batch 333: 0.91 (2019-08-04 13:34:01.555982)\n",
            "Accuracy at step 0 - batch 333: 0.7508\n",
            "training loss at step 0 - batch 334: 0.87 (2019-08-04 13:34:01.570172)\n",
            "Accuracy at step 0 - batch 334: 0.7584\n",
            "training loss at step 0 - batch 335: 0.87 (2019-08-04 13:34:01.582131)\n",
            "Accuracy at step 0 - batch 335: 0.7652\n",
            "training loss at step 0 - batch 336: 0.83 (2019-08-04 13:34:01.704395)\n",
            "Accuracy at step 0 - batch 336: 0.7628\n",
            "training loss at step 0 - batch 337: 0.90 (2019-08-04 13:34:01.716910)\n",
            "Accuracy at step 0 - batch 337: 0.7472\n",
            "training loss at step 0 - batch 338: 0.91 (2019-08-04 13:34:01.729545)\n",
            "Accuracy at step 0 - batch 338: 0.7484\n",
            "training loss at step 0 - batch 339: 0.92 (2019-08-04 13:34:01.741408)\n",
            "Accuracy at step 0 - batch 339: 0.7444\n",
            "training loss at step 0 - batch 340: 0.89 (2019-08-04 13:34:01.753440)\n",
            "Accuracy at step 0 - batch 340: 0.7528\n",
            "training loss at step 0 - batch 341: 0.85 (2019-08-04 13:34:01.881831)\n",
            "Accuracy at step 0 - batch 341: 0.766\n",
            "training loss at step 0 - batch 342: 0.81 (2019-08-04 13:34:01.894307)\n",
            "Accuracy at step 0 - batch 342: 0.7844\n",
            "training loss at step 0 - batch 343: 0.83 (2019-08-04 13:34:01.908049)\n",
            "Accuracy at step 0 - batch 343: 0.76\n",
            "training loss at step 0 - batch 344: 0.79 (2019-08-04 13:34:01.920200)\n",
            "Accuracy at step 0 - batch 344: 0.7828\n",
            "training loss at step 0 - batch 345: 0.83 (2019-08-04 13:34:01.933142)\n",
            "Accuracy at step 0 - batch 345: 0.7656\n",
            "training loss at step 0 - batch 346: 0.86 (2019-08-04 13:34:02.051060)\n",
            "Accuracy at step 0 - batch 346: 0.7632\n",
            "training loss at step 0 - batch 347: 0.93 (2019-08-04 13:34:02.064491)\n",
            "Accuracy at step 0 - batch 347: 0.74\n",
            "training loss at step 0 - batch 348: 0.85 (2019-08-04 13:34:02.076190)\n",
            "Accuracy at step 0 - batch 348: 0.768\n",
            "training loss at step 0 - batch 349: 0.83 (2019-08-04 13:34:02.091580)\n",
            "Accuracy at step 0 - batch 349: 0.7692\n",
            "training loss at step 0 - batch 350: 0.87 (2019-08-04 13:34:02.104924)\n",
            "Accuracy at step 0 - batch 350: 0.752\n",
            "training loss at step 0 - batch 351: 1.04 (2019-08-04 13:34:02.221291)\n",
            "Accuracy at step 0 - batch 351: 0.72\n",
            "training loss at step 0 - batch 352: 0.91 (2019-08-04 13:34:02.239026)\n",
            "Accuracy at step 0 - batch 352: 0.746\n",
            "training loss at step 0 - batch 353: 0.88 (2019-08-04 13:34:02.251757)\n",
            "Accuracy at step 0 - batch 353: 0.7528\n",
            "training loss at step 0 - batch 354: 0.77 (2019-08-04 13:34:02.264169)\n",
            "Accuracy at step 0 - batch 354: 0.7876\n",
            "training loss at step 0 - batch 355: 0.80 (2019-08-04 13:34:02.275978)\n",
            "Accuracy at step 0 - batch 355: 0.7832\n",
            "training loss at step 0 - batch 356: 0.82 (2019-08-04 13:34:02.395347)\n",
            "Accuracy at step 0 - batch 356: 0.7576\n",
            "training loss at step 0 - batch 357: 0.83 (2019-08-04 13:34:02.407427)\n",
            "Accuracy at step 0 - batch 357: 0.7684\n",
            "training loss at step 0 - batch 358: 0.80 (2019-08-04 13:34:02.420411)\n",
            "Accuracy at step 0 - batch 358: 0.7828\n",
            "training loss at step 0 - batch 359: 0.74 (2019-08-04 13:34:02.432671)\n",
            "Accuracy at step 0 - batch 359: 0.794\n",
            "training loss at step 0 - batch 360: 0.72 (2019-08-04 13:34:02.444759)\n",
            "Accuracy at step 0 - batch 360: 0.8012\n",
            "training loss at step 0 - batch 361: 0.70 (2019-08-04 13:34:02.567760)\n",
            "Accuracy at step 0 - batch 361: 0.81\n",
            "training loss at step 0 - batch 362: 0.76 (2019-08-04 13:34:02.582309)\n",
            "Accuracy at step 0 - batch 362: 0.7924\n",
            "training loss at step 0 - batch 363: 0.73 (2019-08-04 13:34:02.594411)\n",
            "Accuracy at step 0 - batch 363: 0.8044\n",
            "training loss at step 0 - batch 364: 0.74 (2019-08-04 13:34:02.609882)\n",
            "Accuracy at step 0 - batch 364: 0.7968\n",
            "training loss at step 0 - batch 365: 0.86 (2019-08-04 13:34:02.621917)\n",
            "Accuracy at step 0 - batch 365: 0.7644\n",
            "training loss at step 0 - batch 366: 0.73 (2019-08-04 13:34:02.743895)\n",
            "Accuracy at step 0 - batch 366: 0.8028\n",
            "training loss at step 0 - batch 367: 0.80 (2019-08-04 13:34:02.756759)\n",
            "Accuracy at step 0 - batch 367: 0.7808\n",
            "training loss at step 0 - batch 368: 0.74 (2019-08-04 13:34:02.770093)\n",
            "Accuracy at step 0 - batch 368: 0.8004\n",
            "training loss at step 0 - batch 369: 0.74 (2019-08-04 13:34:02.783192)\n",
            "Accuracy at step 0 - batch 369: 0.7948\n",
            "training loss at step 0 - batch 370: 0.74 (2019-08-04 13:34:02.795870)\n",
            "Accuracy at step 0 - batch 370: 0.7964\n",
            "training loss at step 0 - batch 371: 0.75 (2019-08-04 13:34:02.919556)\n",
            "Accuracy at step 0 - batch 371: 0.7908\n",
            "training loss at step 0 - batch 372: 0.70 (2019-08-04 13:34:02.932421)\n",
            "Accuracy at step 0 - batch 372: 0.8084\n",
            "training loss at step 0 - batch 373: 0.72 (2019-08-04 13:34:02.944532)\n",
            "Accuracy at step 0 - batch 373: 0.798\n",
            "training loss at step 0 - batch 374: 0.73 (2019-08-04 13:34:02.956216)\n",
            "Accuracy at step 0 - batch 374: 0.7964\n",
            "training loss at step 0 - batch 375: 0.77 (2019-08-04 13:34:02.970177)\n",
            "Accuracy at step 0 - batch 375: 0.7864\n",
            "training loss at step 0 - batch 376: 0.70 (2019-08-04 13:34:03.089219)\n",
            "Accuracy at step 0 - batch 376: 0.8056\n",
            "training loss at step 0 - batch 377: 0.76 (2019-08-04 13:34:03.103116)\n",
            "Accuracy at step 0 - batch 377: 0.7968\n",
            "training loss at step 0 - batch 378: 0.72 (2019-08-04 13:34:03.116246)\n",
            "Accuracy at step 0 - batch 378: 0.8016\n",
            "training loss at step 0 - batch 379: 0.73 (2019-08-04 13:34:03.131904)\n",
            "Accuracy at step 0 - batch 379: 0.7952\n",
            "training loss at step 0 - batch 380: 0.70 (2019-08-04 13:34:03.144781)\n",
            "Accuracy at step 0 - batch 380: 0.8056\n",
            "training loss at step 0 - batch 381: 0.68 (2019-08-04 13:34:03.262593)\n",
            "Accuracy at step 0 - batch 381: 0.8056\n",
            "training loss at step 0 - batch 382: 0.69 (2019-08-04 13:34:03.277914)\n",
            "Accuracy at step 0 - batch 382: 0.8064\n",
            "training loss at step 0 - batch 383: 0.73 (2019-08-04 13:34:03.291392)\n",
            "Accuracy at step 0 - batch 383: 0.7928\n",
            "training loss at step 0 - batch 384: 0.83 (2019-08-04 13:34:03.303893)\n",
            "Accuracy at step 0 - batch 384: 0.7616\n",
            "training loss at step 0 - batch 385: 0.74 (2019-08-04 13:34:03.316210)\n",
            "Accuracy at step 0 - batch 385: 0.7896\n",
            "training loss at step 0 - batch 386: 0.72 (2019-08-04 13:34:03.433865)\n",
            "Accuracy at step 0 - batch 386: 0.7904\n",
            "training loss at step 0 - batch 387: 0.70 (2019-08-04 13:34:03.450053)\n",
            "Accuracy at step 0 - batch 387: 0.8088\n",
            "training loss at step 0 - batch 388: 0.71 (2019-08-04 13:34:03.463473)\n",
            "Accuracy at step 0 - batch 388: 0.8012\n",
            "training loss at step 0 - batch 389: 0.71 (2019-08-04 13:34:03.475563)\n",
            "Accuracy at step 0 - batch 389: 0.8024\n",
            "training loss at step 0 - batch 390: 0.68 (2019-08-04 13:34:03.488284)\n",
            "Accuracy at step 0 - batch 390: 0.812\n",
            "training loss at step 0 - batch 391: 0.74 (2019-08-04 13:34:03.611097)\n",
            "Accuracy at step 0 - batch 391: 0.7992\n",
            "training loss at step 0 - batch 392: 0.76 (2019-08-04 13:34:03.623858)\n",
            "Accuracy at step 0 - batch 392: 0.7856\n",
            "training loss at step 0 - batch 393: 0.79 (2019-08-04 13:34:03.639038)\n",
            "Accuracy at step 0 - batch 393: 0.784\n",
            "training loss at step 0 - batch 394: 0.71 (2019-08-04 13:34:03.653790)\n",
            "Accuracy at step 0 - batch 394: 0.7956\n",
            "training loss at step 0 - batch 395: 0.66 (2019-08-04 13:34:03.668120)\n",
            "Accuracy at step 0 - batch 395: 0.8176\n",
            "training loss at step 0 - batch 396: 0.73 (2019-08-04 13:34:03.796887)\n",
            "Accuracy at step 0 - batch 396: 0.7924\n",
            "training loss at step 0 - batch 397: 0.66 (2019-08-04 13:34:03.813557)\n",
            "Accuracy at step 0 - batch 397: 0.8184\n",
            "training loss at step 0 - batch 398: 0.74 (2019-08-04 13:34:03.825828)\n",
            "Accuracy at step 0 - batch 398: 0.7952\n",
            "training loss at step 0 - batch 399: 0.71 (2019-08-04 13:34:03.837915)\n",
            "Accuracy at step 0 - batch 399: 0.8064\n",
            "training loss at step 0 - batch 400: 0.78 (2019-08-04 13:34:03.854936)\n",
            "Accuracy at step 0 - batch 400: 0.7892\n",
            "training loss at step 0 - batch 401: 0.65 (2019-08-04 13:34:03.976364)\n",
            "Accuracy at step 0 - batch 401: 0.8148\n",
            "training loss at step 0 - batch 402: 0.66 (2019-08-04 13:34:03.993774)\n",
            "Accuracy at step 0 - batch 402: 0.8172\n",
            "training loss at step 0 - batch 403: 0.67 (2019-08-04 13:34:04.006028)\n",
            "Accuracy at step 0 - batch 403: 0.8108\n",
            "training loss at step 0 - batch 404: 0.63 (2019-08-04 13:34:04.018208)\n",
            "Accuracy at step 0 - batch 404: 0.8264\n",
            "training loss at step 0 - batch 405: 0.66 (2019-08-04 13:34:04.030914)\n",
            "Accuracy at step 0 - batch 405: 0.8128\n",
            "training loss at step 0 - batch 406: 0.69 (2019-08-04 13:34:04.156829)\n",
            "Accuracy at step 0 - batch 406: 0.8152\n",
            "training loss at step 0 - batch 407: 0.77 (2019-08-04 13:34:04.170074)\n",
            "Accuracy at step 0 - batch 407: 0.7904\n",
            "training loss at step 0 - batch 408: 0.67 (2019-08-04 13:34:04.182527)\n",
            "Accuracy at step 0 - batch 408: 0.8044\n",
            "training loss at step 0 - batch 409: 0.63 (2019-08-04 13:34:04.194916)\n",
            "Accuracy at step 0 - batch 409: 0.8192\n",
            "training loss at step 0 - batch 410: 0.62 (2019-08-04 13:34:04.208277)\n",
            "Accuracy at step 0 - batch 410: 0.8272\n",
            "training loss at step 0 - batch 411: 0.68 (2019-08-04 13:34:04.328072)\n",
            "Accuracy at step 0 - batch 411: 0.8108\n",
            "training loss at step 0 - batch 412: 0.65 (2019-08-04 13:34:04.343538)\n",
            "Accuracy at step 0 - batch 412: 0.8156\n",
            "training loss at step 0 - batch 413: 0.69 (2019-08-04 13:34:04.356097)\n",
            "Accuracy at step 0 - batch 413: 0.7996\n",
            "training loss at step 0 - batch 414: 0.72 (2019-08-04 13:34:04.371633)\n",
            "Accuracy at step 0 - batch 414: 0.7956\n",
            "training loss at step 0 - batch 415: 0.64 (2019-08-04 13:34:04.384383)\n",
            "Accuracy at step 0 - batch 415: 0.824\n",
            "training loss at step 0 - batch 416: 0.70 (2019-08-04 13:34:04.508070)\n",
            "Accuracy at step 0 - batch 416: 0.8\n",
            "training loss at step 0 - batch 417: 0.67 (2019-08-04 13:34:04.524263)\n",
            "Accuracy at step 0 - batch 417: 0.8128\n",
            "training loss at step 0 - batch 418: 0.63 (2019-08-04 13:34:04.536651)\n",
            "Accuracy at step 0 - batch 418: 0.8232\n",
            "training loss at step 0 - batch 419: 0.59 (2019-08-04 13:34:04.548632)\n",
            "Accuracy at step 0 - batch 419: 0.8268\n",
            "training loss at step 0 - batch 420: 0.61 (2019-08-04 13:34:04.560840)\n",
            "Accuracy at step 0 - batch 420: 0.8304\n",
            "training loss at step 0 - batch 421: 0.60 (2019-08-04 13:34:04.679657)\n",
            "Accuracy at step 0 - batch 421: 0.828\n",
            "training loss at step 0 - batch 422: 0.61 (2019-08-04 13:34:04.700834)\n",
            "Accuracy at step 0 - batch 422: 0.828\n",
            "training loss at step 0 - batch 423: 0.66 (2019-08-04 13:34:04.717163)\n",
            "Accuracy at step 0 - batch 423: 0.8144\n",
            "training loss at step 0 - batch 424: 0.62 (2019-08-04 13:34:04.729910)\n",
            "Accuracy at step 0 - batch 424: 0.8204\n",
            "training loss at step 0 - batch 425: 0.69 (2019-08-04 13:34:04.742262)\n",
            "Accuracy at step 0 - batch 425: 0.8112\n",
            "training loss at step 0 - batch 426: 0.69 (2019-08-04 13:34:04.864628)\n",
            "Accuracy at step 0 - batch 426: 0.8032\n",
            "training loss at step 0 - batch 427: 0.66 (2019-08-04 13:34:04.883850)\n",
            "Accuracy at step 0 - batch 427: 0.8112\n",
            "training loss at step 0 - batch 428: 0.63 (2019-08-04 13:34:04.898396)\n",
            "Accuracy at step 0 - batch 428: 0.8292\n",
            "training loss at step 0 - batch 429: 0.60 (2019-08-04 13:34:04.910849)\n",
            "Accuracy at step 0 - batch 429: 0.8304\n",
            "training loss at step 0 - batch 430: 0.62 (2019-08-04 13:34:04.923192)\n",
            "Accuracy at step 0 - batch 430: 0.826\n",
            "training loss at step 0 - batch 431: 0.65 (2019-08-04 13:34:05.064883)\n",
            "Accuracy at step 0 - batch 431: 0.8156\n",
            "training loss at step 0 - batch 432: 0.64 (2019-08-04 13:34:05.083659)\n",
            "Accuracy at step 0 - batch 432: 0.82\n",
            "training loss at step 0 - batch 433: 0.63 (2019-08-04 13:34:05.099534)\n",
            "Accuracy at step 0 - batch 433: 0.8176\n",
            "training loss at step 0 - batch 434: 0.60 (2019-08-04 13:34:05.111841)\n",
            "Accuracy at step 0 - batch 434: 0.8264\n",
            "training loss at step 0 - batch 435: 0.64 (2019-08-04 13:34:05.123786)\n",
            "Accuracy at step 0 - batch 435: 0.826\n",
            "training loss at step 0 - batch 436: 0.59 (2019-08-04 13:34:05.243755)\n",
            "Accuracy at step 0 - batch 436: 0.8368\n",
            "training loss at step 0 - batch 437: 0.62 (2019-08-04 13:34:05.255793)\n",
            "Accuracy at step 0 - batch 437: 0.8268\n",
            "training loss at step 0 - batch 438: 0.66 (2019-08-04 13:34:05.269068)\n",
            "Accuracy at step 0 - batch 438: 0.8164\n",
            "training loss at step 0 - batch 439: 0.61 (2019-08-04 13:34:05.282350)\n",
            "Accuracy at step 0 - batch 439: 0.8236\n",
            "training loss at step 0 - batch 440: 0.65 (2019-08-04 13:34:05.294865)\n",
            "Accuracy at step 0 - batch 440: 0.816\n",
            "training loss at step 0 - batch 441: 0.64 (2019-08-04 13:34:05.422900)\n",
            "Accuracy at step 0 - batch 441: 0.8244\n",
            "training loss at step 0 - batch 442: 0.68 (2019-08-04 13:34:05.440950)\n",
            "Accuracy at step 0 - batch 442: 0.81\n",
            "training loss at step 0 - batch 443: 0.64 (2019-08-04 13:34:05.454867)\n",
            "Accuracy at step 0 - batch 443: 0.8224\n",
            "training loss at step 0 - batch 444: 0.75 (2019-08-04 13:34:05.468213)\n",
            "Accuracy at step 0 - batch 444: 0.7916\n",
            "training loss at step 0 - batch 445: 0.69 (2019-08-04 13:34:05.481174)\n",
            "Accuracy at step 0 - batch 445: 0.8084\n",
            "training loss at step 0 - batch 446: 0.72 (2019-08-04 13:34:05.601715)\n",
            "Accuracy at step 0 - batch 446: 0.7908\n",
            "training loss at step 0 - batch 447: 0.65 (2019-08-04 13:34:05.619422)\n",
            "Accuracy at step 0 - batch 447: 0.814\n",
            "training loss at step 0 - batch 448: 0.65 (2019-08-04 13:34:05.635147)\n",
            "Accuracy at step 0 - batch 448: 0.816\n",
            "training loss at step 0 - batch 449: 0.58 (2019-08-04 13:34:05.647928)\n",
            "Accuracy at step 0 - batch 449: 0.8412\n",
            "training loss at step 0 - batch 450: 0.60 (2019-08-04 13:34:05.660380)\n",
            "Accuracy at step 0 - batch 450: 0.8292\n",
            "training loss at step 0 - batch 451: 0.61 (2019-08-04 13:34:05.784697)\n",
            "Accuracy at step 0 - batch 451: 0.8224\n",
            "training loss at step 0 - batch 452: 0.62 (2019-08-04 13:34:05.802461)\n",
            "Accuracy at step 0 - batch 452: 0.8292\n",
            "training loss at step 0 - batch 453: 0.59 (2019-08-04 13:34:05.815550)\n",
            "Accuracy at step 0 - batch 453: 0.8396\n",
            "training loss at step 0 - batch 454: 0.59 (2019-08-04 13:34:05.828693)\n",
            "Accuracy at step 0 - batch 454: 0.8316\n",
            "training loss at step 0 - batch 455: 0.61 (2019-08-04 13:34:05.844016)\n",
            "Accuracy at step 0 - batch 455: 0.8272\n",
            "training loss at step 0 - batch 456: 0.59 (2019-08-04 13:34:05.965886)\n",
            "Accuracy at step 0 - batch 456: 0.834\n",
            "training loss at step 0 - batch 457: 0.64 (2019-08-04 13:34:05.981913)\n",
            "Accuracy at step 0 - batch 457: 0.8228\n",
            "training loss at step 0 - batch 458: 0.57 (2019-08-04 13:34:05.994965)\n",
            "Accuracy at step 0 - batch 458: 0.836\n",
            "training loss at step 0 - batch 459: 0.58 (2019-08-04 13:34:06.008022)\n",
            "Accuracy at step 0 - batch 459: 0.8404\n",
            "training loss at step 0 - batch 460: 0.61 (2019-08-04 13:34:06.020113)\n",
            "Accuracy at step 0 - batch 460: 0.8312\n",
            "training loss at step 0 - batch 461: 0.60 (2019-08-04 13:34:06.147080)\n",
            "Accuracy at step 0 - batch 461: 0.8312\n",
            "training loss at step 0 - batch 462: 0.59 (2019-08-04 13:34:06.164432)\n",
            "Accuracy at step 0 - batch 462: 0.8404\n",
            "training loss at step 0 - batch 463: 0.57 (2019-08-04 13:34:06.178023)\n",
            "Accuracy at step 0 - batch 463: 0.838\n",
            "training loss at step 0 - batch 464: 0.57 (2019-08-04 13:34:06.190592)\n",
            "Accuracy at step 0 - batch 464: 0.8396\n",
            "training loss at step 0 - batch 465: 0.58 (2019-08-04 13:34:06.202843)\n",
            "Accuracy at step 0 - batch 465: 0.8316\n",
            "training loss at step 0 - batch 466: 0.56 (2019-08-04 13:34:06.321428)\n",
            "Accuracy at step 0 - batch 466: 0.8408\n",
            "training loss at step 0 - batch 467: 0.61 (2019-08-04 13:34:06.335275)\n",
            "Accuracy at step 0 - batch 467: 0.8284\n",
            "training loss at step 0 - batch 468: 0.56 (2019-08-04 13:34:06.348946)\n",
            "Accuracy at step 0 - batch 468: 0.842\n",
            "training loss at step 0 - batch 469: 0.57 (2019-08-04 13:34:06.365740)\n",
            "Accuracy at step 0 - batch 469: 0.842\n",
            "training loss at step 0 - batch 470: 0.54 (2019-08-04 13:34:06.377981)\n",
            "Accuracy at step 0 - batch 470: 0.8464\n",
            "training loss at step 0 - batch 471: 0.55 (2019-08-04 13:34:06.497369)\n",
            "Accuracy at step 0 - batch 471: 0.8464\n",
            "training loss at step 0 - batch 472: 0.53 (2019-08-04 13:34:06.511561)\n",
            "Accuracy at step 0 - batch 472: 0.8512\n",
            "training loss at step 0 - batch 473: 0.57 (2019-08-04 13:34:06.525993)\n",
            "Accuracy at step 0 - batch 473: 0.844\n",
            "training loss at step 0 - batch 474: 0.56 (2019-08-04 13:34:06.539419)\n",
            "Accuracy at step 0 - batch 474: 0.8432\n",
            "training loss at step 0 - batch 475: 0.60 (2019-08-04 13:34:06.554040)\n",
            "Accuracy at step 0 - batch 475: 0.8348\n",
            "training loss at step 0 - batch 476: 0.61 (2019-08-04 13:34:06.671087)\n",
            "Accuracy at step 0 - batch 476: 0.832\n",
            "training loss at step 0 - batch 477: 0.56 (2019-08-04 13:34:06.688663)\n",
            "Accuracy at step 0 - batch 477: 0.8384\n",
            "training loss at step 0 - batch 478: 0.54 (2019-08-04 13:34:06.702626)\n",
            "Accuracy at step 0 - batch 478: 0.8524\n",
            "training loss at step 0 - batch 479: 0.59 (2019-08-04 13:34:06.714899)\n",
            "Accuracy at step 0 - batch 479: 0.8336\n",
            "training loss at step 0 - batch 480: 0.54 (2019-08-04 13:34:06.730340)\n",
            "Accuracy at step 0 - batch 480: 0.8428\n",
            "training loss at step 0 - batch 481: 0.61 (2019-08-04 13:34:06.870057)\n",
            "Accuracy at step 0 - batch 481: 0.832\n",
            "training loss at step 0 - batch 482: 0.56 (2019-08-04 13:34:06.882976)\n",
            "Accuracy at step 0 - batch 482: 0.8436\n",
            "training loss at step 0 - batch 483: 0.60 (2019-08-04 13:34:06.897109)\n",
            "Accuracy at step 0 - batch 483: 0.8372\n",
            "training loss at step 0 - batch 484: 0.58 (2019-08-04 13:34:06.910157)\n",
            "Accuracy at step 0 - batch 484: 0.836\n",
            "training loss at step 0 - batch 485: 0.59 (2019-08-04 13:34:06.923604)\n",
            "Accuracy at step 0 - batch 485: 0.8304\n",
            "training loss at step 0 - batch 486: 0.57 (2019-08-04 13:34:07.043098)\n",
            "Accuracy at step 0 - batch 486: 0.8372\n",
            "training loss at step 0 - batch 487: 0.54 (2019-08-04 13:34:07.055666)\n",
            "Accuracy at step 0 - batch 487: 0.8508\n",
            "training loss at step 0 - batch 488: 0.55 (2019-08-04 13:34:07.067953)\n",
            "Accuracy at step 0 - batch 488: 0.834\n",
            "training loss at step 0 - batch 489: 0.59 (2019-08-04 13:34:07.082865)\n",
            "Accuracy at step 0 - batch 489: 0.832\n",
            "training loss at step 0 - batch 490: 0.57 (2019-08-04 13:34:07.097168)\n",
            "Accuracy at step 0 - batch 490: 0.842\n",
            "training loss at step 0 - batch 491: 0.55 (2019-08-04 13:34:07.218150)\n",
            "Accuracy at step 0 - batch 491: 0.8456\n",
            "training loss at step 0 - batch 492: 0.55 (2019-08-04 13:34:07.234360)\n",
            "Accuracy at step 0 - batch 492: 0.8464\n",
            "training loss at step 0 - batch 493: 0.50 (2019-08-04 13:34:07.247551)\n",
            "Accuracy at step 0 - batch 493: 0.8596\n",
            "training loss at step 0 - batch 494: 0.55 (2019-08-04 13:34:07.259726)\n",
            "Accuracy at step 0 - batch 494: 0.8444\n",
            "training loss at step 0 - batch 495: 0.54 (2019-08-04 13:34:07.272044)\n",
            "Accuracy at step 0 - batch 495: 0.8452\n",
            "training loss at step 0 - batch 496: 0.57 (2019-08-04 13:34:07.400320)\n",
            "Accuracy at step 0 - batch 496: 0.838\n",
            "training loss at step 0 - batch 497: 0.59 (2019-08-04 13:34:07.413597)\n",
            "Accuracy at step 0 - batch 497: 0.8376\n",
            "training loss at step 0 - batch 498: 0.58 (2019-08-04 13:34:07.425554)\n",
            "Accuracy at step 0 - batch 498: 0.8384\n",
            "training loss at step 0 - batch 499: 0.57 (2019-08-04 13:34:07.438221)\n",
            "Accuracy at step 0 - batch 499: 0.8312\n",
            "training loss at step 0 - batch 500: 0.57 (2019-08-04 13:34:07.451085)\n",
            "Accuracy at step 0 - batch 500: 0.8428\n",
            "training loss at step 0 - batch 501: 0.55 (2019-08-04 13:34:07.701355)\n",
            "Accuracy at step 0 - batch 501: 0.8464\n",
            "training loss at step 0 - batch 502: 0.52 (2019-08-04 13:34:07.714219)\n",
            "Accuracy at step 0 - batch 502: 0.8512\n",
            "training loss at step 0 - batch 503: 0.51 (2019-08-04 13:34:07.726082)\n",
            "Accuracy at step 0 - batch 503: 0.8612\n",
            "training loss at step 0 - batch 504: 0.56 (2019-08-04 13:34:07.740002)\n",
            "Accuracy at step 0 - batch 504: 0.8356\n",
            "training loss at step 0 - batch 505: 0.59 (2019-08-04 13:34:07.754052)\n",
            "Accuracy at step 0 - batch 505: 0.8392\n",
            "training loss at step 0 - batch 506: 0.63 (2019-08-04 13:34:07.881891)\n",
            "Accuracy at step 0 - batch 506: 0.8272\n",
            "training loss at step 0 - batch 507: 0.57 (2019-08-04 13:34:07.900722)\n",
            "Accuracy at step 0 - batch 507: 0.842\n",
            "training loss at step 0 - batch 508: 0.55 (2019-08-04 13:34:07.915853)\n",
            "Accuracy at step 0 - batch 508: 0.8452\n",
            "training loss at step 0 - batch 509: 0.58 (2019-08-04 13:34:07.928483)\n",
            "Accuracy at step 0 - batch 509: 0.8308\n",
            "training loss at step 0 - batch 510: 0.57 (2019-08-04 13:34:07.942110)\n",
            "Accuracy at step 0 - batch 510: 0.8432\n",
            "training loss at step 0 - batch 511: 0.55 (2019-08-04 13:34:08.062474)\n",
            "Accuracy at step 0 - batch 511: 0.8472\n",
            "training loss at step 0 - batch 512: 0.51 (2019-08-04 13:34:08.074596)\n",
            "Accuracy at step 0 - batch 512: 0.8584\n",
            "training loss at step 0 - batch 513: 0.51 (2019-08-04 13:34:08.087188)\n",
            "Accuracy at step 0 - batch 513: 0.8496\n",
            "training loss at step 0 - batch 514: 0.54 (2019-08-04 13:34:08.100326)\n",
            "Accuracy at step 0 - batch 514: 0.8456\n",
            "training loss at step 0 - batch 515: 0.55 (2019-08-04 13:34:08.113290)\n",
            "Accuracy at step 0 - batch 515: 0.8464\n",
            "training loss at step 0 - batch 516: 0.56 (2019-08-04 13:34:08.237405)\n",
            "Accuracy at step 0 - batch 516: 0.8396\n",
            "training loss at step 0 - batch 517: 0.55 (2019-08-04 13:34:08.250686)\n",
            "Accuracy at step 0 - batch 517: 0.8428\n",
            "training loss at step 0 - batch 518: 0.52 (2019-08-04 13:34:08.263035)\n",
            "Accuracy at step 0 - batch 518: 0.858\n",
            "training loss at step 0 - batch 519: 0.54 (2019-08-04 13:34:08.274847)\n",
            "Accuracy at step 0 - batch 519: 0.8504\n",
            "training loss at step 0 - batch 520: 0.54 (2019-08-04 13:34:08.287589)\n",
            "Accuracy at step 0 - batch 520: 0.8432\n",
            "training loss at step 0 - batch 521: 0.55 (2019-08-04 13:34:08.404828)\n",
            "Accuracy at step 0 - batch 521: 0.8408\n",
            "training loss at step 0 - batch 522: 0.54 (2019-08-04 13:34:08.420474)\n",
            "Accuracy at step 0 - batch 522: 0.8424\n",
            "training loss at step 0 - batch 523: 0.52 (2019-08-04 13:34:08.435631)\n",
            "Accuracy at step 0 - batch 523: 0.8588\n",
            "training loss at step 0 - batch 524: 0.53 (2019-08-04 13:34:08.451598)\n",
            "Accuracy at step 0 - batch 524: 0.8532\n",
            "training loss at step 0 - batch 525: 0.55 (2019-08-04 13:34:08.464397)\n",
            "Accuracy at step 0 - batch 525: 0.842\n",
            "training loss at step 0 - batch 526: 0.53 (2019-08-04 13:34:08.593727)\n",
            "Accuracy at step 0 - batch 526: 0.8536\n",
            "training loss at step 0 - batch 527: 0.52 (2019-08-04 13:34:08.607920)\n",
            "Accuracy at step 0 - batch 527: 0.8564\n",
            "training loss at step 0 - batch 528: 0.53 (2019-08-04 13:34:08.620080)\n",
            "Accuracy at step 0 - batch 528: 0.8484\n",
            "training loss at step 0 - batch 529: 0.51 (2019-08-04 13:34:08.634479)\n",
            "Accuracy at step 0 - batch 529: 0.8504\n",
            "training loss at step 0 - batch 530: 0.51 (2019-08-04 13:34:08.646572)\n",
            "Accuracy at step 0 - batch 530: 0.858\n",
            "training loss at step 0 - batch 531: 0.53 (2019-08-04 13:34:08.767825)\n",
            "Accuracy at step 0 - batch 531: 0.8552\n",
            "training loss at step 0 - batch 532: 0.54 (2019-08-04 13:34:08.785743)\n",
            "Accuracy at step 0 - batch 532: 0.844\n",
            "training loss at step 0 - batch 533: 0.55 (2019-08-04 13:34:08.801217)\n",
            "Accuracy at step 0 - batch 533: 0.8448\n",
            "training loss at step 0 - batch 534: 0.52 (2019-08-04 13:34:08.816298)\n",
            "Accuracy at step 0 - batch 534: 0.8576\n",
            "training loss at step 0 - batch 535: 0.54 (2019-08-04 13:34:08.829621)\n",
            "Accuracy at step 0 - batch 535: 0.8532\n",
            "training loss at step 0 - batch 536: 0.52 (2019-08-04 13:34:08.948107)\n",
            "Accuracy at step 0 - batch 536: 0.8552\n",
            "training loss at step 0 - batch 537: 0.49 (2019-08-04 13:34:08.961723)\n",
            "Accuracy at step 0 - batch 537: 0.8608\n",
            "training loss at step 0 - batch 538: 0.52 (2019-08-04 13:34:08.976217)\n",
            "Accuracy at step 0 - batch 538: 0.8468\n",
            "training loss at step 0 - batch 539: 0.52 (2019-08-04 13:34:08.988607)\n",
            "Accuracy at step 0 - batch 539: 0.8508\n",
            "training loss at step 0 - batch 540: 0.52 (2019-08-04 13:34:09.000916)\n",
            "Accuracy at step 0 - batch 540: 0.8556\n",
            "training loss at step 0 - batch 541: 0.50 (2019-08-04 13:34:09.118753)\n",
            "Accuracy at step 0 - batch 541: 0.8556\n",
            "training loss at step 0 - batch 542: 0.49 (2019-08-04 13:34:09.131892)\n",
            "Accuracy at step 0 - batch 542: 0.8524\n",
            "training loss at step 0 - batch 543: 0.52 (2019-08-04 13:34:09.145150)\n",
            "Accuracy at step 0 - batch 543: 0.8452\n",
            "training loss at step 0 - batch 544: 0.52 (2019-08-04 13:34:09.157404)\n",
            "Accuracy at step 0 - batch 544: 0.8488\n",
            "training loss at step 0 - batch 545: 0.51 (2019-08-04 13:34:09.169168)\n",
            "Accuracy at step 0 - batch 545: 0.8524\n",
            "training loss at step 0 - batch 546: 0.52 (2019-08-04 13:34:09.296162)\n",
            "Accuracy at step 0 - batch 546: 0.8528\n",
            "training loss at step 0 - batch 547: 0.49 (2019-08-04 13:34:09.312443)\n",
            "Accuracy at step 0 - batch 547: 0.8616\n",
            "training loss at step 0 - batch 548: 0.52 (2019-08-04 13:34:09.325635)\n",
            "Accuracy at step 0 - batch 548: 0.8612\n",
            "training loss at step 0 - batch 549: 0.49 (2019-08-04 13:34:09.338214)\n",
            "Accuracy at step 0 - batch 549: 0.8608\n",
            "training loss at step 0 - batch 550: 0.51 (2019-08-04 13:34:09.351675)\n",
            "Accuracy at step 0 - batch 550: 0.8516\n",
            "training loss at step 0 - batch 551: 0.54 (2019-08-04 13:34:09.469152)\n",
            "Accuracy at step 0 - batch 551: 0.85\n",
            "training loss at step 0 - batch 552: 0.50 (2019-08-04 13:34:09.486740)\n",
            "Accuracy at step 0 - batch 552: 0.8584\n",
            "training loss at step 0 - batch 553: 0.52 (2019-08-04 13:34:09.501561)\n",
            "Accuracy at step 0 - batch 553: 0.8524\n",
            "training loss at step 0 - batch 554: 0.48 (2019-08-04 13:34:09.515261)\n",
            "Accuracy at step 0 - batch 554: 0.8592\n",
            "training loss at step 0 - batch 555: 0.48 (2019-08-04 13:34:09.531980)\n",
            "Accuracy at step 0 - batch 555: 0.8712\n",
            "training loss at step 0 - batch 556: 0.51 (2019-08-04 13:34:09.656339)\n",
            "Accuracy at step 0 - batch 556: 0.8576\n",
            "training loss at step 0 - batch 557: 0.50 (2019-08-04 13:34:09.669486)\n",
            "Accuracy at step 0 - batch 557: 0.862\n",
            "training loss at step 0 - batch 558: 0.53 (2019-08-04 13:34:09.683121)\n",
            "Accuracy at step 0 - batch 558: 0.8472\n",
            "training loss at step 0 - batch 559: 0.51 (2019-08-04 13:34:09.696067)\n",
            "Accuracy at step 0 - batch 559: 0.8568\n",
            "training loss at step 0 - batch 560: 0.53 (2019-08-04 13:34:09.711141)\n",
            "Accuracy at step 0 - batch 560: 0.85\n",
            "training loss at step 0 - batch 561: 0.50 (2019-08-04 13:34:09.836618)\n",
            "Accuracy at step 0 - batch 561: 0.8536\n",
            "training loss at step 0 - batch 562: 0.52 (2019-08-04 13:34:09.849046)\n",
            "Accuracy at step 0 - batch 562: 0.8552\n",
            "training loss at step 0 - batch 563: 0.46 (2019-08-04 13:34:09.861787)\n",
            "Accuracy at step 0 - batch 563: 0.87\n",
            "training loss at step 0 - batch 564: 0.49 (2019-08-04 13:34:09.875083)\n",
            "Accuracy at step 0 - batch 564: 0.8636\n",
            "training loss at step 0 - batch 565: 0.52 (2019-08-04 13:34:09.887828)\n",
            "Accuracy at step 0 - batch 565: 0.8564\n",
            "training loss at step 0 - batch 566: 0.51 (2019-08-04 13:34:10.026428)\n",
            "Accuracy at step 0 - batch 566: 0.8596\n",
            "training loss at step 0 - batch 567: 0.49 (2019-08-04 13:34:10.039561)\n",
            "Accuracy at step 0 - batch 567: 0.8688\n",
            "training loss at step 0 - batch 568: 0.48 (2019-08-04 13:34:10.053237)\n",
            "Accuracy at step 0 - batch 568: 0.8656\n",
            "training loss at step 0 - batch 569: 0.50 (2019-08-04 13:34:10.069560)\n",
            "Accuracy at step 0 - batch 569: 0.862\n",
            "training loss at step 0 - batch 570: 0.50 (2019-08-04 13:34:10.083637)\n",
            "Accuracy at step 0 - batch 570: 0.86\n",
            "training loss at step 0 - batch 571: 0.49 (2019-08-04 13:34:10.209499)\n",
            "Accuracy at step 0 - batch 571: 0.8624\n",
            "training loss at step 0 - batch 572: 0.46 (2019-08-04 13:34:10.225903)\n",
            "Accuracy at step 0 - batch 572: 0.8672\n",
            "training loss at step 0 - batch 573: 0.52 (2019-08-04 13:34:10.241880)\n",
            "Accuracy at step 0 - batch 573: 0.8528\n",
            "training loss at step 0 - batch 574: 0.49 (2019-08-04 13:34:10.255336)\n",
            "Accuracy at step 0 - batch 574: 0.862\n",
            "training loss at step 0 - batch 575: 0.48 (2019-08-04 13:34:10.268460)\n",
            "Accuracy at step 0 - batch 575: 0.8664\n",
            "training loss at step 0 - batch 576: 0.47 (2019-08-04 13:34:10.381594)\n",
            "Accuracy at step 0 - batch 576: 0.8664\n",
            "training loss at step 0 - batch 577: 0.52 (2019-08-04 13:34:10.395230)\n",
            "Accuracy at step 0 - batch 577: 0.8556\n",
            "training loss at step 0 - batch 578: 0.48 (2019-08-04 13:34:10.407873)\n",
            "Accuracy at step 0 - batch 578: 0.8664\n",
            "training loss at step 0 - batch 579: 0.49 (2019-08-04 13:34:10.420310)\n",
            "Accuracy at step 0 - batch 579: 0.864\n",
            "training loss at step 0 - batch 580: 0.47 (2019-08-04 13:34:10.433275)\n",
            "Accuracy at step 0 - batch 580: 0.8732\n",
            "training loss at step 0 - batch 581: 0.48 (2019-08-04 13:34:10.556834)\n",
            "Accuracy at step 0 - batch 581: 0.8636\n",
            "training loss at step 0 - batch 582: 0.52 (2019-08-04 13:34:10.574487)\n",
            "Accuracy at step 0 - batch 582: 0.86\n",
            "training loss at step 0 - batch 583: 0.50 (2019-08-04 13:34:10.587767)\n",
            "Accuracy at step 0 - batch 583: 0.8632\n",
            "training loss at step 0 - batch 584: 0.50 (2019-08-04 13:34:10.600134)\n",
            "Accuracy at step 0 - batch 584: 0.8592\n",
            "training loss at step 0 - batch 585: 0.48 (2019-08-04 13:34:10.613251)\n",
            "Accuracy at step 0 - batch 585: 0.8664\n",
            "training loss at step 0 - batch 586: 0.52 (2019-08-04 13:34:10.736943)\n",
            "Accuracy at step 0 - batch 586: 0.8544\n",
            "training loss at step 0 - batch 587: 0.49 (2019-08-04 13:34:10.753353)\n",
            "Accuracy at step 0 - batch 587: 0.8652\n",
            "training loss at step 0 - batch 588: 0.47 (2019-08-04 13:34:10.769674)\n",
            "Accuracy at step 0 - batch 588: 0.868\n",
            "training loss at step 0 - batch 589: 0.46 (2019-08-04 13:34:10.782401)\n",
            "Accuracy at step 0 - batch 589: 0.872\n",
            "training loss at step 0 - batch 590: 0.48 (2019-08-04 13:34:10.795234)\n",
            "Accuracy at step 0 - batch 590: 0.8704\n",
            "training loss at step 0 - batch 591: 0.54 (2019-08-04 13:34:10.922881)\n",
            "Accuracy at step 0 - batch 591: 0.8528\n",
            "training loss at step 0 - batch 592: 0.51 (2019-08-04 13:34:10.935257)\n",
            "Accuracy at step 0 - batch 592: 0.8548\n",
            "training loss at step 0 - batch 593: 0.63 (2019-08-04 13:34:10.947357)\n",
            "Accuracy at step 0 - batch 593: 0.8196\n",
            "training loss at step 0 - batch 594: 0.58 (2019-08-04 13:34:10.960043)\n",
            "Accuracy at step 0 - batch 594: 0.834\n",
            "training loss at step 0 - batch 595: 0.91 (2019-08-04 13:34:10.974213)\n",
            "Accuracy at step 0 - batch 595: 0.7664\n",
            "training loss at step 0 - batch 596: 1.14 (2019-08-04 13:34:11.093886)\n",
            "Accuracy at step 0 - batch 596: 0.706\n",
            "training loss at step 0 - batch 597: 2.08 (2019-08-04 13:34:11.106647)\n",
            "Accuracy at step 0 - batch 597: 0.532\n",
            "training loss at step 0 - batch 598: 2.66 (2019-08-04 13:34:11.119177)\n",
            "Accuracy at step 0 - batch 598: 0.4032\n",
            "training loss at step 0 - batch 599: 2.86 (2019-08-04 13:34:11.132258)\n",
            "Accuracy at step 0 - batch 599: 0.3692\n",
            "training loss at step 0 - batch 600: 4.11 (2019-08-04 13:34:11.145361)\n",
            "Accuracy at step 0 - batch 600: 0.2688\n",
            "training loss at step 0 - batch 601: 6.53 (2019-08-04 13:34:11.267029)\n",
            "Accuracy at step 0 - batch 601: 0.2488\n",
            "training loss at step 0 - batch 602: 9.88 (2019-08-04 13:34:11.283041)\n",
            "Accuracy at step 0 - batch 602: 0.0972\n",
            "training loss at step 0 - batch 603: 6.82 (2019-08-04 13:34:11.301945)\n",
            "Accuracy at step 0 - batch 603: 0.1484\n",
            "training loss at step 0 - batch 604: 6.83 (2019-08-04 13:34:11.320201)\n",
            "Accuracy at step 0 - batch 604: 0.0724\n",
            "training loss at step 0 - batch 605: 4.73 (2019-08-04 13:34:11.333161)\n",
            "Accuracy at step 0 - batch 605: 0.1696\n",
            "training loss at step 0 - batch 606: 8.08 (2019-08-04 13:34:11.455664)\n",
            "Accuracy at step 0 - batch 606: 0.0512\n",
            "training loss at step 0 - batch 607: 7.57 (2019-08-04 13:34:11.467691)\n",
            "Accuracy at step 0 - batch 607: 0.1668\n",
            "training loss at step 0 - batch 608: 9.33 (2019-08-04 13:34:11.484215)\n",
            "Accuracy at step 0 - batch 608: 0.1164\n",
            "training loss at step 0 - batch 609: 6.96 (2019-08-04 13:34:11.500025)\n",
            "Accuracy at step 0 - batch 609: 0.0484\n",
            "training loss at step 0 - batch 610: 5.73 (2019-08-04 13:34:11.513722)\n",
            "Accuracy at step 0 - batch 610: 0.1516\n",
            "training loss at step 0 - batch 611: 7.42 (2019-08-04 13:34:11.644361)\n",
            "Accuracy at step 0 - batch 611: 0.0552\n",
            "training loss at step 0 - batch 612: 6.36 (2019-08-04 13:34:11.656765)\n",
            "Accuracy at step 0 - batch 612: 0.0828\n",
            "training loss at step 0 - batch 613: 7.20 (2019-08-04 13:34:11.669415)\n",
            "Accuracy at step 0 - batch 613: 0.1716\n",
            "training loss at step 0 - batch 614: 6.52 (2019-08-04 13:34:11.681707)\n",
            "Accuracy at step 0 - batch 614: 0.0452\n",
            "training loss at step 0 - batch 615: 4.34 (2019-08-04 13:34:11.698863)\n",
            "Accuracy at step 0 - batch 615: 0.1056\n",
            "training loss at step 0 - batch 616: 4.17 (2019-08-04 13:34:11.820417)\n",
            "Accuracy at step 0 - batch 616: 0.212\n",
            "training loss at step 0 - batch 617: 4.83 (2019-08-04 13:34:11.832531)\n",
            "Accuracy at step 0 - batch 617: 0.1028\n",
            "training loss at step 0 - batch 618: 3.84 (2019-08-04 13:34:11.849326)\n",
            "Accuracy at step 0 - batch 618: 0.1356\n",
            "training loss at step 0 - batch 619: 3.21 (2019-08-04 13:34:11.866300)\n",
            "Accuracy at step 0 - batch 619: 0.196\n",
            "training loss at step 0 - batch 620: 3.67 (2019-08-04 13:34:11.880014)\n",
            "Accuracy at step 0 - batch 620: 0.2148\n",
            "training loss at step 0 - batch 621: 3.50 (2019-08-04 13:34:12.008741)\n",
            "Accuracy at step 0 - batch 621: 0.154\n",
            "training loss at step 0 - batch 622: 2.91 (2019-08-04 13:34:12.027436)\n",
            "Accuracy at step 0 - batch 622: 0.2948\n",
            "training loss at step 0 - batch 623: 2.72 (2019-08-04 13:34:12.039553)\n",
            "Accuracy at step 0 - batch 623: 0.2812\n",
            "training loss at step 0 - batch 624: 2.71 (2019-08-04 13:34:12.052256)\n",
            "Accuracy at step 0 - batch 624: 0.2984\n",
            "training loss at step 0 - batch 625: 2.77 (2019-08-04 13:34:12.064944)\n",
            "Accuracy at step 0 - batch 625: 0.3016\n",
            "training loss at step 0 - batch 626: 2.95 (2019-08-04 13:34:12.192843)\n",
            "Accuracy at step 0 - batch 626: 0.2696\n",
            "training loss at step 0 - batch 627: 2.40 (2019-08-04 13:34:12.209922)\n",
            "Accuracy at step 0 - batch 627: 0.3676\n",
            "training loss at step 0 - batch 628: 2.26 (2019-08-04 13:34:12.225827)\n",
            "Accuracy at step 0 - batch 628: 0.4088\n",
            "training loss at step 0 - batch 629: 2.35 (2019-08-04 13:34:12.238167)\n",
            "Accuracy at step 0 - batch 629: 0.3756\n",
            "training loss at step 0 - batch 630: 2.26 (2019-08-04 13:34:12.251273)\n",
            "Accuracy at step 0 - batch 630: 0.3976\n",
            "training loss at step 0 - batch 631: 2.33 (2019-08-04 13:34:12.366380)\n",
            "Accuracy at step 0 - batch 631: 0.3752\n",
            "training loss at step 0 - batch 632: 2.26 (2019-08-04 13:34:12.378457)\n",
            "Accuracy at step 0 - batch 632: 0.4056\n",
            "training loss at step 0 - batch 633: 2.16 (2019-08-04 13:34:12.391236)\n",
            "Accuracy at step 0 - batch 633: 0.402\n",
            "training loss at step 0 - batch 634: 2.06 (2019-08-04 13:34:12.403909)\n",
            "Accuracy at step 0 - batch 634: 0.4456\n",
            "training loss at step 0 - batch 635: 2.23 (2019-08-04 13:34:12.419326)\n",
            "Accuracy at step 0 - batch 635: 0.4076\n",
            "training loss at step 0 - batch 636: 2.26 (2019-08-04 13:34:12.541358)\n",
            "Accuracy at step 0 - batch 636: 0.4036\n",
            "training loss at step 0 - batch 637: 2.13 (2019-08-04 13:34:12.553931)\n",
            "Accuracy at step 0 - batch 637: 0.4176\n",
            "training loss at step 0 - batch 638: 2.32 (2019-08-04 13:34:12.567089)\n",
            "Accuracy at step 0 - batch 638: 0.3696\n",
            "training loss at step 0 - batch 639: 1.92 (2019-08-04 13:34:12.580363)\n",
            "Accuracy at step 0 - batch 639: 0.4604\n",
            "training loss at step 0 - batch 640: 1.87 (2019-08-04 13:34:12.593233)\n",
            "Accuracy at step 0 - batch 640: 0.4904\n",
            "training loss at step 0 - batch 641: 1.80 (2019-08-04 13:34:12.716989)\n",
            "Accuracy at step 0 - batch 641: 0.5164\n",
            "training loss at step 0 - batch 642: 2.07 (2019-08-04 13:34:12.735276)\n",
            "Accuracy at step 0 - batch 642: 0.44\n",
            "training loss at step 0 - batch 643: 1.76 (2019-08-04 13:34:12.748276)\n",
            "Accuracy at step 0 - batch 643: 0.5236\n",
            "training loss at step 0 - batch 644: 1.84 (2019-08-04 13:34:12.761309)\n",
            "Accuracy at step 0 - batch 644: 0.5024\n",
            "training loss at step 0 - batch 645: 1.68 (2019-08-04 13:34:12.774230)\n",
            "Accuracy at step 0 - batch 645: 0.55\n",
            "training loss at step 0 - batch 646: 1.54 (2019-08-04 13:34:12.895084)\n",
            "Accuracy at step 0 - batch 646: 0.574\n",
            "training loss at step 0 - batch 647: 1.55 (2019-08-04 13:34:12.908281)\n",
            "Accuracy at step 0 - batch 647: 0.5824\n",
            "training loss at step 0 - batch 648: 1.51 (2019-08-04 13:34:12.922053)\n",
            "Accuracy at step 0 - batch 648: 0.5908\n",
            "training loss at step 0 - batch 649: 1.82 (2019-08-04 13:34:12.934891)\n",
            "Accuracy at step 0 - batch 649: 0.5336\n",
            "training loss at step 0 - batch 650: 2.06 (2019-08-04 13:34:12.947291)\n",
            "Accuracy at step 0 - batch 650: 0.4528\n",
            "training loss at step 0 - batch 651: 1.70 (2019-08-04 13:34:13.075148)\n",
            "Accuracy at step 0 - batch 651: 0.5424\n",
            "training loss at step 0 - batch 652: 1.53 (2019-08-04 13:34:13.090929)\n",
            "Accuracy at step 0 - batch 652: 0.5788\n",
            "training loss at step 0 - batch 653: 1.47 (2019-08-04 13:34:13.104255)\n",
            "Accuracy at step 0 - batch 653: 0.5988\n",
            "training loss at step 0 - batch 654: 1.50 (2019-08-04 13:34:13.116074)\n",
            "Accuracy at step 0 - batch 654: 0.574\n",
            "training loss at step 0 - batch 655: 1.48 (2019-08-04 13:34:13.130130)\n",
            "Accuracy at step 0 - batch 655: 0.5948\n",
            "training loss at step 0 - batch 656: 1.42 (2019-08-04 13:34:13.250862)\n",
            "Accuracy at step 0 - batch 656: 0.6108\n",
            "training loss at step 0 - batch 657: 1.33 (2019-08-04 13:34:13.267070)\n",
            "Accuracy at step 0 - batch 657: 0.6276\n",
            "training loss at step 0 - batch 658: 1.63 (2019-08-04 13:34:13.280037)\n",
            "Accuracy at step 0 - batch 658: 0.572\n",
            "training loss at step 0 - batch 659: 1.61 (2019-08-04 13:34:13.292421)\n",
            "Accuracy at step 0 - batch 659: 0.5564\n",
            "training loss at step 0 - batch 660: 1.43 (2019-08-04 13:34:13.305347)\n",
            "Accuracy at step 0 - batch 660: 0.6208\n",
            "training loss at step 0 - batch 661: 1.53 (2019-08-04 13:34:13.427702)\n",
            "Accuracy at step 0 - batch 661: 0.5752\n",
            "training loss at step 0 - batch 662: 1.24 (2019-08-04 13:34:13.441253)\n",
            "Accuracy at step 0 - batch 662: 0.6548\n",
            "training loss at step 0 - batch 663: 1.22 (2019-08-04 13:34:13.453225)\n",
            "Accuracy at step 0 - batch 663: 0.6552\n",
            "training loss at step 0 - batch 664: 1.30 (2019-08-04 13:34:13.465732)\n",
            "Accuracy at step 0 - batch 664: 0.6392\n",
            "training loss at step 0 - batch 665: 1.30 (2019-08-04 13:34:13.478232)\n",
            "Accuracy at step 0 - batch 665: 0.6496\n",
            "training loss at step 0 - batch 666: 1.16 (2019-08-04 13:34:13.597277)\n",
            "Accuracy at step 0 - batch 666: 0.682\n",
            "training loss at step 0 - batch 667: 1.15 (2019-08-04 13:34:13.613841)\n",
            "Accuracy at step 0 - batch 667: 0.6856\n",
            "training loss at step 0 - batch 668: 1.28 (2019-08-04 13:34:13.626141)\n",
            "Accuracy at step 0 - batch 668: 0.6448\n",
            "training loss at step 0 - batch 669: 1.32 (2019-08-04 13:34:13.641773)\n",
            "Accuracy at step 0 - batch 669: 0.644\n",
            "training loss at step 0 - batch 670: 1.15 (2019-08-04 13:34:13.654412)\n",
            "Accuracy at step 0 - batch 670: 0.6956\n",
            "training loss at step 0 - batch 671: 1.08 (2019-08-04 13:34:13.780909)\n",
            "Accuracy at step 0 - batch 671: 0.7032\n",
            "training loss at step 0 - batch 672: 1.22 (2019-08-04 13:34:13.794798)\n",
            "Accuracy at step 0 - batch 672: 0.6708\n",
            "training loss at step 0 - batch 673: 1.43 (2019-08-04 13:34:13.808455)\n",
            "Accuracy at step 0 - batch 673: 0.6056\n",
            "training loss at step 0 - batch 674: 1.14 (2019-08-04 13:34:13.820354)\n",
            "Accuracy at step 0 - batch 674: 0.6844\n",
            "training loss at step 0 - batch 675: 1.01 (2019-08-04 13:34:13.832838)\n",
            "Accuracy at step 0 - batch 675: 0.7232\n",
            "training loss at step 0 - batch 676: 0.97 (2019-08-04 13:34:13.964484)\n",
            "Accuracy at step 0 - batch 676: 0.7432\n",
            "training loss at step 0 - batch 677: 1.00 (2019-08-04 13:34:13.977113)\n",
            "Accuracy at step 0 - batch 677: 0.7324\n",
            "training loss at step 0 - batch 678: 0.96 (2019-08-04 13:34:13.990221)\n",
            "Accuracy at step 0 - batch 678: 0.7392\n",
            "training loss at step 0 - batch 679: 0.95 (2019-08-04 13:34:14.003249)\n",
            "Accuracy at step 0 - batch 679: 0.7444\n",
            "training loss at step 0 - batch 680: 0.95 (2019-08-04 13:34:14.016671)\n",
            "Accuracy at step 0 - batch 680: 0.7456\n",
            "training loss at step 0 - batch 681: 1.23 (2019-08-04 13:34:14.137438)\n",
            "Accuracy at step 0 - batch 681: 0.6572\n",
            "training loss at step 0 - batch 682: 1.01 (2019-08-04 13:34:14.154365)\n",
            "Accuracy at step 0 - batch 682: 0.724\n",
            "training loss at step 0 - batch 683: 1.05 (2019-08-04 13:34:14.169705)\n",
            "Accuracy at step 0 - batch 683: 0.7088\n",
            "training loss at step 0 - batch 684: 1.16 (2019-08-04 13:34:14.182886)\n",
            "Accuracy at step 0 - batch 684: 0.6668\n",
            "training loss at step 0 - batch 685: 1.16 (2019-08-04 13:34:14.197933)\n",
            "Accuracy at step 0 - batch 685: 0.6884\n",
            "training loss at step 0 - batch 686: 0.90 (2019-08-04 13:34:14.315120)\n",
            "Accuracy at step 0 - batch 686: 0.7552\n",
            "training loss at step 0 - batch 687: 0.91 (2019-08-04 13:34:14.331482)\n",
            "Accuracy at step 0 - batch 687: 0.75\n",
            "training loss at step 0 - batch 688: 0.94 (2019-08-04 13:34:14.343958)\n",
            "Accuracy at step 0 - batch 688: 0.7428\n",
            "training loss at step 0 - batch 689: 0.97 (2019-08-04 13:34:14.357130)\n",
            "Accuracy at step 0 - batch 689: 0.742\n",
            "training loss at step 0 - batch 690: 0.88 (2019-08-04 13:34:14.370152)\n",
            "Accuracy at step 0 - batch 690: 0.7564\n",
            "training loss at step 0 - batch 691: 0.87 (2019-08-04 13:34:14.494733)\n",
            "Accuracy at step 0 - batch 691: 0.7672\n",
            "training loss at step 0 - batch 692: 0.81 (2019-08-04 13:34:14.512450)\n",
            "Accuracy at step 0 - batch 692: 0.7796\n",
            "training loss at step 0 - batch 693: 0.85 (2019-08-04 13:34:14.528946)\n",
            "Accuracy at step 0 - batch 693: 0.7688\n",
            "training loss at step 0 - batch 694: 0.86 (2019-08-04 13:34:14.543793)\n",
            "Accuracy at step 0 - batch 694: 0.7684\n",
            "training loss at step 0 - batch 695: 0.85 (2019-08-04 13:34:14.557000)\n",
            "Accuracy at step 0 - batch 695: 0.768\n",
            "training loss at step 0 - batch 696: 0.85 (2019-08-04 13:34:14.674965)\n",
            "Accuracy at step 0 - batch 696: 0.7688\n",
            "training loss at step 0 - batch 697: 0.87 (2019-08-04 13:34:14.689473)\n",
            "Accuracy at step 0 - batch 697: 0.7588\n",
            "training loss at step 0 - batch 698: 0.95 (2019-08-04 13:34:14.703927)\n",
            "Accuracy at step 0 - batch 698: 0.7364\n",
            "training loss at step 0 - batch 699: 0.81 (2019-08-04 13:34:14.719025)\n",
            "Accuracy at step 0 - batch 699: 0.7732\n",
            "training loss at step 0 - batch 700: 0.87 (2019-08-04 13:34:14.732150)\n",
            "Accuracy at step 0 - batch 700: 0.7588\n",
            "training loss at step 0 - batch 701: 0.89 (2019-08-04 13:34:14.852412)\n",
            "Accuracy at step 0 - batch 701: 0.7636\n",
            "training loss at step 0 - batch 702: 0.88 (2019-08-04 13:34:14.868207)\n",
            "Accuracy at step 0 - batch 702: 0.7612\n",
            "training loss at step 0 - batch 703: 0.82 (2019-08-04 13:34:14.880490)\n",
            "Accuracy at step 0 - batch 703: 0.7748\n",
            "training loss at step 0 - batch 704: 0.84 (2019-08-04 13:34:14.892544)\n",
            "Accuracy at step 0 - batch 704: 0.7736\n",
            "training loss at step 0 - batch 705: 0.78 (2019-08-04 13:34:14.911815)\n",
            "Accuracy at step 0 - batch 705: 0.7884\n",
            "training loss at step 0 - batch 706: 0.84 (2019-08-04 13:34:15.049247)\n",
            "Accuracy at step 0 - batch 706: 0.7648\n",
            "training loss at step 0 - batch 707: 0.74 (2019-08-04 13:34:15.062582)\n",
            "Accuracy at step 0 - batch 707: 0.802\n",
            "training loss at step 0 - batch 708: 0.77 (2019-08-04 13:34:15.076156)\n",
            "Accuracy at step 0 - batch 708: 0.7892\n",
            "training loss at step 0 - batch 709: 0.77 (2019-08-04 13:34:15.090605)\n",
            "Accuracy at step 0 - batch 709: 0.7964\n",
            "training loss at step 0 - batch 710: 0.76 (2019-08-04 13:34:15.104294)\n",
            "Accuracy at step 0 - batch 710: 0.7892\n",
            "training loss at step 0 - batch 711: 0.70 (2019-08-04 13:34:15.232192)\n",
            "Accuracy at step 0 - batch 711: 0.806\n",
            "training loss at step 0 - batch 712: 0.72 (2019-08-04 13:34:15.245966)\n",
            "Accuracy at step 0 - batch 712: 0.8008\n",
            "training loss at step 0 - batch 713: 0.69 (2019-08-04 13:34:15.258164)\n",
            "Accuracy at step 0 - batch 713: 0.8064\n",
            "training loss at step 0 - batch 714: 0.69 (2019-08-04 13:34:15.270335)\n",
            "Accuracy at step 0 - batch 714: 0.814\n",
            "training loss at step 0 - batch 715: 0.73 (2019-08-04 13:34:15.282688)\n",
            "Accuracy at step 0 - batch 715: 0.8052\n",
            "training loss at step 0 - batch 716: 0.74 (2019-08-04 13:34:15.404744)\n",
            "Accuracy at step 0 - batch 716: 0.7976\n",
            "training loss at step 0 - batch 717: 0.67 (2019-08-04 13:34:15.422674)\n",
            "Accuracy at step 0 - batch 717: 0.8164\n",
            "training loss at step 0 - batch 718: 0.73 (2019-08-04 13:34:15.437019)\n",
            "Accuracy at step 0 - batch 718: 0.796\n",
            "training loss at step 0 - batch 719: 0.70 (2019-08-04 13:34:15.451447)\n",
            "Accuracy at step 0 - batch 719: 0.8068\n",
            "training loss at step 0 - batch 720: 0.66 (2019-08-04 13:34:15.464219)\n",
            "Accuracy at step 0 - batch 720: 0.8144\n",
            "training loss at step 0 - batch 721: 0.66 (2019-08-04 13:34:15.591764)\n",
            "Accuracy at step 0 - batch 721: 0.8208\n",
            "training loss at step 0 - batch 722: 0.66 (2019-08-04 13:34:15.610886)\n",
            "Accuracy at step 0 - batch 722: 0.8208\n",
            "training loss at step 0 - batch 723: 0.70 (2019-08-04 13:34:15.623344)\n",
            "Accuracy at step 0 - batch 723: 0.8024\n",
            "training loss at step 0 - batch 724: 0.78 (2019-08-04 13:34:15.635265)\n",
            "Accuracy at step 0 - batch 724: 0.7792\n",
            "training loss at step 0 - batch 725: 0.66 (2019-08-04 13:34:15.650410)\n",
            "Accuracy at step 0 - batch 725: 0.818\n",
            "training loss at step 0 - batch 726: 0.67 (2019-08-04 13:34:15.766762)\n",
            "Accuracy at step 0 - batch 726: 0.8112\n",
            "training loss at step 0 - batch 727: 0.68 (2019-08-04 13:34:15.779316)\n",
            "Accuracy at step 0 - batch 727: 0.8128\n",
            "training loss at step 0 - batch 728: 0.66 (2019-08-04 13:34:15.792251)\n",
            "Accuracy at step 0 - batch 728: 0.8208\n",
            "training loss at step 0 - batch 729: 0.64 (2019-08-04 13:34:15.805291)\n",
            "Accuracy at step 0 - batch 729: 0.83\n",
            "training loss at step 0 - batch 730: 0.68 (2019-08-04 13:34:15.817401)\n",
            "Accuracy at step 0 - batch 730: 0.8116\n",
            "training loss at step 0 - batch 731: 0.65 (2019-08-04 13:34:15.947327)\n",
            "Accuracy at step 0 - batch 731: 0.8268\n",
            "training loss at step 0 - batch 732: 0.64 (2019-08-04 13:34:15.960093)\n",
            "Accuracy at step 0 - batch 732: 0.8264\n",
            "training loss at step 0 - batch 733: 0.63 (2019-08-04 13:34:15.973636)\n",
            "Accuracy at step 0 - batch 733: 0.8208\n",
            "training loss at step 0 - batch 734: 0.66 (2019-08-04 13:34:15.987883)\n",
            "Accuracy at step 0 - batch 734: 0.8244\n",
            "training loss at step 0 - batch 735: 0.69 (2019-08-04 13:34:16.002720)\n",
            "Accuracy at step 0 - batch 735: 0.8052\n",
            "training loss at step 0 - batch 736: 0.74 (2019-08-04 13:34:16.127636)\n",
            "Accuracy at step 0 - batch 736: 0.7992\n",
            "training loss at step 0 - batch 737: 0.90 (2019-08-04 13:34:16.141389)\n",
            "Accuracy at step 0 - batch 737: 0.75\n",
            "training loss at step 0 - batch 738: 0.70 (2019-08-04 13:34:16.156254)\n",
            "Accuracy at step 0 - batch 738: 0.8068\n",
            "training loss at step 0 - batch 739: 0.66 (2019-08-04 13:34:16.168692)\n",
            "Accuracy at step 0 - batch 739: 0.818\n",
            "training loss at step 0 - batch 740: 0.65 (2019-08-04 13:34:16.180543)\n",
            "Accuracy at step 0 - batch 740: 0.8184\n",
            "training loss at step 0 - batch 741: 0.69 (2019-08-04 13:34:16.299863)\n",
            "Accuracy at step 0 - batch 741: 0.8024\n",
            "training loss at step 0 - batch 742: 0.65 (2019-08-04 13:34:16.315575)\n",
            "Accuracy at step 0 - batch 742: 0.8228\n",
            "training loss at step 0 - batch 743: 0.63 (2019-08-04 13:34:16.331038)\n",
            "Accuracy at step 0 - batch 743: 0.8168\n",
            "training loss at step 0 - batch 744: 0.60 (2019-08-04 13:34:16.344474)\n",
            "Accuracy at step 0 - batch 744: 0.834\n",
            "training loss at step 0 - batch 745: 0.61 (2019-08-04 13:34:16.360049)\n",
            "Accuracy at step 0 - batch 745: 0.8336\n",
            "training loss at step 0 - batch 746: 0.61 (2019-08-04 13:34:16.482828)\n",
            "Accuracy at step 0 - batch 746: 0.8288\n",
            "training loss at step 0 - batch 747: 0.63 (2019-08-04 13:34:16.498617)\n",
            "Accuracy at step 0 - batch 747: 0.8252\n",
            "training loss at step 0 - batch 748: 0.64 (2019-08-04 13:34:16.511101)\n",
            "Accuracy at step 0 - batch 748: 0.8204\n",
            "training loss at step 0 - batch 749: 0.61 (2019-08-04 13:34:16.524374)\n",
            "Accuracy at step 0 - batch 749: 0.8348\n",
            "training loss at step 0 - batch 750: 0.66 (2019-08-04 13:34:16.537928)\n",
            "Accuracy at step 0 - batch 750: 0.8232\n",
            "training loss at step 0 - batch 751: 0.61 (2019-08-04 13:34:16.663687)\n",
            "Accuracy at step 0 - batch 751: 0.834\n",
            "training loss at step 0 - batch 752: 0.65 (2019-08-04 13:34:16.678606)\n",
            "Accuracy at step 0 - batch 752: 0.8224\n",
            "training loss at step 0 - batch 753: 0.63 (2019-08-04 13:34:16.690367)\n",
            "Accuracy at step 0 - batch 753: 0.8288\n",
            "training loss at step 0 - batch 754: 0.61 (2019-08-04 13:34:16.705133)\n",
            "Accuracy at step 0 - batch 754: 0.8392\n",
            "training loss at step 0 - batch 755: 0.63 (2019-08-04 13:34:16.717986)\n",
            "Accuracy at step 0 - batch 755: 0.8292\n",
            "training loss at step 0 - batch 756: 0.62 (2019-08-04 13:34:16.840292)\n",
            "Accuracy at step 0 - batch 756: 0.8304\n",
            "training loss at step 0 - batch 757: 0.60 (2019-08-04 13:34:16.853937)\n",
            "Accuracy at step 0 - batch 757: 0.8352\n",
            "training loss at step 0 - batch 758: 0.63 (2019-08-04 13:34:16.867675)\n",
            "Accuracy at step 0 - batch 758: 0.8264\n",
            "training loss at step 0 - batch 759: 0.63 (2019-08-04 13:34:16.882627)\n",
            "Accuracy at step 0 - batch 759: 0.8328\n",
            "training loss at step 0 - batch 760: 0.60 (2019-08-04 13:34:16.895092)\n",
            "Accuracy at step 0 - batch 760: 0.8356\n",
            "training loss at step 0 - batch 761: 0.58 (2019-08-04 13:34:17.017654)\n",
            "Accuracy at step 0 - batch 761: 0.8412\n",
            "training loss at step 0 - batch 762: 0.59 (2019-08-04 13:34:17.030369)\n",
            "Accuracy at step 0 - batch 762: 0.8464\n",
            "training loss at step 0 - batch 763: 0.59 (2019-08-04 13:34:17.042591)\n",
            "Accuracy at step 0 - batch 763: 0.8376\n",
            "training loss at step 0 - batch 764: 0.57 (2019-08-04 13:34:17.054660)\n",
            "Accuracy at step 0 - batch 764: 0.848\n",
            "training loss at step 0 - batch 765: 0.60 (2019-08-04 13:34:17.066865)\n",
            "Accuracy at step 0 - batch 765: 0.8304\n",
            "training loss at step 0 - batch 766: 0.62 (2019-08-04 13:34:17.192048)\n",
            "Accuracy at step 0 - batch 766: 0.8296\n",
            "training loss at step 0 - batch 767: 0.63 (2019-08-04 13:34:17.205768)\n",
            "Accuracy at step 0 - batch 767: 0.8256\n",
            "training loss at step 0 - batch 768: 0.60 (2019-08-04 13:34:17.220524)\n",
            "Accuracy at step 0 - batch 768: 0.8392\n",
            "training loss at step 0 - batch 769: 0.64 (2019-08-04 13:34:17.233054)\n",
            "Accuracy at step 0 - batch 769: 0.8236\n",
            "training loss at step 0 - batch 770: 0.62 (2019-08-04 13:34:17.245060)\n",
            "Accuracy at step 0 - batch 770: 0.8332\n",
            "training loss at step 0 - batch 771: 0.62 (2019-08-04 13:34:17.367191)\n",
            "Accuracy at step 0 - batch 771: 0.8356\n",
            "training loss at step 0 - batch 772: 0.63 (2019-08-04 13:34:17.383938)\n",
            "Accuracy at step 0 - batch 772: 0.8296\n",
            "training loss at step 0 - batch 773: 0.78 (2019-08-04 13:34:17.399149)\n",
            "Accuracy at step 0 - batch 773: 0.7816\n",
            "training loss at step 0 - batch 774: 0.63 (2019-08-04 13:34:17.413398)\n",
            "Accuracy at step 0 - batch 774: 0.8248\n",
            "training loss at step 0 - batch 775: 0.62 (2019-08-04 13:34:17.426031)\n",
            "Accuracy at step 0 - batch 775: 0.8316\n",
            "training loss at step 0 - batch 776: 0.58 (2019-08-04 13:34:17.545064)\n",
            "Accuracy at step 0 - batch 776: 0.8408\n",
            "training loss at step 0 - batch 777: 0.57 (2019-08-04 13:34:17.558855)\n",
            "Accuracy at step 0 - batch 777: 0.8472\n",
            "training loss at step 0 - batch 778: 0.57 (2019-08-04 13:34:17.571126)\n",
            "Accuracy at step 0 - batch 778: 0.8428\n",
            "training loss at step 0 - batch 779: 0.57 (2019-08-04 13:34:17.584265)\n",
            "Accuracy at step 0 - batch 779: 0.8436\n",
            "training loss at step 1 - batch 0: 0.58 (2019-08-04 13:34:17.600357)\n",
            "Accuracy at step 1 - batch 0: 0.8308\n",
            "training loss at step 1 - batch 1: 0.57 (2019-08-04 13:34:17.731470)\n",
            "Accuracy at step 1 - batch 1: 0.844\n",
            "training loss at step 1 - batch 2: 0.55 (2019-08-04 13:34:17.743641)\n",
            "Accuracy at step 1 - batch 2: 0.8504\n",
            "training loss at step 1 - batch 3: 0.59 (2019-08-04 13:34:17.756107)\n",
            "Accuracy at step 1 - batch 3: 0.8356\n",
            "training loss at step 1 - batch 4: 0.59 (2019-08-04 13:34:17.768074)\n",
            "Accuracy at step 1 - batch 4: 0.8368\n",
            "training loss at step 1 - batch 5: 0.56 (2019-08-04 13:34:17.781266)\n",
            "Accuracy at step 1 - batch 5: 0.8484\n",
            "training loss at step 1 - batch 6: 0.55 (2019-08-04 13:34:17.903970)\n",
            "Accuracy at step 1 - batch 6: 0.8468\n",
            "training loss at step 1 - batch 7: 0.57 (2019-08-04 13:34:17.916704)\n",
            "Accuracy at step 1 - batch 7: 0.8476\n",
            "training loss at step 1 - batch 8: 0.55 (2019-08-04 13:34:17.929793)\n",
            "Accuracy at step 1 - batch 8: 0.8488\n",
            "training loss at step 1 - batch 9: 0.56 (2019-08-04 13:34:17.942335)\n",
            "Accuracy at step 1 - batch 9: 0.846\n",
            "training loss at step 1 - batch 10: 0.57 (2019-08-04 13:34:17.955278)\n",
            "Accuracy at step 1 - batch 10: 0.84\n",
            "training loss at step 1 - batch 11: 0.53 (2019-08-04 13:34:18.085917)\n",
            "Accuracy at step 1 - batch 11: 0.8604\n",
            "training loss at step 1 - batch 12: 0.55 (2019-08-04 13:34:18.099939)\n",
            "Accuracy at step 1 - batch 12: 0.8548\n",
            "training loss at step 1 - batch 13: 0.54 (2019-08-04 13:34:18.117158)\n",
            "Accuracy at step 1 - batch 13: 0.8476\n",
            "training loss at step 1 - batch 14: 0.55 (2019-08-04 13:34:18.132189)\n",
            "Accuracy at step 1 - batch 14: 0.8512\n",
            "training loss at step 1 - batch 15: 0.53 (2019-08-04 13:34:18.145147)\n",
            "Accuracy at step 1 - batch 15: 0.8608\n",
            "training loss at step 1 - batch 16: 0.55 (2019-08-04 13:34:18.265450)\n",
            "Accuracy at step 1 - batch 16: 0.8516\n",
            "training loss at step 1 - batch 17: 0.56 (2019-08-04 13:34:18.279914)\n",
            "Accuracy at step 1 - batch 17: 0.8476\n",
            "training loss at step 1 - batch 18: 0.53 (2019-08-04 13:34:18.292472)\n",
            "Accuracy at step 1 - batch 18: 0.8564\n",
            "training loss at step 1 - batch 19: 0.56 (2019-08-04 13:34:18.304337)\n",
            "Accuracy at step 1 - batch 19: 0.8436\n",
            "training loss at step 1 - batch 20: 0.52 (2019-08-04 13:34:18.317431)\n",
            "Accuracy at step 1 - batch 20: 0.8532\n",
            "training loss at step 1 - batch 21: 0.53 (2019-08-04 13:34:18.439212)\n",
            "Accuracy at step 1 - batch 21: 0.8536\n",
            "training loss at step 1 - batch 22: 0.52 (2019-08-04 13:34:18.453215)\n",
            "Accuracy at step 1 - batch 22: 0.8564\n",
            "training loss at step 1 - batch 23: 0.57 (2019-08-04 13:34:18.465589)\n",
            "Accuracy at step 1 - batch 23: 0.8396\n",
            "training loss at step 1 - batch 24: 0.57 (2019-08-04 13:34:18.477557)\n",
            "Accuracy at step 1 - batch 24: 0.8356\n",
            "training loss at step 1 - batch 25: 0.56 (2019-08-04 13:34:18.489400)\n",
            "Accuracy at step 1 - batch 25: 0.8468\n",
            "training loss at step 1 - batch 26: 0.59 (2019-08-04 13:34:18.605422)\n",
            "Accuracy at step 1 - batch 26: 0.8452\n",
            "training loss at step 1 - batch 27: 0.58 (2019-08-04 13:34:18.619548)\n",
            "Accuracy at step 1 - batch 27: 0.8444\n",
            "training loss at step 1 - batch 28: 0.56 (2019-08-04 13:34:18.631767)\n",
            "Accuracy at step 1 - batch 28: 0.8436\n",
            "training loss at step 1 - batch 29: 0.53 (2019-08-04 13:34:18.645606)\n",
            "Accuracy at step 1 - batch 29: 0.852\n",
            "training loss at step 1 - batch 30: 0.51 (2019-08-04 13:34:18.658366)\n",
            "Accuracy at step 1 - batch 30: 0.8548\n",
            "training loss at step 1 - batch 31: 0.53 (2019-08-04 13:34:18.777677)\n",
            "Accuracy at step 1 - batch 31: 0.8504\n",
            "training loss at step 1 - batch 32: 0.52 (2019-08-04 13:34:18.791340)\n",
            "Accuracy at step 1 - batch 32: 0.8592\n",
            "training loss at step 1 - batch 33: 0.55 (2019-08-04 13:34:18.805511)\n",
            "Accuracy at step 1 - batch 33: 0.8444\n",
            "training loss at step 1 - batch 34: 0.54 (2019-08-04 13:34:18.818615)\n",
            "Accuracy at step 1 - batch 34: 0.8512\n",
            "training loss at step 1 - batch 35: 0.55 (2019-08-04 13:34:18.830621)\n",
            "Accuracy at step 1 - batch 35: 0.8464\n",
            "training loss at step 1 - batch 36: 0.52 (2019-08-04 13:34:18.956455)\n",
            "Accuracy at step 1 - batch 36: 0.86\n",
            "training loss at step 1 - batch 37: 0.54 (2019-08-04 13:34:18.973727)\n",
            "Accuracy at step 1 - batch 37: 0.852\n",
            "training loss at step 1 - batch 38: 0.56 (2019-08-04 13:34:18.988002)\n",
            "Accuracy at step 1 - batch 38: 0.8408\n",
            "training loss at step 1 - batch 39: 0.56 (2019-08-04 13:34:19.005615)\n",
            "Accuracy at step 1 - batch 39: 0.846\n",
            "training loss at step 1 - batch 40: 0.57 (2019-08-04 13:34:19.019303)\n",
            "Accuracy at step 1 - batch 40: 0.8364\n",
            "training loss at step 1 - batch 41: 0.56 (2019-08-04 13:34:19.147022)\n",
            "Accuracy at step 1 - batch 41: 0.8452\n",
            "training loss at step 1 - batch 42: 0.51 (2019-08-04 13:34:19.164050)\n",
            "Accuracy at step 1 - batch 42: 0.8572\n",
            "training loss at step 1 - batch 43: 0.53 (2019-08-04 13:34:19.178603)\n",
            "Accuracy at step 1 - batch 43: 0.8508\n",
            "training loss at step 1 - batch 44: 0.49 (2019-08-04 13:34:19.191340)\n",
            "Accuracy at step 1 - batch 44: 0.8616\n",
            "training loss at step 1 - batch 45: 0.52 (2019-08-04 13:34:19.204119)\n",
            "Accuracy at step 1 - batch 45: 0.8588\n",
            "training loss at step 1 - batch 46: 0.55 (2019-08-04 13:34:19.326228)\n",
            "Accuracy at step 1 - batch 46: 0.8476\n",
            "training loss at step 1 - batch 47: 0.53 (2019-08-04 13:34:19.338436)\n",
            "Accuracy at step 1 - batch 47: 0.8544\n",
            "training loss at step 1 - batch 48: 0.54 (2019-08-04 13:34:19.350580)\n",
            "Accuracy at step 1 - batch 48: 0.8528\n",
            "training loss at step 1 - batch 49: 0.52 (2019-08-04 13:34:19.364319)\n",
            "Accuracy at step 1 - batch 49: 0.854\n",
            "training loss at step 1 - batch 50: 0.55 (2019-08-04 13:34:19.382903)\n",
            "Accuracy at step 1 - batch 50: 0.8544\n",
            "training loss at step 1 - batch 51: 0.51 (2019-08-04 13:34:19.500589)\n",
            "Accuracy at step 1 - batch 51: 0.8588\n",
            "training loss at step 1 - batch 52: 0.49 (2019-08-04 13:34:19.514581)\n",
            "Accuracy at step 1 - batch 52: 0.868\n",
            "training loss at step 1 - batch 53: 0.50 (2019-08-04 13:34:19.527112)\n",
            "Accuracy at step 1 - batch 53: 0.8616\n",
            "training loss at step 1 - batch 54: 0.53 (2019-08-04 13:34:19.539847)\n",
            "Accuracy at step 1 - batch 54: 0.8528\n",
            "training loss at step 1 - batch 55: 0.54 (2019-08-04 13:34:19.552874)\n",
            "Accuracy at step 1 - batch 55: 0.8528\n",
            "training loss at step 1 - batch 56: 0.49 (2019-08-04 13:34:19.681784)\n",
            "Accuracy at step 1 - batch 56: 0.8616\n",
            "training loss at step 1 - batch 57: 0.54 (2019-08-04 13:34:19.699991)\n",
            "Accuracy at step 1 - batch 57: 0.85\n",
            "training loss at step 1 - batch 58: 0.51 (2019-08-04 13:34:19.712975)\n",
            "Accuracy at step 1 - batch 58: 0.8644\n",
            "training loss at step 1 - batch 59: 0.54 (2019-08-04 13:34:19.725223)\n",
            "Accuracy at step 1 - batch 59: 0.8536\n",
            "training loss at step 1 - batch 60: 0.54 (2019-08-04 13:34:19.738084)\n",
            "Accuracy at step 1 - batch 60: 0.8488\n",
            "training loss at step 1 - batch 61: 0.51 (2019-08-04 13:34:19.851723)\n",
            "Accuracy at step 1 - batch 61: 0.8604\n",
            "training loss at step 1 - batch 62: 0.52 (2019-08-04 13:34:19.865166)\n",
            "Accuracy at step 1 - batch 62: 0.8592\n",
            "training loss at step 1 - batch 63: 0.55 (2019-08-04 13:34:19.877284)\n",
            "Accuracy at step 1 - batch 63: 0.854\n",
            "training loss at step 1 - batch 64: 0.48 (2019-08-04 13:34:19.892056)\n",
            "Accuracy at step 1 - batch 64: 0.8656\n",
            "training loss at step 1 - batch 65: 0.52 (2019-08-04 13:34:19.904782)\n",
            "Accuracy at step 1 - batch 65: 0.856\n",
            "training loss at step 1 - batch 66: 0.48 (2019-08-04 13:34:20.027695)\n",
            "Accuracy at step 1 - batch 66: 0.8652\n",
            "training loss at step 1 - batch 67: 0.49 (2019-08-04 13:34:20.042167)\n",
            "Accuracy at step 1 - batch 67: 0.8636\n",
            "training loss at step 1 - batch 68: 0.48 (2019-08-04 13:34:20.054920)\n",
            "Accuracy at step 1 - batch 68: 0.8696\n",
            "training loss at step 1 - batch 69: 0.52 (2019-08-04 13:34:20.068517)\n",
            "Accuracy at step 1 - batch 69: 0.8496\n",
            "training loss at step 1 - batch 70: 0.51 (2019-08-04 13:34:20.080530)\n",
            "Accuracy at step 1 - batch 70: 0.8612\n",
            "training loss at step 1 - batch 71: 0.51 (2019-08-04 13:34:20.226010)\n",
            "Accuracy at step 1 - batch 71: 0.8588\n",
            "training loss at step 1 - batch 72: 0.49 (2019-08-04 13:34:20.240597)\n",
            "Accuracy at step 1 - batch 72: 0.8668\n",
            "training loss at step 1 - batch 73: 0.52 (2019-08-04 13:34:20.252948)\n",
            "Accuracy at step 1 - batch 73: 0.8592\n",
            "training loss at step 1 - batch 74: 0.50 (2019-08-04 13:34:20.265546)\n",
            "Accuracy at step 1 - batch 74: 0.86\n",
            "training loss at step 1 - batch 75: 0.50 (2019-08-04 13:34:20.278918)\n",
            "Accuracy at step 1 - batch 75: 0.8588\n",
            "training loss at step 1 - batch 76: 0.52 (2019-08-04 13:34:20.396584)\n",
            "Accuracy at step 1 - batch 76: 0.858\n",
            "training loss at step 1 - batch 77: 0.51 (2019-08-04 13:34:20.410407)\n",
            "Accuracy at step 1 - batch 77: 0.8548\n",
            "training loss at step 1 - batch 78: 0.50 (2019-08-04 13:34:20.422782)\n",
            "Accuracy at step 1 - batch 78: 0.8616\n",
            "training loss at step 1 - batch 79: 0.47 (2019-08-04 13:34:20.438780)\n",
            "Accuracy at step 1 - batch 79: 0.8764\n",
            "training loss at step 1 - batch 80: 0.49 (2019-08-04 13:34:20.451308)\n",
            "Accuracy at step 1 - batch 80: 0.8592\n",
            "training loss at step 1 - batch 81: 0.49 (2019-08-04 13:34:20.575031)\n",
            "Accuracy at step 1 - batch 81: 0.8664\n",
            "training loss at step 1 - batch 82: 0.52 (2019-08-04 13:34:20.590865)\n",
            "Accuracy at step 1 - batch 82: 0.86\n",
            "training loss at step 1 - batch 83: 0.50 (2019-08-04 13:34:20.603334)\n",
            "Accuracy at step 1 - batch 83: 0.862\n",
            "training loss at step 1 - batch 84: 0.50 (2019-08-04 13:34:20.616905)\n",
            "Accuracy at step 1 - batch 84: 0.8628\n",
            "training loss at step 1 - batch 85: 0.52 (2019-08-04 13:34:20.628981)\n",
            "Accuracy at step 1 - batch 85: 0.8544\n",
            "training loss at step 1 - batch 86: 0.53 (2019-08-04 13:34:20.750397)\n",
            "Accuracy at step 1 - batch 86: 0.8564\n",
            "training loss at step 1 - batch 87: 0.48 (2019-08-04 13:34:20.764042)\n",
            "Accuracy at step 1 - batch 87: 0.8744\n",
            "training loss at step 1 - batch 88: 0.47 (2019-08-04 13:34:20.776629)\n",
            "Accuracy at step 1 - batch 88: 0.8644\n",
            "training loss at step 1 - batch 89: 0.51 (2019-08-04 13:34:20.789361)\n",
            "Accuracy at step 1 - batch 89: 0.8536\n",
            "training loss at step 1 - batch 90: 0.52 (2019-08-04 13:34:20.803038)\n",
            "Accuracy at step 1 - batch 90: 0.8596\n",
            "training loss at step 1 - batch 91: 0.49 (2019-08-04 13:34:20.927071)\n",
            "Accuracy at step 1 - batch 91: 0.8632\n",
            "training loss at step 1 - batch 92: 0.48 (2019-08-04 13:34:20.939067)\n",
            "Accuracy at step 1 - batch 92: 0.8644\n",
            "training loss at step 1 - batch 93: 0.48 (2019-08-04 13:34:20.952006)\n",
            "Accuracy at step 1 - batch 93: 0.8684\n",
            "training loss at step 1 - batch 94: 0.49 (2019-08-04 13:34:20.967133)\n",
            "Accuracy at step 1 - batch 94: 0.8628\n",
            "training loss at step 1 - batch 95: 0.47 (2019-08-04 13:34:20.979446)\n",
            "Accuracy at step 1 - batch 95: 0.8656\n",
            "training loss at step 1 - batch 96: 0.52 (2019-08-04 13:34:21.105612)\n",
            "Accuracy at step 1 - batch 96: 0.856\n",
            "training loss at step 1 - batch 97: 0.49 (2019-08-04 13:34:21.120961)\n",
            "Accuracy at step 1 - batch 97: 0.8604\n",
            "training loss at step 1 - batch 98: 0.50 (2019-08-04 13:34:21.133884)\n",
            "Accuracy at step 1 - batch 98: 0.858\n",
            "training loss at step 1 - batch 99: 0.51 (2019-08-04 13:34:21.145909)\n",
            "Accuracy at step 1 - batch 99: 0.8564\n",
            "training loss at step 1 - batch 100: 0.50 (2019-08-04 13:34:21.160645)\n",
            "Accuracy at step 1 - batch 100: 0.8652\n",
            "training loss at step 1 - batch 101: 0.47 (2019-08-04 13:34:21.283299)\n",
            "Accuracy at step 1 - batch 101: 0.87\n",
            "training loss at step 1 - batch 102: 0.46 (2019-08-04 13:34:21.304573)\n",
            "Accuracy at step 1 - batch 102: 0.8704\n",
            "training loss at step 1 - batch 103: 0.53 (2019-08-04 13:34:21.317320)\n",
            "Accuracy at step 1 - batch 103: 0.8532\n",
            "training loss at step 1 - batch 104: 0.48 (2019-08-04 13:34:21.334933)\n",
            "Accuracy at step 1 - batch 104: 0.8724\n",
            "training loss at step 1 - batch 105: 0.50 (2019-08-04 13:34:21.347357)\n",
            "Accuracy at step 1 - batch 105: 0.8592\n",
            "training loss at step 1 - batch 106: 0.49 (2019-08-04 13:34:21.469268)\n",
            "Accuracy at step 1 - batch 106: 0.868\n",
            "training loss at step 1 - batch 107: 0.48 (2019-08-04 13:34:21.482608)\n",
            "Accuracy at step 1 - batch 107: 0.8724\n",
            "training loss at step 1 - batch 108: 0.48 (2019-08-04 13:34:21.495199)\n",
            "Accuracy at step 1 - batch 108: 0.8672\n",
            "training loss at step 1 - batch 109: 0.49 (2019-08-04 13:34:21.508343)\n",
            "Accuracy at step 1 - batch 109: 0.8616\n",
            "training loss at step 1 - batch 110: 0.49 (2019-08-04 13:34:21.520343)\n",
            "Accuracy at step 1 - batch 110: 0.8636\n",
            "training loss at step 1 - batch 111: 0.50 (2019-08-04 13:34:21.638478)\n",
            "Accuracy at step 1 - batch 111: 0.8648\n",
            "training loss at step 1 - batch 112: 0.48 (2019-08-04 13:34:21.655934)\n",
            "Accuracy at step 1 - batch 112: 0.8648\n",
            "training loss at step 1 - batch 113: 0.48 (2019-08-04 13:34:21.668347)\n",
            "Accuracy at step 1 - batch 113: 0.8672\n",
            "training loss at step 1 - batch 114: 0.50 (2019-08-04 13:34:21.683563)\n",
            "Accuracy at step 1 - batch 114: 0.8568\n",
            "training loss at step 1 - batch 115: 0.47 (2019-08-04 13:34:21.696260)\n",
            "Accuracy at step 1 - batch 115: 0.8696\n",
            "training loss at step 1 - batch 116: 0.47 (2019-08-04 13:34:21.816232)\n",
            "Accuracy at step 1 - batch 116: 0.8716\n",
            "training loss at step 1 - batch 117: 0.46 (2019-08-04 13:34:21.830136)\n",
            "Accuracy at step 1 - batch 117: 0.8648\n",
            "training loss at step 1 - batch 118: 0.49 (2019-08-04 13:34:21.842338)\n",
            "Accuracy at step 1 - batch 118: 0.8612\n",
            "training loss at step 1 - batch 119: 0.49 (2019-08-04 13:34:21.857025)\n",
            "Accuracy at step 1 - batch 119: 0.8584\n",
            "training loss at step 1 - batch 120: 0.50 (2019-08-04 13:34:21.870438)\n",
            "Accuracy at step 1 - batch 120: 0.8628\n",
            "training loss at step 1 - batch 121: 0.46 (2019-08-04 13:34:22.012394)\n",
            "Accuracy at step 1 - batch 121: 0.874\n",
            "training loss at step 1 - batch 122: 0.46 (2019-08-04 13:34:22.027468)\n",
            "Accuracy at step 1 - batch 122: 0.8712\n",
            "training loss at step 1 - batch 123: 0.49 (2019-08-04 13:34:22.040327)\n",
            "Accuracy at step 1 - batch 123: 0.8656\n",
            "training loss at step 1 - batch 124: 0.46 (2019-08-04 13:34:22.066124)\n",
            "Accuracy at step 1 - batch 124: 0.8688\n",
            "training loss at step 1 - batch 125: 0.52 (2019-08-04 13:34:22.079354)\n",
            "Accuracy at step 1 - batch 125: 0.8536\n",
            "training loss at step 1 - batch 126: 0.50 (2019-08-04 13:34:22.207107)\n",
            "Accuracy at step 1 - batch 126: 0.858\n",
            "training loss at step 1 - batch 127: 0.51 (2019-08-04 13:34:22.223758)\n",
            "Accuracy at step 1 - batch 127: 0.8644\n",
            "training loss at step 1 - batch 128: 0.46 (2019-08-04 13:34:22.237340)\n",
            "Accuracy at step 1 - batch 128: 0.876\n",
            "training loss at step 1 - batch 129: 0.48 (2019-08-04 13:34:22.249448)\n",
            "Accuracy at step 1 - batch 129: 0.8628\n",
            "training loss at step 1 - batch 130: 0.49 (2019-08-04 13:34:22.261735)\n",
            "Accuracy at step 1 - batch 130: 0.8672\n",
            "training loss at step 1 - batch 131: 0.51 (2019-08-04 13:34:22.382292)\n",
            "Accuracy at step 1 - batch 131: 0.862\n",
            "training loss at step 1 - batch 132: 0.49 (2019-08-04 13:34:22.398399)\n",
            "Accuracy at step 1 - batch 132: 0.8576\n",
            "training loss at step 1 - batch 133: 0.52 (2019-08-04 13:34:22.410268)\n",
            "Accuracy at step 1 - batch 133: 0.8604\n",
            "training loss at step 1 - batch 134: 0.51 (2019-08-04 13:34:22.422940)\n",
            "Accuracy at step 1 - batch 134: 0.8612\n",
            "training loss at step 1 - batch 135: 0.46 (2019-08-04 13:34:22.438916)\n",
            "Accuracy at step 1 - batch 135: 0.8748\n",
            "training loss at step 1 - batch 136: 0.46 (2019-08-04 13:34:22.564055)\n",
            "Accuracy at step 1 - batch 136: 0.8712\n",
            "training loss at step 1 - batch 137: 0.46 (2019-08-04 13:34:22.578253)\n",
            "Accuracy at step 1 - batch 137: 0.8692\n",
            "training loss at step 1 - batch 138: 0.46 (2019-08-04 13:34:22.591043)\n",
            "Accuracy at step 1 - batch 138: 0.8712\n",
            "training loss at step 1 - batch 139: 0.47 (2019-08-04 13:34:22.604294)\n",
            "Accuracy at step 1 - batch 139: 0.8768\n",
            "training loss at step 1 - batch 140: 0.47 (2019-08-04 13:34:22.619090)\n",
            "Accuracy at step 1 - batch 140: 0.8708\n",
            "training loss at step 1 - batch 141: 0.48 (2019-08-04 13:34:22.744022)\n",
            "Accuracy at step 1 - batch 141: 0.8664\n",
            "training loss at step 1 - batch 142: 0.48 (2019-08-04 13:34:22.757419)\n",
            "Accuracy at step 1 - batch 142: 0.8636\n",
            "training loss at step 1 - batch 143: 0.47 (2019-08-04 13:34:22.769687)\n",
            "Accuracy at step 1 - batch 143: 0.8748\n",
            "training loss at step 1 - batch 144: 0.46 (2019-08-04 13:34:22.782005)\n",
            "Accuracy at step 1 - batch 144: 0.8704\n",
            "training loss at step 1 - batch 145: 0.45 (2019-08-04 13:34:22.795968)\n",
            "Accuracy at step 1 - batch 145: 0.8756\n",
            "training loss at step 1 - batch 146: 0.44 (2019-08-04 13:34:22.911132)\n",
            "Accuracy at step 1 - batch 146: 0.8732\n",
            "training loss at step 1 - batch 147: 0.46 (2019-08-04 13:34:22.924650)\n",
            "Accuracy at step 1 - batch 147: 0.8668\n",
            "training loss at step 1 - batch 148: 0.48 (2019-08-04 13:34:22.938150)\n",
            "Accuracy at step 1 - batch 148: 0.8676\n",
            "training loss at step 1 - batch 149: 0.51 (2019-08-04 13:34:22.953359)\n",
            "Accuracy at step 1 - batch 149: 0.866\n",
            "training loss at step 1 - batch 150: 0.48 (2019-08-04 13:34:22.967046)\n",
            "Accuracy at step 1 - batch 150: 0.8608\n",
            "training loss at step 1 - batch 151: 0.45 (2019-08-04 13:34:23.090640)\n",
            "Accuracy at step 1 - batch 151: 0.8744\n",
            "training loss at step 1 - batch 152: 0.48 (2019-08-04 13:34:23.103463)\n",
            "Accuracy at step 1 - batch 152: 0.8676\n",
            "training loss at step 1 - batch 153: 0.50 (2019-08-04 13:34:23.116125)\n",
            "Accuracy at step 1 - batch 153: 0.8632\n",
            "training loss at step 1 - batch 154: 0.49 (2019-08-04 13:34:23.129354)\n",
            "Accuracy at step 1 - batch 154: 0.8704\n",
            "training loss at step 1 - batch 155: 0.46 (2019-08-04 13:34:23.143471)\n",
            "Accuracy at step 1 - batch 155: 0.8696\n",
            "training loss at step 1 - batch 156: 0.48 (2019-08-04 13:34:23.275065)\n",
            "Accuracy at step 1 - batch 156: 0.8664\n",
            "training loss at step 1 - batch 157: 0.46 (2019-08-04 13:34:23.293002)\n",
            "Accuracy at step 1 - batch 157: 0.8724\n",
            "training loss at step 1 - batch 158: 0.47 (2019-08-04 13:34:23.305096)\n",
            "Accuracy at step 1 - batch 158: 0.8688\n",
            "training loss at step 1 - batch 159: 0.46 (2019-08-04 13:34:23.317106)\n",
            "Accuracy at step 1 - batch 159: 0.874\n",
            "training loss at step 1 - batch 160: 0.48 (2019-08-04 13:34:23.328899)\n",
            "Accuracy at step 1 - batch 160: 0.8608\n",
            "training loss at step 1 - batch 161: 0.50 (2019-08-04 13:34:23.447658)\n",
            "Accuracy at step 1 - batch 161: 0.864\n",
            "training loss at step 1 - batch 162: 0.50 (2019-08-04 13:34:23.463152)\n",
            "Accuracy at step 1 - batch 162: 0.8568\n",
            "training loss at step 1 - batch 163: 0.49 (2019-08-04 13:34:23.476953)\n",
            "Accuracy at step 1 - batch 163: 0.86\n",
            "training loss at step 1 - batch 164: 0.48 (2019-08-04 13:34:23.492067)\n",
            "Accuracy at step 1 - batch 164: 0.8648\n",
            "training loss at step 1 - batch 165: 0.46 (2019-08-04 13:34:23.503856)\n",
            "Accuracy at step 1 - batch 165: 0.8696\n",
            "training loss at step 1 - batch 166: 0.47 (2019-08-04 13:34:23.622293)\n",
            "Accuracy at step 1 - batch 166: 0.8708\n",
            "training loss at step 1 - batch 167: 0.50 (2019-08-04 13:34:23.636537)\n",
            "Accuracy at step 1 - batch 167: 0.86\n",
            "training loss at step 1 - batch 168: 0.46 (2019-08-04 13:34:23.649819)\n",
            "Accuracy at step 1 - batch 168: 0.868\n",
            "training loss at step 1 - batch 169: 0.46 (2019-08-04 13:34:23.662479)\n",
            "Accuracy at step 1 - batch 169: 0.8724\n",
            "training loss at step 1 - batch 170: 0.46 (2019-08-04 13:34:23.674391)\n",
            "Accuracy at step 1 - batch 170: 0.868\n",
            "training loss at step 1 - batch 171: 0.45 (2019-08-04 13:34:23.799867)\n",
            "Accuracy at step 1 - batch 171: 0.8732\n",
            "training loss at step 1 - batch 172: 0.45 (2019-08-04 13:34:23.817321)\n",
            "Accuracy at step 1 - batch 172: 0.8716\n",
            "training loss at step 1 - batch 173: 0.47 (2019-08-04 13:34:23.829333)\n",
            "Accuracy at step 1 - batch 173: 0.8744\n",
            "training loss at step 1 - batch 174: 0.47 (2019-08-04 13:34:23.841754)\n",
            "Accuracy at step 1 - batch 174: 0.8756\n",
            "training loss at step 1 - batch 175: 0.44 (2019-08-04 13:34:23.855254)\n",
            "Accuracy at step 1 - batch 175: 0.872\n",
            "training loss at step 1 - batch 176: 0.44 (2019-08-04 13:34:23.973083)\n",
            "Accuracy at step 1 - batch 176: 0.8752\n",
            "training loss at step 1 - batch 177: 0.44 (2019-08-04 13:34:23.985371)\n",
            "Accuracy at step 1 - batch 177: 0.8748\n",
            "training loss at step 1 - batch 178: 0.47 (2019-08-04 13:34:23.998084)\n",
            "Accuracy at step 1 - batch 178: 0.8712\n",
            "training loss at step 1 - batch 179: 0.45 (2019-08-04 13:34:24.015041)\n",
            "Accuracy at step 1 - batch 179: 0.8732\n",
            "training loss at step 1 - batch 180: 0.45 (2019-08-04 13:34:24.028036)\n",
            "Accuracy at step 1 - batch 180: 0.8744\n",
            "training loss at step 1 - batch 181: 0.47 (2019-08-04 13:34:24.151985)\n",
            "Accuracy at step 1 - batch 181: 0.8648\n",
            "training loss at step 1 - batch 182: 0.45 (2019-08-04 13:34:24.165355)\n",
            "Accuracy at step 1 - batch 182: 0.8776\n",
            "training loss at step 1 - batch 183: 0.46 (2019-08-04 13:34:24.177586)\n",
            "Accuracy at step 1 - batch 183: 0.8708\n",
            "training loss at step 1 - batch 184: 0.48 (2019-08-04 13:34:24.189349)\n",
            "Accuracy at step 1 - batch 184: 0.8612\n",
            "training loss at step 1 - batch 185: 0.46 (2019-08-04 13:34:24.201633)\n",
            "Accuracy at step 1 - batch 185: 0.8768\n",
            "training loss at step 1 - batch 186: 0.44 (2019-08-04 13:34:24.327897)\n",
            "Accuracy at step 1 - batch 186: 0.8776\n",
            "training loss at step 1 - batch 187: 0.50 (2019-08-04 13:34:24.342254)\n",
            "Accuracy at step 1 - batch 187: 0.8656\n",
            "training loss at step 1 - batch 188: 0.47 (2019-08-04 13:34:24.354489)\n",
            "Accuracy at step 1 - batch 188: 0.8688\n",
            "training loss at step 1 - batch 189: 0.48 (2019-08-04 13:34:24.368255)\n",
            "Accuracy at step 1 - batch 189: 0.868\n",
            "training loss at step 1 - batch 190: 0.46 (2019-08-04 13:34:24.381178)\n",
            "Accuracy at step 1 - batch 190: 0.8704\n",
            "training loss at step 1 - batch 191: 0.46 (2019-08-04 13:34:24.500339)\n",
            "Accuracy at step 1 - batch 191: 0.87\n",
            "training loss at step 1 - batch 192: 0.43 (2019-08-04 13:34:24.514396)\n",
            "Accuracy at step 1 - batch 192: 0.8824\n",
            "training loss at step 1 - batch 193: 0.44 (2019-08-04 13:34:24.526208)\n",
            "Accuracy at step 1 - batch 193: 0.8796\n",
            "training loss at step 1 - batch 194: 0.46 (2019-08-04 13:34:24.542267)\n",
            "Accuracy at step 1 - batch 194: 0.8676\n",
            "training loss at step 1 - batch 195: 0.44 (2019-08-04 13:34:24.555398)\n",
            "Accuracy at step 1 - batch 195: 0.8756\n",
            "training loss at step 1 - batch 196: 0.47 (2019-08-04 13:34:24.675587)\n",
            "Accuracy at step 1 - batch 196: 0.876\n",
            "training loss at step 1 - batch 197: 0.44 (2019-08-04 13:34:24.689243)\n",
            "Accuracy at step 1 - batch 197: 0.8772\n",
            "training loss at step 1 - batch 198: 0.46 (2019-08-04 13:34:24.703030)\n",
            "Accuracy at step 1 - batch 198: 0.8692\n",
            "training loss at step 1 - batch 199: 0.47 (2019-08-04 13:34:24.715904)\n",
            "Accuracy at step 1 - batch 199: 0.874\n",
            "training loss at step 1 - batch 200: 0.44 (2019-08-04 13:34:24.729160)\n",
            "Accuracy at step 1 - batch 200: 0.8772\n",
            "training loss at step 1 - batch 201: 0.46 (2019-08-04 13:34:24.848698)\n",
            "Accuracy at step 1 - batch 201: 0.8688\n",
            "training loss at step 1 - batch 202: 0.46 (2019-08-04 13:34:24.863832)\n",
            "Accuracy at step 1 - batch 202: 0.8696\n",
            "training loss at step 1 - batch 203: 0.43 (2019-08-04 13:34:24.877035)\n",
            "Accuracy at step 1 - batch 203: 0.8836\n",
            "training loss at step 1 - batch 204: 0.44 (2019-08-04 13:34:24.890865)\n",
            "Accuracy at step 1 - batch 204: 0.8696\n",
            "training loss at step 1 - batch 205: 0.45 (2019-08-04 13:34:24.904462)\n",
            "Accuracy at step 1 - batch 205: 0.8744\n",
            "training loss at step 1 - batch 206: 0.46 (2019-08-04 13:34:25.026529)\n",
            "Accuracy at step 1 - batch 206: 0.8736\n",
            "training loss at step 1 - batch 207: 0.47 (2019-08-04 13:34:25.040456)\n",
            "Accuracy at step 1 - batch 207: 0.8684\n",
            "training loss at step 1 - batch 208: 0.43 (2019-08-04 13:34:25.054629)\n",
            "Accuracy at step 1 - batch 208: 0.886\n",
            "training loss at step 1 - batch 209: 0.44 (2019-08-04 13:34:25.067082)\n",
            "Accuracy at step 1 - batch 209: 0.8704\n",
            "training loss at step 1 - batch 210: 0.43 (2019-08-04 13:34:25.079867)\n",
            "Accuracy at step 1 - batch 210: 0.8856\n",
            "training loss at step 1 - batch 211: 0.46 (2019-08-04 13:34:25.226103)\n",
            "Accuracy at step 1 - batch 211: 0.874\n",
            "training loss at step 1 - batch 212: 0.46 (2019-08-04 13:34:25.238376)\n",
            "Accuracy at step 1 - batch 212: 0.8732\n",
            "training loss at step 1 - batch 213: 0.44 (2019-08-04 13:34:25.250607)\n",
            "Accuracy at step 1 - batch 213: 0.8692\n",
            "training loss at step 1 - batch 214: 0.45 (2019-08-04 13:34:25.264948)\n",
            "Accuracy at step 1 - batch 214: 0.872\n",
            "training loss at step 1 - batch 215: 0.46 (2019-08-04 13:34:25.277313)\n",
            "Accuracy at step 1 - batch 215: 0.8704\n",
            "training loss at step 1 - batch 216: 0.41 (2019-08-04 13:34:25.400871)\n",
            "Accuracy at step 1 - batch 216: 0.8832\n",
            "training loss at step 1 - batch 217: 0.44 (2019-08-04 13:34:25.414707)\n",
            "Accuracy at step 1 - batch 217: 0.8756\n",
            "training loss at step 1 - batch 218: 0.43 (2019-08-04 13:34:25.427937)\n",
            "Accuracy at step 1 - batch 218: 0.8756\n",
            "training loss at step 1 - batch 219: 0.46 (2019-08-04 13:34:25.441134)\n",
            "Accuracy at step 1 - batch 219: 0.874\n",
            "training loss at step 1 - batch 220: 0.49 (2019-08-04 13:34:25.453908)\n",
            "Accuracy at step 1 - batch 220: 0.8644\n",
            "training loss at step 1 - batch 221: 0.42 (2019-08-04 13:34:25.589601)\n",
            "Accuracy at step 1 - batch 221: 0.8828\n",
            "training loss at step 1 - batch 222: 0.45 (2019-08-04 13:34:25.603120)\n",
            "Accuracy at step 1 - batch 222: 0.8736\n",
            "training loss at step 1 - batch 223: 0.42 (2019-08-04 13:34:25.615296)\n",
            "Accuracy at step 1 - batch 223: 0.8748\n",
            "training loss at step 1 - batch 224: 0.46 (2019-08-04 13:34:25.627599)\n",
            "Accuracy at step 1 - batch 224: 0.8692\n",
            "training loss at step 1 - batch 225: 0.46 (2019-08-04 13:34:25.639736)\n",
            "Accuracy at step 1 - batch 225: 0.8744\n",
            "training loss at step 1 - batch 226: 0.46 (2019-08-04 13:34:25.759029)\n",
            "Accuracy at step 1 - batch 226: 0.8688\n",
            "training loss at step 1 - batch 227: 0.43 (2019-08-04 13:34:25.774900)\n",
            "Accuracy at step 1 - batch 227: 0.878\n",
            "training loss at step 1 - batch 228: 0.45 (2019-08-04 13:34:25.787311)\n",
            "Accuracy at step 1 - batch 228: 0.866\n",
            "training loss at step 1 - batch 229: 0.46 (2019-08-04 13:34:25.801735)\n",
            "Accuracy at step 1 - batch 229: 0.8748\n",
            "training loss at step 1 - batch 230: 0.45 (2019-08-04 13:34:25.814827)\n",
            "Accuracy at step 1 - batch 230: 0.8712\n",
            "training loss at step 1 - batch 231: 0.45 (2019-08-04 13:34:25.935589)\n",
            "Accuracy at step 1 - batch 231: 0.8736\n",
            "training loss at step 1 - batch 232: 0.45 (2019-08-04 13:34:25.948914)\n",
            "Accuracy at step 1 - batch 232: 0.8728\n",
            "training loss at step 1 - batch 233: 0.42 (2019-08-04 13:34:25.963611)\n",
            "Accuracy at step 1 - batch 233: 0.8848\n",
            "training loss at step 1 - batch 234: 0.45 (2019-08-04 13:34:25.975753)\n",
            "Accuracy at step 1 - batch 234: 0.8744\n",
            "training loss at step 1 - batch 235: 0.47 (2019-08-04 13:34:25.988581)\n",
            "Accuracy at step 1 - batch 235: 0.8724\n",
            "training loss at step 1 - batch 236: 0.44 (2019-08-04 13:34:26.116785)\n",
            "Accuracy at step 1 - batch 236: 0.8856\n",
            "training loss at step 1 - batch 237: 0.43 (2019-08-04 13:34:26.133312)\n",
            "Accuracy at step 1 - batch 237: 0.8776\n",
            "training loss at step 1 - batch 238: 0.42 (2019-08-04 13:34:26.149898)\n",
            "Accuracy at step 1 - batch 238: 0.8824\n",
            "training loss at step 1 - batch 239: 0.43 (2019-08-04 13:34:26.167081)\n",
            "Accuracy at step 1 - batch 239: 0.8768\n",
            "training loss at step 1 - batch 240: 0.43 (2019-08-04 13:34:26.180371)\n",
            "Accuracy at step 1 - batch 240: 0.8776\n",
            "training loss at step 1 - batch 241: 0.44 (2019-08-04 13:34:26.304758)\n",
            "Accuracy at step 1 - batch 241: 0.8772\n",
            "training loss at step 1 - batch 242: 0.44 (2019-08-04 13:34:26.321038)\n",
            "Accuracy at step 1 - batch 242: 0.874\n",
            "training loss at step 1 - batch 243: 0.42 (2019-08-04 13:34:26.333988)\n",
            "Accuracy at step 1 - batch 243: 0.8828\n",
            "training loss at step 1 - batch 244: 0.49 (2019-08-04 13:34:26.347242)\n",
            "Accuracy at step 1 - batch 244: 0.8636\n",
            "training loss at step 1 - batch 245: 0.47 (2019-08-04 13:34:26.360544)\n",
            "Accuracy at step 1 - batch 245: 0.8652\n",
            "training loss at step 1 - batch 246: 0.47 (2019-08-04 13:34:26.480476)\n",
            "Accuracy at step 1 - batch 246: 0.8652\n",
            "training loss at step 1 - batch 247: 0.45 (2019-08-04 13:34:26.492459)\n",
            "Accuracy at step 1 - batch 247: 0.8812\n",
            "training loss at step 1 - batch 248: 0.43 (2019-08-04 13:34:26.506502)\n",
            "Accuracy at step 1 - batch 248: 0.8764\n",
            "training loss at step 1 - batch 249: 0.44 (2019-08-04 13:34:26.518779)\n",
            "Accuracy at step 1 - batch 249: 0.8808\n",
            "training loss at step 1 - batch 250: 0.45 (2019-08-04 13:34:26.533478)\n",
            "Accuracy at step 1 - batch 250: 0.8756\n",
            "training loss at step 1 - batch 251: 0.44 (2019-08-04 13:34:26.654082)\n",
            "Accuracy at step 1 - batch 251: 0.8792\n",
            "training loss at step 1 - batch 252: 0.46 (2019-08-04 13:34:26.667274)\n",
            "Accuracy at step 1 - batch 252: 0.874\n",
            "training loss at step 1 - batch 253: 0.42 (2019-08-04 13:34:26.683629)\n",
            "Accuracy at step 1 - batch 253: 0.8796\n",
            "training loss at step 1 - batch 254: 0.44 (2019-08-04 13:34:26.696117)\n",
            "Accuracy at step 1 - batch 254: 0.8812\n",
            "training loss at step 1 - batch 255: 0.43 (2019-08-04 13:34:26.709198)\n",
            "Accuracy at step 1 - batch 255: 0.878\n",
            "training loss at step 1 - batch 256: 0.42 (2019-08-04 13:34:26.830579)\n",
            "Accuracy at step 1 - batch 256: 0.8864\n",
            "training loss at step 1 - batch 257: 0.44 (2019-08-04 13:34:26.846228)\n",
            "Accuracy at step 1 - batch 257: 0.8756\n",
            "training loss at step 1 - batch 258: 0.44 (2019-08-04 13:34:26.858725)\n",
            "Accuracy at step 1 - batch 258: 0.8744\n",
            "training loss at step 1 - batch 259: 0.41 (2019-08-04 13:34:26.871967)\n",
            "Accuracy at step 1 - batch 259: 0.8848\n",
            "training loss at step 1 - batch 260: 0.44 (2019-08-04 13:34:26.884583)\n",
            "Accuracy at step 1 - batch 260: 0.8768\n",
            "training loss at step 1 - batch 261: 0.42 (2019-08-04 13:34:27.001025)\n",
            "Accuracy at step 1 - batch 261: 0.882\n",
            "training loss at step 1 - batch 262: 0.45 (2019-08-04 13:34:27.015144)\n",
            "Accuracy at step 1 - batch 262: 0.868\n",
            "training loss at step 1 - batch 263: 0.45 (2019-08-04 13:34:27.027593)\n",
            "Accuracy at step 1 - batch 263: 0.8736\n",
            "training loss at step 1 - batch 264: 0.44 (2019-08-04 13:34:27.042457)\n",
            "Accuracy at step 1 - batch 264: 0.8732\n",
            "training loss at step 1 - batch 265: 0.44 (2019-08-04 13:34:27.055322)\n",
            "Accuracy at step 1 - batch 265: 0.882\n",
            "training loss at step 1 - batch 266: 0.44 (2019-08-04 13:34:27.178895)\n",
            "Accuracy at step 1 - batch 266: 0.8772\n",
            "training loss at step 1 - batch 267: 0.44 (2019-08-04 13:34:27.196880)\n",
            "Accuracy at step 1 - batch 267: 0.8784\n",
            "training loss at step 1 - batch 268: 0.44 (2019-08-04 13:34:27.210906)\n",
            "Accuracy at step 1 - batch 268: 0.8768\n",
            "training loss at step 1 - batch 269: 0.43 (2019-08-04 13:34:27.223524)\n",
            "Accuracy at step 1 - batch 269: 0.8784\n",
            "training loss at step 1 - batch 270: 0.45 (2019-08-04 13:34:27.236135)\n",
            "Accuracy at step 1 - batch 270: 0.8728\n",
            "training loss at step 1 - batch 271: 0.44 (2019-08-04 13:34:27.358495)\n",
            "Accuracy at step 1 - batch 271: 0.88\n",
            "training loss at step 1 - batch 272: 0.43 (2019-08-04 13:34:27.373066)\n",
            "Accuracy at step 1 - batch 272: 0.8752\n",
            "training loss at step 1 - batch 273: 0.42 (2019-08-04 13:34:27.386274)\n",
            "Accuracy at step 1 - batch 273: 0.886\n",
            "training loss at step 1 - batch 274: 0.43 (2019-08-04 13:34:27.399378)\n",
            "Accuracy at step 1 - batch 274: 0.8756\n",
            "training loss at step 1 - batch 275: 0.43 (2019-08-04 13:34:27.415751)\n",
            "Accuracy at step 1 - batch 275: 0.8752\n",
            "training loss at step 1 - batch 276: 0.44 (2019-08-04 13:34:27.540482)\n",
            "Accuracy at step 1 - batch 276: 0.8716\n",
            "training loss at step 1 - batch 277: 0.44 (2019-08-04 13:34:27.555554)\n",
            "Accuracy at step 1 - batch 277: 0.8728\n",
            "training loss at step 1 - batch 278: 0.43 (2019-08-04 13:34:27.577835)\n",
            "Accuracy at step 1 - batch 278: 0.8804\n",
            "training loss at step 1 - batch 279: 0.46 (2019-08-04 13:34:27.590327)\n",
            "Accuracy at step 1 - batch 279: 0.8768\n",
            "training loss at step 1 - batch 280: 0.44 (2019-08-04 13:34:27.603392)\n",
            "Accuracy at step 1 - batch 280: 0.88\n",
            "training loss at step 1 - batch 281: 0.39 (2019-08-04 13:34:27.720642)\n",
            "Accuracy at step 1 - batch 281: 0.888\n",
            "training loss at step 1 - batch 282: 0.43 (2019-08-04 13:34:27.735642)\n",
            "Accuracy at step 1 - batch 282: 0.8796\n",
            "training loss at step 1 - batch 283: 0.40 (2019-08-04 13:34:27.747622)\n",
            "Accuracy at step 1 - batch 283: 0.8888\n",
            "training loss at step 1 - batch 284: 0.46 (2019-08-04 13:34:27.761046)\n",
            "Accuracy at step 1 - batch 284: 0.8788\n",
            "training loss at step 1 - batch 285: 0.44 (2019-08-04 13:34:27.773120)\n",
            "Accuracy at step 1 - batch 285: 0.87\n",
            "training loss at step 1 - batch 286: 0.44 (2019-08-04 13:34:27.897221)\n",
            "Accuracy at step 1 - batch 286: 0.8732\n",
            "training loss at step 1 - batch 287: 0.43 (2019-08-04 13:34:27.909905)\n",
            "Accuracy at step 1 - batch 287: 0.88\n",
            "training loss at step 1 - batch 288: 0.43 (2019-08-04 13:34:27.922774)\n",
            "Accuracy at step 1 - batch 288: 0.884\n",
            "training loss at step 1 - batch 289: 0.43 (2019-08-04 13:34:27.935587)\n",
            "Accuracy at step 1 - batch 289: 0.876\n",
            "training loss at step 1 - batch 290: 0.45 (2019-08-04 13:34:27.948080)\n",
            "Accuracy at step 1 - batch 290: 0.8736\n",
            "training loss at step 1 - batch 291: 0.49 (2019-08-04 13:34:28.068163)\n",
            "Accuracy at step 1 - batch 291: 0.8612\n",
            "training loss at step 1 - batch 292: 0.42 (2019-08-04 13:34:28.080587)\n",
            "Accuracy at step 1 - batch 292: 0.8708\n",
            "training loss at step 1 - batch 293: 0.45 (2019-08-04 13:34:28.093103)\n",
            "Accuracy at step 1 - batch 293: 0.8736\n",
            "training loss at step 1 - batch 294: 0.46 (2019-08-04 13:34:28.107606)\n",
            "Accuracy at step 1 - batch 294: 0.8708\n",
            "training loss at step 1 - batch 295: 0.44 (2019-08-04 13:34:28.120151)\n",
            "Accuracy at step 1 - batch 295: 0.8764\n",
            "training loss at step 1 - batch 296: 0.43 (2019-08-04 13:34:28.248936)\n",
            "Accuracy at step 1 - batch 296: 0.8832\n",
            "training loss at step 1 - batch 297: 0.42 (2019-08-04 13:34:28.262013)\n",
            "Accuracy at step 1 - batch 297: 0.8792\n",
            "training loss at step 1 - batch 298: 0.45 (2019-08-04 13:34:28.274995)\n",
            "Accuracy at step 1 - batch 298: 0.872\n",
            "training loss at step 1 - batch 299: 0.42 (2019-08-04 13:34:28.288899)\n",
            "Accuracy at step 1 - batch 299: 0.8784\n",
            "training loss at step 1 - batch 300: 0.42 (2019-08-04 13:34:28.301375)\n",
            "Accuracy at step 1 - batch 300: 0.878\n",
            "training loss at step 1 - batch 301: 0.43 (2019-08-04 13:34:28.423315)\n",
            "Accuracy at step 1 - batch 301: 0.8772\n",
            "training loss at step 1 - batch 302: 0.43 (2019-08-04 13:34:28.437255)\n",
            "Accuracy at step 1 - batch 302: 0.8764\n",
            "training loss at step 1 - batch 303: 0.43 (2019-08-04 13:34:28.450359)\n",
            "Accuracy at step 1 - batch 303: 0.8812\n",
            "training loss at step 1 - batch 304: 0.42 (2019-08-04 13:34:28.462886)\n",
            "Accuracy at step 1 - batch 304: 0.8788\n",
            "training loss at step 1 - batch 305: 0.41 (2019-08-04 13:34:28.475116)\n",
            "Accuracy at step 1 - batch 305: 0.8828\n",
            "training loss at step 1 - batch 306: 0.43 (2019-08-04 13:34:28.594792)\n",
            "Accuracy at step 1 - batch 306: 0.8804\n",
            "training loss at step 1 - batch 307: 0.42 (2019-08-04 13:34:28.612444)\n",
            "Accuracy at step 1 - batch 307: 0.8848\n",
            "training loss at step 1 - batch 308: 0.41 (2019-08-04 13:34:28.627067)\n",
            "Accuracy at step 1 - batch 308: 0.8828\n",
            "training loss at step 1 - batch 309: 0.41 (2019-08-04 13:34:28.640549)\n",
            "Accuracy at step 1 - batch 309: 0.8904\n",
            "training loss at step 1 - batch 310: 0.41 (2019-08-04 13:34:28.654691)\n",
            "Accuracy at step 1 - batch 310: 0.8792\n",
            "training loss at step 1 - batch 311: 0.43 (2019-08-04 13:34:28.777000)\n",
            "Accuracy at step 1 - batch 311: 0.8804\n",
            "training loss at step 1 - batch 312: 0.42 (2019-08-04 13:34:28.789860)\n",
            "Accuracy at step 1 - batch 312: 0.8772\n",
            "training loss at step 1 - batch 313: 0.44 (2019-08-04 13:34:28.802706)\n",
            "Accuracy at step 1 - batch 313: 0.8756\n",
            "training loss at step 1 - batch 314: 0.43 (2019-08-04 13:34:28.815411)\n",
            "Accuracy at step 1 - batch 314: 0.8736\n",
            "training loss at step 1 - batch 315: 0.40 (2019-08-04 13:34:28.827325)\n",
            "Accuracy at step 1 - batch 315: 0.8852\n",
            "training loss at step 1 - batch 316: 0.45 (2019-08-04 13:34:28.959287)\n",
            "Accuracy at step 1 - batch 316: 0.8736\n",
            "training loss at step 1 - batch 317: 0.42 (2019-08-04 13:34:28.973530)\n",
            "Accuracy at step 1 - batch 317: 0.878\n",
            "training loss at step 1 - batch 318: 0.43 (2019-08-04 13:34:28.986118)\n",
            "Accuracy at step 1 - batch 318: 0.8804\n",
            "training loss at step 1 - batch 319: 0.42 (2019-08-04 13:34:28.998361)\n",
            "Accuracy at step 1 - batch 319: 0.888\n",
            "training loss at step 1 - batch 320: 0.46 (2019-08-04 13:34:29.010335)\n",
            "Accuracy at step 1 - batch 320: 0.8716\n",
            "training loss at step 1 - batch 321: 0.42 (2019-08-04 13:34:29.132723)\n",
            "Accuracy at step 1 - batch 321: 0.8788\n",
            "training loss at step 1 - batch 322: 0.43 (2019-08-04 13:34:29.146260)\n",
            "Accuracy at step 1 - batch 322: 0.8804\n",
            "training loss at step 1 - batch 323: 0.44 (2019-08-04 13:34:29.159460)\n",
            "Accuracy at step 1 - batch 323: 0.8776\n",
            "training loss at step 1 - batch 324: 0.44 (2019-08-04 13:34:29.175895)\n",
            "Accuracy at step 1 - batch 324: 0.868\n",
            "training loss at step 1 - batch 325: 0.44 (2019-08-04 13:34:29.188944)\n",
            "Accuracy at step 1 - batch 325: 0.8764\n",
            "training loss at step 1 - batch 326: 0.42 (2019-08-04 13:34:29.319632)\n",
            "Accuracy at step 1 - batch 326: 0.8796\n",
            "training loss at step 1 - batch 327: 0.41 (2019-08-04 13:34:29.331979)\n",
            "Accuracy at step 1 - batch 327: 0.8848\n",
            "training loss at step 1 - batch 328: 0.43 (2019-08-04 13:34:29.345471)\n",
            "Accuracy at step 1 - batch 328: 0.8744\n",
            "training loss at step 1 - batch 329: 0.41 (2019-08-04 13:34:29.359455)\n",
            "Accuracy at step 1 - batch 329: 0.882\n",
            "training loss at step 1 - batch 330: 0.43 (2019-08-04 13:34:29.376086)\n",
            "Accuracy at step 1 - batch 330: 0.8776\n",
            "training loss at step 1 - batch 331: 0.41 (2019-08-04 13:34:29.501098)\n",
            "Accuracy at step 1 - batch 331: 0.8892\n",
            "training loss at step 1 - batch 332: 0.43 (2019-08-04 13:34:29.513786)\n",
            "Accuracy at step 1 - batch 332: 0.8804\n",
            "training loss at step 1 - batch 333: 0.42 (2019-08-04 13:34:29.526373)\n",
            "Accuracy at step 1 - batch 333: 0.8852\n",
            "training loss at step 1 - batch 334: 0.42 (2019-08-04 13:34:29.538127)\n",
            "Accuracy at step 1 - batch 334: 0.8848\n",
            "training loss at step 1 - batch 335: 0.43 (2019-08-04 13:34:29.550059)\n",
            "Accuracy at step 1 - batch 335: 0.8824\n",
            "training loss at step 1 - batch 336: 0.42 (2019-08-04 13:34:29.675847)\n",
            "Accuracy at step 1 - batch 336: 0.8788\n",
            "training loss at step 1 - batch 337: 0.43 (2019-08-04 13:34:29.692242)\n",
            "Accuracy at step 1 - batch 337: 0.8804\n",
            "training loss at step 1 - batch 338: 0.43 (2019-08-04 13:34:29.704678)\n",
            "Accuracy at step 1 - batch 338: 0.8764\n",
            "training loss at step 1 - batch 339: 0.42 (2019-08-04 13:34:29.717932)\n",
            "Accuracy at step 1 - batch 339: 0.882\n",
            "training loss at step 1 - batch 340: 0.41 (2019-08-04 13:34:29.731440)\n",
            "Accuracy at step 1 - batch 340: 0.8808\n",
            "training loss at step 1 - batch 341: 0.42 (2019-08-04 13:34:29.849217)\n",
            "Accuracy at step 1 - batch 341: 0.8768\n",
            "training loss at step 1 - batch 342: 0.44 (2019-08-04 13:34:29.861272)\n",
            "Accuracy at step 1 - batch 342: 0.878\n",
            "training loss at step 1 - batch 343: 0.41 (2019-08-04 13:34:29.873878)\n",
            "Accuracy at step 1 - batch 343: 0.882\n",
            "training loss at step 1 - batch 344: 0.42 (2019-08-04 13:34:29.889213)\n",
            "Accuracy at step 1 - batch 344: 0.8796\n",
            "training loss at step 1 - batch 345: 0.40 (2019-08-04 13:34:29.902017)\n",
            "Accuracy at step 1 - batch 345: 0.8848\n",
            "training loss at step 1 - batch 346: 0.40 (2019-08-04 13:34:30.024202)\n",
            "Accuracy at step 1 - batch 346: 0.8776\n",
            "training loss at step 1 - batch 347: 0.42 (2019-08-04 13:34:30.042845)\n",
            "Accuracy at step 1 - batch 347: 0.8828\n",
            "training loss at step 1 - batch 348: 0.44 (2019-08-04 13:34:30.055369)\n",
            "Accuracy at step 1 - batch 348: 0.8744\n",
            "training loss at step 1 - batch 349: 0.41 (2019-08-04 13:34:30.071157)\n",
            "Accuracy at step 1 - batch 349: 0.88\n",
            "training loss at step 1 - batch 350: 0.42 (2019-08-04 13:34:30.084704)\n",
            "Accuracy at step 1 - batch 350: 0.8764\n",
            "training loss at step 1 - batch 351: 0.44 (2019-08-04 13:34:30.232383)\n",
            "Accuracy at step 1 - batch 351: 0.8816\n",
            "training loss at step 1 - batch 352: 0.40 (2019-08-04 13:34:30.248478)\n",
            "Accuracy at step 1 - batch 352: 0.8892\n",
            "training loss at step 1 - batch 353: 0.42 (2019-08-04 13:34:30.264212)\n",
            "Accuracy at step 1 - batch 353: 0.8812\n",
            "training loss at step 1 - batch 354: 0.42 (2019-08-04 13:34:30.278231)\n",
            "Accuracy at step 1 - batch 354: 0.8776\n",
            "training loss at step 1 - batch 355: 0.39 (2019-08-04 13:34:30.292862)\n",
            "Accuracy at step 1 - batch 355: 0.8936\n",
            "training loss at step 1 - batch 356: 0.44 (2019-08-04 13:34:30.418730)\n",
            "Accuracy at step 1 - batch 356: 0.8748\n",
            "training loss at step 1 - batch 357: 0.42 (2019-08-04 13:34:30.432583)\n",
            "Accuracy at step 1 - batch 357: 0.8796\n",
            "training loss at step 1 - batch 358: 0.42 (2019-08-04 13:34:30.446685)\n",
            "Accuracy at step 1 - batch 358: 0.8804\n",
            "training loss at step 1 - batch 359: 0.41 (2019-08-04 13:34:30.459835)\n",
            "Accuracy at step 1 - batch 359: 0.888\n",
            "training loss at step 1 - batch 360: 0.45 (2019-08-04 13:34:30.471821)\n",
            "Accuracy at step 1 - batch 360: 0.8792\n",
            "training loss at step 1 - batch 361: 0.45 (2019-08-04 13:34:30.587018)\n",
            "Accuracy at step 1 - batch 361: 0.8728\n",
            "training loss at step 1 - batch 362: 0.43 (2019-08-04 13:34:30.602370)\n",
            "Accuracy at step 1 - batch 362: 0.8792\n",
            "training loss at step 1 - batch 363: 0.43 (2019-08-04 13:34:30.616798)\n",
            "Accuracy at step 1 - batch 363: 0.8828\n",
            "training loss at step 1 - batch 364: 0.43 (2019-08-04 13:34:30.630232)\n",
            "Accuracy at step 1 - batch 364: 0.8808\n",
            "training loss at step 1 - batch 365: 0.41 (2019-08-04 13:34:30.642658)\n",
            "Accuracy at step 1 - batch 365: 0.8824\n",
            "training loss at step 1 - batch 366: 0.41 (2019-08-04 13:34:30.769090)\n",
            "Accuracy at step 1 - batch 366: 0.8792\n",
            "training loss at step 1 - batch 367: 0.41 (2019-08-04 13:34:30.781753)\n",
            "Accuracy at step 1 - batch 367: 0.8872\n",
            "training loss at step 1 - batch 368: 0.41 (2019-08-04 13:34:30.793815)\n",
            "Accuracy at step 1 - batch 368: 0.8804\n",
            "training loss at step 1 - batch 369: 0.42 (2019-08-04 13:34:30.806770)\n",
            "Accuracy at step 1 - batch 369: 0.8812\n",
            "training loss at step 1 - batch 370: 0.45 (2019-08-04 13:34:30.819379)\n",
            "Accuracy at step 1 - batch 370: 0.8724\n",
            "training loss at step 1 - batch 371: 0.43 (2019-08-04 13:34:30.935861)\n",
            "Accuracy at step 1 - batch 371: 0.88\n",
            "training loss at step 1 - batch 372: 0.41 (2019-08-04 13:34:30.948441)\n",
            "Accuracy at step 1 - batch 372: 0.8828\n",
            "training loss at step 1 - batch 373: 0.43 (2019-08-04 13:34:30.961961)\n",
            "Accuracy at step 1 - batch 373: 0.8748\n",
            "training loss at step 1 - batch 374: 0.42 (2019-08-04 13:34:30.975379)\n",
            "Accuracy at step 1 - batch 374: 0.8792\n",
            "training loss at step 1 - batch 375: 0.41 (2019-08-04 13:34:30.988051)\n",
            "Accuracy at step 1 - batch 375: 0.8932\n",
            "training loss at step 1 - batch 376: 0.42 (2019-08-04 13:34:31.111039)\n",
            "Accuracy at step 1 - batch 376: 0.8836\n",
            "training loss at step 1 - batch 377: 0.42 (2019-08-04 13:34:31.128637)\n",
            "Accuracy at step 1 - batch 377: 0.88\n",
            "training loss at step 1 - batch 378: 0.42 (2019-08-04 13:34:31.142440)\n",
            "Accuracy at step 1 - batch 378: 0.8796\n",
            "training loss at step 1 - batch 379: 0.40 (2019-08-04 13:34:31.155081)\n",
            "Accuracy at step 1 - batch 379: 0.89\n",
            "training loss at step 1 - batch 380: 0.44 (2019-08-04 13:34:31.167843)\n",
            "Accuracy at step 1 - batch 380: 0.88\n",
            "training loss at step 1 - batch 381: 0.42 (2019-08-04 13:34:31.295937)\n",
            "Accuracy at step 1 - batch 381: 0.878\n",
            "training loss at step 1 - batch 382: 0.42 (2019-08-04 13:34:31.308698)\n",
            "Accuracy at step 1 - batch 382: 0.8832\n",
            "training loss at step 1 - batch 383: 0.42 (2019-08-04 13:34:31.321413)\n",
            "Accuracy at step 1 - batch 383: 0.8816\n",
            "training loss at step 1 - batch 384: 0.41 (2019-08-04 13:34:31.334378)\n",
            "Accuracy at step 1 - batch 384: 0.8816\n",
            "training loss at step 1 - batch 385: 0.44 (2019-08-04 13:34:31.347063)\n",
            "Accuracy at step 1 - batch 385: 0.8748\n",
            "training loss at step 1 - batch 386: 0.41 (2019-08-04 13:34:31.465794)\n",
            "Accuracy at step 1 - batch 386: 0.8844\n",
            "training loss at step 1 - batch 387: 0.42 (2019-08-04 13:34:31.478336)\n",
            "Accuracy at step 1 - batch 387: 0.8776\n",
            "training loss at step 1 - batch 388: 0.42 (2019-08-04 13:34:31.491324)\n",
            "Accuracy at step 1 - batch 388: 0.8792\n",
            "training loss at step 1 - batch 389: 0.41 (2019-08-04 13:34:31.505850)\n",
            "Accuracy at step 1 - batch 389: 0.888\n",
            "training loss at step 1 - batch 390: 0.44 (2019-08-04 13:34:31.519162)\n",
            "Accuracy at step 1 - batch 390: 0.8768\n",
            "training loss at step 1 - batch 391: 0.43 (2019-08-04 13:34:31.634784)\n",
            "Accuracy at step 1 - batch 391: 0.878\n",
            "training loss at step 1 - batch 392: 0.42 (2019-08-04 13:34:31.649081)\n",
            "Accuracy at step 1 - batch 392: 0.8816\n",
            "training loss at step 1 - batch 393: 0.44 (2019-08-04 13:34:31.661441)\n",
            "Accuracy at step 1 - batch 393: 0.8724\n",
            "training loss at step 1 - batch 394: 0.44 (2019-08-04 13:34:31.673769)\n",
            "Accuracy at step 1 - batch 394: 0.8768\n",
            "training loss at step 1 - batch 395: 0.41 (2019-08-04 13:34:31.685886)\n",
            "Accuracy at step 1 - batch 395: 0.8852\n",
            "training loss at step 1 - batch 396: 0.42 (2019-08-04 13:34:31.807472)\n",
            "Accuracy at step 1 - batch 396: 0.8808\n",
            "training loss at step 1 - batch 397: 0.42 (2019-08-04 13:34:31.821097)\n",
            "Accuracy at step 1 - batch 397: 0.8788\n",
            "training loss at step 1 - batch 398: 0.40 (2019-08-04 13:34:31.833060)\n",
            "Accuracy at step 1 - batch 398: 0.8792\n",
            "training loss at step 1 - batch 399: 0.42 (2019-08-04 13:34:31.846412)\n",
            "Accuracy at step 1 - batch 399: 0.8772\n",
            "training loss at step 1 - batch 400: 0.43 (2019-08-04 13:34:31.858630)\n",
            "Accuracy at step 1 - batch 400: 0.878\n",
            "training loss at step 1 - batch 401: 0.41 (2019-08-04 13:34:31.977760)\n",
            "Accuracy at step 1 - batch 401: 0.8772\n",
            "training loss at step 1 - batch 402: 0.38 (2019-08-04 13:34:31.995119)\n",
            "Accuracy at step 1 - batch 402: 0.8952\n",
            "training loss at step 1 - batch 403: 0.41 (2019-08-04 13:34:32.011592)\n",
            "Accuracy at step 1 - batch 403: 0.888\n",
            "training loss at step 1 - batch 404: 0.41 (2019-08-04 13:34:32.026732)\n",
            "Accuracy at step 1 - batch 404: 0.886\n",
            "training loss at step 1 - batch 405: 0.43 (2019-08-04 13:34:32.039790)\n",
            "Accuracy at step 1 - batch 405: 0.8768\n",
            "training loss at step 1 - batch 406: 0.39 (2019-08-04 13:34:32.161163)\n",
            "Accuracy at step 1 - batch 406: 0.8856\n",
            "training loss at step 1 - batch 407: 0.44 (2019-08-04 13:34:32.174486)\n",
            "Accuracy at step 1 - batch 407: 0.8784\n",
            "training loss at step 1 - batch 408: 0.43 (2019-08-04 13:34:32.189100)\n",
            "Accuracy at step 1 - batch 408: 0.8788\n",
            "training loss at step 1 - batch 409: 0.41 (2019-08-04 13:34:32.201089)\n",
            "Accuracy at step 1 - batch 409: 0.8856\n",
            "training loss at step 1 - batch 410: 0.40 (2019-08-04 13:34:32.215012)\n",
            "Accuracy at step 1 - batch 410: 0.8916\n",
            "training loss at step 1 - batch 411: 0.40 (2019-08-04 13:34:32.348849)\n",
            "Accuracy at step 1 - batch 411: 0.8888\n",
            "training loss at step 1 - batch 412: 0.42 (2019-08-04 13:34:32.362003)\n",
            "Accuracy at step 1 - batch 412: 0.88\n",
            "training loss at step 1 - batch 413: 0.41 (2019-08-04 13:34:32.374490)\n",
            "Accuracy at step 1 - batch 413: 0.8824\n",
            "training loss at step 1 - batch 414: 0.41 (2019-08-04 13:34:32.386911)\n",
            "Accuracy at step 1 - batch 414: 0.888\n",
            "training loss at step 1 - batch 415: 0.42 (2019-08-04 13:34:32.399357)\n",
            "Accuracy at step 1 - batch 415: 0.88\n",
            "training loss at step 1 - batch 416: 0.40 (2019-08-04 13:34:32.544988)\n",
            "Accuracy at step 1 - batch 416: 0.8836\n",
            "training loss at step 1 - batch 417: 0.41 (2019-08-04 13:34:32.558258)\n",
            "Accuracy at step 1 - batch 417: 0.8808\n",
            "training loss at step 1 - batch 418: 0.42 (2019-08-04 13:34:32.572340)\n",
            "Accuracy at step 1 - batch 418: 0.8784\n",
            "training loss at step 1 - batch 419: 0.41 (2019-08-04 13:34:32.585031)\n",
            "Accuracy at step 1 - batch 419: 0.8808\n",
            "training loss at step 1 - batch 420: 0.39 (2019-08-04 13:34:32.598505)\n",
            "Accuracy at step 1 - batch 420: 0.8864\n",
            "training loss at step 1 - batch 421: 0.42 (2019-08-04 13:34:32.717158)\n",
            "Accuracy at step 1 - batch 421: 0.8796\n",
            "training loss at step 1 - batch 422: 0.39 (2019-08-04 13:34:32.730407)\n",
            "Accuracy at step 1 - batch 422: 0.892\n",
            "training loss at step 1 - batch 423: 0.40 (2019-08-04 13:34:32.743098)\n",
            "Accuracy at step 1 - batch 423: 0.8816\n",
            "training loss at step 1 - batch 424: 0.43 (2019-08-04 13:34:32.758967)\n",
            "Accuracy at step 1 - batch 424: 0.8744\n",
            "training loss at step 1 - batch 425: 0.39 (2019-08-04 13:34:32.772238)\n",
            "Accuracy at step 1 - batch 425: 0.8852\n",
            "training loss at step 1 - batch 426: 0.41 (2019-08-04 13:34:32.890770)\n",
            "Accuracy at step 1 - batch 426: 0.884\n",
            "training loss at step 1 - batch 427: 0.42 (2019-08-04 13:34:32.905701)\n",
            "Accuracy at step 1 - batch 427: 0.8832\n",
            "training loss at step 1 - batch 428: 0.39 (2019-08-04 13:34:32.918252)\n",
            "Accuracy at step 1 - batch 428: 0.8852\n",
            "training loss at step 1 - batch 429: 0.39 (2019-08-04 13:34:32.930204)\n",
            "Accuracy at step 1 - batch 429: 0.8928\n",
            "training loss at step 1 - batch 430: 0.41 (2019-08-04 13:34:32.942131)\n",
            "Accuracy at step 1 - batch 430: 0.8844\n",
            "training loss at step 1 - batch 431: 0.42 (2019-08-04 13:34:33.064873)\n",
            "Accuracy at step 1 - batch 431: 0.8824\n",
            "training loss at step 1 - batch 432: 0.42 (2019-08-04 13:34:33.077249)\n",
            "Accuracy at step 1 - batch 432: 0.8808\n",
            "training loss at step 1 - batch 433: 0.41 (2019-08-04 13:34:33.089014)\n",
            "Accuracy at step 1 - batch 433: 0.8804\n",
            "training loss at step 1 - batch 434: 0.40 (2019-08-04 13:34:33.102019)\n",
            "Accuracy at step 1 - batch 434: 0.8844\n",
            "training loss at step 1 - batch 435: 0.39 (2019-08-04 13:34:33.114712)\n",
            "Accuracy at step 1 - batch 435: 0.89\n",
            "training loss at step 1 - batch 436: 0.42 (2019-08-04 13:34:33.236015)\n",
            "Accuracy at step 1 - batch 436: 0.8796\n",
            "training loss at step 1 - batch 437: 0.42 (2019-08-04 13:34:33.253664)\n",
            "Accuracy at step 1 - batch 437: 0.8848\n",
            "training loss at step 1 - batch 438: 0.40 (2019-08-04 13:34:33.266001)\n",
            "Accuracy at step 1 - batch 438: 0.8832\n",
            "training loss at step 1 - batch 439: 0.43 (2019-08-04 13:34:33.281604)\n",
            "Accuracy at step 1 - batch 439: 0.8792\n",
            "training loss at step 1 - batch 440: 0.41 (2019-08-04 13:34:33.296957)\n",
            "Accuracy at step 1 - batch 440: 0.884\n",
            "training loss at step 1 - batch 441: 0.41 (2019-08-04 13:34:33.427155)\n",
            "Accuracy at step 1 - batch 441: 0.8844\n",
            "training loss at step 1 - batch 442: 0.39 (2019-08-04 13:34:33.444639)\n",
            "Accuracy at step 1 - batch 442: 0.8852\n",
            "training loss at step 1 - batch 443: 0.38 (2019-08-04 13:34:33.456772)\n",
            "Accuracy at step 1 - batch 443: 0.8892\n",
            "training loss at step 1 - batch 444: 0.41 (2019-08-04 13:34:33.469514)\n",
            "Accuracy at step 1 - batch 444: 0.8916\n",
            "training loss at step 1 - batch 445: 0.43 (2019-08-04 13:34:33.485537)\n",
            "Accuracy at step 1 - batch 445: 0.8788\n",
            "training loss at step 1 - batch 446: 0.42 (2019-08-04 13:34:33.605530)\n",
            "Accuracy at step 1 - batch 446: 0.88\n",
            "training loss at step 1 - batch 447: 0.41 (2019-08-04 13:34:33.622339)\n",
            "Accuracy at step 1 - batch 447: 0.8836\n",
            "training loss at step 1 - batch 448: 0.39 (2019-08-04 13:34:33.634557)\n",
            "Accuracy at step 1 - batch 448: 0.8872\n",
            "training loss at step 1 - batch 449: 0.40 (2019-08-04 13:34:33.648082)\n",
            "Accuracy at step 1 - batch 449: 0.8852\n",
            "training loss at step 1 - batch 450: 0.41 (2019-08-04 13:34:33.660586)\n",
            "Accuracy at step 1 - batch 450: 0.8896\n",
            "training loss at step 1 - batch 451: 0.40 (2019-08-04 13:34:33.783666)\n",
            "Accuracy at step 1 - batch 451: 0.8848\n",
            "training loss at step 1 - batch 452: 0.40 (2019-08-04 13:34:33.803935)\n",
            "Accuracy at step 1 - batch 452: 0.8884\n",
            "training loss at step 1 - batch 453: 0.40 (2019-08-04 13:34:33.815909)\n",
            "Accuracy at step 1 - batch 453: 0.8916\n",
            "training loss at step 1 - batch 454: 0.39 (2019-08-04 13:34:33.828950)\n",
            "Accuracy at step 1 - batch 454: 0.8916\n",
            "training loss at step 1 - batch 455: 0.38 (2019-08-04 13:34:33.842503)\n",
            "Accuracy at step 1 - batch 455: 0.8916\n",
            "training loss at step 1 - batch 456: 0.41 (2019-08-04 13:34:33.960029)\n",
            "Accuracy at step 1 - batch 456: 0.8848\n",
            "training loss at step 1 - batch 457: 0.43 (2019-08-04 13:34:33.977990)\n",
            "Accuracy at step 1 - batch 457: 0.8752\n",
            "training loss at step 1 - batch 458: 0.42 (2019-08-04 13:34:33.992707)\n",
            "Accuracy at step 1 - batch 458: 0.8844\n",
            "training loss at step 1 - batch 459: 0.40 (2019-08-04 13:34:34.004907)\n",
            "Accuracy at step 1 - batch 459: 0.8884\n",
            "training loss at step 1 - batch 460: 0.41 (2019-08-04 13:34:34.017106)\n",
            "Accuracy at step 1 - batch 460: 0.8824\n",
            "training loss at step 1 - batch 461: 0.43 (2019-08-04 13:34:34.134954)\n",
            "Accuracy at step 1 - batch 461: 0.8764\n",
            "training loss at step 1 - batch 462: 0.43 (2019-08-04 13:34:34.149356)\n",
            "Accuracy at step 1 - batch 462: 0.8804\n",
            "training loss at step 1 - batch 463: 0.42 (2019-08-04 13:34:34.162252)\n",
            "Accuracy at step 1 - batch 463: 0.8804\n",
            "training loss at step 1 - batch 464: 0.41 (2019-08-04 13:34:34.174597)\n",
            "Accuracy at step 1 - batch 464: 0.8884\n",
            "training loss at step 1 - batch 465: 0.42 (2019-08-04 13:34:34.188211)\n",
            "Accuracy at step 1 - batch 465: 0.8788\n",
            "training loss at step 1 - batch 466: 0.39 (2019-08-04 13:34:34.315598)\n",
            "Accuracy at step 1 - batch 466: 0.89\n",
            "training loss at step 1 - batch 467: 0.37 (2019-08-04 13:34:34.330740)\n",
            "Accuracy at step 1 - batch 467: 0.8908\n",
            "training loss at step 1 - batch 468: 0.43 (2019-08-04 13:34:34.343623)\n",
            "Accuracy at step 1 - batch 468: 0.8764\n",
            "training loss at step 1 - batch 469: 0.40 (2019-08-04 13:34:34.357949)\n",
            "Accuracy at step 1 - batch 469: 0.8896\n",
            "training loss at step 1 - batch 470: 0.42 (2019-08-04 13:34:34.373286)\n",
            "Accuracy at step 1 - batch 470: 0.8796\n",
            "training loss at step 1 - batch 471: 0.41 (2019-08-04 13:34:34.498018)\n",
            "Accuracy at step 1 - batch 471: 0.8836\n",
            "training loss at step 1 - batch 472: 0.40 (2019-08-04 13:34:34.516293)\n",
            "Accuracy at step 1 - batch 472: 0.8892\n",
            "training loss at step 1 - batch 473: 0.41 (2019-08-04 13:34:34.531364)\n",
            "Accuracy at step 1 - batch 473: 0.876\n",
            "training loss at step 1 - batch 474: 0.39 (2019-08-04 13:34:34.543638)\n",
            "Accuracy at step 1 - batch 474: 0.886\n",
            "training loss at step 1 - batch 475: 0.41 (2019-08-04 13:34:34.555869)\n",
            "Accuracy at step 1 - batch 475: 0.8892\n",
            "training loss at step 1 - batch 476: 0.42 (2019-08-04 13:34:34.675647)\n",
            "Accuracy at step 1 - batch 476: 0.8824\n",
            "training loss at step 1 - batch 477: 0.42 (2019-08-04 13:34:34.688168)\n",
            "Accuracy at step 1 - batch 477: 0.8776\n",
            "training loss at step 1 - batch 478: 0.38 (2019-08-04 13:34:34.703001)\n",
            "Accuracy at step 1 - batch 478: 0.8916\n",
            "training loss at step 1 - batch 479: 0.41 (2019-08-04 13:34:34.714765)\n",
            "Accuracy at step 1 - batch 479: 0.8868\n",
            "training loss at step 1 - batch 480: 0.39 (2019-08-04 13:34:34.728745)\n",
            "Accuracy at step 1 - batch 480: 0.8912\n",
            "training loss at step 1 - batch 481: 0.39 (2019-08-04 13:34:34.857848)\n",
            "Accuracy at step 1 - batch 481: 0.8832\n",
            "training loss at step 1 - batch 482: 0.43 (2019-08-04 13:34:34.872756)\n",
            "Accuracy at step 1 - batch 482: 0.882\n",
            "training loss at step 1 - batch 483: 0.39 (2019-08-04 13:34:34.886420)\n",
            "Accuracy at step 1 - batch 483: 0.89\n",
            "training loss at step 1 - batch 484: 0.42 (2019-08-04 13:34:34.899370)\n",
            "Accuracy at step 1 - batch 484: 0.8844\n",
            "training loss at step 1 - batch 485: 0.42 (2019-08-04 13:34:34.912444)\n",
            "Accuracy at step 1 - batch 485: 0.888\n",
            "training loss at step 1 - batch 486: 0.40 (2019-08-04 13:34:35.033219)\n",
            "Accuracy at step 1 - batch 486: 0.8852\n",
            "training loss at step 1 - batch 487: 0.39 (2019-08-04 13:34:35.047249)\n",
            "Accuracy at step 1 - batch 487: 0.8944\n",
            "training loss at step 1 - batch 488: 0.41 (2019-08-04 13:34:35.059563)\n",
            "Accuracy at step 1 - batch 488: 0.882\n",
            "training loss at step 1 - batch 489: 0.40 (2019-08-04 13:34:35.074886)\n",
            "Accuracy at step 1 - batch 489: 0.8892\n",
            "training loss at step 1 - batch 490: 0.41 (2019-08-04 13:34:35.087552)\n",
            "Accuracy at step 1 - batch 490: 0.88\n",
            "training loss at step 1 - batch 491: 0.41 (2019-08-04 13:34:35.228518)\n",
            "Accuracy at step 1 - batch 491: 0.8852\n",
            "training loss at step 1 - batch 492: 0.42 (2019-08-04 13:34:35.245445)\n",
            "Accuracy at step 1 - batch 492: 0.8812\n",
            "training loss at step 1 - batch 493: 0.40 (2019-08-04 13:34:35.257385)\n",
            "Accuracy at step 1 - batch 493: 0.8816\n",
            "training loss at step 1 - batch 494: 0.39 (2019-08-04 13:34:35.273295)\n",
            "Accuracy at step 1 - batch 494: 0.8892\n",
            "training loss at step 1 - batch 495: 0.41 (2019-08-04 13:34:35.286325)\n",
            "Accuracy at step 1 - batch 495: 0.8856\n",
            "training loss at step 1 - batch 496: 0.39 (2019-08-04 13:34:35.415470)\n",
            "Accuracy at step 1 - batch 496: 0.8852\n",
            "training loss at step 1 - batch 497: 0.41 (2019-08-04 13:34:35.429645)\n",
            "Accuracy at step 1 - batch 497: 0.884\n",
            "training loss at step 1 - batch 498: 0.41 (2019-08-04 13:34:35.442077)\n",
            "Accuracy at step 1 - batch 498: 0.8832\n",
            "training loss at step 1 - batch 499: 0.41 (2019-08-04 13:34:35.454862)\n",
            "Accuracy at step 1 - batch 499: 0.8852\n",
            "training loss at step 1 - batch 500: 0.41 (2019-08-04 13:34:35.466647)\n",
            "Accuracy at step 1 - batch 500: 0.8808\n",
            "training loss at step 1 - batch 501: 0.41 (2019-08-04 13:34:35.592483)\n",
            "Accuracy at step 1 - batch 501: 0.8816\n",
            "training loss at step 1 - batch 502: 0.41 (2019-08-04 13:34:35.608617)\n",
            "Accuracy at step 1 - batch 502: 0.8804\n",
            "training loss at step 1 - batch 503: 0.37 (2019-08-04 13:34:35.620618)\n",
            "Accuracy at step 1 - batch 503: 0.8884\n",
            "training loss at step 1 - batch 504: 0.39 (2019-08-04 13:34:35.633286)\n",
            "Accuracy at step 1 - batch 504: 0.8916\n",
            "training loss at step 1 - batch 505: 0.39 (2019-08-04 13:34:35.645577)\n",
            "Accuracy at step 1 - batch 505: 0.8888\n",
            "training loss at step 1 - batch 506: 0.41 (2019-08-04 13:34:35.766960)\n",
            "Accuracy at step 1 - batch 506: 0.8852\n",
            "training loss at step 1 - batch 507: 0.40 (2019-08-04 13:34:35.784427)\n",
            "Accuracy at step 1 - batch 507: 0.8856\n",
            "training loss at step 1 - batch 508: 0.41 (2019-08-04 13:34:35.799022)\n",
            "Accuracy at step 1 - batch 508: 0.8836\n",
            "training loss at step 1 - batch 509: 0.42 (2019-08-04 13:34:35.813593)\n",
            "Accuracy at step 1 - batch 509: 0.8848\n",
            "training loss at step 1 - batch 510: 0.43 (2019-08-04 13:34:35.826348)\n",
            "Accuracy at step 1 - batch 510: 0.874\n",
            "training loss at step 1 - batch 511: 0.41 (2019-08-04 13:34:35.946684)\n",
            "Accuracy at step 1 - batch 511: 0.8856\n",
            "training loss at step 1 - batch 512: 0.43 (2019-08-04 13:34:35.959391)\n",
            "Accuracy at step 1 - batch 512: 0.876\n",
            "training loss at step 1 - batch 513: 0.40 (2019-08-04 13:34:35.973284)\n",
            "Accuracy at step 1 - batch 513: 0.8804\n",
            "training loss at step 1 - batch 514: 0.39 (2019-08-04 13:34:35.986490)\n",
            "Accuracy at step 1 - batch 514: 0.8844\n",
            "training loss at step 1 - batch 515: 0.39 (2019-08-04 13:34:35.999157)\n",
            "Accuracy at step 1 - batch 515: 0.8848\n",
            "training loss at step 1 - batch 516: 0.40 (2019-08-04 13:34:36.121868)\n",
            "Accuracy at step 1 - batch 516: 0.8816\n",
            "training loss at step 1 - batch 517: 0.42 (2019-08-04 13:34:36.136370)\n",
            "Accuracy at step 1 - batch 517: 0.8804\n",
            "training loss at step 1 - batch 518: 0.40 (2019-08-04 13:34:36.149459)\n",
            "Accuracy at step 1 - batch 518: 0.8888\n",
            "training loss at step 1 - batch 519: 0.39 (2019-08-04 13:34:36.162904)\n",
            "Accuracy at step 1 - batch 519: 0.888\n",
            "training loss at step 1 - batch 520: 0.40 (2019-08-04 13:34:36.175324)\n",
            "Accuracy at step 1 - batch 520: 0.8864\n",
            "training loss at step 1 - batch 521: 0.40 (2019-08-04 13:34:36.296814)\n",
            "Accuracy at step 1 - batch 521: 0.8852\n",
            "training loss at step 1 - batch 522: 0.40 (2019-08-04 13:34:36.309278)\n",
            "Accuracy at step 1 - batch 522: 0.884\n",
            "training loss at step 1 - batch 523: 0.40 (2019-08-04 13:34:36.322401)\n",
            "Accuracy at step 1 - batch 523: 0.8848\n",
            "training loss at step 1 - batch 524: 0.39 (2019-08-04 13:34:36.341250)\n",
            "Accuracy at step 1 - batch 524: 0.8876\n",
            "training loss at step 1 - batch 525: 0.40 (2019-08-04 13:34:36.358929)\n",
            "Accuracy at step 1 - batch 525: 0.8864\n",
            "training loss at step 1 - batch 526: 0.40 (2019-08-04 13:34:36.488528)\n",
            "Accuracy at step 1 - batch 526: 0.8868\n",
            "training loss at step 1 - batch 527: 0.40 (2019-08-04 13:34:36.504421)\n",
            "Accuracy at step 1 - batch 527: 0.8844\n",
            "training loss at step 1 - batch 528: 0.36 (2019-08-04 13:34:36.518722)\n",
            "Accuracy at step 1 - batch 528: 0.8928\n",
            "training loss at step 1 - batch 529: 0.39 (2019-08-04 13:34:36.530876)\n",
            "Accuracy at step 1 - batch 529: 0.8852\n",
            "training loss at step 1 - batch 530: 0.42 (2019-08-04 13:34:36.543022)\n",
            "Accuracy at step 1 - batch 530: 0.8792\n",
            "training loss at step 1 - batch 531: 0.39 (2019-08-04 13:34:36.663759)\n",
            "Accuracy at step 1 - batch 531: 0.888\n",
            "training loss at step 1 - batch 532: 0.40 (2019-08-04 13:34:36.680437)\n",
            "Accuracy at step 1 - batch 532: 0.8832\n",
            "training loss at step 1 - batch 533: 0.40 (2019-08-04 13:34:36.692737)\n",
            "Accuracy at step 1 - batch 533: 0.8816\n",
            "training loss at step 1 - batch 534: 0.40 (2019-08-04 13:34:36.704939)\n",
            "Accuracy at step 1 - batch 534: 0.882\n",
            "training loss at step 1 - batch 535: 0.40 (2019-08-04 13:34:36.717324)\n",
            "Accuracy at step 1 - batch 535: 0.888\n",
            "training loss at step 1 - batch 536: 0.40 (2019-08-04 13:34:36.840040)\n",
            "Accuracy at step 1 - batch 536: 0.8876\n",
            "training loss at step 1 - batch 537: 0.37 (2019-08-04 13:34:36.853617)\n",
            "Accuracy at step 1 - batch 537: 0.8972\n",
            "training loss at step 1 - batch 538: 0.39 (2019-08-04 13:34:36.865954)\n",
            "Accuracy at step 1 - batch 538: 0.8864\n",
            "training loss at step 1 - batch 539: 0.41 (2019-08-04 13:34:36.879335)\n",
            "Accuracy at step 1 - batch 539: 0.8796\n",
            "training loss at step 1 - batch 540: 0.40 (2019-08-04 13:34:36.892494)\n",
            "Accuracy at step 1 - batch 540: 0.8864\n",
            "training loss at step 1 - batch 541: 0.40 (2019-08-04 13:34:37.013908)\n",
            "Accuracy at step 1 - batch 541: 0.8844\n",
            "training loss at step 1 - batch 542: 0.38 (2019-08-04 13:34:37.031984)\n",
            "Accuracy at step 1 - batch 542: 0.8924\n",
            "training loss at step 1 - batch 543: 0.39 (2019-08-04 13:34:37.047078)\n",
            "Accuracy at step 1 - batch 543: 0.8872\n",
            "training loss at step 1 - batch 544: 0.38 (2019-08-04 13:34:37.060470)\n",
            "Accuracy at step 1 - batch 544: 0.8888\n",
            "training loss at step 1 - batch 545: 0.38 (2019-08-04 13:34:37.072333)\n",
            "Accuracy at step 1 - batch 545: 0.8936\n",
            "training loss at step 1 - batch 546: 0.42 (2019-08-04 13:34:37.193201)\n",
            "Accuracy at step 1 - batch 546: 0.8788\n",
            "training loss at step 1 - batch 547: 0.38 (2019-08-04 13:34:37.209933)\n",
            "Accuracy at step 1 - batch 547: 0.8904\n",
            "training loss at step 1 - batch 548: 0.38 (2019-08-04 13:34:37.222042)\n",
            "Accuracy at step 1 - batch 548: 0.8896\n",
            "training loss at step 1 - batch 549: 0.42 (2019-08-04 13:34:37.235693)\n",
            "Accuracy at step 1 - batch 549: 0.8836\n",
            "training loss at step 1 - batch 550: 0.39 (2019-08-04 13:34:37.248180)\n",
            "Accuracy at step 1 - batch 550: 0.888\n",
            "training loss at step 1 - batch 551: 0.39 (2019-08-04 13:34:37.371168)\n",
            "Accuracy at step 1 - batch 551: 0.8864\n",
            "training loss at step 1 - batch 552: 0.42 (2019-08-04 13:34:37.389483)\n",
            "Accuracy at step 1 - batch 552: 0.8828\n",
            "training loss at step 1 - batch 553: 0.38 (2019-08-04 13:34:37.405779)\n",
            "Accuracy at step 1 - batch 553: 0.888\n",
            "training loss at step 1 - batch 554: 0.42 (2019-08-04 13:34:37.420530)\n",
            "Accuracy at step 1 - batch 554: 0.8772\n",
            "training loss at step 1 - batch 555: 0.37 (2019-08-04 13:34:37.433079)\n",
            "Accuracy at step 1 - batch 555: 0.8904\n",
            "training loss at step 1 - batch 556: 0.39 (2019-08-04 13:34:37.553940)\n",
            "Accuracy at step 1 - batch 556: 0.8896\n",
            "training loss at step 1 - batch 557: 0.39 (2019-08-04 13:34:37.570258)\n",
            "Accuracy at step 1 - batch 557: 0.888\n",
            "training loss at step 1 - batch 558: 0.38 (2019-08-04 13:34:37.585055)\n",
            "Accuracy at step 1 - batch 558: 0.8896\n",
            "training loss at step 1 - batch 559: 0.42 (2019-08-04 13:34:37.598212)\n",
            "Accuracy at step 1 - batch 559: 0.8772\n",
            "training loss at step 1 - batch 560: 0.38 (2019-08-04 13:34:37.610863)\n",
            "Accuracy at step 1 - batch 560: 0.8908\n",
            "training loss at step 1 - batch 561: 0.41 (2019-08-04 13:34:37.733729)\n",
            "Accuracy at step 1 - batch 561: 0.878\n",
            "training loss at step 1 - batch 562: 0.40 (2019-08-04 13:34:37.750398)\n",
            "Accuracy at step 1 - batch 562: 0.8916\n",
            "training loss at step 1 - batch 563: 0.41 (2019-08-04 13:34:37.762577)\n",
            "Accuracy at step 1 - batch 563: 0.8848\n",
            "training loss at step 1 - batch 564: 0.37 (2019-08-04 13:34:37.776606)\n",
            "Accuracy at step 1 - batch 564: 0.886\n",
            "training loss at step 1 - batch 565: 0.37 (2019-08-04 13:34:37.790344)\n",
            "Accuracy at step 1 - batch 565: 0.8964\n",
            "training loss at step 1 - batch 566: 0.37 (2019-08-04 13:34:37.911506)\n",
            "Accuracy at step 1 - batch 566: 0.8956\n",
            "training loss at step 1 - batch 567: 0.40 (2019-08-04 13:34:37.925512)\n",
            "Accuracy at step 1 - batch 567: 0.8876\n",
            "training loss at step 1 - batch 568: 0.37 (2019-08-04 13:34:37.937738)\n",
            "Accuracy at step 1 - batch 568: 0.8952\n",
            "training loss at step 1 - batch 569: 0.39 (2019-08-04 13:34:37.951287)\n",
            "Accuracy at step 1 - batch 569: 0.89\n",
            "training loss at step 1 - batch 570: 0.39 (2019-08-04 13:34:37.963614)\n",
            "Accuracy at step 1 - batch 570: 0.8872\n",
            "training loss at step 1 - batch 571: 0.38 (2019-08-04 13:34:38.088236)\n",
            "Accuracy at step 1 - batch 571: 0.892\n",
            "training loss at step 1 - batch 572: 0.40 (2019-08-04 13:34:38.103314)\n",
            "Accuracy at step 1 - batch 572: 0.8884\n",
            "training loss at step 1 - batch 573: 0.37 (2019-08-04 13:34:38.115434)\n",
            "Accuracy at step 1 - batch 573: 0.8912\n",
            "training loss at step 1 - batch 574: 0.40 (2019-08-04 13:34:38.128085)\n",
            "Accuracy at step 1 - batch 574: 0.882\n",
            "training loss at step 1 - batch 575: 0.40 (2019-08-04 13:34:38.139486)\n",
            "Accuracy at step 1 - batch 575: 0.8828\n",
            "training loss at step 1 - batch 576: 0.38 (2019-08-04 13:34:38.258473)\n",
            "Accuracy at step 1 - batch 576: 0.8948\n",
            "training loss at step 1 - batch 577: 0.41 (2019-08-04 13:34:38.275055)\n",
            "Accuracy at step 1 - batch 577: 0.888\n",
            "training loss at step 1 - batch 578: 0.40 (2019-08-04 13:34:38.287590)\n",
            "Accuracy at step 1 - batch 578: 0.8908\n",
            "training loss at step 1 - batch 579: 0.39 (2019-08-04 13:34:38.303776)\n",
            "Accuracy at step 1 - batch 579: 0.89\n",
            "training loss at step 1 - batch 580: 0.40 (2019-08-04 13:34:38.317149)\n",
            "Accuracy at step 1 - batch 580: 0.8816\n",
            "training loss at step 1 - batch 581: 0.39 (2019-08-04 13:34:38.444198)\n",
            "Accuracy at step 1 - batch 581: 0.8888\n",
            "training loss at step 1 - batch 582: 0.41 (2019-08-04 13:34:38.456694)\n",
            "Accuracy at step 1 - batch 582: 0.8812\n",
            "training loss at step 1 - batch 583: 0.40 (2019-08-04 13:34:38.469103)\n",
            "Accuracy at step 1 - batch 583: 0.89\n",
            "training loss at step 1 - batch 584: 0.42 (2019-08-04 13:34:38.482157)\n",
            "Accuracy at step 1 - batch 584: 0.8784\n",
            "training loss at step 1 - batch 585: 0.41 (2019-08-04 13:34:38.494877)\n",
            "Accuracy at step 1 - batch 585: 0.8892\n",
            "training loss at step 1 - batch 586: 0.41 (2019-08-04 13:34:38.618028)\n",
            "Accuracy at step 1 - batch 586: 0.89\n",
            "training loss at step 1 - batch 587: 0.40 (2019-08-04 13:34:38.630229)\n",
            "Accuracy at step 1 - batch 587: 0.8816\n",
            "training loss at step 1 - batch 588: 0.41 (2019-08-04 13:34:38.642613)\n",
            "Accuracy at step 1 - batch 588: 0.888\n",
            "training loss at step 1 - batch 589: 0.38 (2019-08-04 13:34:38.655949)\n",
            "Accuracy at step 1 - batch 589: 0.8876\n",
            "training loss at step 1 - batch 590: 0.38 (2019-08-04 13:34:38.668364)\n",
            "Accuracy at step 1 - batch 590: 0.8924\n",
            "training loss at step 1 - batch 591: 0.40 (2019-08-04 13:34:38.783909)\n",
            "Accuracy at step 1 - batch 591: 0.8836\n",
            "training loss at step 1 - batch 592: 0.39 (2019-08-04 13:34:38.797952)\n",
            "Accuracy at step 1 - batch 592: 0.884\n",
            "training loss at step 1 - batch 593: 0.39 (2019-08-04 13:34:38.812381)\n",
            "Accuracy at step 1 - batch 593: 0.886\n",
            "training loss at step 1 - batch 594: 0.38 (2019-08-04 13:34:38.827407)\n",
            "Accuracy at step 1 - batch 594: 0.8856\n",
            "training loss at step 1 - batch 595: 0.39 (2019-08-04 13:34:38.841034)\n",
            "Accuracy at step 1 - batch 595: 0.884\n",
            "training loss at step 1 - batch 596: 0.38 (2019-08-04 13:34:38.959955)\n",
            "Accuracy at step 1 - batch 596: 0.8944\n",
            "training loss at step 1 - batch 597: 0.43 (2019-08-04 13:34:38.973325)\n",
            "Accuracy at step 1 - batch 597: 0.8744\n",
            "training loss at step 1 - batch 598: 0.42 (2019-08-04 13:34:38.985863)\n",
            "Accuracy at step 1 - batch 598: 0.876\n",
            "training loss at step 1 - batch 599: 0.39 (2019-08-04 13:34:38.998896)\n",
            "Accuracy at step 1 - batch 599: 0.886\n",
            "training loss at step 1 - batch 600: 0.37 (2019-08-04 13:34:39.012202)\n",
            "Accuracy at step 1 - batch 600: 0.8948\n",
            "training loss at step 1 - batch 601: 0.40 (2019-08-04 13:34:39.136686)\n",
            "Accuracy at step 1 - batch 601: 0.8804\n",
            "training loss at step 1 - batch 602: 0.38 (2019-08-04 13:34:39.150744)\n",
            "Accuracy at step 1 - batch 602: 0.8884\n",
            "training loss at step 1 - batch 603: 0.41 (2019-08-04 13:34:39.163321)\n",
            "Accuracy at step 1 - batch 603: 0.8844\n",
            "training loss at step 1 - batch 604: 0.40 (2019-08-04 13:34:39.176245)\n",
            "Accuracy at step 1 - batch 604: 0.8868\n",
            "training loss at step 1 - batch 605: 0.40 (2019-08-04 13:34:39.188671)\n",
            "Accuracy at step 1 - batch 605: 0.8816\n",
            "training loss at step 1 - batch 606: 0.42 (2019-08-04 13:34:39.305462)\n",
            "Accuracy at step 1 - batch 606: 0.882\n",
            "training loss at step 1 - batch 607: 0.37 (2019-08-04 13:34:39.319100)\n",
            "Accuracy at step 1 - batch 607: 0.8936\n",
            "training loss at step 1 - batch 608: 0.40 (2019-08-04 13:34:39.333208)\n",
            "Accuracy at step 1 - batch 608: 0.8864\n",
            "training loss at step 1 - batch 609: 0.39 (2019-08-04 13:34:39.348722)\n",
            "Accuracy at step 1 - batch 609: 0.8872\n",
            "training loss at step 1 - batch 610: 0.38 (2019-08-04 13:34:39.362369)\n",
            "Accuracy at step 1 - batch 610: 0.8952\n",
            "training loss at step 1 - batch 611: 0.43 (2019-08-04 13:34:39.493226)\n",
            "Accuracy at step 1 - batch 611: 0.8756\n",
            "training loss at step 1 - batch 612: 0.38 (2019-08-04 13:34:39.506222)\n",
            "Accuracy at step 1 - batch 612: 0.8924\n",
            "training loss at step 1 - batch 613: 0.39 (2019-08-04 13:34:39.519488)\n",
            "Accuracy at step 1 - batch 613: 0.8844\n",
            "training loss at step 1 - batch 614: 0.37 (2019-08-04 13:34:39.531903)\n",
            "Accuracy at step 1 - batch 614: 0.89\n",
            "training loss at step 1 - batch 615: 0.41 (2019-08-04 13:34:39.544233)\n",
            "Accuracy at step 1 - batch 615: 0.8816\n",
            "training loss at step 1 - batch 616: 0.40 (2019-08-04 13:34:39.663323)\n",
            "Accuracy at step 1 - batch 616: 0.8856\n",
            "training loss at step 1 - batch 617: 0.40 (2019-08-04 13:34:39.677662)\n",
            "Accuracy at step 1 - batch 617: 0.8864\n",
            "training loss at step 1 - batch 618: 0.39 (2019-08-04 13:34:39.690098)\n",
            "Accuracy at step 1 - batch 618: 0.8908\n",
            "training loss at step 1 - batch 619: 0.40 (2019-08-04 13:34:39.703490)\n",
            "Accuracy at step 1 - batch 619: 0.8904\n",
            "training loss at step 1 - batch 620: 0.39 (2019-08-04 13:34:39.715956)\n",
            "Accuracy at step 1 - batch 620: 0.8876\n",
            "training loss at step 1 - batch 621: 0.38 (2019-08-04 13:34:39.831687)\n",
            "Accuracy at step 1 - batch 621: 0.8832\n",
            "training loss at step 1 - batch 622: 0.40 (2019-08-04 13:34:39.845935)\n",
            "Accuracy at step 1 - batch 622: 0.8828\n",
            "training loss at step 1 - batch 623: 0.40 (2019-08-04 13:34:39.858193)\n",
            "Accuracy at step 1 - batch 623: 0.8856\n",
            "training loss at step 1 - batch 624: 0.38 (2019-08-04 13:34:39.875743)\n",
            "Accuracy at step 1 - batch 624: 0.896\n",
            "training loss at step 1 - batch 625: 0.38 (2019-08-04 13:34:39.888874)\n",
            "Accuracy at step 1 - batch 625: 0.888\n",
            "training loss at step 1 - batch 626: 0.40 (2019-08-04 13:34:40.012066)\n",
            "Accuracy at step 1 - batch 626: 0.8848\n",
            "training loss at step 1 - batch 627: 0.39 (2019-08-04 13:34:40.024825)\n",
            "Accuracy at step 1 - batch 627: 0.8936\n",
            "training loss at step 1 - batch 628: 0.39 (2019-08-04 13:34:40.038219)\n",
            "Accuracy at step 1 - batch 628: 0.8888\n",
            "training loss at step 1 - batch 629: 0.39 (2019-08-04 13:34:40.050377)\n",
            "Accuracy at step 1 - batch 629: 0.89\n",
            "training loss at step 1 - batch 630: 0.41 (2019-08-04 13:34:40.064171)\n",
            "Accuracy at step 1 - batch 630: 0.8844\n",
            "training loss at step 1 - batch 631: 0.41 (2019-08-04 13:34:40.197118)\n",
            "Accuracy at step 1 - batch 631: 0.8876\n",
            "training loss at step 1 - batch 632: 0.42 (2019-08-04 13:34:40.212453)\n",
            "Accuracy at step 1 - batch 632: 0.8816\n",
            "training loss at step 1 - batch 633: 0.38 (2019-08-04 13:34:40.225576)\n",
            "Accuracy at step 1 - batch 633: 0.8984\n",
            "training loss at step 1 - batch 634: 0.38 (2019-08-04 13:34:40.239137)\n",
            "Accuracy at step 1 - batch 634: 0.8908\n",
            "training loss at step 1 - batch 635: 0.39 (2019-08-04 13:34:40.254929)\n",
            "Accuracy at step 1 - batch 635: 0.8928\n",
            "training loss at step 1 - batch 636: 0.40 (2019-08-04 13:34:40.385244)\n",
            "Accuracy at step 1 - batch 636: 0.882\n",
            "training loss at step 1 - batch 637: 0.38 (2019-08-04 13:34:40.402960)\n",
            "Accuracy at step 1 - batch 637: 0.8912\n",
            "training loss at step 1 - batch 638: 0.41 (2019-08-04 13:34:40.416137)\n",
            "Accuracy at step 1 - batch 638: 0.8876\n",
            "training loss at step 1 - batch 639: 0.36 (2019-08-04 13:34:40.431586)\n",
            "Accuracy at step 1 - batch 639: 0.8984\n",
            "training loss at step 1 - batch 640: 0.40 (2019-08-04 13:34:40.444636)\n",
            "Accuracy at step 1 - batch 640: 0.8912\n",
            "training loss at step 1 - batch 641: 0.39 (2019-08-04 13:34:40.575398)\n",
            "Accuracy at step 1 - batch 641: 0.8804\n",
            "training loss at step 1 - batch 642: 0.41 (2019-08-04 13:34:40.592697)\n",
            "Accuracy at step 1 - batch 642: 0.88\n",
            "training loss at step 1 - batch 643: 0.39 (2019-08-04 13:34:40.607311)\n",
            "Accuracy at step 1 - batch 643: 0.8876\n",
            "training loss at step 1 - batch 644: 0.40 (2019-08-04 13:34:40.623419)\n",
            "Accuracy at step 1 - batch 644: 0.8876\n",
            "training loss at step 1 - batch 645: 0.39 (2019-08-04 13:34:40.635888)\n",
            "Accuracy at step 1 - batch 645: 0.8868\n",
            "training loss at step 1 - batch 646: 0.40 (2019-08-04 13:34:40.755365)\n",
            "Accuracy at step 1 - batch 646: 0.8864\n",
            "training loss at step 1 - batch 647: 0.41 (2019-08-04 13:34:40.768901)\n",
            "Accuracy at step 1 - batch 647: 0.8828\n",
            "training loss at step 1 - batch 648: 0.37 (2019-08-04 13:34:40.781775)\n",
            "Accuracy at step 1 - batch 648: 0.8928\n",
            "training loss at step 1 - batch 649: 0.40 (2019-08-04 13:34:40.793690)\n",
            "Accuracy at step 1 - batch 649: 0.8852\n",
            "training loss at step 1 - batch 650: 0.39 (2019-08-04 13:34:40.807126)\n",
            "Accuracy at step 1 - batch 650: 0.8908\n",
            "training loss at step 1 - batch 651: 0.39 (2019-08-04 13:34:40.937280)\n",
            "Accuracy at step 1 - batch 651: 0.8856\n",
            "training loss at step 1 - batch 652: 0.40 (2019-08-04 13:34:40.953203)\n",
            "Accuracy at step 1 - batch 652: 0.8812\n",
            "training loss at step 1 - batch 653: 0.40 (2019-08-04 13:34:40.966504)\n",
            "Accuracy at step 1 - batch 653: 0.884\n",
            "training loss at step 1 - batch 654: 0.40 (2019-08-04 13:34:40.979108)\n",
            "Accuracy at step 1 - batch 654: 0.8856\n",
            "training loss at step 1 - batch 655: 0.39 (2019-08-04 13:34:40.993601)\n",
            "Accuracy at step 1 - batch 655: 0.89\n",
            "training loss at step 1 - batch 656: 0.38 (2019-08-04 13:34:41.114367)\n",
            "Accuracy at step 1 - batch 656: 0.8884\n",
            "training loss at step 1 - batch 657: 0.38 (2019-08-04 13:34:41.126837)\n",
            "Accuracy at step 1 - batch 657: 0.8936\n",
            "training loss at step 1 - batch 658: 0.38 (2019-08-04 13:34:41.141193)\n",
            "Accuracy at step 1 - batch 658: 0.8888\n",
            "training loss at step 1 - batch 659: 0.40 (2019-08-04 13:34:41.155551)\n",
            "Accuracy at step 1 - batch 659: 0.8852\n",
            "training loss at step 1 - batch 660: 0.37 (2019-08-04 13:34:41.169686)\n",
            "Accuracy at step 1 - batch 660: 0.8896\n",
            "training loss at step 1 - batch 661: 0.39 (2019-08-04 13:34:41.293501)\n",
            "Accuracy at step 1 - batch 661: 0.8892\n",
            "training loss at step 1 - batch 662: 0.40 (2019-08-04 13:34:41.307877)\n",
            "Accuracy at step 1 - batch 662: 0.8844\n",
            "training loss at step 1 - batch 663: 0.36 (2019-08-04 13:34:41.320449)\n",
            "Accuracy at step 1 - batch 663: 0.8964\n",
            "training loss at step 1 - batch 664: 0.39 (2019-08-04 13:34:41.332313)\n",
            "Accuracy at step 1 - batch 664: 0.8812\n",
            "training loss at step 1 - batch 665: 0.42 (2019-08-04 13:34:41.345566)\n",
            "Accuracy at step 1 - batch 665: 0.8784\n",
            "training loss at step 1 - batch 666: 0.38 (2019-08-04 13:34:41.473966)\n",
            "Accuracy at step 1 - batch 666: 0.8896\n",
            "training loss at step 1 - batch 667: 0.38 (2019-08-04 13:34:41.488940)\n",
            "Accuracy at step 1 - batch 667: 0.8924\n",
            "training loss at step 1 - batch 668: 0.38 (2019-08-04 13:34:41.501492)\n",
            "Accuracy at step 1 - batch 668: 0.8924\n",
            "training loss at step 1 - batch 669: 0.41 (2019-08-04 13:34:41.513456)\n",
            "Accuracy at step 1 - batch 669: 0.884\n",
            "training loss at step 1 - batch 670: 0.40 (2019-08-04 13:34:41.528022)\n",
            "Accuracy at step 1 - batch 670: 0.8832\n",
            "training loss at step 1 - batch 671: 0.38 (2019-08-04 13:34:41.782599)\n",
            "Accuracy at step 1 - batch 671: 0.892\n",
            "training loss at step 1 - batch 672: 0.39 (2019-08-04 13:34:41.798780)\n",
            "Accuracy at step 1 - batch 672: 0.892\n",
            "training loss at step 1 - batch 673: 0.39 (2019-08-04 13:34:41.812358)\n",
            "Accuracy at step 1 - batch 673: 0.8816\n",
            "training loss at step 1 - batch 674: 0.38 (2019-08-04 13:34:41.826170)\n",
            "Accuracy at step 1 - batch 674: 0.8932\n",
            "training loss at step 1 - batch 675: 0.39 (2019-08-04 13:34:41.840360)\n",
            "Accuracy at step 1 - batch 675: 0.8892\n",
            "training loss at step 1 - batch 676: 0.39 (2019-08-04 13:34:41.968054)\n",
            "Accuracy at step 1 - batch 676: 0.8948\n",
            "training loss at step 1 - batch 677: 0.40 (2019-08-04 13:34:41.984411)\n",
            "Accuracy at step 1 - batch 677: 0.8868\n",
            "training loss at step 1 - batch 678: 0.40 (2019-08-04 13:34:41.999411)\n",
            "Accuracy at step 1 - batch 678: 0.8872\n",
            "training loss at step 1 - batch 679: 0.40 (2019-08-04 13:34:42.012082)\n",
            "Accuracy at step 1 - batch 679: 0.8896\n",
            "training loss at step 1 - batch 680: 0.39 (2019-08-04 13:34:42.024572)\n",
            "Accuracy at step 1 - batch 680: 0.8856\n",
            "training loss at step 1 - batch 681: 0.38 (2019-08-04 13:34:42.147967)\n",
            "Accuracy at step 1 - batch 681: 0.892\n",
            "training loss at step 1 - batch 682: 0.40 (2019-08-04 13:34:42.165559)\n",
            "Accuracy at step 1 - batch 682: 0.8912\n",
            "training loss at step 1 - batch 683: 0.39 (2019-08-04 13:34:42.177403)\n",
            "Accuracy at step 1 - batch 683: 0.8892\n",
            "training loss at step 1 - batch 684: 0.41 (2019-08-04 13:34:42.192534)\n",
            "Accuracy at step 1 - batch 684: 0.8836\n",
            "training loss at step 1 - batch 685: 0.41 (2019-08-04 13:34:42.206520)\n",
            "Accuracy at step 1 - batch 685: 0.8836\n",
            "training loss at step 1 - batch 686: 0.40 (2019-08-04 13:34:42.325515)\n",
            "Accuracy at step 1 - batch 686: 0.888\n",
            "training loss at step 1 - batch 687: 0.37 (2019-08-04 13:34:42.341189)\n",
            "Accuracy at step 1 - batch 687: 0.896\n",
            "training loss at step 1 - batch 688: 0.40 (2019-08-04 13:34:42.354362)\n",
            "Accuracy at step 1 - batch 688: 0.892\n",
            "training loss at step 1 - batch 689: 0.38 (2019-08-04 13:34:42.367079)\n",
            "Accuracy at step 1 - batch 689: 0.8868\n",
            "training loss at step 1 - batch 690: 0.38 (2019-08-04 13:34:42.379932)\n",
            "Accuracy at step 1 - batch 690: 0.8936\n",
            "training loss at step 1 - batch 691: 0.38 (2019-08-04 13:34:42.508719)\n",
            "Accuracy at step 1 - batch 691: 0.8904\n",
            "training loss at step 1 - batch 692: 0.38 (2019-08-04 13:34:42.521515)\n",
            "Accuracy at step 1 - batch 692: 0.8892\n",
            "training loss at step 1 - batch 693: 0.38 (2019-08-04 13:34:42.534443)\n",
            "Accuracy at step 1 - batch 693: 0.89\n",
            "training loss at step 1 - batch 694: 0.40 (2019-08-04 13:34:42.547937)\n",
            "Accuracy at step 1 - batch 694: 0.89\n",
            "training loss at step 1 - batch 695: 0.38 (2019-08-04 13:34:42.561740)\n",
            "Accuracy at step 1 - batch 695: 0.8904\n",
            "training loss at step 1 - batch 696: 0.39 (2019-08-04 13:34:42.685871)\n",
            "Accuracy at step 1 - batch 696: 0.89\n",
            "training loss at step 1 - batch 697: 0.37 (2019-08-04 13:34:42.698306)\n",
            "Accuracy at step 1 - batch 697: 0.8896\n",
            "training loss at step 1 - batch 698: 0.38 (2019-08-04 13:34:42.713107)\n",
            "Accuracy at step 1 - batch 698: 0.8896\n",
            "training loss at step 1 - batch 699: 0.39 (2019-08-04 13:34:42.730760)\n",
            "Accuracy at step 1 - batch 699: 0.8828\n",
            "training loss at step 1 - batch 700: 0.37 (2019-08-04 13:34:42.745022)\n",
            "Accuracy at step 1 - batch 700: 0.892\n",
            "training loss at step 1 - batch 701: 0.38 (2019-08-04 13:34:42.865988)\n",
            "Accuracy at step 1 - batch 701: 0.8904\n",
            "training loss at step 1 - batch 702: 0.40 (2019-08-04 13:34:42.888015)\n",
            "Accuracy at step 1 - batch 702: 0.8868\n",
            "training loss at step 1 - batch 703: 0.38 (2019-08-04 13:34:42.910012)\n",
            "Accuracy at step 1 - batch 703: 0.8888\n",
            "training loss at step 1 - batch 704: 0.39 (2019-08-04 13:34:42.931986)\n",
            "Accuracy at step 1 - batch 704: 0.8812\n",
            "training loss at step 1 - batch 705: 0.37 (2019-08-04 13:34:42.944858)\n",
            "Accuracy at step 1 - batch 705: 0.8928\n",
            "training loss at step 1 - batch 706: 0.38 (2019-08-04 13:34:43.066208)\n",
            "Accuracy at step 1 - batch 706: 0.8884\n",
            "training loss at step 1 - batch 707: 0.39 (2019-08-04 13:34:43.080116)\n",
            "Accuracy at step 1 - batch 707: 0.886\n",
            "training loss at step 1 - batch 708: 0.38 (2019-08-04 13:34:43.092974)\n",
            "Accuracy at step 1 - batch 708: 0.8924\n",
            "training loss at step 1 - batch 709: 0.40 (2019-08-04 13:34:43.106425)\n",
            "Accuracy at step 1 - batch 709: 0.888\n",
            "training loss at step 1 - batch 710: 0.40 (2019-08-04 13:34:43.118829)\n",
            "Accuracy at step 1 - batch 710: 0.8852\n",
            "training loss at step 1 - batch 711: 0.40 (2019-08-04 13:34:43.239938)\n",
            "Accuracy at step 1 - batch 711: 0.8864\n",
            "training loss at step 1 - batch 712: 0.38 (2019-08-04 13:34:43.258334)\n",
            "Accuracy at step 1 - batch 712: 0.8936\n",
            "training loss at step 1 - batch 713: 0.38 (2019-08-04 13:34:43.270530)\n",
            "Accuracy at step 1 - batch 713: 0.892\n",
            "training loss at step 1 - batch 714: 0.38 (2019-08-04 13:34:43.283212)\n",
            "Accuracy at step 1 - batch 714: 0.8904\n",
            "training loss at step 1 - batch 715: 0.40 (2019-08-04 13:34:43.295974)\n",
            "Accuracy at step 1 - batch 715: 0.884\n",
            "training loss at step 1 - batch 716: 0.39 (2019-08-04 13:34:43.414786)\n",
            "Accuracy at step 1 - batch 716: 0.8856\n",
            "training loss at step 1 - batch 717: 0.37 (2019-08-04 13:34:43.429370)\n",
            "Accuracy at step 1 - batch 717: 0.8944\n",
            "training loss at step 1 - batch 718: 0.38 (2019-08-04 13:34:43.444918)\n",
            "Accuracy at step 1 - batch 718: 0.8868\n",
            "training loss at step 1 - batch 719: 0.36 (2019-08-04 13:34:43.458364)\n",
            "Accuracy at step 1 - batch 719: 0.8952\n",
            "training loss at step 1 - batch 720: 0.38 (2019-08-04 13:34:43.471065)\n",
            "Accuracy at step 1 - batch 720: 0.8892\n",
            "training loss at step 1 - batch 721: 0.37 (2019-08-04 13:34:43.602255)\n",
            "Accuracy at step 1 - batch 721: 0.8896\n",
            "training loss at step 1 - batch 722: 0.39 (2019-08-04 13:34:43.614895)\n",
            "Accuracy at step 1 - batch 722: 0.8896\n",
            "training loss at step 1 - batch 723: 0.37 (2019-08-04 13:34:43.628840)\n",
            "Accuracy at step 1 - batch 723: 0.8924\n",
            "training loss at step 1 - batch 724: 0.39 (2019-08-04 13:34:43.640777)\n",
            "Accuracy at step 1 - batch 724: 0.884\n",
            "training loss at step 1 - batch 725: 0.39 (2019-08-04 13:34:43.655421)\n",
            "Accuracy at step 1 - batch 725: 0.886\n",
            "training loss at step 1 - batch 726: 0.37 (2019-08-04 13:34:43.776081)\n",
            "Accuracy at step 1 - batch 726: 0.8876\n",
            "training loss at step 1 - batch 727: 0.40 (2019-08-04 13:34:43.789862)\n",
            "Accuracy at step 1 - batch 727: 0.8804\n",
            "training loss at step 1 - batch 728: 0.39 (2019-08-04 13:34:43.802529)\n",
            "Accuracy at step 1 - batch 728: 0.8864\n",
            "training loss at step 1 - batch 729: 0.38 (2019-08-04 13:34:43.814678)\n",
            "Accuracy at step 1 - batch 729: 0.8904\n",
            "training loss at step 1 - batch 730: 0.40 (2019-08-04 13:34:43.828264)\n",
            "Accuracy at step 1 - batch 730: 0.8828\n",
            "training loss at step 1 - batch 731: 0.41 (2019-08-04 13:34:43.953325)\n",
            "Accuracy at step 1 - batch 731: 0.8772\n",
            "training loss at step 1 - batch 732: 0.39 (2019-08-04 13:34:43.966459)\n",
            "Accuracy at step 1 - batch 732: 0.8932\n",
            "training loss at step 1 - batch 733: 0.37 (2019-08-04 13:34:43.978853)\n",
            "Accuracy at step 1 - batch 733: 0.8916\n",
            "training loss at step 1 - batch 734: 0.39 (2019-08-04 13:34:43.991985)\n",
            "Accuracy at step 1 - batch 734: 0.8892\n",
            "training loss at step 1 - batch 735: 0.38 (2019-08-04 13:34:44.004382)\n",
            "Accuracy at step 1 - batch 735: 0.8936\n",
            "training loss at step 1 - batch 736: 0.38 (2019-08-04 13:34:44.131234)\n",
            "Accuracy at step 1 - batch 736: 0.8872\n",
            "training loss at step 1 - batch 737: 0.39 (2019-08-04 13:34:44.143403)\n",
            "Accuracy at step 1 - batch 737: 0.888\n",
            "training loss at step 1 - batch 738: 0.39 (2019-08-04 13:34:44.158284)\n",
            "Accuracy at step 1 - batch 738: 0.886\n",
            "training loss at step 1 - batch 739: 0.40 (2019-08-04 13:34:44.173970)\n",
            "Accuracy at step 1 - batch 739: 0.8832\n",
            "training loss at step 1 - batch 740: 0.35 (2019-08-04 13:34:44.187201)\n",
            "Accuracy at step 1 - batch 740: 0.8968\n",
            "training loss at step 1 - batch 741: 0.37 (2019-08-04 13:34:44.305327)\n",
            "Accuracy at step 1 - batch 741: 0.8892\n",
            "training loss at step 1 - batch 742: 0.37 (2019-08-04 13:34:44.319383)\n",
            "Accuracy at step 1 - batch 742: 0.8936\n",
            "training loss at step 1 - batch 743: 0.39 (2019-08-04 13:34:44.332286)\n",
            "Accuracy at step 1 - batch 743: 0.8844\n",
            "training loss at step 1 - batch 744: 0.38 (2019-08-04 13:34:44.345644)\n",
            "Accuracy at step 1 - batch 744: 0.892\n",
            "training loss at step 1 - batch 745: 0.37 (2019-08-04 13:34:44.357891)\n",
            "Accuracy at step 1 - batch 745: 0.89\n",
            "training loss at step 1 - batch 746: 0.37 (2019-08-04 13:34:44.486197)\n",
            "Accuracy at step 1 - batch 746: 0.8876\n",
            "training loss at step 1 - batch 747: 0.38 (2019-08-04 13:34:44.504354)\n",
            "Accuracy at step 1 - batch 747: 0.89\n",
            "training loss at step 1 - batch 748: 0.40 (2019-08-04 13:34:44.516515)\n",
            "Accuracy at step 1 - batch 748: 0.89\n",
            "training loss at step 1 - batch 749: 0.38 (2019-08-04 13:34:44.529258)\n",
            "Accuracy at step 1 - batch 749: 0.894\n",
            "training loss at step 1 - batch 750: 0.37 (2019-08-04 13:34:44.547726)\n",
            "Accuracy at step 1 - batch 750: 0.8952\n",
            "training loss at step 1 - batch 751: 0.38 (2019-08-04 13:34:44.677605)\n",
            "Accuracy at step 1 - batch 751: 0.8836\n",
            "training loss at step 1 - batch 752: 0.38 (2019-08-04 13:34:44.691797)\n",
            "Accuracy at step 1 - batch 752: 0.894\n",
            "training loss at step 1 - batch 753: 0.38 (2019-08-04 13:34:44.707157)\n",
            "Accuracy at step 1 - batch 753: 0.8932\n",
            "training loss at step 1 - batch 754: 0.38 (2019-08-04 13:34:44.721917)\n",
            "Accuracy at step 1 - batch 754: 0.8924\n",
            "training loss at step 1 - batch 755: 0.39 (2019-08-04 13:34:44.733693)\n",
            "Accuracy at step 1 - batch 755: 0.8908\n",
            "training loss at step 1 - batch 756: 0.39 (2019-08-04 13:34:44.853475)\n",
            "Accuracy at step 1 - batch 756: 0.8848\n",
            "training loss at step 1 - batch 757: 0.39 (2019-08-04 13:34:44.866893)\n",
            "Accuracy at step 1 - batch 757: 0.888\n",
            "training loss at step 1 - batch 758: 0.40 (2019-08-04 13:34:44.879103)\n",
            "Accuracy at step 1 - batch 758: 0.8852\n",
            "training loss at step 1 - batch 759: 0.40 (2019-08-04 13:34:44.892200)\n",
            "Accuracy at step 1 - batch 759: 0.8896\n",
            "training loss at step 1 - batch 760: 0.39 (2019-08-04 13:34:44.909203)\n",
            "Accuracy at step 1 - batch 760: 0.8852\n",
            "training loss at step 1 - batch 761: 0.34 (2019-08-04 13:34:45.029926)\n",
            "Accuracy at step 1 - batch 761: 0.9016\n",
            "training loss at step 1 - batch 762: 0.37 (2019-08-04 13:34:45.047386)\n",
            "Accuracy at step 1 - batch 762: 0.8928\n",
            "training loss at step 1 - batch 763: 0.41 (2019-08-04 13:34:45.059384)\n",
            "Accuracy at step 1 - batch 763: 0.8836\n",
            "training loss at step 1 - batch 764: 0.39 (2019-08-04 13:34:45.072335)\n",
            "Accuracy at step 1 - batch 764: 0.8888\n",
            "training loss at step 1 - batch 765: 0.37 (2019-08-04 13:34:45.085451)\n",
            "Accuracy at step 1 - batch 765: 0.8964\n",
            "training loss at step 1 - batch 766: 0.37 (2019-08-04 13:34:45.214900)\n",
            "Accuracy at step 1 - batch 766: 0.8868\n",
            "training loss at step 1 - batch 767: 0.38 (2019-08-04 13:34:45.229005)\n",
            "Accuracy at step 1 - batch 767: 0.8964\n",
            "training loss at step 1 - batch 768: 0.38 (2019-08-04 13:34:45.243036)\n",
            "Accuracy at step 1 - batch 768: 0.8896\n",
            "training loss at step 1 - batch 769: 0.39 (2019-08-04 13:34:45.256753)\n",
            "Accuracy at step 1 - batch 769: 0.886\n",
            "training loss at step 1 - batch 770: 0.40 (2019-08-04 13:34:45.270430)\n",
            "Accuracy at step 1 - batch 770: 0.8848\n",
            "training loss at step 1 - batch 771: 0.37 (2019-08-04 13:34:45.395423)\n",
            "Accuracy at step 1 - batch 771: 0.8944\n",
            "training loss at step 1 - batch 772: 0.36 (2019-08-04 13:34:45.413460)\n",
            "Accuracy at step 1 - batch 772: 0.898\n",
            "training loss at step 1 - batch 773: 0.38 (2019-08-04 13:34:45.429288)\n",
            "Accuracy at step 1 - batch 773: 0.8924\n",
            "training loss at step 1 - batch 774: 0.39 (2019-08-04 13:34:45.442613)\n",
            "Accuracy at step 1 - batch 774: 0.8884\n",
            "training loss at step 1 - batch 775: 0.38 (2019-08-04 13:34:45.455104)\n",
            "Accuracy at step 1 - batch 775: 0.8864\n",
            "training loss at step 1 - batch 776: 0.38 (2019-08-04 13:34:45.583012)\n",
            "Accuracy at step 1 - batch 776: 0.8848\n",
            "training loss at step 1 - batch 777: 0.36 (2019-08-04 13:34:45.595948)\n",
            "Accuracy at step 1 - batch 777: 0.8948\n",
            "training loss at step 1 - batch 778: 0.38 (2019-08-04 13:34:45.609930)\n",
            "Accuracy at step 1 - batch 778: 0.8892\n",
            "training loss at step 1 - batch 779: 0.38 (2019-08-04 13:34:45.624156)\n",
            "Accuracy at step 1 - batch 779: 0.8888\n",
            "training loss at step 2 - batch 0: 0.37 (2019-08-04 13:34:45.640126)\n",
            "Accuracy at step 2 - batch 0: 0.8944\n",
            "training loss at step 2 - batch 1: 0.36 (2019-08-04 13:34:45.763897)\n",
            "Accuracy at step 2 - batch 1: 0.8924\n",
            "training loss at step 2 - batch 2: 0.36 (2019-08-04 13:34:45.777759)\n",
            "Accuracy at step 2 - batch 2: 0.8932\n",
            "training loss at step 2 - batch 3: 0.37 (2019-08-04 13:34:45.790414)\n",
            "Accuracy at step 2 - batch 3: 0.8964\n",
            "training loss at step 2 - batch 4: 0.38 (2019-08-04 13:34:45.802725)\n",
            "Accuracy at step 2 - batch 4: 0.8936\n",
            "training loss at step 2 - batch 5: 0.38 (2019-08-04 13:34:45.815323)\n",
            "Accuracy at step 2 - batch 5: 0.8916\n",
            "training loss at step 2 - batch 6: 0.41 (2019-08-04 13:34:45.939409)\n",
            "Accuracy at step 2 - batch 6: 0.8896\n",
            "training loss at step 2 - batch 7: 0.37 (2019-08-04 13:34:45.954592)\n",
            "Accuracy at step 2 - batch 7: 0.8872\n",
            "training loss at step 2 - batch 8: 0.39 (2019-08-04 13:34:45.966918)\n",
            "Accuracy at step 2 - batch 8: 0.886\n",
            "training loss at step 2 - batch 9: 0.37 (2019-08-04 13:34:45.980213)\n",
            "Accuracy at step 2 - batch 9: 0.8948\n",
            "training loss at step 2 - batch 10: 0.40 (2019-08-04 13:34:45.992993)\n",
            "Accuracy at step 2 - batch 10: 0.8844\n",
            "training loss at step 2 - batch 11: 0.40 (2019-08-04 13:34:46.113198)\n",
            "Accuracy at step 2 - batch 11: 0.8852\n",
            "training loss at step 2 - batch 12: 0.36 (2019-08-04 13:34:46.127170)\n",
            "Accuracy at step 2 - batch 12: 0.8984\n",
            "training loss at step 2 - batch 13: 0.38 (2019-08-04 13:34:46.143342)\n",
            "Accuracy at step 2 - batch 13: 0.8932\n",
            "training loss at step 2 - batch 14: 0.38 (2019-08-04 13:34:46.158538)\n",
            "Accuracy at step 2 - batch 14: 0.8916\n",
            "training loss at step 2 - batch 15: 0.38 (2019-08-04 13:34:46.174405)\n",
            "Accuracy at step 2 - batch 15: 0.8912\n",
            "training loss at step 2 - batch 16: 0.37 (2019-08-04 13:34:46.302154)\n",
            "Accuracy at step 2 - batch 16: 0.8988\n",
            "training loss at step 2 - batch 17: 0.36 (2019-08-04 13:34:46.319224)\n",
            "Accuracy at step 2 - batch 17: 0.8992\n",
            "training loss at step 2 - batch 18: 0.38 (2019-08-04 13:34:46.331672)\n",
            "Accuracy at step 2 - batch 18: 0.8964\n",
            "training loss at step 2 - batch 19: 0.36 (2019-08-04 13:34:46.344606)\n",
            "Accuracy at step 2 - batch 19: 0.8944\n",
            "training loss at step 2 - batch 20: 0.40 (2019-08-04 13:34:46.359798)\n",
            "Accuracy at step 2 - batch 20: 0.8852\n",
            "training loss at step 2 - batch 21: 0.37 (2019-08-04 13:34:46.478299)\n",
            "Accuracy at step 2 - batch 21: 0.8972\n",
            "training loss at step 2 - batch 22: 0.36 (2019-08-04 13:34:46.492863)\n",
            "Accuracy at step 2 - batch 22: 0.89\n",
            "training loss at step 2 - batch 23: 0.39 (2019-08-04 13:34:46.505391)\n",
            "Accuracy at step 2 - batch 23: 0.8884\n",
            "training loss at step 2 - batch 24: 0.37 (2019-08-04 13:34:46.518434)\n",
            "Accuracy at step 2 - batch 24: 0.8972\n",
            "training loss at step 2 - batch 25: 0.37 (2019-08-04 13:34:46.531565)\n",
            "Accuracy at step 2 - batch 25: 0.8896\n",
            "training loss at step 2 - batch 26: 0.36 (2019-08-04 13:34:46.665861)\n",
            "Accuracy at step 2 - batch 26: 0.8948\n",
            "training loss at step 2 - batch 27: 0.37 (2019-08-04 13:34:46.678498)\n",
            "Accuracy at step 2 - batch 27: 0.8972\n",
            "training loss at step 2 - batch 28: 0.40 (2019-08-04 13:34:46.691224)\n",
            "Accuracy at step 2 - batch 28: 0.8884\n",
            "training loss at step 2 - batch 29: 0.37 (2019-08-04 13:34:46.703606)\n",
            "Accuracy at step 2 - batch 29: 0.8936\n",
            "training loss at step 2 - batch 30: 0.35 (2019-08-04 13:34:46.717284)\n",
            "Accuracy at step 2 - batch 30: 0.896\n",
            "training loss at step 2 - batch 31: 0.35 (2019-08-04 13:34:46.842523)\n",
            "Accuracy at step 2 - batch 31: 0.8984\n",
            "training loss at step 2 - batch 32: 0.40 (2019-08-04 13:34:46.855366)\n",
            "Accuracy at step 2 - batch 32: 0.8852\n",
            "training loss at step 2 - batch 33: 0.38 (2019-08-04 13:34:46.869983)\n",
            "Accuracy at step 2 - batch 33: 0.8948\n",
            "training loss at step 2 - batch 34: 0.40 (2019-08-04 13:34:46.883858)\n",
            "Accuracy at step 2 - batch 34: 0.8876\n",
            "training loss at step 2 - batch 35: 0.38 (2019-08-04 13:34:46.896602)\n",
            "Accuracy at step 2 - batch 35: 0.8904\n",
            "training loss at step 2 - batch 36: 0.40 (2019-08-04 13:34:47.022137)\n",
            "Accuracy at step 2 - batch 36: 0.8804\n",
            "training loss at step 2 - batch 37: 0.38 (2019-08-04 13:34:47.039883)\n",
            "Accuracy at step 2 - batch 37: 0.8896\n",
            "training loss at step 2 - batch 38: 0.37 (2019-08-04 13:34:47.052102)\n",
            "Accuracy at step 2 - batch 38: 0.8972\n",
            "training loss at step 2 - batch 39: 0.38 (2019-08-04 13:34:47.065492)\n",
            "Accuracy at step 2 - batch 39: 0.8872\n",
            "training loss at step 2 - batch 40: 0.39 (2019-08-04 13:34:47.081504)\n",
            "Accuracy at step 2 - batch 40: 0.8892\n",
            "training loss at step 2 - batch 41: 0.39 (2019-08-04 13:34:47.198705)\n",
            "Accuracy at step 2 - batch 41: 0.8844\n",
            "training loss at step 2 - batch 42: 0.38 (2019-08-04 13:34:47.212484)\n",
            "Accuracy at step 2 - batch 42: 0.8884\n",
            "training loss at step 2 - batch 43: 0.36 (2019-08-04 13:34:47.224816)\n",
            "Accuracy at step 2 - batch 43: 0.8908\n",
            "training loss at step 2 - batch 44: 0.35 (2019-08-04 13:34:47.237335)\n",
            "Accuracy at step 2 - batch 44: 0.898\n",
            "training loss at step 2 - batch 45: 0.36 (2019-08-04 13:34:47.249824)\n",
            "Accuracy at step 2 - batch 45: 0.8996\n",
            "training loss at step 2 - batch 46: 0.39 (2019-08-04 13:34:47.369153)\n",
            "Accuracy at step 2 - batch 46: 0.886\n",
            "training loss at step 2 - batch 47: 0.38 (2019-08-04 13:34:47.382498)\n",
            "Accuracy at step 2 - batch 47: 0.8932\n",
            "training loss at step 2 - batch 48: 0.39 (2019-08-04 13:34:47.395448)\n",
            "Accuracy at step 2 - batch 48: 0.888\n",
            "training loss at step 2 - batch 49: 0.38 (2019-08-04 13:34:47.408246)\n",
            "Accuracy at step 2 - batch 49: 0.8956\n",
            "training loss at step 2 - batch 50: 0.36 (2019-08-04 13:34:47.420613)\n",
            "Accuracy at step 2 - batch 50: 0.8916\n",
            "training loss at step 2 - batch 51: 0.38 (2019-08-04 13:34:47.543694)\n",
            "Accuracy at step 2 - batch 51: 0.8916\n",
            "training loss at step 2 - batch 52: 0.36 (2019-08-04 13:34:47.559634)\n",
            "Accuracy at step 2 - batch 52: 0.8948\n",
            "training loss at step 2 - batch 53: 0.38 (2019-08-04 13:34:47.574503)\n",
            "Accuracy at step 2 - batch 53: 0.886\n",
            "training loss at step 2 - batch 54: 0.37 (2019-08-04 13:34:47.587603)\n",
            "Accuracy at step 2 - batch 54: 0.89\n",
            "training loss at step 2 - batch 55: 0.38 (2019-08-04 13:34:47.603292)\n",
            "Accuracy at step 2 - batch 55: 0.8896\n",
            "training loss at step 2 - batch 56: 0.39 (2019-08-04 13:34:47.732012)\n",
            "Accuracy at step 2 - batch 56: 0.8908\n",
            "training loss at step 2 - batch 57: 0.38 (2019-08-04 13:34:47.750079)\n",
            "Accuracy at step 2 - batch 57: 0.8904\n",
            "training loss at step 2 - batch 58: 0.40 (2019-08-04 13:34:47.764926)\n",
            "Accuracy at step 2 - batch 58: 0.886\n",
            "training loss at step 2 - batch 59: 0.38 (2019-08-04 13:34:47.781901)\n",
            "Accuracy at step 2 - batch 59: 0.8912\n",
            "training loss at step 2 - batch 60: 0.38 (2019-08-04 13:34:47.795086)\n",
            "Accuracy at step 2 - batch 60: 0.892\n",
            "training loss at step 2 - batch 61: 0.41 (2019-08-04 13:34:47.920197)\n",
            "Accuracy at step 2 - batch 61: 0.8792\n",
            "training loss at step 2 - batch 62: 0.38 (2019-08-04 13:34:47.933541)\n",
            "Accuracy at step 2 - batch 62: 0.8916\n",
            "training loss at step 2 - batch 63: 0.38 (2019-08-04 13:34:47.947326)\n",
            "Accuracy at step 2 - batch 63: 0.8892\n",
            "training loss at step 2 - batch 64: 0.39 (2019-08-04 13:34:47.959712)\n",
            "Accuracy at step 2 - batch 64: 0.8908\n",
            "training loss at step 2 - batch 65: 0.37 (2019-08-04 13:34:47.972689)\n",
            "Accuracy at step 2 - batch 65: 0.89\n",
            "training loss at step 2 - batch 66: 0.36 (2019-08-04 13:34:48.096186)\n",
            "Accuracy at step 2 - batch 66: 0.8932\n",
            "training loss at step 2 - batch 67: 0.36 (2019-08-04 13:34:48.108538)\n",
            "Accuracy at step 2 - batch 67: 0.896\n",
            "training loss at step 2 - batch 68: 0.34 (2019-08-04 13:34:48.122105)\n",
            "Accuracy at step 2 - batch 68: 0.9028\n",
            "training loss at step 2 - batch 69: 0.36 (2019-08-04 13:34:48.134881)\n",
            "Accuracy at step 2 - batch 69: 0.8964\n",
            "training loss at step 2 - batch 70: 0.37 (2019-08-04 13:34:48.147105)\n",
            "Accuracy at step 2 - batch 70: 0.8888\n",
            "training loss at step 2 - batch 71: 0.37 (2019-08-04 13:34:48.269384)\n",
            "Accuracy at step 2 - batch 71: 0.8908\n",
            "training loss at step 2 - batch 72: 0.39 (2019-08-04 13:34:48.283669)\n",
            "Accuracy at step 2 - batch 72: 0.8932\n",
            "training loss at step 2 - batch 73: 0.37 (2019-08-04 13:34:48.296202)\n",
            "Accuracy at step 2 - batch 73: 0.8908\n",
            "training loss at step 2 - batch 74: 0.39 (2019-08-04 13:34:48.314870)\n",
            "Accuracy at step 2 - batch 74: 0.8932\n",
            "training loss at step 2 - batch 75: 0.35 (2019-08-04 13:34:48.328701)\n",
            "Accuracy at step 2 - batch 75: 0.8956\n",
            "training loss at step 2 - batch 76: 0.39 (2019-08-04 13:34:48.444845)\n",
            "Accuracy at step 2 - batch 76: 0.8896\n",
            "training loss at step 2 - batch 77: 0.37 (2019-08-04 13:34:48.458788)\n",
            "Accuracy at step 2 - batch 77: 0.8896\n",
            "training loss at step 2 - batch 78: 0.37 (2019-08-04 13:34:48.472759)\n",
            "Accuracy at step 2 - batch 78: 0.8928\n",
            "training loss at step 2 - batch 79: 0.38 (2019-08-04 13:34:48.485591)\n",
            "Accuracy at step 2 - batch 79: 0.8848\n",
            "training loss at step 2 - batch 80: 0.35 (2019-08-04 13:34:48.500105)\n",
            "Accuracy at step 2 - batch 80: 0.898\n",
            "training loss at step 2 - batch 81: 0.37 (2019-08-04 13:34:48.627857)\n",
            "Accuracy at step 2 - batch 81: 0.8948\n",
            "training loss at step 2 - batch 82: 0.37 (2019-08-04 13:34:48.643841)\n",
            "Accuracy at step 2 - batch 82: 0.8932\n",
            "training loss at step 2 - batch 83: 0.39 (2019-08-04 13:34:48.656560)\n",
            "Accuracy at step 2 - batch 83: 0.8884\n",
            "training loss at step 2 - batch 84: 0.40 (2019-08-04 13:34:48.668580)\n",
            "Accuracy at step 2 - batch 84: 0.8844\n",
            "training loss at step 2 - batch 85: 0.37 (2019-08-04 13:34:48.681515)\n",
            "Accuracy at step 2 - batch 85: 0.8928\n",
            "training loss at step 2 - batch 86: 0.38 (2019-08-04 13:34:48.807410)\n",
            "Accuracy at step 2 - batch 86: 0.8936\n",
            "training loss at step 2 - batch 87: 0.37 (2019-08-04 13:34:48.821784)\n",
            "Accuracy at step 2 - batch 87: 0.8932\n",
            "training loss at step 2 - batch 88: 0.35 (2019-08-04 13:34:48.836752)\n",
            "Accuracy at step 2 - batch 88: 0.898\n",
            "training loss at step 2 - batch 89: 0.36 (2019-08-04 13:34:48.849548)\n",
            "Accuracy at step 2 - batch 89: 0.8956\n",
            "training loss at step 2 - batch 90: 0.36 (2019-08-04 13:34:48.861509)\n",
            "Accuracy at step 2 - batch 90: 0.8956\n",
            "training loss at step 2 - batch 91: 0.39 (2019-08-04 13:34:48.985588)\n",
            "Accuracy at step 2 - batch 91: 0.89\n",
            "training loss at step 2 - batch 92: 0.36 (2019-08-04 13:34:48.999950)\n",
            "Accuracy at step 2 - batch 92: 0.8956\n",
            "training loss at step 2 - batch 93: 0.39 (2019-08-04 13:34:49.012623)\n",
            "Accuracy at step 2 - batch 93: 0.8804\n",
            "training loss at step 2 - batch 94: 0.37 (2019-08-04 13:34:49.025413)\n",
            "Accuracy at step 2 - batch 94: 0.8928\n",
            "training loss at step 2 - batch 95: 0.37 (2019-08-04 13:34:49.038943)\n",
            "Accuracy at step 2 - batch 95: 0.8936\n",
            "training loss at step 2 - batch 96: 0.40 (2019-08-04 13:34:49.162391)\n",
            "Accuracy at step 2 - batch 96: 0.8848\n",
            "training loss at step 2 - batch 97: 0.40 (2019-08-04 13:34:49.179950)\n",
            "Accuracy at step 2 - batch 97: 0.8844\n",
            "training loss at step 2 - batch 98: 0.38 (2019-08-04 13:34:49.192322)\n",
            "Accuracy at step 2 - batch 98: 0.8908\n",
            "training loss at step 2 - batch 99: 0.39 (2019-08-04 13:34:49.205228)\n",
            "Accuracy at step 2 - batch 99: 0.8788\n",
            "training loss at step 2 - batch 100: 0.37 (2019-08-04 13:34:49.218390)\n",
            "Accuracy at step 2 - batch 100: 0.8896\n",
            "training loss at step 2 - batch 101: 0.40 (2019-08-04 13:34:49.347054)\n",
            "Accuracy at step 2 - batch 101: 0.884\n",
            "training loss at step 2 - batch 102: 0.35 (2019-08-04 13:34:49.361429)\n",
            "Accuracy at step 2 - batch 102: 0.898\n",
            "training loss at step 2 - batch 103: 0.35 (2019-08-04 13:34:49.373199)\n",
            "Accuracy at step 2 - batch 103: 0.902\n",
            "training loss at step 2 - batch 104: 0.39 (2019-08-04 13:34:49.385550)\n",
            "Accuracy at step 2 - batch 104: 0.8872\n",
            "training loss at step 2 - batch 105: 0.36 (2019-08-04 13:34:49.398092)\n",
            "Accuracy at step 2 - batch 105: 0.8932\n",
            "training loss at step 2 - batch 106: 0.37 (2019-08-04 13:34:49.515669)\n",
            "Accuracy at step 2 - batch 106: 0.8908\n",
            "training loss at step 2 - batch 107: 0.38 (2019-08-04 13:34:49.532748)\n",
            "Accuracy at step 2 - batch 107: 0.8936\n",
            "training loss at step 2 - batch 108: 0.37 (2019-08-04 13:34:49.545241)\n",
            "Accuracy at step 2 - batch 108: 0.8976\n",
            "training loss at step 2 - batch 109: 0.36 (2019-08-04 13:34:49.561053)\n",
            "Accuracy at step 2 - batch 109: 0.8916\n",
            "training loss at step 2 - batch 110: 0.39 (2019-08-04 13:34:49.574529)\n",
            "Accuracy at step 2 - batch 110: 0.8864\n",
            "training loss at step 2 - batch 111: 0.38 (2019-08-04 13:34:49.704980)\n",
            "Accuracy at step 2 - batch 111: 0.8856\n",
            "training loss at step 2 - batch 112: 0.38 (2019-08-04 13:34:49.717489)\n",
            "Accuracy at step 2 - batch 112: 0.8884\n",
            "training loss at step 2 - batch 113: 0.36 (2019-08-04 13:34:49.730920)\n",
            "Accuracy at step 2 - batch 113: 0.8968\n",
            "training loss at step 2 - batch 114: 0.38 (2019-08-04 13:34:49.743679)\n",
            "Accuracy at step 2 - batch 114: 0.8916\n",
            "training loss at step 2 - batch 115: 0.37 (2019-08-04 13:34:49.755767)\n",
            "Accuracy at step 2 - batch 115: 0.894\n",
            "training loss at step 2 - batch 116: 0.36 (2019-08-04 13:34:49.889365)\n",
            "Accuracy at step 2 - batch 116: 0.8964\n",
            "training loss at step 2 - batch 117: 0.38 (2019-08-04 13:34:49.903366)\n",
            "Accuracy at step 2 - batch 117: 0.8944\n",
            "training loss at step 2 - batch 118: 0.36 (2019-08-04 13:34:49.915761)\n",
            "Accuracy at step 2 - batch 118: 0.8888\n",
            "training loss at step 2 - batch 119: 0.37 (2019-08-04 13:34:49.930201)\n",
            "Accuracy at step 2 - batch 119: 0.8948\n",
            "training loss at step 2 - batch 120: 0.37 (2019-08-04 13:34:49.942673)\n",
            "Accuracy at step 2 - batch 120: 0.8928\n",
            "training loss at step 2 - batch 121: 0.39 (2019-08-04 13:34:50.061788)\n",
            "Accuracy at step 2 - batch 121: 0.8884\n",
            "training loss at step 2 - batch 122: 0.35 (2019-08-04 13:34:50.073923)\n",
            "Accuracy at step 2 - batch 122: 0.9004\n",
            "training loss at step 2 - batch 123: 0.37 (2019-08-04 13:34:50.086443)\n",
            "Accuracy at step 2 - batch 123: 0.8916\n",
            "training loss at step 2 - batch 124: 0.37 (2019-08-04 13:34:50.101480)\n",
            "Accuracy at step 2 - batch 124: 0.8944\n",
            "training loss at step 2 - batch 125: 0.39 (2019-08-04 13:34:50.113673)\n",
            "Accuracy at step 2 - batch 125: 0.886\n",
            "training loss at step 2 - batch 126: 0.39 (2019-08-04 13:34:50.239397)\n",
            "Accuracy at step 2 - batch 126: 0.8844\n",
            "training loss at step 2 - batch 127: 0.37 (2019-08-04 13:34:50.255712)\n",
            "Accuracy at step 2 - batch 127: 0.8916\n",
            "training loss at step 2 - batch 128: 0.38 (2019-08-04 13:34:50.268411)\n",
            "Accuracy at step 2 - batch 128: 0.8984\n",
            "training loss at step 2 - batch 129: 0.35 (2019-08-04 13:34:50.281038)\n",
            "Accuracy at step 2 - batch 129: 0.8968\n",
            "training loss at step 2 - batch 130: 0.38 (2019-08-04 13:34:50.294030)\n",
            "Accuracy at step 2 - batch 130: 0.8944\n",
            "training loss at step 2 - batch 131: 0.39 (2019-08-04 13:34:50.422088)\n",
            "Accuracy at step 2 - batch 131: 0.8924\n",
            "training loss at step 2 - batch 132: 0.37 (2019-08-04 13:34:50.436085)\n",
            "Accuracy at step 2 - batch 132: 0.8908\n",
            "training loss at step 2 - batch 133: 0.38 (2019-08-04 13:34:50.449336)\n",
            "Accuracy at step 2 - batch 133: 0.8848\n",
            "training loss at step 2 - batch 134: 0.42 (2019-08-04 13:34:50.461549)\n",
            "Accuracy at step 2 - batch 134: 0.8784\n",
            "training loss at step 2 - batch 135: 0.39 (2019-08-04 13:34:50.473573)\n",
            "Accuracy at step 2 - batch 135: 0.888\n",
            "training loss at step 2 - batch 136: 0.35 (2019-08-04 13:34:50.590074)\n",
            "Accuracy at step 2 - batch 136: 0.8992\n",
            "training loss at step 2 - batch 137: 0.38 (2019-08-04 13:34:50.603074)\n",
            "Accuracy at step 2 - batch 137: 0.8948\n",
            "training loss at step 2 - batch 138: 0.39 (2019-08-04 13:34:50.615089)\n",
            "Accuracy at step 2 - batch 138: 0.8876\n",
            "training loss at step 2 - batch 139: 0.35 (2019-08-04 13:34:50.631737)\n",
            "Accuracy at step 2 - batch 139: 0.894\n",
            "training loss at step 2 - batch 140: 0.37 (2019-08-04 13:34:50.643851)\n",
            "Accuracy at step 2 - batch 140: 0.8952\n",
            "training loss at step 2 - batch 141: 0.37 (2019-08-04 13:34:50.772945)\n",
            "Accuracy at step 2 - batch 141: 0.8928\n",
            "training loss at step 2 - batch 142: 0.37 (2019-08-04 13:34:50.786947)\n",
            "Accuracy at step 2 - batch 142: 0.8896\n",
            "training loss at step 2 - batch 143: 0.38 (2019-08-04 13:34:50.800327)\n",
            "Accuracy at step 2 - batch 143: 0.8892\n",
            "training loss at step 2 - batch 144: 0.38 (2019-08-04 13:34:50.813340)\n",
            "Accuracy at step 2 - batch 144: 0.892\n",
            "training loss at step 2 - batch 145: 0.37 (2019-08-04 13:34:50.827160)\n",
            "Accuracy at step 2 - batch 145: 0.8944\n",
            "training loss at step 2 - batch 146: 0.35 (2019-08-04 13:34:50.954792)\n",
            "Accuracy at step 2 - batch 146: 0.8968\n",
            "training loss at step 2 - batch 147: 0.36 (2019-08-04 13:34:50.972980)\n",
            "Accuracy at step 2 - batch 147: 0.8944\n",
            "training loss at step 2 - batch 148: 0.37 (2019-08-04 13:34:50.985374)\n",
            "Accuracy at step 2 - batch 148: 0.8948\n",
            "training loss at step 2 - batch 149: 0.39 (2019-08-04 13:34:50.998219)\n",
            "Accuracy at step 2 - batch 149: 0.8896\n",
            "training loss at step 2 - batch 150: 0.40 (2019-08-04 13:34:51.011294)\n",
            "Accuracy at step 2 - batch 150: 0.8912\n",
            "training loss at step 2 - batch 151: 0.36 (2019-08-04 13:34:51.131304)\n",
            "Accuracy at step 2 - batch 151: 0.8936\n",
            "training loss at step 2 - batch 152: 0.37 (2019-08-04 13:34:51.143706)\n",
            "Accuracy at step 2 - batch 152: 0.8944\n",
            "training loss at step 2 - batch 153: 0.38 (2019-08-04 13:34:51.159403)\n",
            "Accuracy at step 2 - batch 153: 0.892\n",
            "training loss at step 2 - batch 154: 0.36 (2019-08-04 13:34:51.172299)\n",
            "Accuracy at step 2 - batch 154: 0.9004\n",
            "training loss at step 2 - batch 155: 0.38 (2019-08-04 13:34:51.185312)\n",
            "Accuracy at step 2 - batch 155: 0.8924\n",
            "training loss at step 2 - batch 156: 0.37 (2019-08-04 13:34:51.304223)\n",
            "Accuracy at step 2 - batch 156: 0.8932\n",
            "training loss at step 2 - batch 157: 0.38 (2019-08-04 13:34:51.316467)\n",
            "Accuracy at step 2 - batch 157: 0.8912\n",
            "training loss at step 2 - batch 158: 0.36 (2019-08-04 13:34:51.329434)\n",
            "Accuracy at step 2 - batch 158: 0.8956\n",
            "training loss at step 2 - batch 159: 0.37 (2019-08-04 13:34:51.342282)\n",
            "Accuracy at step 2 - batch 159: 0.892\n",
            "training loss at step 2 - batch 160: 0.36 (2019-08-04 13:34:51.355004)\n",
            "Accuracy at step 2 - batch 160: 0.8944\n",
            "training loss at step 2 - batch 161: 0.39 (2019-08-04 13:34:51.482209)\n",
            "Accuracy at step 2 - batch 161: 0.8836\n",
            "training loss at step 2 - batch 162: 0.39 (2019-08-04 13:34:51.496481)\n",
            "Accuracy at step 2 - batch 162: 0.8884\n",
            "training loss at step 2 - batch 163: 0.37 (2019-08-04 13:34:51.510266)\n",
            "Accuracy at step 2 - batch 163: 0.8928\n",
            "training loss at step 2 - batch 164: 0.38 (2019-08-04 13:34:51.522650)\n",
            "Accuracy at step 2 - batch 164: 0.886\n",
            "training loss at step 2 - batch 165: 0.39 (2019-08-04 13:34:51.535172)\n",
            "Accuracy at step 2 - batch 165: 0.8844\n",
            "training loss at step 2 - batch 166: 0.37 (2019-08-04 13:34:51.650877)\n",
            "Accuracy at step 2 - batch 166: 0.8932\n",
            "training loss at step 2 - batch 167: 0.38 (2019-08-04 13:34:51.666241)\n",
            "Accuracy at step 2 - batch 167: 0.8876\n",
            "training loss at step 2 - batch 168: 0.35 (2019-08-04 13:34:51.679088)\n",
            "Accuracy at step 2 - batch 168: 0.8964\n",
            "training loss at step 2 - batch 169: 0.38 (2019-08-04 13:34:51.696932)\n",
            "Accuracy at step 2 - batch 169: 0.8876\n",
            "training loss at step 2 - batch 170: 0.36 (2019-08-04 13:34:51.712827)\n",
            "Accuracy at step 2 - batch 170: 0.894\n",
            "training loss at step 2 - batch 171: 0.37 (2019-08-04 13:34:51.842024)\n",
            "Accuracy at step 2 - batch 171: 0.8952\n",
            "training loss at step 2 - batch 172: 0.35 (2019-08-04 13:34:51.855495)\n",
            "Accuracy at step 2 - batch 172: 0.9\n",
            "training loss at step 2 - batch 173: 0.38 (2019-08-04 13:34:51.867832)\n",
            "Accuracy at step 2 - batch 173: 0.8896\n",
            "training loss at step 2 - batch 174: 0.36 (2019-08-04 13:34:51.879952)\n",
            "Accuracy at step 2 - batch 174: 0.8988\n",
            "training loss at step 2 - batch 175: 0.37 (2019-08-04 13:34:51.892155)\n",
            "Accuracy at step 2 - batch 175: 0.8956\n",
            "training loss at step 2 - batch 176: 0.36 (2019-08-04 13:34:52.016791)\n",
            "Accuracy at step 2 - batch 176: 0.8904\n",
            "training loss at step 2 - batch 177: 0.35 (2019-08-04 13:34:52.034232)\n",
            "Accuracy at step 2 - batch 177: 0.9004\n",
            "training loss at step 2 - batch 178: 0.38 (2019-08-04 13:34:52.047442)\n",
            "Accuracy at step 2 - batch 178: 0.8896\n",
            "training loss at step 2 - batch 179: 0.36 (2019-08-04 13:34:52.059544)\n",
            "Accuracy at step 2 - batch 179: 0.8944\n",
            "training loss at step 2 - batch 180: 0.38 (2019-08-04 13:34:52.072173)\n",
            "Accuracy at step 2 - batch 180: 0.8888\n",
            "training loss at step 2 - batch 181: 0.38 (2019-08-04 13:34:52.191692)\n",
            "Accuracy at step 2 - batch 181: 0.8932\n",
            "training loss at step 2 - batch 182: 0.37 (2019-08-04 13:34:52.209275)\n",
            "Accuracy at step 2 - batch 182: 0.8932\n",
            "training loss at step 2 - batch 183: 0.36 (2019-08-04 13:34:52.225609)\n",
            "Accuracy at step 2 - batch 183: 0.8964\n",
            "training loss at step 2 - batch 184: 0.36 (2019-08-04 13:34:52.238535)\n",
            "Accuracy at step 2 - batch 184: 0.8892\n",
            "training loss at step 2 - batch 185: 0.38 (2019-08-04 13:34:52.251092)\n",
            "Accuracy at step 2 - batch 185: 0.884\n",
            "training loss at step 2 - batch 186: 0.39 (2019-08-04 13:34:52.376343)\n",
            "Accuracy at step 2 - batch 186: 0.8908\n",
            "training loss at step 2 - batch 187: 0.37 (2019-08-04 13:34:52.393171)\n",
            "Accuracy at step 2 - batch 187: 0.8988\n",
            "training loss at step 2 - batch 188: 0.40 (2019-08-04 13:34:52.406326)\n",
            "Accuracy at step 2 - batch 188: 0.8876\n",
            "training loss at step 2 - batch 189: 0.37 (2019-08-04 13:34:52.418196)\n",
            "Accuracy at step 2 - batch 189: 0.8932\n",
            "training loss at step 2 - batch 190: 0.38 (2019-08-04 13:34:52.432334)\n",
            "Accuracy at step 2 - batch 190: 0.8916\n",
            "training loss at step 2 - batch 191: 0.37 (2019-08-04 13:34:52.552784)\n",
            "Accuracy at step 2 - batch 191: 0.8932\n",
            "training loss at step 2 - batch 192: 0.37 (2019-08-04 13:34:52.569863)\n",
            "Accuracy at step 2 - batch 192: 0.8856\n",
            "training loss at step 2 - batch 193: 0.37 (2019-08-04 13:34:52.583141)\n",
            "Accuracy at step 2 - batch 193: 0.898\n",
            "training loss at step 2 - batch 194: 0.35 (2019-08-04 13:34:52.596379)\n",
            "Accuracy at step 2 - batch 194: 0.8976\n",
            "training loss at step 2 - batch 195: 0.37 (2019-08-04 13:34:52.609039)\n",
            "Accuracy at step 2 - batch 195: 0.892\n",
            "training loss at step 2 - batch 196: 0.37 (2019-08-04 13:34:52.735944)\n",
            "Accuracy at step 2 - batch 196: 0.8868\n",
            "training loss at step 2 - batch 197: 0.37 (2019-08-04 13:34:52.748985)\n",
            "Accuracy at step 2 - batch 197: 0.8952\n",
            "training loss at step 2 - batch 198: 0.37 (2019-08-04 13:34:52.762213)\n",
            "Accuracy at step 2 - batch 198: 0.896\n",
            "training loss at step 2 - batch 199: 0.38 (2019-08-04 13:34:52.774600)\n",
            "Accuracy at step 2 - batch 199: 0.8904\n",
            "training loss at step 2 - batch 200: 0.39 (2019-08-04 13:34:52.786973)\n",
            "Accuracy at step 2 - batch 200: 0.8864\n",
            "training loss at step 2 - batch 201: 0.37 (2019-08-04 13:34:52.910326)\n",
            "Accuracy at step 2 - batch 201: 0.896\n",
            "training loss at step 2 - batch 202: 0.37 (2019-08-04 13:34:52.930106)\n",
            "Accuracy at step 2 - batch 202: 0.8908\n",
            "training loss at step 2 - batch 203: 0.36 (2019-08-04 13:34:52.944398)\n",
            "Accuracy at step 2 - batch 203: 0.8968\n",
            "training loss at step 2 - batch 204: 0.37 (2019-08-04 13:34:52.957426)\n",
            "Accuracy at step 2 - batch 204: 0.892\n",
            "training loss at step 2 - batch 205: 0.34 (2019-08-04 13:34:52.969916)\n",
            "Accuracy at step 2 - batch 205: 0.9012\n",
            "training loss at step 2 - batch 206: 0.37 (2019-08-04 13:34:53.096478)\n",
            "Accuracy at step 2 - batch 206: 0.8936\n",
            "training loss at step 2 - batch 207: 0.38 (2019-08-04 13:34:53.108544)\n",
            "Accuracy at step 2 - batch 207: 0.8852\n",
            "training loss at step 2 - batch 208: 0.38 (2019-08-04 13:34:53.121715)\n",
            "Accuracy at step 2 - batch 208: 0.8896\n",
            "training loss at step 2 - batch 209: 0.35 (2019-08-04 13:34:53.135608)\n",
            "Accuracy at step 2 - batch 209: 0.8996\n",
            "training loss at step 2 - batch 210: 0.36 (2019-08-04 13:34:53.149751)\n",
            "Accuracy at step 2 - batch 210: 0.892\n",
            "training loss at step 2 - batch 211: 0.36 (2019-08-04 13:34:53.269197)\n",
            "Accuracy at step 2 - batch 211: 0.9\n",
            "training loss at step 2 - batch 212: 0.37 (2019-08-04 13:34:53.286792)\n",
            "Accuracy at step 2 - batch 212: 0.8916\n",
            "training loss at step 2 - batch 213: 0.38 (2019-08-04 13:34:53.299180)\n",
            "Accuracy at step 2 - batch 213: 0.8888\n",
            "training loss at step 2 - batch 214: 0.37 (2019-08-04 13:34:53.311516)\n",
            "Accuracy at step 2 - batch 214: 0.8976\n",
            "training loss at step 2 - batch 215: 0.36 (2019-08-04 13:34:53.326726)\n",
            "Accuracy at step 2 - batch 215: 0.898\n",
            "training loss at step 2 - batch 216: 0.36 (2019-08-04 13:34:53.451446)\n",
            "Accuracy at step 2 - batch 216: 0.894\n",
            "training loss at step 2 - batch 217: 0.36 (2019-08-04 13:34:53.465179)\n",
            "Accuracy at step 2 - batch 217: 0.896\n",
            "training loss at step 2 - batch 218: 0.35 (2019-08-04 13:34:53.478233)\n",
            "Accuracy at step 2 - batch 218: 0.896\n",
            "training loss at step 2 - batch 219: 0.36 (2019-08-04 13:34:53.489968)\n",
            "Accuracy at step 2 - batch 219: 0.8936\n",
            "training loss at step 2 - batch 220: 0.36 (2019-08-04 13:34:53.502139)\n",
            "Accuracy at step 2 - batch 220: 0.8984\n",
            "training loss at step 2 - batch 221: 0.39 (2019-08-04 13:34:53.618258)\n",
            "Accuracy at step 2 - batch 221: 0.8852\n",
            "training loss at step 2 - batch 222: 0.37 (2019-08-04 13:34:53.634362)\n",
            "Accuracy at step 2 - batch 222: 0.896\n",
            "training loss at step 2 - batch 223: 0.37 (2019-08-04 13:34:53.648488)\n",
            "Accuracy at step 2 - batch 223: 0.892\n",
            "training loss at step 2 - batch 224: 0.37 (2019-08-04 13:34:53.667136)\n",
            "Accuracy at step 2 - batch 224: 0.888\n",
            "training loss at step 2 - batch 225: 0.38 (2019-08-04 13:34:53.679412)\n",
            "Accuracy at step 2 - batch 225: 0.8876\n",
            "training loss at step 2 - batch 226: 0.37 (2019-08-04 13:34:53.833033)\n",
            "Accuracy at step 2 - batch 226: 0.8948\n",
            "training loss at step 2 - batch 227: 0.39 (2019-08-04 13:34:53.846946)\n",
            "Accuracy at step 2 - batch 227: 0.8896\n",
            "training loss at step 2 - batch 228: 0.38 (2019-08-04 13:34:53.860544)\n",
            "Accuracy at step 2 - batch 228: 0.8912\n",
            "training loss at step 2 - batch 229: 0.36 (2019-08-04 13:34:53.876609)\n",
            "Accuracy at step 2 - batch 229: 0.8968\n",
            "training loss at step 2 - batch 230: 0.37 (2019-08-04 13:34:53.889107)\n",
            "Accuracy at step 2 - batch 230: 0.8932\n",
            "training loss at step 2 - batch 231: 0.37 (2019-08-04 13:34:54.006351)\n",
            "Accuracy at step 2 - batch 231: 0.89\n",
            "training loss at step 2 - batch 232: 0.38 (2019-08-04 13:34:54.018726)\n",
            "Accuracy at step 2 - batch 232: 0.8928\n",
            "training loss at step 2 - batch 233: 0.37 (2019-08-04 13:34:54.030910)\n",
            "Accuracy at step 2 - batch 233: 0.8972\n",
            "training loss at step 2 - batch 234: 0.35 (2019-08-04 13:34:54.043234)\n",
            "Accuracy at step 2 - batch 234: 0.9008\n",
            "training loss at step 2 - batch 235: 0.36 (2019-08-04 13:34:54.056045)\n",
            "Accuracy at step 2 - batch 235: 0.8952\n",
            "training loss at step 2 - batch 236: 0.38 (2019-08-04 13:34:54.181522)\n",
            "Accuracy at step 2 - batch 236: 0.896\n",
            "training loss at step 2 - batch 237: 0.36 (2019-08-04 13:34:54.194043)\n",
            "Accuracy at step 2 - batch 237: 0.8948\n",
            "training loss at step 2 - batch 238: 0.34 (2019-08-04 13:34:54.208309)\n",
            "Accuracy at step 2 - batch 238: 0.8992\n",
            "training loss at step 2 - batch 239: 0.35 (2019-08-04 13:34:54.220044)\n",
            "Accuracy at step 2 - batch 239: 0.9\n",
            "training loss at step 2 - batch 240: 0.38 (2019-08-04 13:34:54.231827)\n",
            "Accuracy at step 2 - batch 240: 0.8904\n",
            "training loss at step 2 - batch 241: 0.37 (2019-08-04 13:34:54.352697)\n",
            "Accuracy at step 2 - batch 241: 0.896\n",
            "training loss at step 2 - batch 242: 0.36 (2019-08-04 13:34:54.370019)\n",
            "Accuracy at step 2 - batch 242: 0.8956\n",
            "training loss at step 2 - batch 243: 0.39 (2019-08-04 13:34:54.382010)\n",
            "Accuracy at step 2 - batch 243: 0.8912\n",
            "training loss at step 2 - batch 244: 0.33 (2019-08-04 13:34:54.399092)\n",
            "Accuracy at step 2 - batch 244: 0.9048\n",
            "training loss at step 2 - batch 245: 0.40 (2019-08-04 13:34:54.412316)\n",
            "Accuracy at step 2 - batch 245: 0.8868\n",
            "training loss at step 2 - batch 246: 0.35 (2019-08-04 13:34:54.528072)\n",
            "Accuracy at step 2 - batch 246: 0.8944\n",
            "training loss at step 2 - batch 247: 0.40 (2019-08-04 13:34:54.546718)\n",
            "Accuracy at step 2 - batch 247: 0.8876\n",
            "training loss at step 2 - batch 248: 0.36 (2019-08-04 13:34:54.559981)\n",
            "Accuracy at step 2 - batch 248: 0.8992\n",
            "training loss at step 2 - batch 249: 0.34 (2019-08-04 13:34:54.573333)\n",
            "Accuracy at step 2 - batch 249: 0.8964\n",
            "training loss at step 2 - batch 250: 0.38 (2019-08-04 13:34:54.588722)\n",
            "Accuracy at step 2 - batch 250: 0.8912\n",
            "training loss at step 2 - batch 251: 0.38 (2019-08-04 13:34:54.712413)\n",
            "Accuracy at step 2 - batch 251: 0.8944\n",
            "training loss at step 2 - batch 252: 0.37 (2019-08-04 13:34:54.724871)\n",
            "Accuracy at step 2 - batch 252: 0.892\n",
            "training loss at step 2 - batch 253: 0.37 (2019-08-04 13:34:54.739293)\n",
            "Accuracy at step 2 - batch 253: 0.8944\n",
            "training loss at step 2 - batch 254: 0.35 (2019-08-04 13:34:54.751434)\n",
            "Accuracy at step 2 - batch 254: 0.9028\n",
            "training loss at step 2 - batch 255: 0.35 (2019-08-04 13:34:54.770126)\n",
            "Accuracy at step 2 - batch 255: 0.8976\n",
            "training loss at step 2 - batch 256: 0.37 (2019-08-04 13:34:54.901150)\n",
            "Accuracy at step 2 - batch 256: 0.9004\n",
            "training loss at step 2 - batch 257: 0.38 (2019-08-04 13:34:54.918417)\n",
            "Accuracy at step 2 - batch 257: 0.8912\n",
            "training loss at step 2 - batch 258: 0.37 (2019-08-04 13:34:54.931178)\n",
            "Accuracy at step 2 - batch 258: 0.892\n",
            "training loss at step 2 - batch 259: 0.37 (2019-08-04 13:34:54.944371)\n",
            "Accuracy at step 2 - batch 259: 0.8956\n",
            "training loss at step 2 - batch 260: 0.36 (2019-08-04 13:34:54.957024)\n",
            "Accuracy at step 2 - batch 260: 0.8912\n",
            "training loss at step 2 - batch 261: 0.37 (2019-08-04 13:34:55.075132)\n",
            "Accuracy at step 2 - batch 261: 0.8956\n",
            "training loss at step 2 - batch 262: 0.36 (2019-08-04 13:34:55.089304)\n",
            "Accuracy at step 2 - batch 262: 0.8976\n",
            "training loss at step 2 - batch 263: 0.39 (2019-08-04 13:34:55.105906)\n",
            "Accuracy at step 2 - batch 263: 0.8888\n",
            "training loss at step 2 - batch 264: 0.36 (2019-08-04 13:34:55.119094)\n",
            "Accuracy at step 2 - batch 264: 0.8928\n",
            "training loss at step 2 - batch 265: 0.36 (2019-08-04 13:34:55.132145)\n",
            "Accuracy at step 2 - batch 265: 0.8944\n",
            "training loss at step 2 - batch 266: 0.37 (2019-08-04 13:34:55.258700)\n",
            "Accuracy at step 2 - batch 266: 0.8948\n",
            "training loss at step 2 - batch 267: 0.38 (2019-08-04 13:34:55.275886)\n",
            "Accuracy at step 2 - batch 267: 0.8908\n",
            "training loss at step 2 - batch 268: 0.39 (2019-08-04 13:34:55.289374)\n",
            "Accuracy at step 2 - batch 268: 0.884\n",
            "training loss at step 2 - batch 269: 0.35 (2019-08-04 13:34:55.302735)\n",
            "Accuracy at step 2 - batch 269: 0.8944\n",
            "training loss at step 2 - batch 270: 0.36 (2019-08-04 13:34:55.318705)\n",
            "Accuracy at step 2 - batch 270: 0.8972\n",
            "training loss at step 2 - batch 271: 0.38 (2019-08-04 13:34:55.441031)\n",
            "Accuracy at step 2 - batch 271: 0.8964\n",
            "training loss at step 2 - batch 272: 0.36 (2019-08-04 13:34:55.457981)\n",
            "Accuracy at step 2 - batch 272: 0.8972\n",
            "training loss at step 2 - batch 273: 0.36 (2019-08-04 13:34:55.471357)\n",
            "Accuracy at step 2 - batch 273: 0.892\n",
            "training loss at step 2 - batch 274: 0.35 (2019-08-04 13:34:55.483735)\n",
            "Accuracy at step 2 - batch 274: 0.9004\n",
            "training loss at step 2 - batch 275: 0.36 (2019-08-04 13:34:55.497840)\n",
            "Accuracy at step 2 - batch 275: 0.894\n",
            "training loss at step 2 - batch 276: 0.37 (2019-08-04 13:34:55.626011)\n",
            "Accuracy at step 2 - batch 276: 0.8928\n",
            "training loss at step 2 - batch 277: 0.38 (2019-08-04 13:34:55.638934)\n",
            "Accuracy at step 2 - batch 277: 0.8896\n",
            "training loss at step 2 - batch 278: 0.38 (2019-08-04 13:34:55.651601)\n",
            "Accuracy at step 2 - batch 278: 0.894\n",
            "training loss at step 2 - batch 279: 0.37 (2019-08-04 13:34:55.663716)\n",
            "Accuracy at step 2 - batch 279: 0.892\n",
            "training loss at step 2 - batch 280: 0.37 (2019-08-04 13:34:55.675862)\n",
            "Accuracy at step 2 - batch 280: 0.892\n",
            "training loss at step 2 - batch 281: 0.36 (2019-08-04 13:34:55.807959)\n",
            "Accuracy at step 2 - batch 281: 0.8968\n",
            "training loss at step 2 - batch 282: 0.34 (2019-08-04 13:34:55.820403)\n",
            "Accuracy at step 2 - batch 282: 0.9004\n",
            "training loss at step 2 - batch 283: 0.36 (2019-08-04 13:34:55.836054)\n",
            "Accuracy at step 2 - batch 283: 0.8952\n",
            "training loss at step 2 - batch 284: 0.33 (2019-08-04 13:34:55.849107)\n",
            "Accuracy at step 2 - batch 284: 0.9068\n",
            "training loss at step 2 - batch 285: 0.39 (2019-08-04 13:34:55.862391)\n",
            "Accuracy at step 2 - batch 285: 0.8912\n",
            "training loss at step 2 - batch 286: 0.37 (2019-08-04 13:34:55.986517)\n",
            "Accuracy at step 2 - batch 286: 0.894\n",
            "training loss at step 2 - batch 287: 0.38 (2019-08-04 13:34:56.003675)\n",
            "Accuracy at step 2 - batch 287: 0.89\n",
            "training loss at step 2 - batch 288: 0.35 (2019-08-04 13:34:56.016121)\n",
            "Accuracy at step 2 - batch 288: 0.8996\n",
            "training loss at step 2 - batch 289: 0.37 (2019-08-04 13:34:56.029289)\n",
            "Accuracy at step 2 - batch 289: 0.8936\n",
            "training loss at step 2 - batch 290: 0.34 (2019-08-04 13:34:56.045190)\n",
            "Accuracy at step 2 - batch 290: 0.9024\n",
            "training loss at step 2 - batch 291: 0.40 (2019-08-04 13:34:56.167292)\n",
            "Accuracy at step 2 - batch 291: 0.8936\n",
            "training loss at step 2 - batch 292: 0.37 (2019-08-04 13:34:56.182820)\n",
            "Accuracy at step 2 - batch 292: 0.8932\n",
            "training loss at step 2 - batch 293: 0.36 (2019-08-04 13:34:56.195126)\n",
            "Accuracy at step 2 - batch 293: 0.896\n",
            "training loss at step 2 - batch 294: 0.38 (2019-08-04 13:34:56.208329)\n",
            "Accuracy at step 2 - batch 294: 0.892\n",
            "training loss at step 2 - batch 295: 0.37 (2019-08-04 13:34:56.222963)\n",
            "Accuracy at step 2 - batch 295: 0.8956\n",
            "training loss at step 2 - batch 296: 0.37 (2019-08-04 13:34:56.347500)\n",
            "Accuracy at step 2 - batch 296: 0.8968\n",
            "training loss at step 2 - batch 297: 0.35 (2019-08-04 13:34:56.361785)\n",
            "Accuracy at step 2 - batch 297: 0.8936\n",
            "training loss at step 2 - batch 298: 0.38 (2019-08-04 13:34:56.373875)\n",
            "Accuracy at step 2 - batch 298: 0.8916\n",
            "training loss at step 2 - batch 299: 0.37 (2019-08-04 13:34:56.386199)\n",
            "Accuracy at step 2 - batch 299: 0.8908\n",
            "training loss at step 2 - batch 300: 0.35 (2019-08-04 13:34:56.399242)\n",
            "Accuracy at step 2 - batch 300: 0.8988\n",
            "training loss at step 2 - batch 301: 0.37 (2019-08-04 13:34:56.518391)\n",
            "Accuracy at step 2 - batch 301: 0.8924\n",
            "training loss at step 2 - batch 302: 0.38 (2019-08-04 13:34:56.535574)\n",
            "Accuracy at step 2 - batch 302: 0.8924\n",
            "training loss at step 2 - batch 303: 0.38 (2019-08-04 13:34:56.547925)\n",
            "Accuracy at step 2 - batch 303: 0.8876\n",
            "training loss at step 2 - batch 304: 0.36 (2019-08-04 13:34:56.563525)\n",
            "Accuracy at step 2 - batch 304: 0.9032\n",
            "training loss at step 2 - batch 305: 0.35 (2019-08-04 13:34:56.575757)\n",
            "Accuracy at step 2 - batch 305: 0.9044\n",
            "training loss at step 2 - batch 306: 0.35 (2019-08-04 13:34:56.697910)\n",
            "Accuracy at step 2 - batch 306: 0.894\n",
            "training loss at step 2 - batch 307: 0.37 (2019-08-04 13:34:56.713619)\n",
            "Accuracy at step 2 - batch 307: 0.8948\n",
            "training loss at step 2 - batch 308: 0.35 (2019-08-04 13:34:56.726136)\n",
            "Accuracy at step 2 - batch 308: 0.8948\n",
            "training loss at step 2 - batch 309: 0.34 (2019-08-04 13:34:56.739310)\n",
            "Accuracy at step 2 - batch 309: 0.9032\n",
            "training loss at step 2 - batch 310: 0.33 (2019-08-04 13:34:56.752974)\n",
            "Accuracy at step 2 - batch 310: 0.9064\n",
            "training loss at step 2 - batch 311: 0.35 (2019-08-04 13:34:56.883409)\n",
            "Accuracy at step 2 - batch 311: 0.8912\n",
            "training loss at step 2 - batch 312: 0.38 (2019-08-04 13:34:56.896375)\n",
            "Accuracy at step 2 - batch 312: 0.892\n",
            "training loss at step 2 - batch 313: 0.35 (2019-08-04 13:34:56.909205)\n",
            "Accuracy at step 2 - batch 313: 0.8968\n",
            "training loss at step 2 - batch 314: 0.37 (2019-08-04 13:34:56.921350)\n",
            "Accuracy at step 2 - batch 314: 0.89\n",
            "training loss at step 2 - batch 315: 0.36 (2019-08-04 13:34:56.934310)\n",
            "Accuracy at step 2 - batch 315: 0.8964\n",
            "training loss at step 2 - batch 316: 0.37 (2019-08-04 13:34:57.054047)\n",
            "Accuracy at step 2 - batch 316: 0.8936\n",
            "training loss at step 2 - batch 317: 0.38 (2019-08-04 13:34:57.070438)\n",
            "Accuracy at step 2 - batch 317: 0.8888\n",
            "training loss at step 2 - batch 318: 0.36 (2019-08-04 13:34:57.083746)\n",
            "Accuracy at step 2 - batch 318: 0.8904\n",
            "training loss at step 2 - batch 319: 0.37 (2019-08-04 13:34:57.100304)\n",
            "Accuracy at step 2 - batch 319: 0.8944\n",
            "training loss at step 2 - batch 320: 0.38 (2019-08-04 13:34:57.113367)\n",
            "Accuracy at step 2 - batch 320: 0.8952\n",
            "training loss at step 2 - batch 321: 0.39 (2019-08-04 13:34:57.236867)\n",
            "Accuracy at step 2 - batch 321: 0.8852\n",
            "training loss at step 2 - batch 322: 0.36 (2019-08-04 13:34:57.250917)\n",
            "Accuracy at step 2 - batch 322: 0.8924\n",
            "training loss at step 2 - batch 323: 0.34 (2019-08-04 13:34:57.264114)\n",
            "Accuracy at step 2 - batch 323: 0.8976\n",
            "training loss at step 2 - batch 324: 0.36 (2019-08-04 13:34:57.276837)\n",
            "Accuracy at step 2 - batch 324: 0.892\n",
            "training loss at step 2 - batch 325: 0.36 (2019-08-04 13:34:57.290084)\n",
            "Accuracy at step 2 - batch 325: 0.892\n",
            "training loss at step 2 - batch 326: 0.37 (2019-08-04 13:34:57.406384)\n",
            "Accuracy at step 2 - batch 326: 0.898\n",
            "training loss at step 2 - batch 327: 0.35 (2019-08-04 13:34:57.419796)\n",
            "Accuracy at step 2 - batch 327: 0.898\n",
            "training loss at step 2 - batch 328: 0.36 (2019-08-04 13:34:57.433390)\n",
            "Accuracy at step 2 - batch 328: 0.898\n",
            "training loss at step 2 - batch 329: 0.37 (2019-08-04 13:34:57.445474)\n",
            "Accuracy at step 2 - batch 329: 0.894\n",
            "training loss at step 2 - batch 330: 0.36 (2019-08-04 13:34:57.458132)\n",
            "Accuracy at step 2 - batch 330: 0.892\n",
            "training loss at step 2 - batch 331: 0.36 (2019-08-04 13:34:57.583040)\n",
            "Accuracy at step 2 - batch 331: 0.8964\n",
            "training loss at step 2 - batch 332: 0.37 (2019-08-04 13:34:57.596409)\n",
            "Accuracy at step 2 - batch 332: 0.8952\n",
            "training loss at step 2 - batch 333: 0.37 (2019-08-04 13:34:57.609277)\n",
            "Accuracy at step 2 - batch 333: 0.8952\n",
            "training loss at step 2 - batch 334: 0.35 (2019-08-04 13:34:57.622408)\n",
            "Accuracy at step 2 - batch 334: 0.8976\n",
            "training loss at step 2 - batch 335: 0.36 (2019-08-04 13:34:57.634775)\n",
            "Accuracy at step 2 - batch 335: 0.9016\n",
            "training loss at step 2 - batch 336: 0.37 (2019-08-04 13:34:57.751733)\n",
            "Accuracy at step 2 - batch 336: 0.8964\n",
            "training loss at step 2 - batch 337: 0.37 (2019-08-04 13:34:57.768669)\n",
            "Accuracy at step 2 - batch 337: 0.8928\n",
            "training loss at step 2 - batch 338: 0.37 (2019-08-04 13:34:57.780998)\n",
            "Accuracy at step 2 - batch 338: 0.8952\n",
            "training loss at step 2 - batch 339: 0.36 (2019-08-04 13:34:57.796656)\n",
            "Accuracy at step 2 - batch 339: 0.8916\n",
            "training loss at step 2 - batch 340: 0.36 (2019-08-04 13:34:57.809267)\n",
            "Accuracy at step 2 - batch 340: 0.8948\n",
            "training loss at step 2 - batch 341: 0.36 (2019-08-04 13:34:57.934401)\n",
            "Accuracy at step 2 - batch 341: 0.8932\n",
            "training loss at step 2 - batch 342: 0.38 (2019-08-04 13:34:57.948188)\n",
            "Accuracy at step 2 - batch 342: 0.8912\n",
            "training loss at step 2 - batch 343: 0.36 (2019-08-04 13:34:57.961262)\n",
            "Accuracy at step 2 - batch 343: 0.8964\n",
            "training loss at step 2 - batch 344: 0.37 (2019-08-04 13:34:57.973964)\n",
            "Accuracy at step 2 - batch 344: 0.89\n",
            "training loss at step 2 - batch 345: 0.36 (2019-08-04 13:34:57.985873)\n",
            "Accuracy at step 2 - batch 345: 0.8952\n",
            "training loss at step 2 - batch 346: 0.35 (2019-08-04 13:34:58.116229)\n",
            "Accuracy at step 2 - batch 346: 0.8944\n",
            "training loss at step 2 - batch 347: 0.35 (2019-08-04 13:34:58.132296)\n",
            "Accuracy at step 2 - batch 347: 0.8956\n",
            "training loss at step 2 - batch 348: 0.35 (2019-08-04 13:34:58.145533)\n",
            "Accuracy at step 2 - batch 348: 0.9008\n",
            "training loss at step 2 - batch 349: 0.37 (2019-08-04 13:34:58.160283)\n",
            "Accuracy at step 2 - batch 349: 0.8888\n",
            "training loss at step 2 - batch 350: 0.37 (2019-08-04 13:34:58.172470)\n",
            "Accuracy at step 2 - batch 350: 0.8908\n",
            "training loss at step 2 - batch 351: 0.37 (2019-08-04 13:34:58.292882)\n",
            "Accuracy at step 2 - batch 351: 0.8896\n",
            "training loss at step 2 - batch 352: 0.38 (2019-08-04 13:34:58.310180)\n",
            "Accuracy at step 2 - batch 352: 0.8892\n",
            "training loss at step 2 - batch 353: 0.33 (2019-08-04 13:34:58.327738)\n",
            "Accuracy at step 2 - batch 353: 0.912\n",
            "training loss at step 2 - batch 354: 0.37 (2019-08-04 13:34:58.341912)\n",
            "Accuracy at step 2 - batch 354: 0.8956\n",
            "training loss at step 2 - batch 355: 0.35 (2019-08-04 13:34:58.356183)\n",
            "Accuracy at step 2 - batch 355: 0.8968\n",
            "training loss at step 2 - batch 356: 0.34 (2019-08-04 13:34:58.476756)\n",
            "Accuracy at step 2 - batch 356: 0.9032\n",
            "training loss at step 2 - batch 357: 0.36 (2019-08-04 13:34:58.490198)\n",
            "Accuracy at step 2 - batch 357: 0.894\n",
            "training loss at step 2 - batch 358: 0.39 (2019-08-04 13:34:58.504441)\n",
            "Accuracy at step 2 - batch 358: 0.8908\n",
            "training loss at step 2 - batch 359: 0.35 (2019-08-04 13:34:58.517056)\n",
            "Accuracy at step 2 - batch 359: 0.904\n",
            "training loss at step 2 - batch 360: 0.35 (2019-08-04 13:34:58.531428)\n",
            "Accuracy at step 2 - batch 360: 0.9004\n",
            "training loss at step 2 - batch 361: 0.42 (2019-08-04 13:34:58.646856)\n",
            "Accuracy at step 2 - batch 361: 0.8824\n",
            "training loss at step 2 - batch 362: 0.37 (2019-08-04 13:34:58.659502)\n",
            "Accuracy at step 2 - batch 362: 0.8932\n",
            "training loss at step 2 - batch 363: 0.36 (2019-08-04 13:34:58.672039)\n",
            "Accuracy at step 2 - batch 363: 0.9012\n",
            "training loss at step 2 - batch 364: 0.38 (2019-08-04 13:34:58.685018)\n",
            "Accuracy at step 2 - batch 364: 0.8908\n",
            "training loss at step 2 - batch 365: 0.37 (2019-08-04 13:34:58.697493)\n",
            "Accuracy at step 2 - batch 365: 0.8924\n",
            "training loss at step 2 - batch 366: 0.35 (2019-08-04 13:34:58.820550)\n",
            "Accuracy at step 2 - batch 366: 0.9056\n",
            "training loss at step 2 - batch 367: 0.35 (2019-08-04 13:34:58.834571)\n",
            "Accuracy at step 2 - batch 367: 0.8988\n",
            "training loss at step 2 - batch 368: 0.36 (2019-08-04 13:34:58.857165)\n",
            "Accuracy at step 2 - batch 368: 0.894\n",
            "training loss at step 2 - batch 369: 0.35 (2019-08-04 13:34:58.870435)\n",
            "Accuracy at step 2 - batch 369: 0.8984\n",
            "training loss at step 2 - batch 370: 0.37 (2019-08-04 13:34:58.882997)\n",
            "Accuracy at step 2 - batch 370: 0.894\n",
            "training loss at step 2 - batch 371: 0.38 (2019-08-04 13:34:59.002974)\n",
            "Accuracy at step 2 - batch 371: 0.8916\n",
            "training loss at step 2 - batch 372: 0.37 (2019-08-04 13:34:59.015349)\n",
            "Accuracy at step 2 - batch 372: 0.8932\n",
            "training loss at step 2 - batch 373: 0.36 (2019-08-04 13:34:59.029794)\n",
            "Accuracy at step 2 - batch 373: 0.8944\n",
            "training loss at step 2 - batch 374: 0.37 (2019-08-04 13:34:59.042265)\n",
            "Accuracy at step 2 - batch 374: 0.8852\n",
            "training loss at step 2 - batch 375: 0.37 (2019-08-04 13:34:59.054813)\n",
            "Accuracy at step 2 - batch 375: 0.8912\n",
            "training loss at step 2 - batch 376: 0.34 (2019-08-04 13:34:59.172912)\n",
            "Accuracy at step 2 - batch 376: 0.9028\n",
            "training loss at step 2 - batch 377: 0.37 (2019-08-04 13:34:59.186605)\n",
            "Accuracy at step 2 - batch 377: 0.8932\n",
            "training loss at step 2 - batch 378: 0.36 (2019-08-04 13:34:59.198667)\n",
            "Accuracy at step 2 - batch 378: 0.8992\n",
            "training loss at step 2 - batch 379: 0.37 (2019-08-04 13:34:59.210892)\n",
            "Accuracy at step 2 - batch 379: 0.894\n",
            "training loss at step 2 - batch 380: 0.35 (2019-08-04 13:34:59.223428)\n",
            "Accuracy at step 2 - batch 380: 0.8988\n",
            "training loss at step 2 - batch 381: 0.38 (2019-08-04 13:34:59.349954)\n",
            "Accuracy at step 2 - batch 381: 0.8884\n",
            "training loss at step 2 - batch 382: 0.35 (2019-08-04 13:34:59.362524)\n",
            "Accuracy at step 2 - batch 382: 0.898\n",
            "training loss at step 2 - batch 383: 0.38 (2019-08-04 13:34:59.374533)\n",
            "Accuracy at step 2 - batch 383: 0.8848\n",
            "training loss at step 2 - batch 384: 0.36 (2019-08-04 13:34:59.387280)\n",
            "Accuracy at step 2 - batch 384: 0.9024\n",
            "training loss at step 2 - batch 385: 0.38 (2019-08-04 13:34:59.400214)\n",
            "Accuracy at step 2 - batch 385: 0.8876\n",
            "training loss at step 2 - batch 386: 0.38 (2019-08-04 13:34:59.526375)\n",
            "Accuracy at step 2 - batch 386: 0.888\n",
            "training loss at step 2 - batch 387: 0.36 (2019-08-04 13:34:59.539053)\n",
            "Accuracy at step 2 - batch 387: 0.8936\n",
            "training loss at step 2 - batch 388: 0.38 (2019-08-04 13:34:59.555179)\n",
            "Accuracy at step 2 - batch 388: 0.888\n",
            "training loss at step 2 - batch 389: 0.37 (2019-08-04 13:34:59.568070)\n",
            "Accuracy at step 2 - batch 389: 0.8932\n",
            "training loss at step 2 - batch 390: 0.34 (2019-08-04 13:34:59.580918)\n",
            "Accuracy at step 2 - batch 390: 0.9012\n",
            "training loss at step 2 - batch 391: 0.39 (2019-08-04 13:34:59.695678)\n",
            "Accuracy at step 2 - batch 391: 0.8852\n",
            "training loss at step 2 - batch 392: 0.37 (2019-08-04 13:34:59.708233)\n",
            "Accuracy at step 2 - batch 392: 0.8972\n",
            "training loss at step 2 - batch 393: 0.36 (2019-08-04 13:34:59.724134)\n",
            "Accuracy at step 2 - batch 393: 0.898\n",
            "training loss at step 2 - batch 394: 0.39 (2019-08-04 13:34:59.736615)\n",
            "Accuracy at step 2 - batch 394: 0.8788\n",
            "training loss at step 2 - batch 395: 0.37 (2019-08-04 13:34:59.749169)\n",
            "Accuracy at step 2 - batch 395: 0.8892\n",
            "training loss at step 2 - batch 396: 0.37 (2019-08-04 13:34:59.874837)\n",
            "Accuracy at step 2 - batch 396: 0.8888\n",
            "training loss at step 2 - batch 397: 0.36 (2019-08-04 13:34:59.889657)\n",
            "Accuracy at step 2 - batch 397: 0.8916\n",
            "training loss at step 2 - batch 398: 0.37 (2019-08-04 13:34:59.902276)\n",
            "Accuracy at step 2 - batch 398: 0.8916\n",
            "training loss at step 2 - batch 399: 0.37 (2019-08-04 13:34:59.914346)\n",
            "Accuracy at step 2 - batch 399: 0.8844\n",
            "training loss at step 2 - batch 400: 0.38 (2019-08-04 13:34:59.927282)\n",
            "Accuracy at step 2 - batch 400: 0.8864\n",
            "training loss at step 2 - batch 401: 0.37 (2019-08-04 13:35:00.050358)\n",
            "Accuracy at step 2 - batch 401: 0.896\n",
            "training loss at step 2 - batch 402: 0.36 (2019-08-04 13:35:00.064230)\n",
            "Accuracy at step 2 - batch 402: 0.894\n",
            "training loss at step 2 - batch 403: 0.34 (2019-08-04 13:35:00.079868)\n",
            "Accuracy at step 2 - batch 403: 0.9028\n",
            "training loss at step 2 - batch 404: 0.35 (2019-08-04 13:35:00.095559)\n",
            "Accuracy at step 2 - batch 404: 0.8948\n",
            "training loss at step 2 - batch 405: 0.36 (2019-08-04 13:35:00.107897)\n",
            "Accuracy at step 2 - batch 405: 0.894\n",
            "training loss at step 2 - batch 406: 0.36 (2019-08-04 13:35:00.231202)\n",
            "Accuracy at step 2 - batch 406: 0.8884\n",
            "training loss at step 2 - batch 407: 0.33 (2019-08-04 13:35:00.244610)\n",
            "Accuracy at step 2 - batch 407: 0.9028\n",
            "training loss at step 2 - batch 408: 0.38 (2019-08-04 13:35:00.256906)\n",
            "Accuracy at step 2 - batch 408: 0.8928\n",
            "training loss at step 2 - batch 409: 0.36 (2019-08-04 13:35:00.270470)\n",
            "Accuracy at step 2 - batch 409: 0.8936\n",
            "training loss at step 2 - batch 410: 0.38 (2019-08-04 13:35:00.282545)\n",
            "Accuracy at step 2 - batch 410: 0.8916\n",
            "training loss at step 2 - batch 411: 0.36 (2019-08-04 13:35:00.430127)\n",
            "Accuracy at step 2 - batch 411: 0.8972\n",
            "training loss at step 2 - batch 412: 0.35 (2019-08-04 13:35:00.444446)\n",
            "Accuracy at step 2 - batch 412: 0.894\n",
            "training loss at step 2 - batch 413: 0.38 (2019-08-04 13:35:00.456859)\n",
            "Accuracy at step 2 - batch 413: 0.888\n",
            "training loss at step 2 - batch 414: 0.37 (2019-08-04 13:35:00.469346)\n",
            "Accuracy at step 2 - batch 414: 0.8956\n",
            "training loss at step 2 - batch 415: 0.36 (2019-08-04 13:35:00.482289)\n",
            "Accuracy at step 2 - batch 415: 0.8944\n",
            "training loss at step 2 - batch 416: 0.36 (2019-08-04 13:35:00.604426)\n",
            "Accuracy at step 2 - batch 416: 0.8976\n",
            "training loss at step 2 - batch 417: 0.35 (2019-08-04 13:35:00.620990)\n",
            "Accuracy at step 2 - batch 417: 0.8928\n",
            "training loss at step 2 - batch 418: 0.37 (2019-08-04 13:35:00.635273)\n",
            "Accuracy at step 2 - batch 418: 0.8912\n",
            "training loss at step 2 - batch 419: 0.36 (2019-08-04 13:35:00.647729)\n",
            "Accuracy at step 2 - batch 419: 0.8952\n",
            "training loss at step 2 - batch 420: 0.34 (2019-08-04 13:35:00.659628)\n",
            "Accuracy at step 2 - batch 420: 0.898\n",
            "training loss at step 2 - batch 421: 0.34 (2019-08-04 13:35:00.781726)\n",
            "Accuracy at step 2 - batch 421: 0.8976\n",
            "training loss at step 2 - batch 422: 0.38 (2019-08-04 13:35:00.797411)\n",
            "Accuracy at step 2 - batch 422: 0.888\n",
            "training loss at step 2 - batch 423: 0.34 (2019-08-04 13:35:00.809488)\n",
            "Accuracy at step 2 - batch 423: 0.9048\n",
            "training loss at step 2 - batch 424: 0.37 (2019-08-04 13:35:00.821470)\n",
            "Accuracy at step 2 - batch 424: 0.8884\n",
            "training loss at step 2 - batch 425: 0.37 (2019-08-04 13:35:00.833785)\n",
            "Accuracy at step 2 - batch 425: 0.8868\n",
            "training loss at step 2 - batch 426: 0.34 (2019-08-04 13:35:00.968768)\n",
            "Accuracy at step 2 - batch 426: 0.896\n",
            "training loss at step 2 - batch 427: 0.35 (2019-08-04 13:35:00.983294)\n",
            "Accuracy at step 2 - batch 427: 0.8976\n",
            "training loss at step 2 - batch 428: 0.36 (2019-08-04 13:35:00.996891)\n",
            "Accuracy at step 2 - batch 428: 0.894\n",
            "training loss at step 2 - batch 429: 0.35 (2019-08-04 13:35:01.010285)\n",
            "Accuracy at step 2 - batch 429: 0.896\n",
            "training loss at step 2 - batch 430: 0.33 (2019-08-04 13:35:01.023980)\n",
            "Accuracy at step 2 - batch 430: 0.904\n",
            "training loss at step 2 - batch 431: 0.36 (2019-08-04 13:35:01.146839)\n",
            "Accuracy at step 2 - batch 431: 0.8968\n",
            "training loss at step 2 - batch 432: 0.38 (2019-08-04 13:35:01.158930)\n",
            "Accuracy at step 2 - batch 432: 0.8896\n",
            "training loss at step 2 - batch 433: 0.37 (2019-08-04 13:35:01.173759)\n",
            "Accuracy at step 2 - batch 433: 0.8916\n",
            "training loss at step 2 - batch 434: 0.36 (2019-08-04 13:35:01.188183)\n",
            "Accuracy at step 2 - batch 434: 0.894\n",
            "training loss at step 2 - batch 435: 0.35 (2019-08-04 13:35:01.200554)\n",
            "Accuracy at step 2 - batch 435: 0.8972\n",
            "training loss at step 2 - batch 436: 0.35 (2019-08-04 13:35:01.320441)\n",
            "Accuracy at step 2 - batch 436: 0.9\n",
            "training loss at step 2 - batch 437: 0.37 (2019-08-04 13:35:01.337926)\n",
            "Accuracy at step 2 - batch 437: 0.8896\n",
            "training loss at step 2 - batch 438: 0.37 (2019-08-04 13:35:01.351870)\n",
            "Accuracy at step 2 - batch 438: 0.8952\n",
            "training loss at step 2 - batch 439: 0.37 (2019-08-04 13:35:01.364036)\n",
            "Accuracy at step 2 - batch 439: 0.8872\n",
            "training loss at step 2 - batch 440: 0.38 (2019-08-04 13:35:01.378540)\n",
            "Accuracy at step 2 - batch 440: 0.894\n",
            "training loss at step 2 - batch 441: 0.36 (2019-08-04 13:35:01.503394)\n",
            "Accuracy at step 2 - batch 441: 0.8916\n",
            "training loss at step 2 - batch 442: 0.36 (2019-08-04 13:35:01.518765)\n",
            "Accuracy at step 2 - batch 442: 0.8928\n",
            "training loss at step 2 - batch 443: 0.34 (2019-08-04 13:35:01.533443)\n",
            "Accuracy at step 2 - batch 443: 0.9008\n",
            "training loss at step 2 - batch 444: 0.32 (2019-08-04 13:35:01.545540)\n",
            "Accuracy at step 2 - batch 444: 0.9076\n",
            "training loss at step 2 - batch 445: 0.37 (2019-08-04 13:35:01.558387)\n",
            "Accuracy at step 2 - batch 445: 0.8956\n",
            "training loss at step 2 - batch 446: 0.37 (2019-08-04 13:35:01.685654)\n",
            "Accuracy at step 2 - batch 446: 0.8968\n",
            "training loss at step 2 - batch 447: 0.36 (2019-08-04 13:35:01.700616)\n",
            "Accuracy at step 2 - batch 447: 0.8924\n",
            "training loss at step 2 - batch 448: 0.35 (2019-08-04 13:35:01.713030)\n",
            "Accuracy at step 2 - batch 448: 0.8984\n",
            "training loss at step 2 - batch 449: 0.34 (2019-08-04 13:35:01.726183)\n",
            "Accuracy at step 2 - batch 449: 0.8988\n",
            "training loss at step 2 - batch 450: 0.36 (2019-08-04 13:35:01.739223)\n",
            "Accuracy at step 2 - batch 450: 0.8956\n",
            "training loss at step 2 - batch 451: 0.35 (2019-08-04 13:35:01.856436)\n",
            "Accuracy at step 2 - batch 451: 0.902\n",
            "training loss at step 2 - batch 452: 0.37 (2019-08-04 13:35:01.869713)\n",
            "Accuracy at step 2 - batch 452: 0.8944\n",
            "training loss at step 2 - batch 453: 0.35 (2019-08-04 13:35:01.882354)\n",
            "Accuracy at step 2 - batch 453: 0.8996\n",
            "training loss at step 2 - batch 454: 0.34 (2019-08-04 13:35:01.897787)\n",
            "Accuracy at step 2 - batch 454: 0.9008\n",
            "training loss at step 2 - batch 455: 0.34 (2019-08-04 13:35:01.914888)\n",
            "Accuracy at step 2 - batch 455: 0.9016\n",
            "training loss at step 2 - batch 456: 0.36 (2019-08-04 13:35:02.045228)\n",
            "Accuracy at step 2 - batch 456: 0.8984\n",
            "training loss at step 2 - batch 457: 0.37 (2019-08-04 13:35:02.057298)\n",
            "Accuracy at step 2 - batch 457: 0.8912\n",
            "training loss at step 2 - batch 458: 0.36 (2019-08-04 13:35:02.070391)\n",
            "Accuracy at step 2 - batch 458: 0.8944\n",
            "training loss at step 2 - batch 459: 0.37 (2019-08-04 13:35:02.083873)\n",
            "Accuracy at step 2 - batch 459: 0.8904\n",
            "training loss at step 2 - batch 460: 0.35 (2019-08-04 13:35:02.096368)\n",
            "Accuracy at step 2 - batch 460: 0.9008\n",
            "training loss at step 2 - batch 461: 0.37 (2019-08-04 13:35:02.216259)\n",
            "Accuracy at step 2 - batch 461: 0.8928\n",
            "training loss at step 2 - batch 462: 0.38 (2019-08-04 13:35:02.229336)\n",
            "Accuracy at step 2 - batch 462: 0.892\n",
            "training loss at step 2 - batch 463: 0.36 (2019-08-04 13:35:02.241693)\n",
            "Accuracy at step 2 - batch 463: 0.8976\n",
            "training loss at step 2 - batch 464: 0.36 (2019-08-04 13:35:02.254518)\n",
            "Accuracy at step 2 - batch 464: 0.896\n",
            "training loss at step 2 - batch 465: 0.38 (2019-08-04 13:35:02.266613)\n",
            "Accuracy at step 2 - batch 465: 0.8904\n",
            "training loss at step 2 - batch 466: 0.38 (2019-08-04 13:35:02.388958)\n",
            "Accuracy at step 2 - batch 466: 0.8848\n",
            "training loss at step 2 - batch 467: 0.34 (2019-08-04 13:35:02.403027)\n",
            "Accuracy at step 2 - batch 467: 0.9036\n",
            "training loss at step 2 - batch 468: 0.35 (2019-08-04 13:35:02.416262)\n",
            "Accuracy at step 2 - batch 468: 0.8932\n",
            "training loss at step 2 - batch 469: 0.40 (2019-08-04 13:35:02.433004)\n",
            "Accuracy at step 2 - batch 469: 0.882\n",
            "training loss at step 2 - batch 470: 0.35 (2019-08-04 13:35:02.445047)\n",
            "Accuracy at step 2 - batch 470: 0.904\n",
            "training loss at step 2 - batch 471: 0.35 (2019-08-04 13:35:02.567252)\n",
            "Accuracy at step 2 - batch 471: 0.8968\n",
            "training loss at step 2 - batch 472: 0.36 (2019-08-04 13:35:02.581008)\n",
            "Accuracy at step 2 - batch 472: 0.8924\n",
            "training loss at step 2 - batch 473: 0.34 (2019-08-04 13:35:02.593415)\n",
            "Accuracy at step 2 - batch 473: 0.9024\n",
            "training loss at step 2 - batch 474: 0.35 (2019-08-04 13:35:02.606170)\n",
            "Accuracy at step 2 - batch 474: 0.8952\n",
            "training loss at step 2 - batch 475: 0.34 (2019-08-04 13:35:02.619304)\n",
            "Accuracy at step 2 - batch 475: 0.904\n",
            "training loss at step 2 - batch 476: 0.36 (2019-08-04 13:35:02.744062)\n",
            "Accuracy at step 2 - batch 476: 0.898\n",
            "training loss at step 2 - batch 477: 0.36 (2019-08-04 13:35:02.758005)\n",
            "Accuracy at step 2 - batch 477: 0.8904\n",
            "training loss at step 2 - batch 478: 0.35 (2019-08-04 13:35:02.772363)\n",
            "Accuracy at step 2 - batch 478: 0.8988\n",
            "training loss at step 2 - batch 479: 0.35 (2019-08-04 13:35:02.785335)\n",
            "Accuracy at step 2 - batch 479: 0.8964\n",
            "training loss at step 2 - batch 480: 0.34 (2019-08-04 13:35:02.800120)\n",
            "Accuracy at step 2 - batch 480: 0.902\n",
            "training loss at step 2 - batch 481: 0.35 (2019-08-04 13:35:02.927740)\n",
            "Accuracy at step 2 - batch 481: 0.8984\n",
            "training loss at step 2 - batch 482: 0.35 (2019-08-04 13:35:02.942416)\n",
            "Accuracy at step 2 - batch 482: 0.8968\n",
            "training loss at step 2 - batch 483: 0.38 (2019-08-04 13:35:02.959860)\n",
            "Accuracy at step 2 - batch 483: 0.8936\n",
            "training loss at step 2 - batch 484: 0.36 (2019-08-04 13:35:02.971983)\n",
            "Accuracy at step 2 - batch 484: 0.9012\n",
            "training loss at step 2 - batch 485: 0.38 (2019-08-04 13:35:02.985917)\n",
            "Accuracy at step 2 - batch 485: 0.8904\n",
            "training loss at step 2 - batch 486: 0.36 (2019-08-04 13:35:03.108431)\n",
            "Accuracy at step 2 - batch 486: 0.8964\n",
            "training loss at step 2 - batch 487: 0.33 (2019-08-04 13:35:03.124314)\n",
            "Accuracy at step 2 - batch 487: 0.9052\n",
            "training loss at step 2 - batch 488: 0.35 (2019-08-04 13:35:03.137427)\n",
            "Accuracy at step 2 - batch 488: 0.8988\n",
            "training loss at step 2 - batch 489: 0.38 (2019-08-04 13:35:03.149671)\n",
            "Accuracy at step 2 - batch 489: 0.8868\n",
            "training loss at step 2 - batch 490: 0.36 (2019-08-04 13:35:03.164494)\n",
            "Accuracy at step 2 - batch 490: 0.898\n",
            "training loss at step 2 - batch 491: 0.36 (2019-08-04 13:35:03.284161)\n",
            "Accuracy at step 2 - batch 491: 0.8964\n",
            "training loss at step 2 - batch 492: 0.36 (2019-08-04 13:35:03.300432)\n",
            "Accuracy at step 2 - batch 492: 0.8952\n",
            "training loss at step 2 - batch 493: 0.37 (2019-08-04 13:35:03.314250)\n",
            "Accuracy at step 2 - batch 493: 0.8936\n",
            "training loss at step 2 - batch 494: 0.36 (2019-08-04 13:35:03.327238)\n",
            "Accuracy at step 2 - batch 494: 0.896\n",
            "training loss at step 2 - batch 495: 0.35 (2019-08-04 13:35:03.340581)\n",
            "Accuracy at step 2 - batch 495: 0.8952\n",
            "training loss at step 2 - batch 496: 0.36 (2019-08-04 13:35:03.463693)\n",
            "Accuracy at step 2 - batch 496: 0.8944\n",
            "training loss at step 2 - batch 497: 0.35 (2019-08-04 13:35:03.480158)\n",
            "Accuracy at step 2 - batch 497: 0.9004\n",
            "training loss at step 2 - batch 498: 0.36 (2019-08-04 13:35:03.492404)\n",
            "Accuracy at step 2 - batch 498: 0.896\n",
            "training loss at step 2 - batch 499: 0.37 (2019-08-04 13:35:03.505163)\n",
            "Accuracy at step 2 - batch 499: 0.8996\n",
            "training loss at step 2 - batch 500: 0.37 (2019-08-04 13:35:03.517907)\n",
            "Accuracy at step 2 - batch 500: 0.896\n",
            "training loss at step 2 - batch 501: 0.36 (2019-08-04 13:35:03.636019)\n",
            "Accuracy at step 2 - batch 501: 0.9028\n",
            "training loss at step 2 - batch 502: 0.38 (2019-08-04 13:35:03.649693)\n",
            "Accuracy at step 2 - batch 502: 0.8868\n",
            "training loss at step 2 - batch 503: 0.36 (2019-08-04 13:35:03.662033)\n",
            "Accuracy at step 2 - batch 503: 0.8964\n",
            "training loss at step 2 - batch 504: 0.34 (2019-08-04 13:35:03.677755)\n",
            "Accuracy at step 2 - batch 504: 0.9016\n",
            "training loss at step 2 - batch 505: 0.35 (2019-08-04 13:35:03.690688)\n",
            "Accuracy at step 2 - batch 505: 0.8972\n",
            "training loss at step 2 - batch 506: 0.35 (2019-08-04 13:35:03.817072)\n",
            "Accuracy at step 2 - batch 506: 0.9016\n",
            "training loss at step 2 - batch 507: 0.36 (2019-08-04 13:35:03.831482)\n",
            "Accuracy at step 2 - batch 507: 0.8984\n",
            "training loss at step 2 - batch 508: 0.37 (2019-08-04 13:35:03.843426)\n",
            "Accuracy at step 2 - batch 508: 0.8936\n",
            "training loss at step 2 - batch 509: 0.37 (2019-08-04 13:35:03.855894)\n",
            "Accuracy at step 2 - batch 509: 0.8908\n",
            "training loss at step 2 - batch 510: 0.37 (2019-08-04 13:35:03.868959)\n",
            "Accuracy at step 2 - batch 510: 0.8948\n",
            "training loss at step 2 - batch 511: 0.37 (2019-08-04 13:35:04.000286)\n",
            "Accuracy at step 2 - batch 511: 0.89\n",
            "training loss at step 2 - batch 512: 0.37 (2019-08-04 13:35:04.013332)\n",
            "Accuracy at step 2 - batch 512: 0.8988\n",
            "training loss at step 2 - batch 513: 0.37 (2019-08-04 13:35:04.026474)\n",
            "Accuracy at step 2 - batch 513: 0.8896\n",
            "training loss at step 2 - batch 514: 0.35 (2019-08-04 13:35:04.038335)\n",
            "Accuracy at step 2 - batch 514: 0.8932\n",
            "training loss at step 2 - batch 515: 0.35 (2019-08-04 13:35:04.050708)\n",
            "Accuracy at step 2 - batch 515: 0.8932\n",
            "training loss at step 2 - batch 516: 0.35 (2019-08-04 13:35:04.190579)\n",
            "Accuracy at step 2 - batch 516: 0.8992\n",
            "training loss at step 2 - batch 517: 0.36 (2019-08-04 13:35:04.205717)\n",
            "Accuracy at step 2 - batch 517: 0.8948\n",
            "training loss at step 2 - batch 518: 0.37 (2019-08-04 13:35:04.219215)\n",
            "Accuracy at step 2 - batch 518: 0.8908\n",
            "training loss at step 2 - batch 519: 0.34 (2019-08-04 13:35:04.232570)\n",
            "Accuracy at step 2 - batch 519: 0.9016\n",
            "training loss at step 2 - batch 520: 0.36 (2019-08-04 13:35:04.244424)\n",
            "Accuracy at step 2 - batch 520: 0.8904\n",
            "training loss at step 2 - batch 521: 0.37 (2019-08-04 13:35:04.366772)\n",
            "Accuracy at step 2 - batch 521: 0.8948\n",
            "training loss at step 2 - batch 522: 0.35 (2019-08-04 13:35:04.384654)\n",
            "Accuracy at step 2 - batch 522: 0.894\n",
            "training loss at step 2 - batch 523: 0.36 (2019-08-04 13:35:04.397140)\n",
            "Accuracy at step 2 - batch 523: 0.8956\n",
            "training loss at step 2 - batch 524: 0.36 (2019-08-04 13:35:04.410786)\n",
            "Accuracy at step 2 - batch 524: 0.896\n",
            "training loss at step 2 - batch 525: 0.35 (2019-08-04 13:35:04.427002)\n",
            "Accuracy at step 2 - batch 525: 0.8972\n",
            "training loss at step 2 - batch 526: 0.39 (2019-08-04 13:35:04.543214)\n",
            "Accuracy at step 2 - batch 526: 0.8896\n",
            "training loss at step 2 - batch 527: 0.34 (2019-08-04 13:35:04.557251)\n",
            "Accuracy at step 2 - batch 527: 0.9024\n",
            "training loss at step 2 - batch 528: 0.35 (2019-08-04 13:35:04.569630)\n",
            "Accuracy at step 2 - batch 528: 0.8916\n",
            "training loss at step 2 - batch 529: 0.32 (2019-08-04 13:35:04.582087)\n",
            "Accuracy at step 2 - batch 529: 0.9072\n",
            "training loss at step 2 - batch 530: 0.35 (2019-08-04 13:35:04.595459)\n",
            "Accuracy at step 2 - batch 530: 0.8936\n",
            "training loss at step 2 - batch 531: 0.37 (2019-08-04 13:35:04.713531)\n",
            "Accuracy at step 2 - batch 531: 0.8896\n",
            "training loss at step 2 - batch 532: 0.35 (2019-08-04 13:35:04.726487)\n",
            "Accuracy at step 2 - batch 532: 0.902\n",
            "training loss at step 2 - batch 533: 0.36 (2019-08-04 13:35:04.739974)\n",
            "Accuracy at step 2 - batch 533: 0.8912\n",
            "training loss at step 2 - batch 534: 0.35 (2019-08-04 13:35:04.752513)\n",
            "Accuracy at step 2 - batch 534: 0.892\n",
            "training loss at step 2 - batch 535: 0.36 (2019-08-04 13:35:04.764597)\n",
            "Accuracy at step 2 - batch 535: 0.8944\n",
            "training loss at step 2 - batch 536: 0.36 (2019-08-04 13:35:04.885340)\n",
            "Accuracy at step 2 - batch 536: 0.898\n",
            "training loss at step 2 - batch 537: 0.34 (2019-08-04 13:35:04.902968)\n",
            "Accuracy at step 2 - batch 537: 0.9008\n",
            "training loss at step 2 - batch 538: 0.34 (2019-08-04 13:35:04.918301)\n",
            "Accuracy at step 2 - batch 538: 0.8996\n",
            "training loss at step 2 - batch 539: 0.34 (2019-08-04 13:35:04.931076)\n",
            "Accuracy at step 2 - batch 539: 0.898\n",
            "training loss at step 2 - batch 540: 0.36 (2019-08-04 13:35:04.944977)\n",
            "Accuracy at step 2 - batch 540: 0.892\n",
            "training loss at step 2 - batch 541: 0.37 (2019-08-04 13:35:05.076385)\n",
            "Accuracy at step 2 - batch 541: 0.8932\n",
            "training loss at step 2 - batch 542: 0.36 (2019-08-04 13:35:05.089258)\n",
            "Accuracy at step 2 - batch 542: 0.8944\n",
            "training loss at step 2 - batch 543: 0.34 (2019-08-04 13:35:05.102359)\n",
            "Accuracy at step 2 - batch 543: 0.902\n",
            "training loss at step 2 - batch 544: 0.35 (2019-08-04 13:35:05.114451)\n",
            "Accuracy at step 2 - batch 544: 0.8924\n",
            "training loss at step 2 - batch 545: 0.34 (2019-08-04 13:35:05.130612)\n",
            "Accuracy at step 2 - batch 545: 0.9016\n",
            "training loss at step 2 - batch 546: 0.35 (2019-08-04 13:35:05.247680)\n",
            "Accuracy at step 2 - batch 546: 0.8976\n",
            "training loss at step 2 - batch 547: 0.34 (2019-08-04 13:35:05.260303)\n",
            "Accuracy at step 2 - batch 547: 0.8956\n",
            "training loss at step 2 - batch 548: 0.34 (2019-08-04 13:35:05.274061)\n",
            "Accuracy at step 2 - batch 548: 0.8956\n",
            "training loss at step 2 - batch 549: 0.36 (2019-08-04 13:35:05.286450)\n",
            "Accuracy at step 2 - batch 549: 0.896\n",
            "training loss at step 2 - batch 550: 0.36 (2019-08-04 13:35:05.298719)\n",
            "Accuracy at step 2 - batch 550: 0.8976\n",
            "training loss at step 2 - batch 551: 0.38 (2019-08-04 13:35:05.447774)\n",
            "Accuracy at step 2 - batch 551: 0.8904\n",
            "training loss at step 2 - batch 552: 0.36 (2019-08-04 13:35:05.462820)\n",
            "Accuracy at step 2 - batch 552: 0.8944\n",
            "training loss at step 2 - batch 553: 0.36 (2019-08-04 13:35:05.475737)\n",
            "Accuracy at step 2 - batch 553: 0.89\n",
            "training loss at step 2 - batch 554: 0.36 (2019-08-04 13:35:05.488254)\n",
            "Accuracy at step 2 - batch 554: 0.894\n",
            "training loss at step 2 - batch 555: 0.36 (2019-08-04 13:35:05.501042)\n",
            "Accuracy at step 2 - batch 555: 0.8912\n",
            "training loss at step 2 - batch 556: 0.33 (2019-08-04 13:35:05.621067)\n",
            "Accuracy at step 2 - batch 556: 0.9\n",
            "training loss at step 2 - batch 557: 0.36 (2019-08-04 13:35:05.638665)\n",
            "Accuracy at step 2 - batch 557: 0.8948\n",
            "training loss at step 2 - batch 558: 0.35 (2019-08-04 13:35:05.653657)\n",
            "Accuracy at step 2 - batch 558: 0.8972\n",
            "training loss at step 2 - batch 559: 0.34 (2019-08-04 13:35:05.666272)\n",
            "Accuracy at step 2 - batch 559: 0.8968\n",
            "training loss at step 2 - batch 560: 0.39 (2019-08-04 13:35:05.679451)\n",
            "Accuracy at step 2 - batch 560: 0.884\n",
            "training loss at step 2 - batch 561: 0.35 (2019-08-04 13:35:05.799160)\n",
            "Accuracy at step 2 - batch 561: 0.904\n",
            "training loss at step 2 - batch 562: 0.36 (2019-08-04 13:35:05.813621)\n",
            "Accuracy at step 2 - batch 562: 0.8948\n",
            "training loss at step 2 - batch 563: 0.36 (2019-08-04 13:35:05.825865)\n",
            "Accuracy at step 2 - batch 563: 0.8976\n",
            "training loss at step 2 - batch 564: 0.37 (2019-08-04 13:35:05.837854)\n",
            "Accuracy at step 2 - batch 564: 0.8932\n",
            "training loss at step 2 - batch 565: 0.34 (2019-08-04 13:35:05.850490)\n",
            "Accuracy at step 2 - batch 565: 0.8972\n",
            "training loss at step 2 - batch 566: 0.32 (2019-08-04 13:35:05.973931)\n",
            "Accuracy at step 2 - batch 566: 0.9084\n",
            "training loss at step 2 - batch 567: 0.34 (2019-08-04 13:35:05.990482)\n",
            "Accuracy at step 2 - batch 567: 0.9012\n",
            "training loss at step 2 - batch 568: 0.36 (2019-08-04 13:35:06.006371)\n",
            "Accuracy at step 2 - batch 568: 0.894\n",
            "training loss at step 2 - batch 569: 0.33 (2019-08-04 13:35:06.019145)\n",
            "Accuracy at step 2 - batch 569: 0.9004\n",
            "training loss at step 2 - batch 570: 0.35 (2019-08-04 13:35:06.032793)\n",
            "Accuracy at step 2 - batch 570: 0.8956\n",
            "training loss at step 2 - batch 571: 0.35 (2019-08-04 13:35:06.159967)\n",
            "Accuracy at step 2 - batch 571: 0.8948\n",
            "training loss at step 2 - batch 572: 0.34 (2019-08-04 13:35:06.179073)\n",
            "Accuracy at step 2 - batch 572: 0.8972\n",
            "training loss at step 2 - batch 573: 0.37 (2019-08-04 13:35:06.192094)\n",
            "Accuracy at step 2 - batch 573: 0.8956\n",
            "training loss at step 2 - batch 574: 0.32 (2019-08-04 13:35:06.204617)\n",
            "Accuracy at step 2 - batch 574: 0.9068\n",
            "training loss at step 2 - batch 575: 0.36 (2019-08-04 13:35:06.216908)\n",
            "Accuracy at step 2 - batch 575: 0.8952\n",
            "training loss at step 2 - batch 576: 0.37 (2019-08-04 13:35:06.338228)\n",
            "Accuracy at step 2 - batch 576: 0.8916\n",
            "training loss at step 2 - batch 577: 0.34 (2019-08-04 13:35:06.354646)\n",
            "Accuracy at step 2 - batch 577: 0.9036\n",
            "training loss at step 2 - batch 578: 0.37 (2019-08-04 13:35:06.367066)\n",
            "Accuracy at step 2 - batch 578: 0.896\n",
            "training loss at step 2 - batch 579: 0.36 (2019-08-04 13:35:06.380976)\n",
            "Accuracy at step 2 - batch 579: 0.8972\n",
            "training loss at step 2 - batch 580: 0.36 (2019-08-04 13:35:06.396860)\n",
            "Accuracy at step 2 - batch 580: 0.8996\n",
            "training loss at step 2 - batch 581: 0.35 (2019-08-04 13:35:06.516512)\n",
            "Accuracy at step 2 - batch 581: 0.8972\n",
            "training loss at step 2 - batch 582: 0.37 (2019-08-04 13:35:06.530428)\n",
            "Accuracy at step 2 - batch 582: 0.8944\n",
            "training loss at step 2 - batch 583: 0.34 (2019-08-04 13:35:06.543784)\n",
            "Accuracy at step 2 - batch 583: 0.9008\n",
            "training loss at step 2 - batch 584: 0.38 (2019-08-04 13:35:06.556180)\n",
            "Accuracy at step 2 - batch 584: 0.8936\n",
            "training loss at step 2 - batch 585: 0.36 (2019-08-04 13:35:06.569250)\n",
            "Accuracy at step 2 - batch 585: 0.8968\n",
            "training loss at step 2 - batch 586: 0.37 (2019-08-04 13:35:06.690475)\n",
            "Accuracy at step 2 - batch 586: 0.8908\n",
            "training loss at step 2 - batch 587: 0.37 (2019-08-04 13:35:06.706827)\n",
            "Accuracy at step 2 - batch 587: 0.898\n",
            "training loss at step 2 - batch 588: 0.36 (2019-08-04 13:35:06.719789)\n",
            "Accuracy at step 2 - batch 588: 0.8932\n",
            "training loss at step 2 - batch 589: 0.36 (2019-08-04 13:35:06.732226)\n",
            "Accuracy at step 2 - batch 589: 0.8972\n",
            "training loss at step 2 - batch 590: 0.34 (2019-08-04 13:35:06.746632)\n",
            "Accuracy at step 2 - batch 590: 0.9032\n",
            "training loss at step 2 - batch 591: 0.36 (2019-08-04 13:35:06.862823)\n",
            "Accuracy at step 2 - batch 591: 0.8964\n",
            "training loss at step 2 - batch 592: 0.36 (2019-08-04 13:35:06.879193)\n",
            "Accuracy at step 2 - batch 592: 0.8944\n",
            "training loss at step 2 - batch 593: 0.35 (2019-08-04 13:35:06.892967)\n",
            "Accuracy at step 2 - batch 593: 0.8908\n",
            "training loss at step 2 - batch 594: 0.35 (2019-08-04 13:35:06.911249)\n",
            "Accuracy at step 2 - batch 594: 0.898\n",
            "training loss at step 2 - batch 595: 0.35 (2019-08-04 13:35:06.923387)\n",
            "Accuracy at step 2 - batch 595: 0.898\n",
            "training loss at step 2 - batch 596: 0.35 (2019-08-04 13:35:07.055170)\n",
            "Accuracy at step 2 - batch 596: 0.8984\n",
            "training loss at step 2 - batch 597: 0.35 (2019-08-04 13:35:07.068497)\n",
            "Accuracy at step 2 - batch 597: 0.898\n",
            "training loss at step 2 - batch 598: 0.37 (2019-08-04 13:35:07.080620)\n",
            "Accuracy at step 2 - batch 598: 0.8884\n",
            "training loss at step 2 - batch 599: 0.39 (2019-08-04 13:35:07.095090)\n",
            "Accuracy at step 2 - batch 599: 0.8836\n",
            "training loss at step 2 - batch 600: 0.35 (2019-08-04 13:35:07.109266)\n",
            "Accuracy at step 2 - batch 600: 0.9016\n",
            "training loss at step 2 - batch 601: 0.35 (2019-08-04 13:35:07.234696)\n",
            "Accuracy at step 2 - batch 601: 0.896\n",
            "training loss at step 2 - batch 602: 0.35 (2019-08-04 13:35:07.251346)\n",
            "Accuracy at step 2 - batch 602: 0.9\n",
            "training loss at step 2 - batch 603: 0.34 (2019-08-04 13:35:07.263173)\n",
            "Accuracy at step 2 - batch 603: 0.8976\n",
            "training loss at step 2 - batch 604: 0.37 (2019-08-04 13:35:07.275561)\n",
            "Accuracy at step 2 - batch 604: 0.896\n",
            "training loss at step 2 - batch 605: 0.37 (2019-08-04 13:35:07.287787)\n",
            "Accuracy at step 2 - batch 605: 0.8948\n",
            "training loss at step 2 - batch 606: 0.35 (2019-08-04 13:35:07.407507)\n",
            "Accuracy at step 2 - batch 606: 0.8972\n",
            "training loss at step 2 - batch 607: 0.38 (2019-08-04 13:35:07.420582)\n",
            "Accuracy at step 2 - batch 607: 0.8916\n",
            "training loss at step 2 - batch 608: 0.35 (2019-08-04 13:35:07.434008)\n",
            "Accuracy at step 2 - batch 608: 0.8924\n",
            "training loss at step 2 - batch 609: 0.35 (2019-08-04 13:35:07.447399)\n",
            "Accuracy at step 2 - batch 609: 0.8984\n",
            "training loss at step 2 - batch 610: 0.36 (2019-08-04 13:35:07.461214)\n",
            "Accuracy at step 2 - batch 610: 0.8984\n",
            "training loss at step 2 - batch 611: 0.35 (2019-08-04 13:35:07.582117)\n",
            "Accuracy at step 2 - batch 611: 0.8988\n",
            "training loss at step 2 - batch 612: 0.36 (2019-08-04 13:35:07.599031)\n",
            "Accuracy at step 2 - batch 612: 0.896\n",
            "training loss at step 2 - batch 613: 0.36 (2019-08-04 13:35:07.613947)\n",
            "Accuracy at step 2 - batch 613: 0.8924\n",
            "training loss at step 2 - batch 614: 0.34 (2019-08-04 13:35:07.628167)\n",
            "Accuracy at step 2 - batch 614: 0.896\n",
            "training loss at step 2 - batch 615: 0.35 (2019-08-04 13:35:07.640345)\n",
            "Accuracy at step 2 - batch 615: 0.8988\n",
            "training loss at step 2 - batch 616: 0.37 (2019-08-04 13:35:07.758520)\n",
            "Accuracy at step 2 - batch 616: 0.8956\n",
            "training loss at step 2 - batch 617: 0.39 (2019-08-04 13:35:07.774648)\n",
            "Accuracy at step 2 - batch 617: 0.8908\n",
            "training loss at step 2 - batch 618: 0.35 (2019-08-04 13:35:07.787402)\n",
            "Accuracy at step 2 - batch 618: 0.8968\n",
            "training loss at step 2 - batch 619: 0.36 (2019-08-04 13:35:07.800429)\n",
            "Accuracy at step 2 - batch 619: 0.8984\n",
            "training loss at step 2 - batch 620: 0.36 (2019-08-04 13:35:07.813055)\n",
            "Accuracy at step 2 - batch 620: 0.8956\n",
            "training loss at step 2 - batch 621: 0.35 (2019-08-04 13:35:07.933946)\n",
            "Accuracy at step 2 - batch 621: 0.8952\n",
            "training loss at step 2 - batch 622: 0.34 (2019-08-04 13:35:07.946720)\n",
            "Accuracy at step 2 - batch 622: 0.8992\n",
            "training loss at step 2 - batch 623: 0.36 (2019-08-04 13:35:07.960854)\n",
            "Accuracy at step 2 - batch 623: 0.8868\n",
            "training loss at step 2 - batch 624: 0.34 (2019-08-04 13:35:07.973271)\n",
            "Accuracy at step 2 - batch 624: 0.8984\n",
            "training loss at step 2 - batch 625: 0.34 (2019-08-04 13:35:07.987718)\n",
            "Accuracy at step 2 - batch 625: 0.9028\n",
            "training loss at step 2 - batch 626: 0.37 (2019-08-04 13:35:08.113718)\n",
            "Accuracy at step 2 - batch 626: 0.894\n",
            "training loss at step 2 - batch 627: 0.36 (2019-08-04 13:35:08.127261)\n",
            "Accuracy at step 2 - batch 627: 0.8976\n",
            "training loss at step 2 - batch 628: 0.36 (2019-08-04 13:35:08.143918)\n",
            "Accuracy at step 2 - batch 628: 0.8984\n",
            "training loss at step 2 - batch 629: 0.36 (2019-08-04 13:35:08.156583)\n",
            "Accuracy at step 2 - batch 629: 0.892\n",
            "training loss at step 2 - batch 630: 0.35 (2019-08-04 13:35:08.169231)\n",
            "Accuracy at step 2 - batch 630: 0.8988\n",
            "training loss at step 2 - batch 631: 0.35 (2019-08-04 13:35:08.288958)\n",
            "Accuracy at step 2 - batch 631: 0.8972\n",
            "training loss at step 2 - batch 632: 0.39 (2019-08-04 13:35:08.301295)\n",
            "Accuracy at step 2 - batch 632: 0.8896\n",
            "training loss at step 2 - batch 633: 0.37 (2019-08-04 13:35:08.314051)\n",
            "Accuracy at step 2 - batch 633: 0.892\n",
            "training loss at step 2 - batch 634: 0.34 (2019-08-04 13:35:08.329214)\n",
            "Accuracy at step 2 - batch 634: 0.9032\n",
            "training loss at step 2 - batch 635: 0.35 (2019-08-04 13:35:08.342497)\n",
            "Accuracy at step 2 - batch 635: 0.8984\n",
            "training loss at step 2 - batch 636: 0.35 (2019-08-04 13:35:08.465067)\n",
            "Accuracy at step 2 - batch 636: 0.8988\n",
            "training loss at step 2 - batch 637: 0.37 (2019-08-04 13:35:08.479100)\n",
            "Accuracy at step 2 - batch 637: 0.8932\n",
            "training loss at step 2 - batch 638: 0.34 (2019-08-04 13:35:08.492237)\n",
            "Accuracy at step 2 - batch 638: 0.902\n",
            "training loss at step 2 - batch 639: 0.37 (2019-08-04 13:35:08.504900)\n",
            "Accuracy at step 2 - batch 639: 0.898\n",
            "training loss at step 2 - batch 640: 0.34 (2019-08-04 13:35:08.518153)\n",
            "Accuracy at step 2 - batch 640: 0.9\n",
            "training loss at step 2 - batch 641: 0.34 (2019-08-04 13:35:08.641087)\n",
            "Accuracy at step 2 - batch 641: 0.9028\n",
            "training loss at step 2 - batch 642: 0.35 (2019-08-04 13:35:08.657734)\n",
            "Accuracy at step 2 - batch 642: 0.896\n",
            "training loss at step 2 - batch 643: 0.37 (2019-08-04 13:35:08.672289)\n",
            "Accuracy at step 2 - batch 643: 0.8892\n",
            "training loss at step 2 - batch 644: 0.35 (2019-08-04 13:35:08.685855)\n",
            "Accuracy at step 2 - batch 644: 0.902\n",
            "training loss at step 2 - batch 645: 0.37 (2019-08-04 13:35:08.700096)\n",
            "Accuracy at step 2 - batch 645: 0.8904\n",
            "training loss at step 2 - batch 646: 0.36 (2019-08-04 13:35:08.823098)\n",
            "Accuracy at step 2 - batch 646: 0.894\n",
            "training loss at step 2 - batch 647: 0.36 (2019-08-04 13:35:08.836822)\n",
            "Accuracy at step 2 - batch 647: 0.8948\n",
            "training loss at step 2 - batch 648: 0.37 (2019-08-04 13:35:08.849417)\n",
            "Accuracy at step 2 - batch 648: 0.8916\n",
            "training loss at step 2 - batch 649: 0.34 (2019-08-04 13:35:08.861621)\n",
            "Accuracy at step 2 - batch 649: 0.8992\n",
            "training loss at step 2 - batch 650: 0.35 (2019-08-04 13:35:08.876745)\n",
            "Accuracy at step 2 - batch 650: 0.9016\n",
            "training loss at step 2 - batch 651: 0.36 (2019-08-04 13:35:08.996901)\n",
            "Accuracy at step 2 - batch 651: 0.8936\n",
            "training loss at step 2 - batch 652: 0.34 (2019-08-04 13:35:09.010277)\n",
            "Accuracy at step 2 - batch 652: 0.8988\n",
            "training loss at step 2 - batch 653: 0.36 (2019-08-04 13:35:09.024088)\n",
            "Accuracy at step 2 - batch 653: 0.8936\n",
            "training loss at step 2 - batch 654: 0.35 (2019-08-04 13:35:09.036914)\n",
            "Accuracy at step 2 - batch 654: 0.8952\n",
            "training loss at step 2 - batch 655: 0.37 (2019-08-04 13:35:09.051027)\n",
            "Accuracy at step 2 - batch 655: 0.894\n",
            "training loss at step 2 - batch 656: 0.34 (2019-08-04 13:35:09.186310)\n",
            "Accuracy at step 2 - batch 656: 0.8992\n",
            "training loss at step 2 - batch 657: 0.34 (2019-08-04 13:35:09.200363)\n",
            "Accuracy at step 2 - batch 657: 0.8964\n",
            "training loss at step 2 - batch 658: 0.35 (2019-08-04 13:35:09.212436)\n",
            "Accuracy at step 2 - batch 658: 0.8992\n",
            "training loss at step 2 - batch 659: 0.34 (2019-08-04 13:35:09.224184)\n",
            "Accuracy at step 2 - batch 659: 0.8964\n",
            "training loss at step 2 - batch 660: 0.34 (2019-08-04 13:35:09.237159)\n",
            "Accuracy at step 2 - batch 660: 0.9012\n",
            "training loss at step 2 - batch 661: 0.34 (2019-08-04 13:35:09.365492)\n",
            "Accuracy at step 2 - batch 661: 0.9056\n",
            "training loss at step 2 - batch 662: 0.35 (2019-08-04 13:35:09.378424)\n",
            "Accuracy at step 2 - batch 662: 0.8944\n",
            "training loss at step 2 - batch 663: 0.36 (2019-08-04 13:35:09.392678)\n",
            "Accuracy at step 2 - batch 663: 0.8956\n",
            "training loss at step 2 - batch 664: 0.33 (2019-08-04 13:35:09.405315)\n",
            "Accuracy at step 2 - batch 664: 0.898\n",
            "training loss at step 2 - batch 665: 0.36 (2019-08-04 13:35:09.418888)\n",
            "Accuracy at step 2 - batch 665: 0.8908\n",
            "training loss at step 2 - batch 666: 0.38 (2019-08-04 13:35:09.533472)\n",
            "Accuracy at step 2 - batch 666: 0.8864\n",
            "training loss at step 2 - batch 667: 0.34 (2019-08-04 13:35:09.545940)\n",
            "Accuracy at step 2 - batch 667: 0.9008\n",
            "training loss at step 2 - batch 668: 0.34 (2019-08-04 13:35:09.558836)\n",
            "Accuracy at step 2 - batch 668: 0.8968\n",
            "training loss at step 2 - batch 669: 0.36 (2019-08-04 13:35:09.572421)\n",
            "Accuracy at step 2 - batch 669: 0.896\n",
            "training loss at step 2 - batch 670: 0.38 (2019-08-04 13:35:09.585129)\n",
            "Accuracy at step 2 - batch 670: 0.8876\n",
            "training loss at step 2 - batch 671: 0.35 (2019-08-04 13:35:09.713486)\n",
            "Accuracy at step 2 - batch 671: 0.8928\n",
            "training loss at step 2 - batch 672: 0.34 (2019-08-04 13:35:09.728303)\n",
            "Accuracy at step 2 - batch 672: 0.9024\n",
            "training loss at step 2 - batch 673: 0.36 (2019-08-04 13:35:09.740965)\n",
            "Accuracy at step 2 - batch 673: 0.8964\n",
            "training loss at step 2 - batch 674: 0.34 (2019-08-04 13:35:09.753133)\n",
            "Accuracy at step 2 - batch 674: 0.898\n",
            "training loss at step 2 - batch 675: 0.33 (2019-08-04 13:35:09.766501)\n",
            "Accuracy at step 2 - batch 675: 0.9016\n",
            "training loss at step 2 - batch 676: 0.35 (2019-08-04 13:35:09.885016)\n",
            "Accuracy at step 2 - batch 676: 0.8952\n",
            "training loss at step 2 - batch 677: 0.35 (2019-08-04 13:35:09.898313)\n",
            "Accuracy at step 2 - batch 677: 0.904\n",
            "training loss at step 2 - batch 678: 0.36 (2019-08-04 13:35:09.911048)\n",
            "Accuracy at step 2 - batch 678: 0.8996\n",
            "training loss at step 2 - batch 679: 0.35 (2019-08-04 13:35:09.926211)\n",
            "Accuracy at step 2 - batch 679: 0.9024\n",
            "training loss at step 2 - batch 680: 0.36 (2019-08-04 13:35:09.938308)\n",
            "Accuracy at step 2 - batch 680: 0.9016\n",
            "training loss at step 2 - batch 681: 0.34 (2019-08-04 13:35:10.056866)\n",
            "Accuracy at step 2 - batch 681: 0.9012\n",
            "training loss at step 2 - batch 682: 0.36 (2019-08-04 13:35:10.075853)\n",
            "Accuracy at step 2 - batch 682: 0.8944\n",
            "training loss at step 2 - batch 683: 0.37 (2019-08-04 13:35:10.092370)\n",
            "Accuracy at step 2 - batch 683: 0.8948\n",
            "training loss at step 2 - batch 684: 0.37 (2019-08-04 13:35:10.106025)\n",
            "Accuracy at step 2 - batch 684: 0.8948\n",
            "training loss at step 2 - batch 685: 0.37 (2019-08-04 13:35:10.119149)\n",
            "Accuracy at step 2 - batch 685: 0.896\n",
            "training loss at step 2 - batch 686: 0.37 (2019-08-04 13:35:10.246143)\n",
            "Accuracy at step 2 - batch 686: 0.8956\n",
            "training loss at step 2 - batch 687: 0.38 (2019-08-04 13:35:10.262928)\n",
            "Accuracy at step 2 - batch 687: 0.8876\n",
            "training loss at step 2 - batch 688: 0.36 (2019-08-04 13:35:10.275976)\n",
            "Accuracy at step 2 - batch 688: 0.896\n",
            "training loss at step 2 - batch 689: 0.35 (2019-08-04 13:35:10.289586)\n",
            "Accuracy at step 2 - batch 689: 0.8968\n",
            "training loss at step 2 - batch 690: 0.37 (2019-08-04 13:35:10.302457)\n",
            "Accuracy at step 2 - batch 690: 0.8888\n",
            "training loss at step 2 - batch 691: 0.33 (2019-08-04 13:35:10.441988)\n",
            "Accuracy at step 2 - batch 691: 0.9012\n",
            "training loss at step 2 - batch 692: 0.36 (2019-08-04 13:35:10.458756)\n",
            "Accuracy at step 2 - batch 692: 0.8968\n",
            "training loss at step 2 - batch 693: 0.33 (2019-08-04 13:35:10.470862)\n",
            "Accuracy at step 2 - batch 693: 0.9016\n",
            "training loss at step 2 - batch 694: 0.36 (2019-08-04 13:35:10.485360)\n",
            "Accuracy at step 2 - batch 694: 0.8968\n",
            "training loss at step 2 - batch 695: 0.37 (2019-08-04 13:35:10.499360)\n",
            "Accuracy at step 2 - batch 695: 0.8964\n",
            "training loss at step 2 - batch 696: 0.33 (2019-08-04 13:35:10.624278)\n",
            "Accuracy at step 2 - batch 696: 0.9072\n",
            "training loss at step 2 - batch 697: 0.36 (2019-08-04 13:35:10.637994)\n",
            "Accuracy at step 2 - batch 697: 0.8948\n",
            "training loss at step 2 - batch 698: 0.34 (2019-08-04 13:35:10.650904)\n",
            "Accuracy at step 2 - batch 698: 0.9004\n",
            "training loss at step 2 - batch 699: 0.34 (2019-08-04 13:35:10.665270)\n",
            "Accuracy at step 2 - batch 699: 0.8948\n",
            "training loss at step 2 - batch 700: 0.37 (2019-08-04 13:35:10.678913)\n",
            "Accuracy at step 2 - batch 700: 0.8904\n",
            "training loss at step 2 - batch 701: 0.33 (2019-08-04 13:35:10.795058)\n",
            "Accuracy at step 2 - batch 701: 0.9024\n",
            "training loss at step 2 - batch 702: 0.36 (2019-08-04 13:35:10.808834)\n",
            "Accuracy at step 2 - batch 702: 0.894\n",
            "training loss at step 2 - batch 703: 0.37 (2019-08-04 13:35:10.823149)\n",
            "Accuracy at step 2 - batch 703: 0.8892\n",
            "training loss at step 2 - batch 704: 0.34 (2019-08-04 13:35:10.836125)\n",
            "Accuracy at step 2 - batch 704: 0.8988\n",
            "training loss at step 2 - batch 705: 0.35 (2019-08-04 13:35:10.850340)\n",
            "Accuracy at step 2 - batch 705: 0.8984\n",
            "training loss at step 2 - batch 706: 0.34 (2019-08-04 13:35:10.978454)\n",
            "Accuracy at step 2 - batch 706: 0.9056\n",
            "training loss at step 2 - batch 707: 0.36 (2019-08-04 13:35:10.992445)\n",
            "Accuracy at step 2 - batch 707: 0.8904\n",
            "training loss at step 2 - batch 708: 0.35 (2019-08-04 13:35:11.004917)\n",
            "Accuracy at step 2 - batch 708: 0.8948\n",
            "training loss at step 2 - batch 709: 0.36 (2019-08-04 13:35:11.017305)\n",
            "Accuracy at step 2 - batch 709: 0.8988\n",
            "training loss at step 2 - batch 710: 0.36 (2019-08-04 13:35:11.030059)\n",
            "Accuracy at step 2 - batch 710: 0.8972\n",
            "training loss at step 2 - batch 711: 0.37 (2019-08-04 13:35:11.153895)\n",
            "Accuracy at step 2 - batch 711: 0.8952\n",
            "training loss at step 2 - batch 712: 0.37 (2019-08-04 13:35:11.167392)\n",
            "Accuracy at step 2 - batch 712: 0.894\n",
            "training loss at step 2 - batch 713: 0.36 (2019-08-04 13:35:11.183162)\n",
            "Accuracy at step 2 - batch 713: 0.8984\n",
            "training loss at step 2 - batch 714: 0.35 (2019-08-04 13:35:11.197407)\n",
            "Accuracy at step 2 - batch 714: 0.9032\n",
            "training loss at step 2 - batch 715: 0.34 (2019-08-04 13:35:11.209394)\n",
            "Accuracy at step 2 - batch 715: 0.9016\n",
            "training loss at step 2 - batch 716: 0.37 (2019-08-04 13:35:11.331821)\n",
            "Accuracy at step 2 - batch 716: 0.8936\n",
            "training loss at step 2 - batch 717: 0.36 (2019-08-04 13:35:11.345993)\n",
            "Accuracy at step 2 - batch 717: 0.8912\n",
            "training loss at step 2 - batch 718: 0.33 (2019-08-04 13:35:11.360046)\n",
            "Accuracy at step 2 - batch 718: 0.9032\n",
            "training loss at step 2 - batch 719: 0.35 (2019-08-04 13:35:11.372352)\n",
            "Accuracy at step 2 - batch 719: 0.8936\n",
            "training loss at step 2 - batch 720: 0.32 (2019-08-04 13:35:11.388133)\n",
            "Accuracy at step 2 - batch 720: 0.9088\n",
            "training loss at step 2 - batch 721: 0.36 (2019-08-04 13:35:11.503300)\n",
            "Accuracy at step 2 - batch 721: 0.894\n",
            "training loss at step 2 - batch 722: 0.34 (2019-08-04 13:35:11.515867)\n",
            "Accuracy at step 2 - batch 722: 0.898\n",
            "training loss at step 2 - batch 723: 0.34 (2019-08-04 13:35:11.529061)\n",
            "Accuracy at step 2 - batch 723: 0.9\n",
            "training loss at step 2 - batch 724: 0.34 (2019-08-04 13:35:11.543424)\n",
            "Accuracy at step 2 - batch 724: 0.898\n",
            "training loss at step 2 - batch 725: 0.37 (2019-08-04 13:35:11.556764)\n",
            "Accuracy at step 2 - batch 725: 0.896\n",
            "training loss at step 2 - batch 726: 0.33 (2019-08-04 13:35:11.680646)\n",
            "Accuracy at step 2 - batch 726: 0.9036\n",
            "training loss at step 2 - batch 727: 0.37 (2019-08-04 13:35:11.697263)\n",
            "Accuracy at step 2 - batch 727: 0.8912\n",
            "training loss at step 2 - batch 728: 0.36 (2019-08-04 13:35:11.710432)\n",
            "Accuracy at step 2 - batch 728: 0.89\n",
            "training loss at step 2 - batch 729: 0.36 (2019-08-04 13:35:11.724713)\n",
            "Accuracy at step 2 - batch 729: 0.8972\n",
            "training loss at step 2 - batch 730: 0.36 (2019-08-04 13:35:11.737217)\n",
            "Accuracy at step 2 - batch 730: 0.8916\n",
            "training loss at step 2 - batch 731: 0.36 (2019-08-04 13:35:11.859409)\n",
            "Accuracy at step 2 - batch 731: 0.8912\n",
            "training loss at step 2 - batch 732: 0.38 (2019-08-04 13:35:11.872013)\n",
            "Accuracy at step 2 - batch 732: 0.888\n",
            "training loss at step 2 - batch 733: 0.36 (2019-08-04 13:35:11.887056)\n",
            "Accuracy at step 2 - batch 733: 0.9008\n",
            "training loss at step 2 - batch 734: 0.34 (2019-08-04 13:35:11.901425)\n",
            "Accuracy at step 2 - batch 734: 0.8956\n",
            "training loss at step 2 - batch 735: 0.35 (2019-08-04 13:35:11.914919)\n",
            "Accuracy at step 2 - batch 735: 0.8996\n",
            "training loss at step 2 - batch 736: 0.36 (2019-08-04 13:35:12.032916)\n",
            "Accuracy at step 2 - batch 736: 0.8956\n",
            "training loss at step 2 - batch 737: 0.36 (2019-08-04 13:35:12.048310)\n",
            "Accuracy at step 2 - batch 737: 0.8932\n",
            "training loss at step 2 - batch 738: 0.34 (2019-08-04 13:35:12.060328)\n",
            "Accuracy at step 2 - batch 738: 0.8952\n",
            "training loss at step 2 - batch 739: 0.35 (2019-08-04 13:35:12.076651)\n",
            "Accuracy at step 2 - batch 739: 0.8936\n",
            "training loss at step 2 - batch 740: 0.36 (2019-08-04 13:35:12.092840)\n",
            "Accuracy at step 2 - batch 740: 0.8964\n",
            "training loss at step 2 - batch 741: 0.32 (2019-08-04 13:35:12.221873)\n",
            "Accuracy at step 2 - batch 741: 0.9032\n",
            "training loss at step 2 - batch 742: 0.35 (2019-08-04 13:35:12.235223)\n",
            "Accuracy at step 2 - batch 742: 0.9004\n",
            "training loss at step 2 - batch 743: 0.35 (2019-08-04 13:35:12.248434)\n",
            "Accuracy at step 2 - batch 743: 0.8992\n",
            "training loss at step 2 - batch 744: 0.37 (2019-08-04 13:35:12.261037)\n",
            "Accuracy at step 2 - batch 744: 0.8896\n",
            "training loss at step 2 - batch 745: 0.34 (2019-08-04 13:35:12.274994)\n",
            "Accuracy at step 2 - batch 745: 0.8984\n",
            "training loss at step 2 - batch 746: 0.35 (2019-08-04 13:35:12.401043)\n",
            "Accuracy at step 2 - batch 746: 0.8952\n",
            "training loss at step 2 - batch 747: 0.35 (2019-08-04 13:35:12.413170)\n",
            "Accuracy at step 2 - batch 747: 0.8928\n",
            "training loss at step 2 - batch 748: 0.35 (2019-08-04 13:35:12.426250)\n",
            "Accuracy at step 2 - batch 748: 0.8992\n",
            "training loss at step 2 - batch 749: 0.38 (2019-08-04 13:35:12.441904)\n",
            "Accuracy at step 2 - batch 749: 0.8964\n",
            "training loss at step 2 - batch 750: 0.33 (2019-08-04 13:35:12.454429)\n",
            "Accuracy at step 2 - batch 750: 0.904\n",
            "training loss at step 2 - batch 751: 0.35 (2019-08-04 13:35:12.580332)\n",
            "Accuracy at step 2 - batch 751: 0.9\n",
            "training loss at step 2 - batch 752: 0.34 (2019-08-04 13:35:12.596941)\n",
            "Accuracy at step 2 - batch 752: 0.8972\n",
            "training loss at step 2 - batch 753: 0.35 (2019-08-04 13:35:12.612437)\n",
            "Accuracy at step 2 - batch 753: 0.8988\n",
            "training loss at step 2 - batch 754: 0.35 (2019-08-04 13:35:12.624990)\n",
            "Accuracy at step 2 - batch 754: 0.8988\n",
            "training loss at step 2 - batch 755: 0.34 (2019-08-04 13:35:12.638261)\n",
            "Accuracy at step 2 - batch 755: 0.9028\n",
            "training loss at step 2 - batch 756: 0.37 (2019-08-04 13:35:12.754563)\n",
            "Accuracy at step 2 - batch 756: 0.8916\n",
            "training loss at step 2 - batch 757: 0.37 (2019-08-04 13:35:12.768561)\n",
            "Accuracy at step 2 - batch 757: 0.8904\n",
            "training loss at step 2 - batch 758: 0.34 (2019-08-04 13:35:12.781132)\n",
            "Accuracy at step 2 - batch 758: 0.8976\n",
            "training loss at step 2 - batch 759: 0.34 (2019-08-04 13:35:12.793640)\n",
            "Accuracy at step 2 - batch 759: 0.9016\n",
            "training loss at step 2 - batch 760: 0.37 (2019-08-04 13:35:12.805397)\n",
            "Accuracy at step 2 - batch 760: 0.8936\n",
            "training loss at step 2 - batch 761: 0.35 (2019-08-04 13:35:12.930997)\n",
            "Accuracy at step 2 - batch 761: 0.8996\n",
            "training loss at step 2 - batch 762: 0.33 (2019-08-04 13:35:12.947095)\n",
            "Accuracy at step 2 - batch 762: 0.904\n",
            "training loss at step 2 - batch 763: 0.35 (2019-08-04 13:35:12.960244)\n",
            "Accuracy at step 2 - batch 763: 0.8988\n",
            "training loss at step 2 - batch 764: 0.36 (2019-08-04 13:35:12.972260)\n",
            "Accuracy at step 2 - batch 764: 0.8932\n",
            "training loss at step 2 - batch 765: 0.34 (2019-08-04 13:35:12.984781)\n",
            "Accuracy at step 2 - batch 765: 0.8996\n",
            "training loss at step 2 - batch 766: 0.34 (2019-08-04 13:35:13.101647)\n",
            "Accuracy at step 2 - batch 766: 0.9012\n",
            "training loss at step 2 - batch 767: 0.34 (2019-08-04 13:35:13.115423)\n",
            "Accuracy at step 2 - batch 767: 0.8972\n",
            "training loss at step 2 - batch 768: 0.36 (2019-08-04 13:35:13.128522)\n",
            "Accuracy at step 2 - batch 768: 0.9024\n",
            "training loss at step 2 - batch 769: 0.33 (2019-08-04 13:35:13.149799)\n",
            "Accuracy at step 2 - batch 769: 0.8992\n",
            "training loss at step 2 - batch 770: 0.37 (2019-08-04 13:35:13.164055)\n",
            "Accuracy at step 2 - batch 770: 0.8888\n",
            "training loss at step 2 - batch 771: 0.35 (2019-08-04 13:35:13.290059)\n",
            "Accuracy at step 2 - batch 771: 0.8984\n",
            "training loss at step 2 - batch 772: 0.36 (2019-08-04 13:35:13.305684)\n",
            "Accuracy at step 2 - batch 772: 0.8976\n",
            "training loss at step 2 - batch 773: 0.33 (2019-08-04 13:35:13.320862)\n",
            "Accuracy at step 2 - batch 773: 0.9032\n",
            "training loss at step 2 - batch 774: 0.36 (2019-08-04 13:35:13.334769)\n",
            "Accuracy at step 2 - batch 774: 0.8976\n",
            "training loss at step 2 - batch 775: 0.33 (2019-08-04 13:35:13.347597)\n",
            "Accuracy at step 2 - batch 775: 0.9024\n",
            "training loss at step 2 - batch 776: 0.36 (2019-08-04 13:35:13.477758)\n",
            "Accuracy at step 2 - batch 776: 0.8908\n",
            "training loss at step 2 - batch 777: 0.35 (2019-08-04 13:35:13.490264)\n",
            "Accuracy at step 2 - batch 777: 0.8936\n",
            "training loss at step 2 - batch 778: 0.34 (2019-08-04 13:35:13.502370)\n",
            "Accuracy at step 2 - batch 778: 0.9032\n",
            "training loss at step 2 - batch 779: 0.35 (2019-08-04 13:35:13.517087)\n",
            "Accuracy at step 2 - batch 779: 0.8992\n",
            "training loss at step 3 - batch 0: 0.37 (2019-08-04 13:35:13.530615)\n",
            "Accuracy at step 3 - batch 0: 0.8968\n",
            "training loss at step 3 - batch 1: 0.34 (2019-08-04 13:35:13.648760)\n",
            "Accuracy at step 3 - batch 1: 0.8988\n",
            "training loss at step 3 - batch 2: 0.33 (2019-08-04 13:35:13.666433)\n",
            "Accuracy at step 3 - batch 2: 0.9048\n",
            "training loss at step 3 - batch 3: 0.32 (2019-08-04 13:35:13.679866)\n",
            "Accuracy at step 3 - batch 3: 0.908\n",
            "training loss at step 3 - batch 4: 0.36 (2019-08-04 13:35:13.696242)\n",
            "Accuracy at step 3 - batch 4: 0.8948\n",
            "training loss at step 3 - batch 5: 0.35 (2019-08-04 13:35:13.708994)\n",
            "Accuracy at step 3 - batch 5: 0.9044\n",
            "training loss at step 3 - batch 6: 0.36 (2019-08-04 13:35:13.826758)\n",
            "Accuracy at step 3 - batch 6: 0.8944\n",
            "training loss at step 3 - batch 7: 0.37 (2019-08-04 13:35:13.839411)\n",
            "Accuracy at step 3 - batch 7: 0.892\n",
            "training loss at step 3 - batch 8: 0.34 (2019-08-04 13:35:13.852318)\n",
            "Accuracy at step 3 - batch 8: 0.898\n",
            "training loss at step 3 - batch 9: 0.35 (2019-08-04 13:35:13.864867)\n",
            "Accuracy at step 3 - batch 9: 0.898\n",
            "training loss at step 3 - batch 10: 0.36 (2019-08-04 13:35:13.878161)\n",
            "Accuracy at step 3 - batch 10: 0.8944\n",
            "training loss at step 3 - batch 11: 0.37 (2019-08-04 13:35:14.002758)\n",
            "Accuracy at step 3 - batch 11: 0.8896\n",
            "training loss at step 3 - batch 12: 0.35 (2019-08-04 13:35:14.015350)\n",
            "Accuracy at step 3 - batch 12: 0.8996\n",
            "training loss at step 3 - batch 13: 0.34 (2019-08-04 13:35:14.028362)\n",
            "Accuracy at step 3 - batch 13: 0.9016\n",
            "training loss at step 3 - batch 14: 0.34 (2019-08-04 13:35:14.040935)\n",
            "Accuracy at step 3 - batch 14: 0.9012\n",
            "training loss at step 3 - batch 15: 0.35 (2019-08-04 13:35:14.052627)\n",
            "Accuracy at step 3 - batch 15: 0.8996\n",
            "training loss at step 3 - batch 16: 0.35 (2019-08-04 13:35:14.171229)\n",
            "Accuracy at step 3 - batch 16: 0.9008\n",
            "training loss at step 3 - batch 17: 0.32 (2019-08-04 13:35:14.186127)\n",
            "Accuracy at step 3 - batch 17: 0.9092\n",
            "training loss at step 3 - batch 18: 0.35 (2019-08-04 13:35:14.198082)\n",
            "Accuracy at step 3 - batch 18: 0.9012\n",
            "training loss at step 3 - batch 19: 0.35 (2019-08-04 13:35:14.213254)\n",
            "Accuracy at step 3 - batch 19: 0.9044\n",
            "training loss at step 3 - batch 20: 0.35 (2019-08-04 13:35:14.226232)\n",
            "Accuracy at step 3 - batch 20: 0.8956\n",
            "training loss at step 3 - batch 21: 0.36 (2019-08-04 13:35:14.351379)\n",
            "Accuracy at step 3 - batch 21: 0.8964\n",
            "training loss at step 3 - batch 22: 0.33 (2019-08-04 13:35:14.365755)\n",
            "Accuracy at step 3 - batch 22: 0.9032\n",
            "training loss at step 3 - batch 23: 0.34 (2019-08-04 13:35:14.377769)\n",
            "Accuracy at step 3 - batch 23: 0.8952\n",
            "training loss at step 3 - batch 24: 0.36 (2019-08-04 13:35:14.389915)\n",
            "Accuracy at step 3 - batch 24: 0.894\n",
            "training loss at step 3 - batch 25: 0.35 (2019-08-04 13:35:14.401755)\n",
            "Accuracy at step 3 - batch 25: 0.8996\n",
            "training loss at step 3 - batch 26: 0.35 (2019-08-04 13:35:14.527668)\n",
            "Accuracy at step 3 - batch 26: 0.8956\n",
            "training loss at step 3 - batch 27: 0.33 (2019-08-04 13:35:14.540146)\n",
            "Accuracy at step 3 - batch 27: 0.9036\n",
            "training loss at step 3 - batch 28: 0.33 (2019-08-04 13:35:14.553963)\n",
            "Accuracy at step 3 - batch 28: 0.9032\n",
            "training loss at step 3 - batch 29: 0.38 (2019-08-04 13:35:14.565663)\n",
            "Accuracy at step 3 - batch 29: 0.8924\n",
            "training loss at step 3 - batch 30: 0.34 (2019-08-04 13:35:14.578036)\n",
            "Accuracy at step 3 - batch 30: 0.8992\n",
            "training loss at step 3 - batch 31: 0.34 (2019-08-04 13:35:14.698336)\n",
            "Accuracy at step 3 - batch 31: 0.8964\n",
            "training loss at step 3 - batch 32: 0.33 (2019-08-04 13:35:14.712634)\n",
            "Accuracy at step 3 - batch 32: 0.906\n",
            "training loss at step 3 - batch 33: 0.35 (2019-08-04 13:35:14.727784)\n",
            "Accuracy at step 3 - batch 33: 0.8936\n",
            "training loss at step 3 - batch 34: 0.35 (2019-08-04 13:35:14.743744)\n",
            "Accuracy at step 3 - batch 34: 0.8992\n",
            "training loss at step 3 - batch 35: 0.36 (2019-08-04 13:35:14.755948)\n",
            "Accuracy at step 3 - batch 35: 0.8952\n",
            "training loss at step 3 - batch 36: 0.35 (2019-08-04 13:35:14.875862)\n",
            "Accuracy at step 3 - batch 36: 0.8976\n",
            "training loss at step 3 - batch 37: 0.37 (2019-08-04 13:35:14.888350)\n",
            "Accuracy at step 3 - batch 37: 0.892\n",
            "training loss at step 3 - batch 38: 0.36 (2019-08-04 13:35:14.901006)\n",
            "Accuracy at step 3 - batch 38: 0.8956\n",
            "training loss at step 3 - batch 39: 0.34 (2019-08-04 13:35:14.918618)\n",
            "Accuracy at step 3 - batch 39: 0.9056\n",
            "training loss at step 3 - batch 40: 0.35 (2019-08-04 13:35:14.934587)\n",
            "Accuracy at step 3 - batch 40: 0.9004\n",
            "training loss at step 3 - batch 41: 0.37 (2019-08-04 13:35:15.075741)\n",
            "Accuracy at step 3 - batch 41: 0.8956\n",
            "training loss at step 3 - batch 42: 0.37 (2019-08-04 13:35:15.091831)\n",
            "Accuracy at step 3 - batch 42: 0.8908\n",
            "training loss at step 3 - batch 43: 0.34 (2019-08-04 13:35:15.105098)\n",
            "Accuracy at step 3 - batch 43: 0.898\n",
            "training loss at step 3 - batch 44: 0.33 (2019-08-04 13:35:15.117114)\n",
            "Accuracy at step 3 - batch 44: 0.9036\n",
            "training loss at step 3 - batch 45: 0.33 (2019-08-04 13:35:15.130264)\n",
            "Accuracy at step 3 - batch 45: 0.902\n",
            "training loss at step 3 - batch 46: 0.34 (2019-08-04 13:35:15.256296)\n",
            "Accuracy at step 3 - batch 46: 0.9008\n",
            "training loss at step 3 - batch 47: 0.36 (2019-08-04 13:35:15.270318)\n",
            "Accuracy at step 3 - batch 47: 0.8896\n",
            "training loss at step 3 - batch 48: 0.34 (2019-08-04 13:35:15.284533)\n",
            "Accuracy at step 3 - batch 48: 0.9028\n",
            "training loss at step 3 - batch 49: 0.37 (2019-08-04 13:35:15.298001)\n",
            "Accuracy at step 3 - batch 49: 0.8908\n",
            "training loss at step 3 - batch 50: 0.34 (2019-08-04 13:35:15.310878)\n",
            "Accuracy at step 3 - batch 50: 0.9012\n",
            "training loss at step 3 - batch 51: 0.34 (2019-08-04 13:35:15.446446)\n",
            "Accuracy at step 3 - batch 51: 0.9012\n",
            "training loss at step 3 - batch 52: 0.35 (2019-08-04 13:35:15.461855)\n",
            "Accuracy at step 3 - batch 52: 0.8968\n",
            "training loss at step 3 - batch 53: 0.34 (2019-08-04 13:35:15.475448)\n",
            "Accuracy at step 3 - batch 53: 0.9004\n",
            "training loss at step 3 - batch 54: 0.35 (2019-08-04 13:35:15.487262)\n",
            "Accuracy at step 3 - batch 54: 0.8972\n",
            "training loss at step 3 - batch 55: 0.34 (2019-08-04 13:35:15.500282)\n",
            "Accuracy at step 3 - batch 55: 0.8984\n",
            "training loss at step 3 - batch 56: 0.34 (2019-08-04 13:35:15.756678)\n",
            "Accuracy at step 3 - batch 56: 0.9032\n",
            "training loss at step 3 - batch 57: 0.36 (2019-08-04 13:35:15.777311)\n",
            "Accuracy at step 3 - batch 57: 0.8972\n",
            "training loss at step 3 - batch 58: 0.36 (2019-08-04 13:35:15.790184)\n",
            "Accuracy at step 3 - batch 58: 0.894\n",
            "training loss at step 3 - batch 59: 0.35 (2019-08-04 13:35:15.803165)\n",
            "Accuracy at step 3 - batch 59: 0.8988\n",
            "training loss at step 3 - batch 60: 0.37 (2019-08-04 13:35:15.816380)\n",
            "Accuracy at step 3 - batch 60: 0.8956\n",
            "training loss at step 3 - batch 61: 0.36 (2019-08-04 13:35:15.936486)\n",
            "Accuracy at step 3 - batch 61: 0.8952\n",
            "training loss at step 3 - batch 62: 0.37 (2019-08-04 13:35:15.953010)\n",
            "Accuracy at step 3 - batch 62: 0.8928\n",
            "training loss at step 3 - batch 63: 0.36 (2019-08-04 13:35:15.967453)\n",
            "Accuracy at step 3 - batch 63: 0.8976\n",
            "training loss at step 3 - batch 64: 0.36 (2019-08-04 13:35:15.980768)\n",
            "Accuracy at step 3 - batch 64: 0.8948\n",
            "training loss at step 3 - batch 65: 0.35 (2019-08-04 13:35:15.993287)\n",
            "Accuracy at step 3 - batch 65: 0.8984\n",
            "training loss at step 3 - batch 66: 0.34 (2019-08-04 13:35:16.113332)\n",
            "Accuracy at step 3 - batch 66: 0.8956\n",
            "training loss at step 3 - batch 67: 0.33 (2019-08-04 13:35:16.131537)\n",
            "Accuracy at step 3 - batch 67: 0.9032\n",
            "training loss at step 3 - batch 68: 0.34 (2019-08-04 13:35:16.144069)\n",
            "Accuracy at step 3 - batch 68: 0.9008\n",
            "training loss at step 3 - batch 69: 0.31 (2019-08-04 13:35:16.156459)\n",
            "Accuracy at step 3 - batch 69: 0.9076\n",
            "training loss at step 3 - batch 70: 0.33 (2019-08-04 13:35:16.171735)\n",
            "Accuracy at step 3 - batch 70: 0.902\n",
            "training loss at step 3 - batch 71: 0.34 (2019-08-04 13:35:16.298192)\n",
            "Accuracy at step 3 - batch 71: 0.8988\n",
            "training loss at step 3 - batch 72: 0.36 (2019-08-04 13:35:16.310547)\n",
            "Accuracy at step 3 - batch 72: 0.8944\n",
            "training loss at step 3 - batch 73: 0.35 (2019-08-04 13:35:16.324020)\n",
            "Accuracy at step 3 - batch 73: 0.9012\n",
            "training loss at step 3 - batch 74: 0.35 (2019-08-04 13:35:16.336761)\n",
            "Accuracy at step 3 - batch 74: 0.8952\n",
            "training loss at step 3 - batch 75: 0.35 (2019-08-04 13:35:16.351670)\n",
            "Accuracy at step 3 - batch 75: 0.902\n",
            "training loss at step 3 - batch 76: 0.35 (2019-08-04 13:35:16.479493)\n",
            "Accuracy at step 3 - batch 76: 0.8904\n",
            "training loss at step 3 - batch 77: 0.36 (2019-08-04 13:35:16.492754)\n",
            "Accuracy at step 3 - batch 77: 0.896\n",
            "training loss at step 3 - batch 78: 0.35 (2019-08-04 13:35:16.505382)\n",
            "Accuracy at step 3 - batch 78: 0.8972\n",
            "training loss at step 3 - batch 79: 0.34 (2019-08-04 13:35:16.518078)\n",
            "Accuracy at step 3 - batch 79: 0.8964\n",
            "training loss at step 3 - batch 80: 0.34 (2019-08-04 13:35:16.531573)\n",
            "Accuracy at step 3 - batch 80: 0.9004\n",
            "training loss at step 3 - batch 81: 0.33 (2019-08-04 13:35:16.649975)\n",
            "Accuracy at step 3 - batch 81: 0.9044\n",
            "training loss at step 3 - batch 82: 0.34 (2019-08-04 13:35:16.663762)\n",
            "Accuracy at step 3 - batch 82: 0.8996\n",
            "training loss at step 3 - batch 83: 0.33 (2019-08-04 13:35:16.677725)\n",
            "Accuracy at step 3 - batch 83: 0.8972\n",
            "training loss at step 3 - batch 84: 0.36 (2019-08-04 13:35:16.694260)\n",
            "Accuracy at step 3 - batch 84: 0.8976\n",
            "training loss at step 3 - batch 85: 0.36 (2019-08-04 13:35:16.707071)\n",
            "Accuracy at step 3 - batch 85: 0.894\n",
            "training loss at step 3 - batch 86: 0.36 (2019-08-04 13:35:16.826538)\n",
            "Accuracy at step 3 - batch 86: 0.8952\n",
            "training loss at step 3 - batch 87: 0.35 (2019-08-04 13:35:16.844657)\n",
            "Accuracy at step 3 - batch 87: 0.8988\n",
            "training loss at step 3 - batch 88: 0.33 (2019-08-04 13:35:16.857365)\n",
            "Accuracy at step 3 - batch 88: 0.9028\n",
            "training loss at step 3 - batch 89: 0.32 (2019-08-04 13:35:16.871152)\n",
            "Accuracy at step 3 - batch 89: 0.9032\n",
            "training loss at step 3 - batch 90: 0.35 (2019-08-04 13:35:16.883650)\n",
            "Accuracy at step 3 - batch 90: 0.8976\n",
            "training loss at step 3 - batch 91: 0.33 (2019-08-04 13:35:17.002682)\n",
            "Accuracy at step 3 - batch 91: 0.9092\n",
            "training loss at step 3 - batch 92: 0.37 (2019-08-04 13:35:17.019533)\n",
            "Accuracy at step 3 - batch 92: 0.8928\n",
            "training loss at step 3 - batch 93: 0.34 (2019-08-04 13:35:17.031704)\n",
            "Accuracy at step 3 - batch 93: 0.8956\n",
            "training loss at step 3 - batch 94: 0.37 (2019-08-04 13:35:17.044713)\n",
            "Accuracy at step 3 - batch 94: 0.8908\n",
            "training loss at step 3 - batch 95: 0.36 (2019-08-04 13:35:17.057393)\n",
            "Accuracy at step 3 - batch 95: 0.8924\n",
            "training loss at step 3 - batch 96: 0.33 (2019-08-04 13:35:17.184522)\n",
            "Accuracy at step 3 - batch 96: 0.9076\n",
            "training loss at step 3 - batch 97: 0.37 (2019-08-04 13:35:17.199072)\n",
            "Accuracy at step 3 - batch 97: 0.8872\n",
            "training loss at step 3 - batch 98: 0.36 (2019-08-04 13:35:17.214882)\n",
            "Accuracy at step 3 - batch 98: 0.8956\n",
            "training loss at step 3 - batch 99: 0.34 (2019-08-04 13:35:17.230514)\n",
            "Accuracy at step 3 - batch 99: 0.9012\n",
            "training loss at step 3 - batch 100: 0.36 (2019-08-04 13:35:17.245214)\n",
            "Accuracy at step 3 - batch 100: 0.8968\n",
            "training loss at step 3 - batch 101: 0.35 (2019-08-04 13:35:17.367487)\n",
            "Accuracy at step 3 - batch 101: 0.9008\n",
            "training loss at step 3 - batch 102: 0.36 (2019-08-04 13:35:17.384458)\n",
            "Accuracy at step 3 - batch 102: 0.8924\n",
            "training loss at step 3 - batch 103: 0.32 (2019-08-04 13:35:17.397074)\n",
            "Accuracy at step 3 - batch 103: 0.9044\n",
            "training loss at step 3 - batch 104: 0.33 (2019-08-04 13:35:17.409739)\n",
            "Accuracy at step 3 - batch 104: 0.9072\n",
            "training loss at step 3 - batch 105: 0.37 (2019-08-04 13:35:17.426253)\n",
            "Accuracy at step 3 - batch 105: 0.8896\n",
            "training loss at step 3 - batch 106: 0.33 (2019-08-04 13:35:17.555793)\n",
            "Accuracy at step 3 - batch 106: 0.8976\n",
            "training loss at step 3 - batch 107: 0.35 (2019-08-04 13:35:17.569554)\n",
            "Accuracy at step 3 - batch 107: 0.8988\n",
            "training loss at step 3 - batch 108: 0.36 (2019-08-04 13:35:17.582013)\n",
            "Accuracy at step 3 - batch 108: 0.8968\n",
            "training loss at step 3 - batch 109: 0.34 (2019-08-04 13:35:17.595279)\n",
            "Accuracy at step 3 - batch 109: 0.9044\n",
            "training loss at step 3 - batch 110: 0.34 (2019-08-04 13:35:17.608893)\n",
            "Accuracy at step 3 - batch 110: 0.896\n",
            "training loss at step 3 - batch 111: 0.35 (2019-08-04 13:35:17.734524)\n",
            "Accuracy at step 3 - batch 111: 0.8916\n",
            "training loss at step 3 - batch 112: 0.34 (2019-08-04 13:35:17.751924)\n",
            "Accuracy at step 3 - batch 112: 0.8992\n",
            "training loss at step 3 - batch 113: 0.35 (2019-08-04 13:35:17.765180)\n",
            "Accuracy at step 3 - batch 113: 0.8964\n",
            "training loss at step 3 - batch 114: 0.34 (2019-08-04 13:35:17.778318)\n",
            "Accuracy at step 3 - batch 114: 0.904\n",
            "training loss at step 3 - batch 115: 0.36 (2019-08-04 13:35:17.790970)\n",
            "Accuracy at step 3 - batch 115: 0.8944\n",
            "training loss at step 3 - batch 116: 0.33 (2019-08-04 13:35:17.911299)\n",
            "Accuracy at step 3 - batch 116: 0.9072\n",
            "training loss at step 3 - batch 117: 0.33 (2019-08-04 13:35:17.925436)\n",
            "Accuracy at step 3 - batch 117: 0.902\n",
            "training loss at step 3 - batch 118: 0.34 (2019-08-04 13:35:17.941049)\n",
            "Accuracy at step 3 - batch 118: 0.8956\n",
            "training loss at step 3 - batch 119: 0.33 (2019-08-04 13:35:17.953895)\n",
            "Accuracy at step 3 - batch 119: 0.9016\n",
            "training loss at step 3 - batch 120: 0.35 (2019-08-04 13:35:17.966176)\n",
            "Accuracy at step 3 - batch 120: 0.8984\n",
            "training loss at step 3 - batch 121: 0.34 (2019-08-04 13:35:18.083924)\n",
            "Accuracy at step 3 - batch 121: 0.8972\n",
            "training loss at step 3 - batch 122: 0.37 (2019-08-04 13:35:18.097246)\n",
            "Accuracy at step 3 - batch 122: 0.8904\n",
            "training loss at step 3 - batch 123: 0.31 (2019-08-04 13:35:18.110343)\n",
            "Accuracy at step 3 - batch 123: 0.9088\n",
            "training loss at step 3 - batch 124: 0.36 (2019-08-04 13:35:18.123996)\n",
            "Accuracy at step 3 - batch 124: 0.8904\n",
            "training loss at step 3 - batch 125: 0.33 (2019-08-04 13:35:18.138772)\n",
            "Accuracy at step 3 - batch 125: 0.9064\n",
            "training loss at step 3 - batch 126: 0.37 (2019-08-04 13:35:18.263228)\n",
            "Accuracy at step 3 - batch 126: 0.8932\n",
            "training loss at step 3 - batch 127: 0.35 (2019-08-04 13:35:18.278254)\n",
            "Accuracy at step 3 - batch 127: 0.8944\n",
            "training loss at step 3 - batch 128: 0.33 (2019-08-04 13:35:18.290955)\n",
            "Accuracy at step 3 - batch 128: 0.9024\n",
            "training loss at step 3 - batch 129: 0.35 (2019-08-04 13:35:18.304098)\n",
            "Accuracy at step 3 - batch 129: 0.904\n",
            "training loss at step 3 - batch 130: 0.33 (2019-08-04 13:35:18.316762)\n",
            "Accuracy at step 3 - batch 130: 0.9\n",
            "training loss at step 3 - batch 131: 0.36 (2019-08-04 13:35:18.441227)\n",
            "Accuracy at step 3 - batch 131: 0.8948\n",
            "training loss at step 3 - batch 132: 0.36 (2019-08-04 13:35:18.457393)\n",
            "Accuracy at step 3 - batch 132: 0.9\n",
            "training loss at step 3 - batch 133: 0.35 (2019-08-04 13:35:18.471262)\n",
            "Accuracy at step 3 - batch 133: 0.8936\n",
            "training loss at step 3 - batch 134: 0.34 (2019-08-04 13:35:18.483675)\n",
            "Accuracy at step 3 - batch 134: 0.8984\n",
            "training loss at step 3 - batch 135: 0.40 (2019-08-04 13:35:18.495665)\n",
            "Accuracy at step 3 - batch 135: 0.882\n",
            "training loss at step 3 - batch 136: 0.34 (2019-08-04 13:35:18.612741)\n",
            "Accuracy at step 3 - batch 136: 0.9016\n",
            "training loss at step 3 - batch 137: 0.33 (2019-08-04 13:35:18.629324)\n",
            "Accuracy at step 3 - batch 137: 0.9048\n",
            "training loss at step 3 - batch 138: 0.35 (2019-08-04 13:35:18.641826)\n",
            "Accuracy at step 3 - batch 138: 0.8996\n",
            "training loss at step 3 - batch 139: 0.37 (2019-08-04 13:35:18.655153)\n",
            "Accuracy at step 3 - batch 139: 0.8908\n",
            "training loss at step 3 - batch 140: 0.33 (2019-08-04 13:35:18.666973)\n",
            "Accuracy at step 3 - batch 140: 0.9008\n",
            "training loss at step 3 - batch 141: 0.34 (2019-08-04 13:35:18.790701)\n",
            "Accuracy at step 3 - batch 141: 0.9012\n",
            "training loss at step 3 - batch 142: 0.35 (2019-08-04 13:35:18.808302)\n",
            "Accuracy at step 3 - batch 142: 0.8956\n",
            "training loss at step 3 - batch 143: 0.34 (2019-08-04 13:35:18.821705)\n",
            "Accuracy at step 3 - batch 143: 0.8928\n",
            "training loss at step 3 - batch 144: 0.34 (2019-08-04 13:35:18.833797)\n",
            "Accuracy at step 3 - batch 144: 0.8972\n",
            "training loss at step 3 - batch 145: 0.35 (2019-08-04 13:35:18.847042)\n",
            "Accuracy at step 3 - batch 145: 0.9\n",
            "training loss at step 3 - batch 146: 0.33 (2019-08-04 13:35:18.962034)\n",
            "Accuracy at step 3 - batch 146: 0.9116\n",
            "training loss at step 3 - batch 147: 0.34 (2019-08-04 13:35:18.976843)\n",
            "Accuracy at step 3 - batch 147: 0.8988\n",
            "training loss at step 3 - batch 148: 0.34 (2019-08-04 13:35:18.990123)\n",
            "Accuracy at step 3 - batch 148: 0.9008\n",
            "training loss at step 3 - batch 149: 0.35 (2019-08-04 13:35:19.006393)\n",
            "Accuracy at step 3 - batch 149: 0.8972\n",
            "training loss at step 3 - batch 150: 0.38 (2019-08-04 13:35:19.020525)\n",
            "Accuracy at step 3 - batch 150: 0.8964\n",
            "training loss at step 3 - batch 151: 0.37 (2019-08-04 13:35:19.138853)\n",
            "Accuracy at step 3 - batch 151: 0.8968\n",
            "training loss at step 3 - batch 152: 0.33 (2019-08-04 13:35:19.156674)\n",
            "Accuracy at step 3 - batch 152: 0.9052\n",
            "training loss at step 3 - batch 153: 0.34 (2019-08-04 13:35:19.169059)\n",
            "Accuracy at step 3 - batch 153: 0.9028\n",
            "training loss at step 3 - batch 154: 0.37 (2019-08-04 13:35:19.182159)\n",
            "Accuracy at step 3 - batch 154: 0.8936\n",
            "training loss at step 3 - batch 155: 0.33 (2019-08-04 13:35:19.194753)\n",
            "Accuracy at step 3 - batch 155: 0.9096\n",
            "training loss at step 3 - batch 156: 0.36 (2019-08-04 13:35:19.321140)\n",
            "Accuracy at step 3 - batch 156: 0.8952\n",
            "training loss at step 3 - batch 157: 0.33 (2019-08-04 13:35:19.334503)\n",
            "Accuracy at step 3 - batch 157: 0.8988\n",
            "training loss at step 3 - batch 158: 0.34 (2019-08-04 13:35:19.346305)\n",
            "Accuracy at step 3 - batch 158: 0.8988\n",
            "training loss at step 3 - batch 159: 0.36 (2019-08-04 13:35:19.358356)\n",
            "Accuracy at step 3 - batch 159: 0.8992\n",
            "training loss at step 3 - batch 160: 0.35 (2019-08-04 13:35:19.371231)\n",
            "Accuracy at step 3 - batch 160: 0.896\n",
            "training loss at step 3 - batch 161: 0.34 (2019-08-04 13:35:19.493219)\n",
            "Accuracy at step 3 - batch 161: 0.8988\n",
            "training loss at step 3 - batch 162: 0.36 (2019-08-04 13:35:19.510088)\n",
            "Accuracy at step 3 - batch 162: 0.8948\n",
            "training loss at step 3 - batch 163: 0.36 (2019-08-04 13:35:19.528760)\n",
            "Accuracy at step 3 - batch 163: 0.8968\n",
            "training loss at step 3 - batch 164: 0.36 (2019-08-04 13:35:19.542189)\n",
            "Accuracy at step 3 - batch 164: 0.892\n",
            "training loss at step 3 - batch 165: 0.35 (2019-08-04 13:35:19.554375)\n",
            "Accuracy at step 3 - batch 165: 0.8948\n",
            "training loss at step 3 - batch 166: 0.35 (2019-08-04 13:35:19.675389)\n",
            "Accuracy at step 3 - batch 166: 0.898\n",
            "training loss at step 3 - batch 167: 0.35 (2019-08-04 13:35:19.687709)\n",
            "Accuracy at step 3 - batch 167: 0.9012\n",
            "training loss at step 3 - batch 168: 0.36 (2019-08-04 13:35:19.700452)\n",
            "Accuracy at step 3 - batch 168: 0.8904\n",
            "training loss at step 3 - batch 169: 0.32 (2019-08-04 13:35:19.712666)\n",
            "Accuracy at step 3 - batch 169: 0.9032\n",
            "training loss at step 3 - batch 170: 0.36 (2019-08-04 13:35:19.725109)\n",
            "Accuracy at step 3 - batch 170: 0.8956\n",
            "training loss at step 3 - batch 171: 0.33 (2019-08-04 13:35:19.844555)\n",
            "Accuracy at step 3 - batch 171: 0.8972\n",
            "training loss at step 3 - batch 172: 0.34 (2019-08-04 13:35:19.857367)\n",
            "Accuracy at step 3 - batch 172: 0.904\n",
            "training loss at step 3 - batch 173: 0.32 (2019-08-04 13:35:19.870357)\n",
            "Accuracy at step 3 - batch 173: 0.9068\n",
            "training loss at step 3 - batch 174: 0.35 (2019-08-04 13:35:19.883327)\n",
            "Accuracy at step 3 - batch 174: 0.8956\n",
            "training loss at step 3 - batch 175: 0.35 (2019-08-04 13:35:19.896791)\n",
            "Accuracy at step 3 - batch 175: 0.9032\n",
            "training loss at step 3 - batch 176: 0.32 (2019-08-04 13:35:20.018488)\n",
            "Accuracy at step 3 - batch 176: 0.9052\n",
            "training loss at step 3 - batch 177: 0.33 (2019-08-04 13:35:20.035423)\n",
            "Accuracy at step 3 - batch 177: 0.9036\n",
            "training loss at step 3 - batch 178: 0.34 (2019-08-04 13:35:20.050117)\n",
            "Accuracy at step 3 - batch 178: 0.896\n",
            "training loss at step 3 - batch 179: 0.37 (2019-08-04 13:35:20.063885)\n",
            "Accuracy at step 3 - batch 179: 0.8892\n",
            "training loss at step 3 - batch 180: 0.34 (2019-08-04 13:35:20.076923)\n",
            "Accuracy at step 3 - batch 180: 0.9016\n",
            "training loss at step 3 - batch 181: 0.34 (2019-08-04 13:35:20.195163)\n",
            "Accuracy at step 3 - batch 181: 0.9032\n",
            "training loss at step 3 - batch 182: 0.35 (2019-08-04 13:35:20.207756)\n",
            "Accuracy at step 3 - batch 182: 0.8996\n",
            "training loss at step 3 - batch 183: 0.34 (2019-08-04 13:35:20.222100)\n",
            "Accuracy at step 3 - batch 183: 0.9004\n",
            "training loss at step 3 - batch 184: 0.33 (2019-08-04 13:35:20.235641)\n",
            "Accuracy at step 3 - batch 184: 0.9012\n",
            "training loss at step 3 - batch 185: 0.35 (2019-08-04 13:35:20.247932)\n",
            "Accuracy at step 3 - batch 185: 0.8916\n",
            "training loss at step 3 - batch 186: 0.34 (2019-08-04 13:35:20.385541)\n",
            "Accuracy at step 3 - batch 186: 0.9008\n",
            "training loss at step 3 - batch 187: 0.37 (2019-08-04 13:35:20.399949)\n",
            "Accuracy at step 3 - batch 187: 0.8936\n",
            "training loss at step 3 - batch 188: 0.35 (2019-08-04 13:35:20.414069)\n",
            "Accuracy at step 3 - batch 188: 0.8968\n",
            "training loss at step 3 - batch 189: 0.35 (2019-08-04 13:35:20.428133)\n",
            "Accuracy at step 3 - batch 189: 0.8988\n",
            "training loss at step 3 - batch 190: 0.35 (2019-08-04 13:35:20.442434)\n",
            "Accuracy at step 3 - batch 190: 0.8944\n",
            "training loss at step 3 - batch 191: 0.35 (2019-08-04 13:35:20.566779)\n",
            "Accuracy at step 3 - batch 191: 0.8988\n",
            "training loss at step 3 - batch 192: 0.34 (2019-08-04 13:35:20.584721)\n",
            "Accuracy at step 3 - batch 192: 0.9\n",
            "training loss at step 3 - batch 193: 0.36 (2019-08-04 13:35:20.600866)\n",
            "Accuracy at step 3 - batch 193: 0.894\n",
            "training loss at step 3 - batch 194: 0.34 (2019-08-04 13:35:20.614121)\n",
            "Accuracy at step 3 - batch 194: 0.904\n",
            "training loss at step 3 - batch 195: 0.34 (2019-08-04 13:35:20.628421)\n",
            "Accuracy at step 3 - batch 195: 0.896\n",
            "training loss at step 3 - batch 196: 0.34 (2019-08-04 13:35:20.746334)\n",
            "Accuracy at step 3 - batch 196: 0.9004\n",
            "training loss at step 3 - batch 197: 0.35 (2019-08-04 13:35:20.762870)\n",
            "Accuracy at step 3 - batch 197: 0.8972\n",
            "training loss at step 3 - batch 198: 0.35 (2019-08-04 13:35:20.775355)\n",
            "Accuracy at step 3 - batch 198: 0.8964\n",
            "training loss at step 3 - batch 199: 0.36 (2019-08-04 13:35:20.788176)\n",
            "Accuracy at step 3 - batch 199: 0.8988\n",
            "training loss at step 3 - batch 200: 0.37 (2019-08-04 13:35:20.800848)\n",
            "Accuracy at step 3 - batch 200: 0.8908\n",
            "training loss at step 3 - batch 201: 0.35 (2019-08-04 13:35:20.922212)\n",
            "Accuracy at step 3 - batch 201: 0.8968\n",
            "training loss at step 3 - batch 202: 0.36 (2019-08-04 13:35:20.939042)\n",
            "Accuracy at step 3 - batch 202: 0.892\n",
            "training loss at step 3 - batch 203: 0.34 (2019-08-04 13:35:20.950900)\n",
            "Accuracy at step 3 - batch 203: 0.9008\n",
            "training loss at step 3 - batch 204: 0.33 (2019-08-04 13:35:20.963848)\n",
            "Accuracy at step 3 - batch 204: 0.8984\n",
            "training loss at step 3 - batch 205: 0.35 (2019-08-04 13:35:20.975742)\n",
            "Accuracy at step 3 - batch 205: 0.8968\n",
            "training loss at step 3 - batch 206: 0.32 (2019-08-04 13:35:21.098832)\n",
            "Accuracy at step 3 - batch 206: 0.9024\n",
            "training loss at step 3 - batch 207: 0.35 (2019-08-04 13:35:21.113368)\n",
            "Accuracy at step 3 - batch 207: 0.8984\n",
            "training loss at step 3 - batch 208: 0.36 (2019-08-04 13:35:21.126460)\n",
            "Accuracy at step 3 - batch 208: 0.8928\n",
            "training loss at step 3 - batch 209: 0.33 (2019-08-04 13:35:21.138579)\n",
            "Accuracy at step 3 - batch 209: 0.9016\n",
            "training loss at step 3 - batch 210: 0.34 (2019-08-04 13:35:21.150772)\n",
            "Accuracy at step 3 - batch 210: 0.9\n",
            "training loss at step 3 - batch 211: 0.33 (2019-08-04 13:35:21.265746)\n",
            "Accuracy at step 3 - batch 211: 0.9008\n",
            "training loss at step 3 - batch 212: 0.34 (2019-08-04 13:35:21.279136)\n",
            "Accuracy at step 3 - batch 212: 0.9012\n",
            "training loss at step 3 - batch 213: 0.34 (2019-08-04 13:35:21.290912)\n",
            "Accuracy at step 3 - batch 213: 0.9\n",
            "training loss at step 3 - batch 214: 0.34 (2019-08-04 13:35:21.305681)\n",
            "Accuracy at step 3 - batch 214: 0.8992\n",
            "training loss at step 3 - batch 215: 0.35 (2019-08-04 13:35:21.325290)\n",
            "Accuracy at step 3 - batch 215: 0.9\n",
            "training loss at step 3 - batch 216: 0.33 (2019-08-04 13:35:21.453306)\n",
            "Accuracy at step 3 - batch 216: 0.9032\n",
            "training loss at step 3 - batch 217: 0.34 (2019-08-04 13:35:21.467578)\n",
            "Accuracy at step 3 - batch 217: 0.8984\n",
            "training loss at step 3 - batch 218: 0.33 (2019-08-04 13:35:21.481907)\n",
            "Accuracy at step 3 - batch 218: 0.9\n",
            "training loss at step 3 - batch 219: 0.33 (2019-08-04 13:35:21.495328)\n",
            "Accuracy at step 3 - batch 219: 0.902\n",
            "training loss at step 3 - batch 220: 0.34 (2019-08-04 13:35:21.511039)\n",
            "Accuracy at step 3 - batch 220: 0.8968\n",
            "training loss at step 3 - batch 221: 0.34 (2019-08-04 13:35:21.632580)\n",
            "Accuracy at step 3 - batch 221: 0.9\n",
            "training loss at step 3 - batch 222: 0.36 (2019-08-04 13:35:21.650734)\n",
            "Accuracy at step 3 - batch 222: 0.8944\n",
            "training loss at step 3 - batch 223: 0.35 (2019-08-04 13:35:21.665215)\n",
            "Accuracy at step 3 - batch 223: 0.8984\n",
            "training loss at step 3 - batch 224: 0.35 (2019-08-04 13:35:21.678280)\n",
            "Accuracy at step 3 - batch 224: 0.8976\n",
            "training loss at step 3 - batch 225: 0.34 (2019-08-04 13:35:21.690986)\n",
            "Accuracy at step 3 - batch 225: 0.898\n",
            "training loss at step 3 - batch 226: 0.36 (2019-08-04 13:35:21.816228)\n",
            "Accuracy at step 3 - batch 226: 0.8964\n",
            "training loss at step 3 - batch 227: 0.36 (2019-08-04 13:35:21.830830)\n",
            "Accuracy at step 3 - batch 227: 0.8988\n",
            "training loss at step 3 - batch 228: 0.35 (2019-08-04 13:35:21.843139)\n",
            "Accuracy at step 3 - batch 228: 0.8984\n",
            "training loss at step 3 - batch 229: 0.36 (2019-08-04 13:35:21.856347)\n",
            "Accuracy at step 3 - batch 229: 0.8976\n",
            "training loss at step 3 - batch 230: 0.33 (2019-08-04 13:35:21.868648)\n",
            "Accuracy at step 3 - batch 230: 0.9004\n",
            "training loss at step 3 - batch 231: 0.36 (2019-08-04 13:35:21.988452)\n",
            "Accuracy at step 3 - batch 231: 0.8976\n",
            "training loss at step 3 - batch 232: 0.35 (2019-08-04 13:35:22.002705)\n",
            "Accuracy at step 3 - batch 232: 0.8972\n",
            "training loss at step 3 - batch 233: 0.35 (2019-08-04 13:35:22.016436)\n",
            "Accuracy at step 3 - batch 233: 0.9\n",
            "training loss at step 3 - batch 234: 0.34 (2019-08-04 13:35:22.031985)\n",
            "Accuracy at step 3 - batch 234: 0.9044\n",
            "training loss at step 3 - batch 235: 0.34 (2019-08-04 13:35:22.044917)\n",
            "Accuracy at step 3 - batch 235: 0.8972\n",
            "training loss at step 3 - batch 236: 0.33 (2019-08-04 13:35:22.168705)\n",
            "Accuracy at step 3 - batch 236: 0.904\n",
            "training loss at step 3 - batch 237: 0.37 (2019-08-04 13:35:22.181681)\n",
            "Accuracy at step 3 - batch 237: 0.8944\n",
            "training loss at step 3 - batch 238: 0.32 (2019-08-04 13:35:22.194360)\n",
            "Accuracy at step 3 - batch 238: 0.9032\n",
            "training loss at step 3 - batch 239: 0.31 (2019-08-04 13:35:22.206983)\n",
            "Accuracy at step 3 - batch 239: 0.9092\n",
            "training loss at step 3 - batch 240: 0.34 (2019-08-04 13:35:22.222746)\n",
            "Accuracy at step 3 - batch 240: 0.9028\n",
            "training loss at step 3 - batch 241: 0.35 (2019-08-04 13:35:22.351781)\n",
            "Accuracy at step 3 - batch 241: 0.8952\n",
            "training loss at step 3 - batch 242: 0.35 (2019-08-04 13:35:22.368797)\n",
            "Accuracy at step 3 - batch 242: 0.8992\n",
            "training loss at step 3 - batch 243: 0.34 (2019-08-04 13:35:22.381426)\n",
            "Accuracy at step 3 - batch 243: 0.8984\n",
            "training loss at step 3 - batch 244: 0.34 (2019-08-04 13:35:22.394310)\n",
            "Accuracy at step 3 - batch 244: 0.9052\n",
            "training loss at step 3 - batch 245: 0.33 (2019-08-04 13:35:22.407248)\n",
            "Accuracy at step 3 - batch 245: 0.906\n",
            "training loss at step 3 - batch 246: 0.36 (2019-08-04 13:35:22.537284)\n",
            "Accuracy at step 3 - batch 246: 0.894\n",
            "training loss at step 3 - batch 247: 0.36 (2019-08-04 13:35:22.554456)\n",
            "Accuracy at step 3 - batch 247: 0.894\n",
            "training loss at step 3 - batch 248: 0.36 (2019-08-04 13:35:22.566759)\n",
            "Accuracy at step 3 - batch 248: 0.8948\n",
            "training loss at step 3 - batch 249: 0.32 (2019-08-04 13:35:22.579150)\n",
            "Accuracy at step 3 - batch 249: 0.9044\n",
            "training loss at step 3 - batch 250: 0.32 (2019-08-04 13:35:22.591089)\n",
            "Accuracy at step 3 - batch 250: 0.904\n",
            "training loss at step 3 - batch 251: 0.36 (2019-08-04 13:35:22.707156)\n",
            "Accuracy at step 3 - batch 251: 0.8988\n",
            "training loss at step 3 - batch 252: 0.35 (2019-08-04 13:35:22.722070)\n",
            "Accuracy at step 3 - batch 252: 0.8988\n",
            "training loss at step 3 - batch 253: 0.35 (2019-08-04 13:35:22.735539)\n",
            "Accuracy at step 3 - batch 253: 0.898\n",
            "training loss at step 3 - batch 254: 0.34 (2019-08-04 13:35:22.751856)\n",
            "Accuracy at step 3 - batch 254: 0.9024\n",
            "training loss at step 3 - batch 255: 0.33 (2019-08-04 13:35:22.764495)\n",
            "Accuracy at step 3 - batch 255: 0.9072\n",
            "training loss at step 3 - batch 256: 0.35 (2019-08-04 13:35:22.890092)\n",
            "Accuracy at step 3 - batch 256: 0.9028\n",
            "training loss at step 3 - batch 257: 0.33 (2019-08-04 13:35:22.902074)\n",
            "Accuracy at step 3 - batch 257: 0.9044\n",
            "training loss at step 3 - batch 258: 0.35 (2019-08-04 13:35:22.915238)\n",
            "Accuracy at step 3 - batch 258: 0.8976\n",
            "training loss at step 3 - batch 259: 0.34 (2019-08-04 13:35:22.927564)\n",
            "Accuracy at step 3 - batch 259: 0.9012\n",
            "training loss at step 3 - batch 260: 0.34 (2019-08-04 13:35:22.939316)\n",
            "Accuracy at step 3 - batch 260: 0.8964\n",
            "training loss at step 3 - batch 261: 0.34 (2019-08-04 13:35:23.059273)\n",
            "Accuracy at step 3 - batch 261: 0.8984\n",
            "training loss at step 3 - batch 262: 0.34 (2019-08-04 13:35:23.075337)\n",
            "Accuracy at step 3 - batch 262: 0.9052\n",
            "training loss at step 3 - batch 263: 0.36 (2019-08-04 13:35:23.088206)\n",
            "Accuracy at step 3 - batch 263: 0.8948\n",
            "training loss at step 3 - batch 264: 0.36 (2019-08-04 13:35:23.101463)\n",
            "Accuracy at step 3 - batch 264: 0.8896\n",
            "training loss at step 3 - batch 265: 0.32 (2019-08-04 13:35:23.113660)\n",
            "Accuracy at step 3 - batch 265: 0.9032\n",
            "training loss at step 3 - batch 266: 0.34 (2019-08-04 13:35:23.231715)\n",
            "Accuracy at step 3 - batch 266: 0.8988\n",
            "training loss at step 3 - batch 267: 0.35 (2019-08-04 13:35:23.245487)\n",
            "Accuracy at step 3 - batch 267: 0.9012\n",
            "training loss at step 3 - batch 268: 0.35 (2019-08-04 13:35:23.258454)\n",
            "Accuracy at step 3 - batch 268: 0.8976\n",
            "training loss at step 3 - batch 269: 0.36 (2019-08-04 13:35:23.273305)\n",
            "Accuracy at step 3 - batch 269: 0.8932\n",
            "training loss at step 3 - batch 270: 0.34 (2019-08-04 13:35:23.285454)\n",
            "Accuracy at step 3 - batch 270: 0.9044\n",
            "training loss at step 3 - batch 271: 0.34 (2019-08-04 13:35:23.412012)\n",
            "Accuracy at step 3 - batch 271: 0.9028\n",
            "training loss at step 3 - batch 272: 0.34 (2019-08-04 13:35:23.424734)\n",
            "Accuracy at step 3 - batch 272: 0.9016\n",
            "training loss at step 3 - batch 273: 0.36 (2019-08-04 13:35:23.437899)\n",
            "Accuracy at step 3 - batch 273: 0.8972\n",
            "training loss at step 3 - batch 274: 0.33 (2019-08-04 13:35:23.451314)\n",
            "Accuracy at step 3 - batch 274: 0.9044\n",
            "training loss at step 3 - batch 275: 0.33 (2019-08-04 13:35:23.464256)\n",
            "Accuracy at step 3 - batch 275: 0.9096\n",
            "training loss at step 3 - batch 276: 0.35 (2019-08-04 13:35:23.587239)\n",
            "Accuracy at step 3 - batch 276: 0.8968\n",
            "training loss at step 3 - batch 277: 0.33 (2019-08-04 13:35:23.600342)\n",
            "Accuracy at step 3 - batch 277: 0.9024\n",
            "training loss at step 3 - batch 278: 0.37 (2019-08-04 13:35:23.612353)\n",
            "Accuracy at step 3 - batch 278: 0.892\n",
            "training loss at step 3 - batch 279: 0.35 (2019-08-04 13:35:23.625331)\n",
            "Accuracy at step 3 - batch 279: 0.8972\n",
            "training loss at step 3 - batch 280: 0.35 (2019-08-04 13:35:23.638460)\n",
            "Accuracy at step 3 - batch 280: 0.894\n",
            "training loss at step 3 - batch 281: 0.35 (2019-08-04 13:35:23.755649)\n",
            "Accuracy at step 3 - batch 281: 0.9\n",
            "training loss at step 3 - batch 282: 0.34 (2019-08-04 13:35:23.772161)\n",
            "Accuracy at step 3 - batch 282: 0.9028\n",
            "training loss at step 3 - batch 283: 0.32 (2019-08-04 13:35:23.785055)\n",
            "Accuracy at step 3 - batch 283: 0.9064\n",
            "training loss at step 3 - batch 284: 0.34 (2019-08-04 13:35:23.799587)\n",
            "Accuracy at step 3 - batch 284: 0.9\n",
            "training loss at step 3 - batch 285: 0.32 (2019-08-04 13:35:23.813346)\n",
            "Accuracy at step 3 - batch 285: 0.9052\n",
            "training loss at step 3 - batch 286: 0.37 (2019-08-04 13:35:23.939264)\n",
            "Accuracy at step 3 - batch 286: 0.8956\n",
            "training loss at step 3 - batch 287: 0.36 (2019-08-04 13:35:23.954302)\n",
            "Accuracy at step 3 - batch 287: 0.892\n",
            "training loss at step 3 - batch 288: 0.34 (2019-08-04 13:35:23.966203)\n",
            "Accuracy at step 3 - batch 288: 0.9016\n",
            "training loss at step 3 - batch 289: 0.34 (2019-08-04 13:35:23.979383)\n",
            "Accuracy at step 3 - batch 289: 0.9036\n",
            "training loss at step 3 - batch 290: 0.34 (2019-08-04 13:35:23.993741)\n",
            "Accuracy at step 3 - batch 290: 0.9\n",
            "training loss at step 3 - batch 291: 0.32 (2019-08-04 13:35:24.121526)\n",
            "Accuracy at step 3 - batch 291: 0.9036\n",
            "training loss at step 3 - batch 292: 0.36 (2019-08-04 13:35:24.133621)\n",
            "Accuracy at step 3 - batch 292: 0.9016\n",
            "training loss at step 3 - batch 293: 0.33 (2019-08-04 13:35:24.147612)\n",
            "Accuracy at step 3 - batch 293: 0.8968\n",
            "training loss at step 3 - batch 294: 0.35 (2019-08-04 13:35:24.159979)\n",
            "Accuracy at step 3 - batch 294: 0.8948\n",
            "training loss at step 3 - batch 295: 0.36 (2019-08-04 13:35:24.172994)\n",
            "Accuracy at step 3 - batch 295: 0.8928\n",
            "training loss at step 3 - batch 296: 0.36 (2019-08-04 13:35:24.288484)\n",
            "Accuracy at step 3 - batch 296: 0.8968\n",
            "training loss at step 3 - batch 297: 0.34 (2019-08-04 13:35:24.303602)\n",
            "Accuracy at step 3 - batch 297: 0.9036\n",
            "training loss at step 3 - batch 298: 0.34 (2019-08-04 13:35:24.315866)\n",
            "Accuracy at step 3 - batch 298: 0.8996\n",
            "training loss at step 3 - batch 299: 0.35 (2019-08-04 13:35:24.330522)\n",
            "Accuracy at step 3 - batch 299: 0.8984\n",
            "training loss at step 3 - batch 300: 0.35 (2019-08-04 13:35:24.343548)\n",
            "Accuracy at step 3 - batch 300: 0.8964\n",
            "training loss at step 3 - batch 301: 0.33 (2019-08-04 13:35:24.473136)\n",
            "Accuracy at step 3 - batch 301: 0.9028\n",
            "training loss at step 3 - batch 302: 0.34 (2019-08-04 13:35:24.486370)\n",
            "Accuracy at step 3 - batch 302: 0.8984\n",
            "training loss at step 3 - batch 303: 0.36 (2019-08-04 13:35:24.499497)\n",
            "Accuracy at step 3 - batch 303: 0.8988\n",
            "training loss at step 3 - batch 304: 0.33 (2019-08-04 13:35:24.512529)\n",
            "Accuracy at step 3 - batch 304: 0.9\n",
            "training loss at step 3 - batch 305: 0.32 (2019-08-04 13:35:24.525791)\n",
            "Accuracy at step 3 - batch 305: 0.9072\n",
            "training loss at step 3 - batch 306: 0.34 (2019-08-04 13:35:24.651671)\n",
            "Accuracy at step 3 - batch 306: 0.9\n",
            "training loss at step 3 - batch 307: 0.33 (2019-08-04 13:35:24.668610)\n",
            "Accuracy at step 3 - batch 307: 0.9016\n",
            "training loss at step 3 - batch 308: 0.35 (2019-08-04 13:35:24.680786)\n",
            "Accuracy at step 3 - batch 308: 0.8976\n",
            "training loss at step 3 - batch 309: 0.34 (2019-08-04 13:35:24.693209)\n",
            "Accuracy at step 3 - batch 309: 0.8976\n",
            "training loss at step 3 - batch 310: 0.31 (2019-08-04 13:35:24.706905)\n",
            "Accuracy at step 3 - batch 310: 0.9084\n",
            "training loss at step 3 - batch 311: 0.33 (2019-08-04 13:35:24.831631)\n",
            "Accuracy at step 3 - batch 311: 0.906\n",
            "training loss at step 3 - batch 312: 0.34 (2019-08-04 13:35:24.847926)\n",
            "Accuracy at step 3 - batch 312: 0.8988\n",
            "training loss at step 3 - batch 313: 0.35 (2019-08-04 13:35:24.863675)\n",
            "Accuracy at step 3 - batch 313: 0.8984\n",
            "training loss at step 3 - batch 314: 0.34 (2019-08-04 13:35:24.876229)\n",
            "Accuracy at step 3 - batch 314: 0.9016\n",
            "training loss at step 3 - batch 315: 0.36 (2019-08-04 13:35:24.890360)\n",
            "Accuracy at step 3 - batch 315: 0.894\n",
            "training loss at step 3 - batch 316: 0.34 (2019-08-04 13:35:25.009443)\n",
            "Accuracy at step 3 - batch 316: 0.8992\n",
            "training loss at step 3 - batch 317: 0.36 (2019-08-04 13:35:25.023631)\n",
            "Accuracy at step 3 - batch 317: 0.8956\n",
            "training loss at step 3 - batch 318: 0.35 (2019-08-04 13:35:25.035511)\n",
            "Accuracy at step 3 - batch 318: 0.898\n",
            "training loss at step 3 - batch 319: 0.34 (2019-08-04 13:35:25.047953)\n",
            "Accuracy at step 3 - batch 319: 0.9004\n",
            "training loss at step 3 - batch 320: 0.34 (2019-08-04 13:35:25.060825)\n",
            "Accuracy at step 3 - batch 320: 0.9016\n",
            "training loss at step 3 - batch 321: 0.35 (2019-08-04 13:35:25.180831)\n",
            "Accuracy at step 3 - batch 321: 0.898\n",
            "training loss at step 3 - batch 322: 0.36 (2019-08-04 13:35:25.199497)\n",
            "Accuracy at step 3 - batch 322: 0.8904\n",
            "training loss at step 3 - batch 323: 0.34 (2019-08-04 13:35:25.211887)\n",
            "Accuracy at step 3 - batch 323: 0.8976\n",
            "training loss at step 3 - batch 324: 0.33 (2019-08-04 13:35:25.223932)\n",
            "Accuracy at step 3 - batch 324: 0.9004\n",
            "training loss at step 3 - batch 325: 0.35 (2019-08-04 13:35:25.237163)\n",
            "Accuracy at step 3 - batch 325: 0.894\n",
            "training loss at step 3 - batch 326: 0.35 (2019-08-04 13:35:25.362710)\n",
            "Accuracy at step 3 - batch 326: 0.8932\n",
            "training loss at step 3 - batch 327: 0.35 (2019-08-04 13:35:25.378972)\n",
            "Accuracy at step 3 - batch 327: 0.9004\n",
            "training loss at step 3 - batch 328: 0.34 (2019-08-04 13:35:25.393212)\n",
            "Accuracy at step 3 - batch 328: 0.902\n",
            "training loss at step 3 - batch 329: 0.34 (2019-08-04 13:35:25.411001)\n",
            "Accuracy at step 3 - batch 329: 0.8984\n",
            "training loss at step 3 - batch 330: 0.35 (2019-08-04 13:35:25.435358)\n",
            "Accuracy at step 3 - batch 330: 0.8968\n",
            "training loss at step 3 - batch 331: 0.35 (2019-08-04 13:35:25.573440)\n",
            "Accuracy at step 3 - batch 331: 0.898\n",
            "training loss at step 3 - batch 332: 0.34 (2019-08-04 13:35:25.585885)\n",
            "Accuracy at step 3 - batch 332: 0.9008\n",
            "training loss at step 3 - batch 333: 0.33 (2019-08-04 13:35:25.600861)\n",
            "Accuracy at step 3 - batch 333: 0.9032\n",
            "training loss at step 3 - batch 334: 0.35 (2019-08-04 13:35:25.613962)\n",
            "Accuracy at step 3 - batch 334: 0.898\n",
            "training loss at step 3 - batch 335: 0.34 (2019-08-04 13:35:25.626774)\n",
            "Accuracy at step 3 - batch 335: 0.8984\n",
            "training loss at step 3 - batch 336: 0.34 (2019-08-04 13:35:25.759077)\n",
            "Accuracy at step 3 - batch 336: 0.9012\n",
            "training loss at step 3 - batch 337: 0.34 (2019-08-04 13:35:25.776046)\n",
            "Accuracy at step 3 - batch 337: 0.9012\n",
            "training loss at step 3 - batch 338: 0.34 (2019-08-04 13:35:25.789356)\n",
            "Accuracy at step 3 - batch 338: 0.898\n",
            "training loss at step 3 - batch 339: 0.35 (2019-08-04 13:35:25.802452)\n",
            "Accuracy at step 3 - batch 339: 0.894\n",
            "training loss at step 3 - batch 340: 0.34 (2019-08-04 13:35:25.817576)\n",
            "Accuracy at step 3 - batch 340: 0.8976\n",
            "training loss at step 3 - batch 341: 0.33 (2019-08-04 13:35:25.943991)\n",
            "Accuracy at step 3 - batch 341: 0.9016\n",
            "training loss at step 3 - batch 342: 0.34 (2019-08-04 13:35:25.958627)\n",
            "Accuracy at step 3 - batch 342: 0.8988\n",
            "training loss at step 3 - batch 343: 0.36 (2019-08-04 13:35:25.971191)\n",
            "Accuracy at step 3 - batch 343: 0.8936\n",
            "training loss at step 3 - batch 344: 0.35 (2019-08-04 13:35:25.986306)\n",
            "Accuracy at step 3 - batch 344: 0.8964\n",
            "training loss at step 3 - batch 345: 0.35 (2019-08-04 13:35:25.998605)\n",
            "Accuracy at step 3 - batch 345: 0.894\n",
            "training loss at step 3 - batch 346: 0.34 (2019-08-04 13:35:26.127077)\n",
            "Accuracy at step 3 - batch 346: 0.8976\n",
            "training loss at step 3 - batch 347: 0.33 (2019-08-04 13:35:26.139779)\n",
            "Accuracy at step 3 - batch 347: 0.9012\n",
            "training loss at step 3 - batch 348: 0.34 (2019-08-04 13:35:26.152417)\n",
            "Accuracy at step 3 - batch 348: 0.8988\n",
            "training loss at step 3 - batch 349: 0.33 (2019-08-04 13:35:26.165089)\n",
            "Accuracy at step 3 - batch 349: 0.9076\n",
            "training loss at step 3 - batch 350: 0.35 (2019-08-04 13:35:26.178933)\n",
            "Accuracy at step 3 - batch 350: 0.8932\n",
            "training loss at step 3 - batch 351: 0.35 (2019-08-04 13:35:26.299520)\n",
            "Accuracy at step 3 - batch 351: 0.8928\n",
            "training loss at step 3 - batch 352: 0.34 (2019-08-04 13:35:26.314230)\n",
            "Accuracy at step 3 - batch 352: 0.9012\n",
            "training loss at step 3 - batch 353: 0.33 (2019-08-04 13:35:26.326926)\n",
            "Accuracy at step 3 - batch 353: 0.9052\n",
            "training loss at step 3 - batch 354: 0.33 (2019-08-04 13:35:26.344431)\n",
            "Accuracy at step 3 - batch 354: 0.9068\n",
            "training loss at step 3 - batch 355: 0.35 (2019-08-04 13:35:26.358036)\n",
            "Accuracy at step 3 - batch 355: 0.8988\n",
            "training loss at step 3 - batch 356: 0.33 (2019-08-04 13:35:26.492814)\n",
            "Accuracy at step 3 - batch 356: 0.9076\n",
            "training loss at step 3 - batch 357: 0.33 (2019-08-04 13:35:26.505604)\n",
            "Accuracy at step 3 - batch 357: 0.9052\n",
            "training loss at step 3 - batch 358: 0.34 (2019-08-04 13:35:26.518357)\n",
            "Accuracy at step 3 - batch 358: 0.9028\n",
            "training loss at step 3 - batch 359: 0.36 (2019-08-04 13:35:26.530175)\n",
            "Accuracy at step 3 - batch 359: 0.8956\n",
            "training loss at step 3 - batch 360: 0.33 (2019-08-04 13:35:26.542136)\n",
            "Accuracy at step 3 - batch 360: 0.908\n",
            "training loss at step 3 - batch 361: 0.34 (2019-08-04 13:35:26.667081)\n",
            "Accuracy at step 3 - batch 361: 0.9\n",
            "training loss at step 3 - batch 362: 0.38 (2019-08-04 13:35:26.680103)\n",
            "Accuracy at step 3 - batch 362: 0.8892\n",
            "training loss at step 3 - batch 363: 0.35 (2019-08-04 13:35:26.692882)\n",
            "Accuracy at step 3 - batch 363: 0.8972\n",
            "training loss at step 3 - batch 364: 0.34 (2019-08-04 13:35:26.706428)\n",
            "Accuracy at step 3 - batch 364: 0.904\n",
            "training loss at step 3 - batch 365: 0.36 (2019-08-04 13:35:26.719204)\n",
            "Accuracy at step 3 - batch 365: 0.8956\n",
            "training loss at step 3 - batch 366: 0.35 (2019-08-04 13:35:26.840658)\n",
            "Accuracy at step 3 - batch 366: 0.8992\n",
            "training loss at step 3 - batch 367: 0.33 (2019-08-04 13:35:26.857377)\n",
            "Accuracy at step 3 - batch 367: 0.9032\n",
            "training loss at step 3 - batch 368: 0.35 (2019-08-04 13:35:26.872500)\n",
            "Accuracy at step 3 - batch 368: 0.8984\n",
            "training loss at step 3 - batch 369: 0.33 (2019-08-04 13:35:26.885232)\n",
            "Accuracy at step 3 - batch 369: 0.9076\n",
            "training loss at step 3 - batch 370: 0.34 (2019-08-04 13:35:26.897549)\n",
            "Accuracy at step 3 - batch 370: 0.9016\n",
            "training loss at step 3 - batch 371: 0.35 (2019-08-04 13:35:27.021693)\n",
            "Accuracy at step 3 - batch 371: 0.9012\n",
            "training loss at step 3 - batch 372: 0.37 (2019-08-04 13:35:27.037929)\n",
            "Accuracy at step 3 - batch 372: 0.8952\n",
            "training loss at step 3 - batch 373: 0.35 (2019-08-04 13:35:27.049918)\n",
            "Accuracy at step 3 - batch 373: 0.9004\n",
            "training loss at step 3 - batch 374: 0.33 (2019-08-04 13:35:27.063494)\n",
            "Accuracy at step 3 - batch 374: 0.898\n",
            "training loss at step 3 - batch 375: 0.35 (2019-08-04 13:35:27.078503)\n",
            "Accuracy at step 3 - batch 375: 0.8936\n",
            "training loss at step 3 - batch 376: 0.36 (2019-08-04 13:35:27.204464)\n",
            "Accuracy at step 3 - batch 376: 0.8944\n",
            "training loss at step 3 - batch 377: 0.33 (2019-08-04 13:35:27.220410)\n",
            "Accuracy at step 3 - batch 377: 0.9056\n",
            "training loss at step 3 - batch 378: 0.35 (2019-08-04 13:35:27.232721)\n",
            "Accuracy at step 3 - batch 378: 0.9032\n",
            "training loss at step 3 - batch 379: 0.32 (2019-08-04 13:35:27.245980)\n",
            "Accuracy at step 3 - batch 379: 0.9032\n",
            "training loss at step 3 - batch 380: 0.33 (2019-08-04 13:35:27.259835)\n",
            "Accuracy at step 3 - batch 380: 0.9052\n",
            "training loss at step 3 - batch 381: 0.32 (2019-08-04 13:35:27.383417)\n",
            "Accuracy at step 3 - batch 381: 0.9064\n",
            "training loss at step 3 - batch 382: 0.36 (2019-08-04 13:35:27.397250)\n",
            "Accuracy at step 3 - batch 382: 0.8944\n",
            "training loss at step 3 - batch 383: 0.34 (2019-08-04 13:35:27.409600)\n",
            "Accuracy at step 3 - batch 383: 0.9004\n",
            "training loss at step 3 - batch 384: 0.35 (2019-08-04 13:35:27.422653)\n",
            "Accuracy at step 3 - batch 384: 0.8956\n",
            "training loss at step 3 - batch 385: 0.33 (2019-08-04 13:35:27.435274)\n",
            "Accuracy at step 3 - batch 385: 0.9056\n",
            "training loss at step 3 - batch 386: 0.37 (2019-08-04 13:35:27.564249)\n",
            "Accuracy at step 3 - batch 386: 0.8904\n",
            "training loss at step 3 - batch 387: 0.36 (2019-08-04 13:35:27.576858)\n",
            "Accuracy at step 3 - batch 387: 0.8924\n",
            "training loss at step 3 - batch 388: 0.36 (2019-08-04 13:35:27.591181)\n",
            "Accuracy at step 3 - batch 388: 0.8952\n",
            "training loss at step 3 - batch 389: 0.36 (2019-08-04 13:35:27.603620)\n",
            "Accuracy at step 3 - batch 389: 0.8936\n",
            "training loss at step 3 - batch 390: 0.34 (2019-08-04 13:35:27.615898)\n",
            "Accuracy at step 3 - batch 390: 0.9008\n",
            "training loss at step 3 - batch 391: 0.33 (2019-08-04 13:35:27.736404)\n",
            "Accuracy at step 3 - batch 391: 0.9076\n",
            "training loss at step 3 - batch 392: 0.34 (2019-08-04 13:35:27.752035)\n",
            "Accuracy at step 3 - batch 392: 0.8996\n",
            "training loss at step 3 - batch 393: 0.34 (2019-08-04 13:35:27.765657)\n",
            "Accuracy at step 3 - batch 393: 0.9036\n",
            "training loss at step 3 - batch 394: 0.34 (2019-08-04 13:35:27.777250)\n",
            "Accuracy at step 3 - batch 394: 0.9016\n",
            "training loss at step 3 - batch 395: 0.37 (2019-08-04 13:35:27.789667)\n",
            "Accuracy at step 3 - batch 395: 0.888\n",
            "training loss at step 3 - batch 396: 0.36 (2019-08-04 13:35:27.908525)\n",
            "Accuracy at step 3 - batch 396: 0.8964\n",
            "training loss at step 3 - batch 397: 0.35 (2019-08-04 13:35:27.921247)\n",
            "Accuracy at step 3 - batch 397: 0.8968\n",
            "training loss at step 3 - batch 398: 0.35 (2019-08-04 13:35:27.934345)\n",
            "Accuracy at step 3 - batch 398: 0.8956\n",
            "training loss at step 3 - batch 399: 0.36 (2019-08-04 13:35:27.946871)\n",
            "Accuracy at step 3 - batch 399: 0.8928\n",
            "training loss at step 3 - batch 400: 0.36 (2019-08-04 13:35:27.960416)\n",
            "Accuracy at step 3 - batch 400: 0.8916\n",
            "training loss at step 3 - batch 401: 0.35 (2019-08-04 13:35:28.080473)\n",
            "Accuracy at step 3 - batch 401: 0.8956\n",
            "training loss at step 3 - batch 402: 0.33 (2019-08-04 13:35:28.093039)\n",
            "Accuracy at step 3 - batch 402: 0.9068\n",
            "training loss at step 3 - batch 403: 0.33 (2019-08-04 13:35:28.106085)\n",
            "Accuracy at step 3 - batch 403: 0.9024\n",
            "training loss at step 3 - batch 404: 0.33 (2019-08-04 13:35:28.120990)\n",
            "Accuracy at step 3 - batch 404: 0.9008\n",
            "training loss at step 3 - batch 405: 0.32 (2019-08-04 13:35:28.134165)\n",
            "Accuracy at step 3 - batch 405: 0.9044\n",
            "training loss at step 3 - batch 406: 0.36 (2019-08-04 13:35:28.254115)\n",
            "Accuracy at step 3 - batch 406: 0.8972\n",
            "training loss at step 3 - batch 407: 0.33 (2019-08-04 13:35:28.272034)\n",
            "Accuracy at step 3 - batch 407: 0.9004\n",
            "training loss at step 3 - batch 408: 0.33 (2019-08-04 13:35:28.285959)\n",
            "Accuracy at step 3 - batch 408: 0.9028\n",
            "training loss at step 3 - batch 409: 0.35 (2019-08-04 13:35:28.298297)\n",
            "Accuracy at step 3 - batch 409: 0.896\n",
            "training loss at step 3 - batch 410: 0.35 (2019-08-04 13:35:28.311415)\n",
            "Accuracy at step 3 - batch 410: 0.9004\n",
            "training loss at step 3 - batch 411: 0.35 (2019-08-04 13:35:28.434507)\n",
            "Accuracy at step 3 - batch 411: 0.8992\n",
            "training loss at step 3 - batch 412: 0.33 (2019-08-04 13:35:28.450442)\n",
            "Accuracy at step 3 - batch 412: 0.9036\n",
            "training loss at step 3 - batch 413: 0.34 (2019-08-04 13:35:28.463136)\n",
            "Accuracy at step 3 - batch 413: 0.8972\n",
            "training loss at step 3 - batch 414: 0.36 (2019-08-04 13:35:28.479549)\n",
            "Accuracy at step 3 - batch 414: 0.8968\n",
            "training loss at step 3 - batch 415: 0.34 (2019-08-04 13:35:28.495394)\n",
            "Accuracy at step 3 - batch 415: 0.8972\n",
            "training loss at step 3 - batch 416: 0.35 (2019-08-04 13:35:28.619273)\n",
            "Accuracy at step 3 - batch 416: 0.8988\n",
            "training loss at step 3 - batch 417: 0.34 (2019-08-04 13:35:28.634795)\n",
            "Accuracy at step 3 - batch 417: 0.9036\n",
            "training loss at step 3 - batch 418: 0.33 (2019-08-04 13:35:28.650690)\n",
            "Accuracy at step 3 - batch 418: 0.8992\n",
            "training loss at step 3 - batch 419: 0.35 (2019-08-04 13:35:28.663852)\n",
            "Accuracy at step 3 - batch 419: 0.8956\n",
            "training loss at step 3 - batch 420: 0.34 (2019-08-04 13:35:28.677110)\n",
            "Accuracy at step 3 - batch 420: 0.8988\n",
            "training loss at step 3 - batch 421: 0.32 (2019-08-04 13:35:28.799868)\n",
            "Accuracy at step 3 - batch 421: 0.9024\n",
            "training loss at step 3 - batch 422: 0.33 (2019-08-04 13:35:28.814732)\n",
            "Accuracy at step 3 - batch 422: 0.898\n",
            "training loss at step 3 - batch 423: 0.34 (2019-08-04 13:35:28.827627)\n",
            "Accuracy at step 3 - batch 423: 0.898\n",
            "training loss at step 3 - batch 424: 0.33 (2019-08-04 13:35:28.842258)\n",
            "Accuracy at step 3 - batch 424: 0.9044\n",
            "training loss at step 3 - batch 425: 0.37 (2019-08-04 13:35:28.855080)\n",
            "Accuracy at step 3 - batch 425: 0.8872\n",
            "training loss at step 3 - batch 426: 0.34 (2019-08-04 13:35:28.973538)\n",
            "Accuracy at step 3 - batch 426: 0.896\n",
            "training loss at step 3 - batch 427: 0.33 (2019-08-04 13:35:28.987454)\n",
            "Accuracy at step 3 - batch 427: 0.9048\n",
            "training loss at step 3 - batch 428: 0.34 (2019-08-04 13:35:29.002391)\n",
            "Accuracy at step 3 - batch 428: 0.8984\n",
            "training loss at step 3 - batch 429: 0.33 (2019-08-04 13:35:29.015294)\n",
            "Accuracy at step 3 - batch 429: 0.9048\n",
            "training loss at step 3 - batch 430: 0.33 (2019-08-04 13:35:29.028797)\n",
            "Accuracy at step 3 - batch 430: 0.902\n",
            "training loss at step 3 - batch 431: 0.30 (2019-08-04 13:35:29.150798)\n",
            "Accuracy at step 3 - batch 431: 0.9116\n",
            "training loss at step 3 - batch 432: 0.36 (2019-08-04 13:35:29.164628)\n",
            "Accuracy at step 3 - batch 432: 0.8968\n",
            "training loss at step 3 - batch 433: 0.36 (2019-08-04 13:35:29.177069)\n",
            "Accuracy at step 3 - batch 433: 0.8956\n",
            "training loss at step 3 - batch 434: 0.35 (2019-08-04 13:35:29.190238)\n",
            "Accuracy at step 3 - batch 434: 0.892\n",
            "training loss at step 3 - batch 435: 0.35 (2019-08-04 13:35:29.203443)\n",
            "Accuracy at step 3 - batch 435: 0.8952\n",
            "training loss at step 3 - batch 436: 0.33 (2019-08-04 13:35:29.321185)\n",
            "Accuracy at step 3 - batch 436: 0.8976\n",
            "training loss at step 3 - batch 437: 0.33 (2019-08-04 13:35:29.338341)\n",
            "Accuracy at step 3 - batch 437: 0.9044\n",
            "training loss at step 3 - batch 438: 0.35 (2019-08-04 13:35:29.354947)\n",
            "Accuracy at step 3 - batch 438: 0.8976\n",
            "training loss at step 3 - batch 439: 0.36 (2019-08-04 13:35:29.369556)\n",
            "Accuracy at step 3 - batch 439: 0.8948\n",
            "training loss at step 3 - batch 440: 0.36 (2019-08-04 13:35:29.383372)\n",
            "Accuracy at step 3 - batch 440: 0.8932\n",
            "training loss at step 3 - batch 441: 0.36 (2019-08-04 13:35:29.504771)\n",
            "Accuracy at step 3 - batch 441: 0.8912\n",
            "training loss at step 3 - batch 442: 0.35 (2019-08-04 13:35:29.519044)\n",
            "Accuracy at step 3 - batch 442: 0.894\n",
            "training loss at step 3 - batch 443: 0.33 (2019-08-04 13:35:29.534028)\n",
            "Accuracy at step 3 - batch 443: 0.9012\n",
            "training loss at step 3 - batch 444: 0.31 (2019-08-04 13:35:29.546725)\n",
            "Accuracy at step 3 - batch 444: 0.9088\n",
            "training loss at step 3 - batch 445: 0.31 (2019-08-04 13:35:29.561041)\n",
            "Accuracy at step 3 - batch 445: 0.91\n",
            "training loss at step 3 - batch 446: 0.37 (2019-08-04 13:35:29.685876)\n",
            "Accuracy at step 3 - batch 446: 0.8956\n",
            "training loss at step 3 - batch 447: 0.35 (2019-08-04 13:35:29.704033)\n",
            "Accuracy at step 3 - batch 447: 0.8968\n",
            "training loss at step 3 - batch 448: 0.35 (2019-08-04 13:35:29.716871)\n",
            "Accuracy at step 3 - batch 448: 0.8972\n",
            "training loss at step 3 - batch 449: 0.31 (2019-08-04 13:35:29.730774)\n",
            "Accuracy at step 3 - batch 449: 0.9124\n",
            "training loss at step 3 - batch 450: 0.34 (2019-08-04 13:35:29.743352)\n",
            "Accuracy at step 3 - batch 450: 0.8996\n",
            "training loss at step 3 - batch 451: 0.34 (2019-08-04 13:35:29.868539)\n",
            "Accuracy at step 3 - batch 451: 0.9\n",
            "training loss at step 3 - batch 452: 0.33 (2019-08-04 13:35:29.882246)\n",
            "Accuracy at step 3 - batch 452: 0.904\n",
            "training loss at step 3 - batch 453: 0.35 (2019-08-04 13:35:29.895909)\n",
            "Accuracy at step 3 - batch 453: 0.9008\n",
            "training loss at step 3 - batch 454: 0.34 (2019-08-04 13:35:29.908857)\n",
            "Accuracy at step 3 - batch 454: 0.904\n",
            "training loss at step 3 - batch 455: 0.32 (2019-08-04 13:35:29.921386)\n",
            "Accuracy at step 3 - batch 455: 0.9116\n",
            "training loss at step 3 - batch 456: 0.32 (2019-08-04 13:35:30.035510)\n",
            "Accuracy at step 3 - batch 456: 0.9052\n",
            "training loss at step 3 - batch 457: 0.36 (2019-08-04 13:35:30.048378)\n",
            "Accuracy at step 3 - batch 457: 0.8924\n",
            "training loss at step 3 - batch 458: 0.34 (2019-08-04 13:35:30.061061)\n",
            "Accuracy at step 3 - batch 458: 0.8968\n",
            "training loss at step 3 - batch 459: 0.36 (2019-08-04 13:35:30.074907)\n",
            "Accuracy at step 3 - batch 459: 0.8972\n",
            "training loss at step 3 - batch 460: 0.35 (2019-08-04 13:35:30.087993)\n",
            "Accuracy at step 3 - batch 460: 0.8948\n",
            "training loss at step 3 - batch 461: 0.32 (2019-08-04 13:35:30.209347)\n",
            "Accuracy at step 3 - batch 461: 0.906\n",
            "training loss at step 3 - batch 462: 0.36 (2019-08-04 13:35:30.227690)\n",
            "Accuracy at step 3 - batch 462: 0.8968\n",
            "training loss at step 3 - batch 463: 0.36 (2019-08-04 13:35:30.242027)\n",
            "Accuracy at step 3 - batch 463: 0.9\n",
            "training loss at step 3 - batch 464: 0.35 (2019-08-04 13:35:30.255374)\n",
            "Accuracy at step 3 - batch 464: 0.8944\n",
            "training loss at step 3 - batch 465: 0.34 (2019-08-04 13:35:30.267370)\n",
            "Accuracy at step 3 - batch 465: 0.902\n",
            "training loss at step 3 - batch 466: 0.35 (2019-08-04 13:35:30.394167)\n",
            "Accuracy at step 3 - batch 466: 0.8932\n",
            "training loss at step 3 - batch 467: 0.36 (2019-08-04 13:35:30.406409)\n",
            "Accuracy at step 3 - batch 467: 0.8944\n",
            "training loss at step 3 - batch 468: 0.32 (2019-08-04 13:35:30.418910)\n",
            "Accuracy at step 3 - batch 468: 0.9072\n",
            "training loss at step 3 - batch 469: 0.36 (2019-08-04 13:35:30.430525)\n",
            "Accuracy at step 3 - batch 469: 0.8888\n",
            "training loss at step 3 - batch 470: 0.34 (2019-08-04 13:35:30.444794)\n",
            "Accuracy at step 3 - batch 470: 0.9004\n",
            "training loss at step 3 - batch 471: 0.36 (2019-08-04 13:35:30.590064)\n",
            "Accuracy at step 3 - batch 471: 0.8992\n",
            "training loss at step 3 - batch 472: 0.32 (2019-08-04 13:35:30.605281)\n",
            "Accuracy at step 3 - batch 472: 0.9076\n",
            "training loss at step 3 - batch 473: 0.33 (2019-08-04 13:35:30.618256)\n",
            "Accuracy at step 3 - batch 473: 0.9016\n",
            "training loss at step 3 - batch 474: 0.32 (2019-08-04 13:35:30.631117)\n",
            "Accuracy at step 3 - batch 474: 0.9048\n",
            "training loss at step 3 - batch 475: 0.33 (2019-08-04 13:35:30.643384)\n",
            "Accuracy at step 3 - batch 475: 0.9064\n",
            "training loss at step 3 - batch 476: 0.34 (2019-08-04 13:35:30.770813)\n",
            "Accuracy at step 3 - batch 476: 0.9036\n",
            "training loss at step 3 - batch 477: 0.34 (2019-08-04 13:35:30.787120)\n",
            "Accuracy at step 3 - batch 477: 0.9028\n",
            "training loss at step 3 - batch 478: 0.35 (2019-08-04 13:35:30.800426)\n",
            "Accuracy at step 3 - batch 478: 0.8964\n",
            "training loss at step 3 - batch 479: 0.33 (2019-08-04 13:35:30.815080)\n",
            "Accuracy at step 3 - batch 479: 0.9092\n",
            "training loss at step 3 - batch 480: 0.35 (2019-08-04 13:35:30.827445)\n",
            "Accuracy at step 3 - batch 480: 0.8984\n",
            "training loss at step 3 - batch 481: 0.32 (2019-08-04 13:35:30.945145)\n",
            "Accuracy at step 3 - batch 481: 0.9028\n",
            "training loss at step 3 - batch 482: 0.32 (2019-08-04 13:35:30.961091)\n",
            "Accuracy at step 3 - batch 482: 0.9068\n",
            "training loss at step 3 - batch 483: 0.36 (2019-08-04 13:35:30.975229)\n",
            "Accuracy at step 3 - batch 483: 0.8968\n",
            "training loss at step 3 - batch 484: 0.34 (2019-08-04 13:35:30.989618)\n",
            "Accuracy at step 3 - batch 484: 0.904\n",
            "training loss at step 3 - batch 485: 0.35 (2019-08-04 13:35:31.003033)\n",
            "Accuracy at step 3 - batch 485: 0.9012\n",
            "training loss at step 3 - batch 486: 0.35 (2019-08-04 13:35:31.126338)\n",
            "Accuracy at step 3 - batch 486: 0.8976\n",
            "training loss at step 3 - batch 487: 0.34 (2019-08-04 13:35:31.140370)\n",
            "Accuracy at step 3 - batch 487: 0.9012\n",
            "training loss at step 3 - batch 488: 0.33 (2019-08-04 13:35:31.152234)\n",
            "Accuracy at step 3 - batch 488: 0.9068\n",
            "training loss at step 3 - batch 489: 0.33 (2019-08-04 13:35:31.164253)\n",
            "Accuracy at step 3 - batch 489: 0.9016\n",
            "training loss at step 3 - batch 490: 0.35 (2019-08-04 13:35:31.176355)\n",
            "Accuracy at step 3 - batch 490: 0.892\n",
            "training loss at step 3 - batch 491: 0.35 (2019-08-04 13:35:31.296836)\n",
            "Accuracy at step 3 - batch 491: 0.9\n",
            "training loss at step 3 - batch 492: 0.35 (2019-08-04 13:35:31.310915)\n",
            "Accuracy at step 3 - batch 492: 0.9008\n",
            "training loss at step 3 - batch 493: 0.35 (2019-08-04 13:35:31.324439)\n",
            "Accuracy at step 3 - batch 493: 0.8984\n",
            "training loss at step 3 - batch 494: 0.34 (2019-08-04 13:35:31.339177)\n",
            "Accuracy at step 3 - batch 494: 0.8996\n",
            "training loss at step 3 - batch 495: 0.35 (2019-08-04 13:35:31.352115)\n",
            "Accuracy at step 3 - batch 495: 0.8932\n",
            "training loss at step 3 - batch 496: 0.32 (2019-08-04 13:35:31.471527)\n",
            "Accuracy at step 3 - batch 496: 0.9076\n",
            "training loss at step 3 - batch 497: 0.36 (2019-08-04 13:35:31.488215)\n",
            "Accuracy at step 3 - batch 497: 0.894\n",
            "training loss at step 3 - batch 498: 0.32 (2019-08-04 13:35:31.501469)\n",
            "Accuracy at step 3 - batch 498: 0.9048\n",
            "training loss at step 3 - batch 499: 0.35 (2019-08-04 13:35:31.513497)\n",
            "Accuracy at step 3 - batch 499: 0.8908\n",
            "training loss at step 3 - batch 500: 0.33 (2019-08-04 13:35:31.525570)\n",
            "Accuracy at step 3 - batch 500: 0.9028\n",
            "training loss at step 3 - batch 501: 0.34 (2019-08-04 13:35:31.657774)\n",
            "Accuracy at step 3 - batch 501: 0.9024\n",
            "training loss at step 3 - batch 502: 0.34 (2019-08-04 13:35:31.672739)\n",
            "Accuracy at step 3 - batch 502: 0.8992\n",
            "training loss at step 3 - batch 503: 0.35 (2019-08-04 13:35:31.686148)\n",
            "Accuracy at step 3 - batch 503: 0.894\n",
            "training loss at step 3 - batch 504: 0.34 (2019-08-04 13:35:31.699251)\n",
            "Accuracy at step 3 - batch 504: 0.8992\n",
            "training loss at step 3 - batch 505: 0.32 (2019-08-04 13:35:31.712874)\n",
            "Accuracy at step 3 - batch 505: 0.9076\n",
            "training loss at step 3 - batch 506: 0.32 (2019-08-04 13:35:31.834930)\n",
            "Accuracy at step 3 - batch 506: 0.908\n",
            "training loss at step 3 - batch 507: 0.35 (2019-08-04 13:35:31.848914)\n",
            "Accuracy at step 3 - batch 507: 0.8992\n",
            "training loss at step 3 - batch 508: 0.33 (2019-08-04 13:35:31.863900)\n",
            "Accuracy at step 3 - batch 508: 0.9036\n",
            "training loss at step 3 - batch 509: 0.36 (2019-08-04 13:35:31.877943)\n",
            "Accuracy at step 3 - batch 509: 0.8964\n",
            "training loss at step 3 - batch 510: 0.35 (2019-08-04 13:35:31.891238)\n",
            "Accuracy at step 3 - batch 510: 0.8952\n",
            "training loss at step 3 - batch 511: 0.36 (2019-08-04 13:35:32.011852)\n",
            "Accuracy at step 3 - batch 511: 0.8964\n",
            "training loss at step 3 - batch 512: 0.35 (2019-08-04 13:35:32.028585)\n",
            "Accuracy at step 3 - batch 512: 0.8944\n",
            "training loss at step 3 - batch 513: 0.34 (2019-08-04 13:35:32.041203)\n",
            "Accuracy at step 3 - batch 513: 0.9044\n",
            "training loss at step 3 - batch 514: 0.36 (2019-08-04 13:35:32.053266)\n",
            "Accuracy at step 3 - batch 514: 0.8952\n",
            "training loss at step 3 - batch 515: 0.35 (2019-08-04 13:35:32.066570)\n",
            "Accuracy at step 3 - batch 515: 0.888\n",
            "training loss at step 3 - batch 516: 0.32 (2019-08-04 13:35:32.194895)\n",
            "Accuracy at step 3 - batch 516: 0.9008\n",
            "training loss at step 3 - batch 517: 0.33 (2019-08-04 13:35:32.212653)\n",
            "Accuracy at step 3 - batch 517: 0.9028\n",
            "training loss at step 3 - batch 518: 0.34 (2019-08-04 13:35:32.226823)\n",
            "Accuracy at step 3 - batch 518: 0.8956\n",
            "training loss at step 3 - batch 519: 0.34 (2019-08-04 13:35:32.238778)\n",
            "Accuracy at step 3 - batch 519: 0.9036\n",
            "training loss at step 3 - batch 520: 0.32 (2019-08-04 13:35:32.253418)\n",
            "Accuracy at step 3 - batch 520: 0.9068\n",
            "training loss at step 3 - batch 521: 0.34 (2019-08-04 13:35:32.377590)\n",
            "Accuracy at step 3 - batch 521: 0.8972\n",
            "training loss at step 3 - batch 522: 0.34 (2019-08-04 13:35:32.393423)\n",
            "Accuracy at step 3 - batch 522: 0.8952\n",
            "training loss at step 3 - batch 523: 0.34 (2019-08-04 13:35:32.405695)\n",
            "Accuracy at step 3 - batch 523: 0.8996\n",
            "training loss at step 3 - batch 524: 0.34 (2019-08-04 13:35:32.418214)\n",
            "Accuracy at step 3 - batch 524: 0.898\n",
            "training loss at step 3 - batch 525: 0.34 (2019-08-04 13:35:32.430528)\n",
            "Accuracy at step 3 - batch 525: 0.9012\n",
            "training loss at step 3 - batch 526: 0.34 (2019-08-04 13:35:32.554194)\n",
            "Accuracy at step 3 - batch 526: 0.902\n",
            "training loss at step 3 - batch 527: 0.35 (2019-08-04 13:35:32.570518)\n",
            "Accuracy at step 3 - batch 527: 0.8928\n",
            "training loss at step 3 - batch 528: 0.34 (2019-08-04 13:35:32.590217)\n",
            "Accuracy at step 3 - batch 528: 0.9016\n",
            "training loss at step 3 - batch 529: 0.32 (2019-08-04 13:35:32.602635)\n",
            "Accuracy at step 3 - batch 529: 0.8984\n",
            "training loss at step 3 - batch 530: 0.32 (2019-08-04 13:35:32.614687)\n",
            "Accuracy at step 3 - batch 530: 0.9036\n",
            "training loss at step 3 - batch 531: 0.34 (2019-08-04 13:35:32.735130)\n",
            "Accuracy at step 3 - batch 531: 0.8996\n",
            "training loss at step 3 - batch 532: 0.36 (2019-08-04 13:35:32.747388)\n",
            "Accuracy at step 3 - batch 532: 0.8944\n",
            "training loss at step 3 - batch 533: 0.34 (2019-08-04 13:35:32.759945)\n",
            "Accuracy at step 3 - batch 533: 0.9004\n",
            "training loss at step 3 - batch 534: 0.33 (2019-08-04 13:35:32.771868)\n",
            "Accuracy at step 3 - batch 534: 0.9012\n",
            "training loss at step 3 - batch 535: 0.34 (2019-08-04 13:35:32.784282)\n",
            "Accuracy at step 3 - batch 535: 0.8944\n",
            "training loss at step 3 - batch 536: 0.32 (2019-08-04 13:35:32.910980)\n",
            "Accuracy at step 3 - batch 536: 0.9024\n",
            "training loss at step 3 - batch 537: 0.36 (2019-08-04 13:35:32.927935)\n",
            "Accuracy at step 3 - batch 537: 0.8996\n",
            "training loss at step 3 - batch 538: 0.31 (2019-08-04 13:35:32.939785)\n",
            "Accuracy at step 3 - batch 538: 0.9076\n",
            "training loss at step 3 - batch 539: 0.33 (2019-08-04 13:35:32.951644)\n",
            "Accuracy at step 3 - batch 539: 0.9048\n",
            "training loss at step 3 - batch 540: 0.34 (2019-08-04 13:35:32.963937)\n",
            "Accuracy at step 3 - batch 540: 0.8976\n",
            "training loss at step 3 - batch 541: 0.33 (2019-08-04 13:35:33.085380)\n",
            "Accuracy at step 3 - batch 541: 0.902\n",
            "training loss at step 3 - batch 542: 0.36 (2019-08-04 13:35:33.098001)\n",
            "Accuracy at step 3 - batch 542: 0.896\n",
            "training loss at step 3 - batch 543: 0.34 (2019-08-04 13:35:33.111116)\n",
            "Accuracy at step 3 - batch 543: 0.904\n",
            "training loss at step 3 - batch 544: 0.32 (2019-08-04 13:35:33.129911)\n",
            "Accuracy at step 3 - batch 544: 0.9056\n",
            "training loss at step 3 - batch 545: 0.34 (2019-08-04 13:35:33.143332)\n",
            "Accuracy at step 3 - batch 545: 0.896\n",
            "training loss at step 3 - batch 546: 0.32 (2019-08-04 13:35:33.266540)\n",
            "Accuracy at step 3 - batch 546: 0.912\n",
            "training loss at step 3 - batch 547: 0.35 (2019-08-04 13:35:33.279936)\n",
            "Accuracy at step 3 - batch 547: 0.8948\n",
            "training loss at step 3 - batch 548: 0.33 (2019-08-04 13:35:33.292850)\n",
            "Accuracy at step 3 - batch 548: 0.9028\n",
            "training loss at step 3 - batch 549: 0.32 (2019-08-04 13:35:33.305592)\n",
            "Accuracy at step 3 - batch 549: 0.9032\n",
            "training loss at step 3 - batch 550: 0.34 (2019-08-04 13:35:33.322328)\n",
            "Accuracy at step 3 - batch 550: 0.8996\n",
            "training loss at step 3 - batch 551: 0.34 (2019-08-04 13:35:33.440127)\n",
            "Accuracy at step 3 - batch 551: 0.9008\n",
            "training loss at step 3 - batch 552: 0.36 (2019-08-04 13:35:33.455480)\n",
            "Accuracy at step 3 - batch 552: 0.8944\n",
            "training loss at step 3 - batch 553: 0.35 (2019-08-04 13:35:33.469033)\n",
            "Accuracy at step 3 - batch 553: 0.9012\n",
            "training loss at step 3 - batch 554: 0.34 (2019-08-04 13:35:33.482336)\n",
            "Accuracy at step 3 - batch 554: 0.8984\n",
            "training loss at step 3 - batch 555: 0.35 (2019-08-04 13:35:33.495397)\n",
            "Accuracy at step 3 - batch 555: 0.8928\n",
            "training loss at step 3 - batch 556: 0.32 (2019-08-04 13:35:33.617474)\n",
            "Accuracy at step 3 - batch 556: 0.902\n",
            "training loss at step 3 - batch 557: 0.32 (2019-08-04 13:35:33.629759)\n",
            "Accuracy at step 3 - batch 557: 0.9008\n",
            "training loss at step 3 - batch 558: 0.33 (2019-08-04 13:35:33.641606)\n",
            "Accuracy at step 3 - batch 558: 0.9028\n",
            "training loss at step 3 - batch 559: 0.33 (2019-08-04 13:35:33.655147)\n",
            "Accuracy at step 3 - batch 559: 0.8996\n",
            "training loss at step 3 - batch 560: 0.33 (2019-08-04 13:35:33.668311)\n",
            "Accuracy at step 3 - batch 560: 0.8968\n",
            "training loss at step 3 - batch 561: 0.35 (2019-08-04 13:35:33.789684)\n",
            "Accuracy at step 3 - batch 561: 0.8948\n",
            "training loss at step 3 - batch 562: 0.34 (2019-08-04 13:35:33.802051)\n",
            "Accuracy at step 3 - batch 562: 0.9024\n",
            "training loss at step 3 - batch 563: 0.34 (2019-08-04 13:35:33.815346)\n",
            "Accuracy at step 3 - batch 563: 0.8996\n",
            "training loss at step 3 - batch 564: 0.35 (2019-08-04 13:35:33.830938)\n",
            "Accuracy at step 3 - batch 564: 0.8996\n",
            "training loss at step 3 - batch 565: 0.34 (2019-08-04 13:35:33.843704)\n",
            "Accuracy at step 3 - batch 565: 0.8976\n",
            "training loss at step 3 - batch 566: 0.32 (2019-08-04 13:35:33.964017)\n",
            "Accuracy at step 3 - batch 566: 0.902\n",
            "training loss at step 3 - batch 567: 0.32 (2019-08-04 13:35:33.982214)\n",
            "Accuracy at step 3 - batch 567: 0.9084\n",
            "training loss at step 3 - batch 568: 0.32 (2019-08-04 13:35:33.995182)\n",
            "Accuracy at step 3 - batch 568: 0.9036\n",
            "training loss at step 3 - batch 569: 0.33 (2019-08-04 13:35:34.008145)\n",
            "Accuracy at step 3 - batch 569: 0.9016\n",
            "training loss at step 3 - batch 570: 0.32 (2019-08-04 13:35:34.020175)\n",
            "Accuracy at step 3 - batch 570: 0.906\n",
            "training loss at step 3 - batch 571: 0.33 (2019-08-04 13:35:34.141448)\n",
            "Accuracy at step 3 - batch 571: 0.9012\n",
            "training loss at step 3 - batch 572: 0.34 (2019-08-04 13:35:34.155534)\n",
            "Accuracy at step 3 - batch 572: 0.8952\n",
            "training loss at step 3 - batch 573: 0.33 (2019-08-04 13:35:34.167757)\n",
            "Accuracy at step 3 - batch 573: 0.8992\n",
            "training loss at step 3 - batch 574: 0.33 (2019-08-04 13:35:34.180607)\n",
            "Accuracy at step 3 - batch 574: 0.9028\n",
            "training loss at step 3 - batch 575: 0.32 (2019-08-04 13:35:34.192902)\n",
            "Accuracy at step 3 - batch 575: 0.908\n",
            "training loss at step 3 - batch 576: 0.33 (2019-08-04 13:35:34.309847)\n",
            "Accuracy at step 3 - batch 576: 0.9032\n",
            "training loss at step 3 - batch 577: 0.35 (2019-08-04 13:35:34.325854)\n",
            "Accuracy at step 3 - batch 577: 0.8964\n",
            "training loss at step 3 - batch 578: 0.33 (2019-08-04 13:35:34.338434)\n",
            "Accuracy at step 3 - batch 578: 0.9044\n",
            "training loss at step 3 - batch 579: 0.34 (2019-08-04 13:35:34.353121)\n",
            "Accuracy at step 3 - batch 579: 0.8984\n",
            "training loss at step 3 - batch 580: 0.36 (2019-08-04 13:35:34.366737)\n",
            "Accuracy at step 3 - batch 580: 0.8944\n",
            "training loss at step 3 - batch 581: 0.33 (2019-08-04 13:35:34.485187)\n",
            "Accuracy at step 3 - batch 581: 0.9048\n",
            "training loss at step 3 - batch 582: 0.34 (2019-08-04 13:35:34.502166)\n",
            "Accuracy at step 3 - batch 582: 0.9024\n",
            "training loss at step 3 - batch 583: 0.34 (2019-08-04 13:35:34.514967)\n",
            "Accuracy at step 3 - batch 583: 0.902\n",
            "training loss at step 3 - batch 584: 0.33 (2019-08-04 13:35:34.526717)\n",
            "Accuracy at step 3 - batch 584: 0.9028\n",
            "training loss at step 3 - batch 585: 0.36 (2019-08-04 13:35:34.540115)\n",
            "Accuracy at step 3 - batch 585: 0.9004\n",
            "training loss at step 3 - batch 586: 0.35 (2019-08-04 13:35:34.672007)\n",
            "Accuracy at step 3 - batch 586: 0.8972\n",
            "training loss at step 3 - batch 587: 0.35 (2019-08-04 13:35:34.684662)\n",
            "Accuracy at step 3 - batch 587: 0.8968\n",
            "training loss at step 3 - batch 588: 0.37 (2019-08-04 13:35:34.697320)\n",
            "Accuracy at step 3 - batch 588: 0.8992\n",
            "training loss at step 3 - batch 589: 0.35 (2019-08-04 13:35:34.710381)\n",
            "Accuracy at step 3 - batch 589: 0.9004\n",
            "training loss at step 3 - batch 590: 0.35 (2019-08-04 13:35:34.725038)\n",
            "Accuracy at step 3 - batch 590: 0.9\n",
            "training loss at step 3 - batch 591: 0.31 (2019-08-04 13:35:34.849453)\n",
            "Accuracy at step 3 - batch 591: 0.9088\n",
            "training loss at step 3 - batch 592: 0.34 (2019-08-04 13:35:34.866182)\n",
            "Accuracy at step 3 - batch 592: 0.9012\n",
            "training loss at step 3 - batch 593: 0.34 (2019-08-04 13:35:34.881602)\n",
            "Accuracy at step 3 - batch 593: 0.9004\n",
            "training loss at step 3 - batch 594: 0.33 (2019-08-04 13:35:34.894016)\n",
            "Accuracy at step 3 - batch 594: 0.9004\n",
            "training loss at step 3 - batch 595: 0.34 (2019-08-04 13:35:34.906790)\n",
            "Accuracy at step 3 - batch 595: 0.8968\n",
            "training loss at step 3 - batch 596: 0.34 (2019-08-04 13:35:35.027438)\n",
            "Accuracy at step 3 - batch 596: 0.9\n",
            "training loss at step 3 - batch 597: 0.34 (2019-08-04 13:35:35.043704)\n",
            "Accuracy at step 3 - batch 597: 0.9004\n",
            "training loss at step 3 - batch 598: 0.34 (2019-08-04 13:35:35.056151)\n",
            "Accuracy at step 3 - batch 598: 0.9008\n",
            "training loss at step 3 - batch 599: 0.35 (2019-08-04 13:35:35.069733)\n",
            "Accuracy at step 3 - batch 599: 0.894\n",
            "training loss at step 3 - batch 600: 0.37 (2019-08-04 13:35:35.082260)\n",
            "Accuracy at step 3 - batch 600: 0.89\n",
            "training loss at step 3 - batch 601: 0.33 (2019-08-04 13:35:35.205958)\n",
            "Accuracy at step 3 - batch 601: 0.906\n",
            "training loss at step 3 - batch 602: 0.33 (2019-08-04 13:35:35.222715)\n",
            "Accuracy at step 3 - batch 602: 0.8996\n",
            "training loss at step 3 - batch 603: 0.32 (2019-08-04 13:35:35.235316)\n",
            "Accuracy at step 3 - batch 603: 0.904\n",
            "training loss at step 3 - batch 604: 0.35 (2019-08-04 13:35:35.249605)\n",
            "Accuracy at step 3 - batch 604: 0.896\n",
            "training loss at step 3 - batch 605: 0.34 (2019-08-04 13:35:35.263601)\n",
            "Accuracy at step 3 - batch 605: 0.9044\n",
            "training loss at step 3 - batch 606: 0.34 (2019-08-04 13:35:35.388238)\n",
            "Accuracy at step 3 - batch 606: 0.8992\n",
            "training loss at step 3 - batch 607: 0.36 (2019-08-04 13:35:35.402319)\n",
            "Accuracy at step 3 - batch 607: 0.8976\n",
            "training loss at step 3 - batch 608: 0.35 (2019-08-04 13:35:35.416418)\n",
            "Accuracy at step 3 - batch 608: 0.8976\n",
            "training loss at step 3 - batch 609: 0.33 (2019-08-04 13:35:35.429493)\n",
            "Accuracy at step 3 - batch 609: 0.8956\n",
            "training loss at step 3 - batch 610: 0.33 (2019-08-04 13:35:35.442378)\n",
            "Accuracy at step 3 - batch 610: 0.9004\n",
            "training loss at step 3 - batch 611: 0.33 (2019-08-04 13:35:35.567710)\n",
            "Accuracy at step 3 - batch 611: 0.9036\n",
            "training loss at step 3 - batch 612: 0.33 (2019-08-04 13:35:35.585911)\n",
            "Accuracy at step 3 - batch 612: 0.9028\n",
            "training loss at step 3 - batch 613: 0.35 (2019-08-04 13:35:35.601311)\n",
            "Accuracy at step 3 - batch 613: 0.896\n",
            "training loss at step 3 - batch 614: 0.33 (2019-08-04 13:35:35.617717)\n",
            "Accuracy at step 3 - batch 614: 0.8996\n",
            "training loss at step 3 - batch 615: 0.33 (2019-08-04 13:35:35.632822)\n",
            "Accuracy at step 3 - batch 615: 0.9024\n",
            "training loss at step 3 - batch 616: 0.34 (2019-08-04 13:35:35.763406)\n",
            "Accuracy at step 3 - batch 616: 0.902\n",
            "training loss at step 3 - batch 617: 0.35 (2019-08-04 13:35:35.776664)\n",
            "Accuracy at step 3 - batch 617: 0.8992\n",
            "training loss at step 3 - batch 618: 0.37 (2019-08-04 13:35:35.790839)\n",
            "Accuracy at step 3 - batch 618: 0.8932\n",
            "training loss at step 3 - batch 619: 0.33 (2019-08-04 13:35:35.806005)\n",
            "Accuracy at step 3 - batch 619: 0.8996\n",
            "training loss at step 3 - batch 620: 0.32 (2019-08-04 13:35:35.818939)\n",
            "Accuracy at step 3 - batch 620: 0.902\n",
            "training loss at step 3 - batch 621: 0.36 (2019-08-04 13:35:35.941709)\n",
            "Accuracy at step 3 - batch 621: 0.8884\n",
            "training loss at step 3 - batch 622: 0.33 (2019-08-04 13:35:35.962359)\n",
            "Accuracy at step 3 - batch 622: 0.9012\n",
            "training loss at step 3 - batch 623: 0.34 (2019-08-04 13:35:35.977125)\n",
            "Accuracy at step 3 - batch 623: 0.8984\n",
            "training loss at step 3 - batch 624: 0.34 (2019-08-04 13:35:35.996182)\n",
            "Accuracy at step 3 - batch 624: 0.896\n",
            "training loss at step 3 - batch 625: 0.33 (2019-08-04 13:35:36.010526)\n",
            "Accuracy at step 3 - batch 625: 0.9\n",
            "training loss at step 3 - batch 626: 0.33 (2019-08-04 13:35:36.133331)\n",
            "Accuracy at step 3 - batch 626: 0.9024\n",
            "training loss at step 3 - batch 627: 0.35 (2019-08-04 13:35:36.146339)\n",
            "Accuracy at step 3 - batch 627: 0.8952\n",
            "training loss at step 3 - batch 628: 0.35 (2019-08-04 13:35:36.161085)\n",
            "Accuracy at step 3 - batch 628: 0.9016\n",
            "training loss at step 3 - batch 629: 0.33 (2019-08-04 13:35:36.174227)\n",
            "Accuracy at step 3 - batch 629: 0.9024\n",
            "training loss at step 3 - batch 630: 0.35 (2019-08-04 13:35:36.186318)\n",
            "Accuracy at step 3 - batch 630: 0.8948\n",
            "training loss at step 3 - batch 631: 0.33 (2019-08-04 13:35:36.314329)\n",
            "Accuracy at step 3 - batch 631: 0.9008\n",
            "training loss at step 3 - batch 632: 0.36 (2019-08-04 13:35:36.332753)\n",
            "Accuracy at step 3 - batch 632: 0.896\n",
            "training loss at step 3 - batch 633: 0.36 (2019-08-04 13:35:36.346157)\n",
            "Accuracy at step 3 - batch 633: 0.8956\n",
            "training loss at step 3 - batch 634: 0.35 (2019-08-04 13:35:36.358919)\n",
            "Accuracy at step 3 - batch 634: 0.8964\n",
            "training loss at step 3 - batch 635: 0.32 (2019-08-04 13:35:36.372877)\n",
            "Accuracy at step 3 - batch 635: 0.9072\n",
            "training loss at step 3 - batch 636: 0.33 (2019-08-04 13:35:36.493046)\n",
            "Accuracy at step 3 - batch 636: 0.9012\n",
            "training loss at step 3 - batch 637: 0.34 (2019-08-04 13:35:36.507827)\n",
            "Accuracy at step 3 - batch 637: 0.9012\n",
            "training loss at step 3 - batch 638: 0.35 (2019-08-04 13:35:36.522868)\n",
            "Accuracy at step 3 - batch 638: 0.896\n",
            "training loss at step 3 - batch 639: 0.33 (2019-08-04 13:35:36.535531)\n",
            "Accuracy at step 3 - batch 639: 0.9044\n",
            "training loss at step 3 - batch 640: 0.35 (2019-08-04 13:35:36.548117)\n",
            "Accuracy at step 3 - batch 640: 0.8992\n",
            "training loss at step 3 - batch 641: 0.32 (2019-08-04 13:35:36.671707)\n",
            "Accuracy at step 3 - batch 641: 0.9072\n",
            "training loss at step 3 - batch 642: 0.32 (2019-08-04 13:35:36.687610)\n",
            "Accuracy at step 3 - batch 642: 0.906\n",
            "training loss at step 3 - batch 643: 0.34 (2019-08-04 13:35:36.700538)\n",
            "Accuracy at step 3 - batch 643: 0.8984\n",
            "training loss at step 3 - batch 644: 0.35 (2019-08-04 13:35:36.713550)\n",
            "Accuracy at step 3 - batch 644: 0.8988\n",
            "training loss at step 3 - batch 645: 0.32 (2019-08-04 13:35:36.728972)\n",
            "Accuracy at step 3 - batch 645: 0.9076\n",
            "training loss at step 3 - batch 646: 0.35 (2019-08-04 13:35:36.849787)\n",
            "Accuracy at step 3 - batch 646: 0.892\n",
            "training loss at step 3 - batch 647: 0.34 (2019-08-04 13:35:36.864110)\n",
            "Accuracy at step 3 - batch 647: 0.8984\n",
            "training loss at step 3 - batch 648: 0.35 (2019-08-04 13:35:36.876342)\n",
            "Accuracy at step 3 - batch 648: 0.8992\n",
            "training loss at step 3 - batch 649: 0.34 (2019-08-04 13:35:36.889468)\n",
            "Accuracy at step 3 - batch 649: 0.8996\n",
            "training loss at step 3 - batch 650: 0.33 (2019-08-04 13:35:36.901840)\n",
            "Accuracy at step 3 - batch 650: 0.9048\n",
            "training loss at step 3 - batch 651: 0.32 (2019-08-04 13:35:37.029719)\n",
            "Accuracy at step 3 - batch 651: 0.904\n",
            "training loss at step 3 - batch 652: 0.36 (2019-08-04 13:35:37.045937)\n",
            "Accuracy at step 3 - batch 652: 0.8944\n",
            "training loss at step 3 - batch 653: 0.33 (2019-08-04 13:35:37.059296)\n",
            "Accuracy at step 3 - batch 653: 0.9076\n",
            "training loss at step 3 - batch 654: 0.35 (2019-08-04 13:35:37.072306)\n",
            "Accuracy at step 3 - batch 654: 0.8968\n",
            "training loss at step 3 - batch 655: 0.33 (2019-08-04 13:35:37.085727)\n",
            "Accuracy at step 3 - batch 655: 0.8956\n",
            "training loss at step 3 - batch 656: 0.35 (2019-08-04 13:35:37.204310)\n",
            "Accuracy at step 3 - batch 656: 0.8968\n",
            "training loss at step 3 - batch 657: 0.34 (2019-08-04 13:35:37.221679)\n",
            "Accuracy at step 3 - batch 657: 0.8936\n",
            "training loss at step 3 - batch 658: 0.32 (2019-08-04 13:35:37.235454)\n",
            "Accuracy at step 3 - batch 658: 0.9044\n",
            "training loss at step 3 - batch 659: 0.34 (2019-08-04 13:35:37.248661)\n",
            "Accuracy at step 3 - batch 659: 0.904\n",
            "training loss at step 3 - batch 660: 0.32 (2019-08-04 13:35:37.263003)\n",
            "Accuracy at step 3 - batch 660: 0.9032\n",
            "training loss at step 3 - batch 661: 0.34 (2019-08-04 13:35:37.377899)\n",
            "Accuracy at step 3 - batch 661: 0.8984\n",
            "training loss at step 3 - batch 662: 0.32 (2019-08-04 13:35:37.392541)\n",
            "Accuracy at step 3 - batch 662: 0.9096\n",
            "training loss at step 3 - batch 663: 0.34 (2019-08-04 13:35:37.406949)\n",
            "Accuracy at step 3 - batch 663: 0.8992\n",
            "training loss at step 3 - batch 664: 0.34 (2019-08-04 13:35:37.419624)\n",
            "Accuracy at step 3 - batch 664: 0.904\n",
            "training loss at step 3 - batch 665: 0.33 (2019-08-04 13:35:37.431503)\n",
            "Accuracy at step 3 - batch 665: 0.898\n",
            "training loss at step 3 - batch 666: 0.34 (2019-08-04 13:35:37.557433)\n",
            "Accuracy at step 3 - batch 666: 0.9012\n",
            "training loss at step 3 - batch 667: 0.35 (2019-08-04 13:35:37.570291)\n",
            "Accuracy at step 3 - batch 667: 0.8956\n",
            "training loss at step 3 - batch 668: 0.34 (2019-08-04 13:35:37.583148)\n",
            "Accuracy at step 3 - batch 668: 0.898\n",
            "training loss at step 3 - batch 669: 0.33 (2019-08-04 13:35:37.597467)\n",
            "Accuracy at step 3 - batch 669: 0.906\n",
            "training loss at step 3 - batch 670: 0.34 (2019-08-04 13:35:37.609825)\n",
            "Accuracy at step 3 - batch 670: 0.904\n",
            "training loss at step 3 - batch 671: 0.36 (2019-08-04 13:35:37.730484)\n",
            "Accuracy at step 3 - batch 671: 0.8924\n",
            "training loss at step 3 - batch 672: 0.34 (2019-08-04 13:35:37.745108)\n",
            "Accuracy at step 3 - batch 672: 0.898\n",
            "training loss at step 3 - batch 673: 0.32 (2019-08-04 13:35:37.758217)\n",
            "Accuracy at step 3 - batch 673: 0.9032\n",
            "training loss at step 3 - batch 674: 0.34 (2019-08-04 13:35:37.775243)\n",
            "Accuracy at step 3 - batch 674: 0.9004\n",
            "training loss at step 3 - batch 675: 0.33 (2019-08-04 13:35:37.788032)\n",
            "Accuracy at step 3 - batch 675: 0.9016\n",
            "training loss at step 3 - batch 676: 0.31 (2019-08-04 13:35:37.912829)\n",
            "Accuracy at step 3 - batch 676: 0.9044\n",
            "training loss at step 3 - batch 677: 0.35 (2019-08-04 13:35:37.929977)\n",
            "Accuracy at step 3 - batch 677: 0.902\n",
            "training loss at step 3 - batch 678: 0.34 (2019-08-04 13:35:37.942325)\n",
            "Accuracy at step 3 - batch 678: 0.9024\n",
            "training loss at step 3 - batch 679: 0.35 (2019-08-04 13:35:37.954266)\n",
            "Accuracy at step 3 - batch 679: 0.9032\n",
            "training loss at step 3 - batch 680: 0.33 (2019-08-04 13:35:37.967045)\n",
            "Accuracy at step 3 - batch 680: 0.9064\n",
            "training loss at step 3 - batch 681: 0.34 (2019-08-04 13:35:38.089949)\n",
            "Accuracy at step 3 - batch 681: 0.902\n",
            "training loss at step 3 - batch 682: 0.34 (2019-08-04 13:35:38.105451)\n",
            "Accuracy at step 3 - batch 682: 0.9008\n",
            "training loss at step 3 - batch 683: 0.34 (2019-08-04 13:35:38.118391)\n",
            "Accuracy at step 3 - batch 683: 0.9\n",
            "training loss at step 3 - batch 684: 0.34 (2019-08-04 13:35:38.135872)\n",
            "Accuracy at step 3 - batch 684: 0.9\n",
            "training loss at step 3 - batch 685: 0.36 (2019-08-04 13:35:38.148102)\n",
            "Accuracy at step 3 - batch 685: 0.8988\n",
            "training loss at step 3 - batch 686: 0.34 (2019-08-04 13:35:38.269492)\n",
            "Accuracy at step 3 - batch 686: 0.9008\n",
            "training loss at step 3 - batch 687: 0.35 (2019-08-04 13:35:38.283549)\n",
            "Accuracy at step 3 - batch 687: 0.9052\n",
            "training loss at step 3 - batch 688: 0.35 (2019-08-04 13:35:38.298138)\n",
            "Accuracy at step 3 - batch 688: 0.8928\n",
            "training loss at step 3 - batch 689: 0.37 (2019-08-04 13:35:38.313781)\n",
            "Accuracy at step 3 - batch 689: 0.8948\n",
            "training loss at step 3 - batch 690: 0.35 (2019-08-04 13:35:38.325790)\n",
            "Accuracy at step 3 - batch 690: 0.8972\n",
            "training loss at step 3 - batch 691: 0.33 (2019-08-04 13:35:38.440215)\n",
            "Accuracy at step 3 - batch 691: 0.9036\n",
            "training loss at step 3 - batch 692: 0.32 (2019-08-04 13:35:38.453383)\n",
            "Accuracy at step 3 - batch 692: 0.904\n",
            "training loss at step 3 - batch 693: 0.35 (2019-08-04 13:35:38.466341)\n",
            "Accuracy at step 3 - batch 693: 0.8968\n",
            "training loss at step 3 - batch 694: 0.32 (2019-08-04 13:35:38.479966)\n",
            "Accuracy at step 3 - batch 694: 0.9004\n",
            "training loss at step 3 - batch 695: 0.35 (2019-08-04 13:35:38.492863)\n",
            "Accuracy at step 3 - batch 695: 0.8952\n",
            "training loss at step 3 - batch 696: 0.33 (2019-08-04 13:35:38.614873)\n",
            "Accuracy at step 3 - batch 696: 0.904\n",
            "training loss at step 3 - batch 697: 0.31 (2019-08-04 13:35:38.629699)\n",
            "Accuracy at step 3 - batch 697: 0.9072\n",
            "training loss at step 3 - batch 698: 0.34 (2019-08-04 13:35:38.642262)\n",
            "Accuracy at step 3 - batch 698: 0.8952\n",
            "training loss at step 3 - batch 699: 0.33 (2019-08-04 13:35:38.656652)\n",
            "Accuracy at step 3 - batch 699: 0.9052\n",
            "training loss at step 3 - batch 700: 0.33 (2019-08-04 13:35:38.669189)\n",
            "Accuracy at step 3 - batch 700: 0.8996\n",
            "training loss at step 3 - batch 701: 0.34 (2019-08-04 13:35:38.794132)\n",
            "Accuracy at step 3 - batch 701: 0.9048\n",
            "training loss at step 3 - batch 702: 0.33 (2019-08-04 13:35:38.806654)\n",
            "Accuracy at step 3 - batch 702: 0.898\n",
            "training loss at step 3 - batch 703: 0.33 (2019-08-04 13:35:38.820451)\n",
            "Accuracy at step 3 - batch 703: 0.8992\n",
            "training loss at step 3 - batch 704: 0.35 (2019-08-04 13:35:38.833119)\n",
            "Accuracy at step 3 - batch 704: 0.902\n",
            "training loss at step 3 - batch 705: 0.34 (2019-08-04 13:35:38.846297)\n",
            "Accuracy at step 3 - batch 705: 0.8952\n",
            "training loss at step 3 - batch 706: 0.33 (2019-08-04 13:35:38.965546)\n",
            "Accuracy at step 3 - batch 706: 0.9036\n",
            "training loss at step 3 - batch 707: 0.31 (2019-08-04 13:35:38.983021)\n",
            "Accuracy at step 3 - batch 707: 0.9076\n",
            "training loss at step 3 - batch 708: 0.34 (2019-08-04 13:35:38.996422)\n",
            "Accuracy at step 3 - batch 708: 0.8992\n",
            "training loss at step 3 - batch 709: 0.33 (2019-08-04 13:35:39.008711)\n",
            "Accuracy at step 3 - batch 709: 0.906\n",
            "training loss at step 3 - batch 710: 0.35 (2019-08-04 13:35:39.021375)\n",
            "Accuracy at step 3 - batch 710: 0.8984\n",
            "training loss at step 3 - batch 711: 0.35 (2019-08-04 13:35:39.150969)\n",
            "Accuracy at step 3 - batch 711: 0.8992\n",
            "training loss at step 3 - batch 712: 0.35 (2019-08-04 13:35:39.163872)\n",
            "Accuracy at step 3 - batch 712: 0.9004\n",
            "training loss at step 3 - batch 713: 0.35 (2019-08-04 13:35:39.175694)\n",
            "Accuracy at step 3 - batch 713: 0.8964\n",
            "training loss at step 3 - batch 714: 0.34 (2019-08-04 13:35:39.187485)\n",
            "Accuracy at step 3 - batch 714: 0.904\n",
            "training loss at step 3 - batch 715: 0.32 (2019-08-04 13:35:39.200057)\n",
            "Accuracy at step 3 - batch 715: 0.906\n",
            "training loss at step 3 - batch 716: 0.34 (2019-08-04 13:35:39.320496)\n",
            "Accuracy at step 3 - batch 716: 0.8988\n",
            "training loss at step 3 - batch 717: 0.34 (2019-08-04 13:35:39.333135)\n",
            "Accuracy at step 3 - batch 717: 0.8988\n",
            "training loss at step 3 - batch 718: 0.33 (2019-08-04 13:35:39.345920)\n",
            "Accuracy at step 3 - batch 718: 0.9012\n",
            "training loss at step 3 - batch 719: 0.32 (2019-08-04 13:35:39.359531)\n",
            "Accuracy at step 3 - batch 719: 0.9016\n",
            "training loss at step 3 - batch 720: 0.34 (2019-08-04 13:35:39.372261)\n",
            "Accuracy at step 3 - batch 720: 0.9032\n",
            "training loss at step 3 - batch 721: 0.31 (2019-08-04 13:35:39.489881)\n",
            "Accuracy at step 3 - batch 721: 0.9092\n",
            "training loss at step 3 - batch 722: 0.33 (2019-08-04 13:35:39.506320)\n",
            "Accuracy at step 3 - batch 722: 0.8972\n",
            "training loss at step 3 - batch 723: 0.33 (2019-08-04 13:35:39.518889)\n",
            "Accuracy at step 3 - batch 723: 0.9008\n",
            "training loss at step 3 - batch 724: 0.32 (2019-08-04 13:35:39.533696)\n",
            "Accuracy at step 3 - batch 724: 0.9048\n",
            "training loss at step 3 - batch 725: 0.34 (2019-08-04 13:35:39.546305)\n",
            "Accuracy at step 3 - batch 725: 0.9004\n",
            "training loss at step 3 - batch 726: 0.35 (2019-08-04 13:35:39.670279)\n",
            "Accuracy at step 3 - batch 726: 0.8952\n",
            "training loss at step 3 - batch 727: 0.31 (2019-08-04 13:35:39.683843)\n",
            "Accuracy at step 3 - batch 727: 0.9084\n",
            "training loss at step 3 - batch 728: 0.36 (2019-08-04 13:35:39.695698)\n",
            "Accuracy at step 3 - batch 728: 0.8976\n",
            "training loss at step 3 - batch 729: 0.34 (2019-08-04 13:35:39.708310)\n",
            "Accuracy at step 3 - batch 729: 0.9\n",
            "training loss at step 3 - batch 730: 0.34 (2019-08-04 13:35:39.720685)\n",
            "Accuracy at step 3 - batch 730: 0.8984\n",
            "training loss at step 3 - batch 731: 0.35 (2019-08-04 13:35:39.846873)\n",
            "Accuracy at step 3 - batch 731: 0.8932\n",
            "training loss at step 3 - batch 732: 0.35 (2019-08-04 13:35:39.859925)\n",
            "Accuracy at step 3 - batch 732: 0.8924\n",
            "training loss at step 3 - batch 733: 0.35 (2019-08-04 13:35:39.873541)\n",
            "Accuracy at step 3 - batch 733: 0.9\n",
            "training loss at step 3 - batch 734: 0.35 (2019-08-04 13:35:39.887069)\n",
            "Accuracy at step 3 - batch 734: 0.902\n",
            "training loss at step 3 - batch 735: 0.32 (2019-08-04 13:35:39.898856)\n",
            "Accuracy at step 3 - batch 735: 0.9004\n",
            "training loss at step 3 - batch 736: 0.33 (2019-08-04 13:35:40.021595)\n",
            "Accuracy at step 3 - batch 736: 0.9028\n",
            "training loss at step 3 - batch 737: 0.34 (2019-08-04 13:35:40.039007)\n",
            "Accuracy at step 3 - batch 737: 0.8948\n",
            "training loss at step 3 - batch 738: 0.34 (2019-08-04 13:35:40.053266)\n",
            "Accuracy at step 3 - batch 738: 0.8976\n",
            "training loss at step 3 - batch 739: 0.32 (2019-08-04 13:35:40.065476)\n",
            "Accuracy at step 3 - batch 739: 0.902\n",
            "training loss at step 3 - batch 740: 0.36 (2019-08-04 13:35:40.078060)\n",
            "Accuracy at step 3 - batch 740: 0.8952\n",
            "training loss at step 3 - batch 741: 0.33 (2019-08-04 13:35:40.198490)\n",
            "Accuracy at step 3 - batch 741: 0.9036\n",
            "training loss at step 3 - batch 742: 0.33 (2019-08-04 13:35:40.216184)\n",
            "Accuracy at step 3 - batch 742: 0.9016\n",
            "training loss at step 3 - batch 743: 0.33 (2019-08-04 13:35:40.230637)\n",
            "Accuracy at step 3 - batch 743: 0.904\n",
            "training loss at step 3 - batch 744: 0.35 (2019-08-04 13:35:40.243114)\n",
            "Accuracy at step 3 - batch 744: 0.896\n",
            "training loss at step 3 - batch 745: 0.35 (2019-08-04 13:35:40.257593)\n",
            "Accuracy at step 3 - batch 745: 0.8988\n",
            "training loss at step 3 - batch 746: 0.34 (2019-08-04 13:35:40.380832)\n",
            "Accuracy at step 3 - batch 746: 0.9004\n",
            "training loss at step 3 - batch 747: 0.33 (2019-08-04 13:35:40.394873)\n",
            "Accuracy at step 3 - batch 747: 0.8984\n",
            "training loss at step 3 - batch 748: 0.34 (2019-08-04 13:35:40.407556)\n",
            "Accuracy at step 3 - batch 748: 0.8964\n",
            "training loss at step 3 - batch 749: 0.34 (2019-08-04 13:35:40.419599)\n",
            "Accuracy at step 3 - batch 749: 0.9048\n",
            "training loss at step 3 - batch 750: 0.35 (2019-08-04 13:35:40.432107)\n",
            "Accuracy at step 3 - batch 750: 0.9004\n",
            "training loss at step 3 - batch 751: 0.32 (2019-08-04 13:35:40.557154)\n",
            "Accuracy at step 3 - batch 751: 0.9076\n",
            "training loss at step 3 - batch 752: 0.33 (2019-08-04 13:35:40.570454)\n",
            "Accuracy at step 3 - batch 752: 0.904\n",
            "training loss at step 3 - batch 753: 0.34 (2019-08-04 13:35:40.582740)\n",
            "Accuracy at step 3 - batch 753: 0.9012\n",
            "training loss at step 3 - batch 754: 0.33 (2019-08-04 13:35:40.597131)\n",
            "Accuracy at step 3 - batch 754: 0.9044\n",
            "training loss at step 3 - batch 755: 0.31 (2019-08-04 13:35:40.609044)\n",
            "Accuracy at step 3 - batch 755: 0.91\n",
            "training loss at step 3 - batch 756: 0.35 (2019-08-04 13:35:40.744396)\n",
            "Accuracy at step 3 - batch 756: 0.8988\n",
            "training loss at step 3 - batch 757: 0.34 (2019-08-04 13:35:40.765942)\n",
            "Accuracy at step 3 - batch 757: 0.898\n",
            "training loss at step 3 - batch 758: 0.35 (2019-08-04 13:35:40.780510)\n",
            "Accuracy at step 3 - batch 758: 0.9\n",
            "training loss at step 3 - batch 759: 0.33 (2019-08-04 13:35:40.792945)\n",
            "Accuracy at step 3 - batch 759: 0.9028\n",
            "training loss at step 3 - batch 760: 0.33 (2019-08-04 13:35:40.805363)\n",
            "Accuracy at step 3 - batch 760: 0.906\n",
            "training loss at step 3 - batch 761: 0.36 (2019-08-04 13:35:40.927338)\n",
            "Accuracy at step 3 - batch 761: 0.8964\n",
            "training loss at step 3 - batch 762: 0.33 (2019-08-04 13:35:40.941864)\n",
            "Accuracy at step 3 - batch 762: 0.9032\n",
            "training loss at step 3 - batch 763: 0.30 (2019-08-04 13:35:40.955180)\n",
            "Accuracy at step 3 - batch 763: 0.9128\n",
            "training loss at step 3 - batch 764: 0.34 (2019-08-04 13:35:40.967597)\n",
            "Accuracy at step 3 - batch 764: 0.8984\n",
            "training loss at step 3 - batch 765: 0.33 (2019-08-04 13:35:40.983890)\n",
            "Accuracy at step 3 - batch 765: 0.904\n",
            "training loss at step 3 - batch 766: 0.33 (2019-08-04 13:35:41.105655)\n",
            "Accuracy at step 3 - batch 766: 0.8996\n",
            "training loss at step 3 - batch 767: 0.32 (2019-08-04 13:35:41.120719)\n",
            "Accuracy at step 3 - batch 767: 0.9064\n",
            "training loss at step 3 - batch 768: 0.33 (2019-08-04 13:35:41.134040)\n",
            "Accuracy at step 3 - batch 768: 0.898\n",
            "training loss at step 3 - batch 769: 0.32 (2019-08-04 13:35:41.147060)\n",
            "Accuracy at step 3 - batch 769: 0.9064\n",
            "training loss at step 3 - batch 770: 0.33 (2019-08-04 13:35:41.159877)\n",
            "Accuracy at step 3 - batch 770: 0.904\n",
            "training loss at step 3 - batch 771: 0.36 (2019-08-04 13:35:41.288334)\n",
            "Accuracy at step 3 - batch 771: 0.894\n",
            "training loss at step 3 - batch 772: 0.32 (2019-08-04 13:35:41.304403)\n",
            "Accuracy at step 3 - batch 772: 0.902\n",
            "training loss at step 3 - batch 773: 0.35 (2019-08-04 13:35:41.316987)\n",
            "Accuracy at step 3 - batch 773: 0.9024\n",
            "training loss at step 3 - batch 774: 0.32 (2019-08-04 13:35:41.331262)\n",
            "Accuracy at step 3 - batch 774: 0.906\n",
            "training loss at step 3 - batch 775: 0.34 (2019-08-04 13:35:41.345160)\n",
            "Accuracy at step 3 - batch 775: 0.902\n",
            "training loss at step 3 - batch 776: 0.33 (2019-08-04 13:35:41.463546)\n",
            "Accuracy at step 3 - batch 776: 0.9012\n",
            "training loss at step 3 - batch 777: 0.34 (2019-08-04 13:35:41.480493)\n",
            "Accuracy at step 3 - batch 777: 0.8952\n",
            "training loss at step 3 - batch 778: 0.32 (2019-08-04 13:35:41.494495)\n",
            "Accuracy at step 3 - batch 778: 0.9044\n",
            "training loss at step 3 - batch 779: 0.33 (2019-08-04 13:35:41.508547)\n",
            "Accuracy at step 3 - batch 779: 0.9048\n",
            "training loss at step 4 - batch 0: 0.35 (2019-08-04 13:35:41.521693)\n",
            "Accuracy at step 4 - batch 0: 0.8984\n",
            "training loss at step 4 - batch 1: 0.34 (2019-08-04 13:35:41.647545)\n",
            "Accuracy at step 4 - batch 1: 0.9028\n",
            "training loss at step 4 - batch 2: 0.34 (2019-08-04 13:35:41.660123)\n",
            "Accuracy at step 4 - batch 2: 0.9036\n",
            "training loss at step 4 - batch 3: 0.31 (2019-08-04 13:35:41.672925)\n",
            "Accuracy at step 4 - batch 3: 0.9088\n",
            "training loss at step 4 - batch 4: 0.30 (2019-08-04 13:35:41.685591)\n",
            "Accuracy at step 4 - batch 4: 0.9104\n",
            "training loss at step 4 - batch 5: 0.35 (2019-08-04 13:35:41.699487)\n",
            "Accuracy at step 4 - batch 5: 0.8952\n",
            "training loss at step 4 - batch 6: 0.34 (2019-08-04 13:35:41.822643)\n",
            "Accuracy at step 4 - batch 6: 0.9052\n",
            "training loss at step 4 - batch 7: 0.34 (2019-08-04 13:35:41.834947)\n",
            "Accuracy at step 4 - batch 7: 0.9016\n",
            "training loss at step 4 - batch 8: 0.34 (2019-08-04 13:35:41.847790)\n",
            "Accuracy at step 4 - batch 8: 0.8972\n",
            "training loss at step 4 - batch 9: 0.35 (2019-08-04 13:35:41.861542)\n",
            "Accuracy at step 4 - batch 9: 0.8984\n",
            "training loss at step 4 - batch 10: 0.33 (2019-08-04 13:35:41.874087)\n",
            "Accuracy at step 4 - batch 10: 0.8992\n",
            "training loss at step 4 - batch 11: 0.34 (2019-08-04 13:35:42.004860)\n",
            "Accuracy at step 4 - batch 11: 0.8984\n",
            "training loss at step 4 - batch 12: 0.36 (2019-08-04 13:35:42.021710)\n",
            "Accuracy at step 4 - batch 12: 0.8968\n",
            "training loss at step 4 - batch 13: 0.34 (2019-08-04 13:35:42.036316)\n",
            "Accuracy at step 4 - batch 13: 0.9024\n",
            "training loss at step 4 - batch 14: 0.32 (2019-08-04 13:35:42.048381)\n",
            "Accuracy at step 4 - batch 14: 0.9044\n",
            "training loss at step 4 - batch 15: 0.34 (2019-08-04 13:35:42.061041)\n",
            "Accuracy at step 4 - batch 15: 0.9024\n",
            "training loss at step 4 - batch 16: 0.33 (2019-08-04 13:35:42.180526)\n",
            "Accuracy at step 4 - batch 16: 0.9048\n",
            "training loss at step 4 - batch 17: 0.33 (2019-08-04 13:35:42.197749)\n",
            "Accuracy at step 4 - batch 17: 0.9072\n",
            "training loss at step 4 - batch 18: 0.31 (2019-08-04 13:35:42.215094)\n",
            "Accuracy at step 4 - batch 18: 0.9112\n",
            "training loss at step 4 - batch 19: 0.35 (2019-08-04 13:35:42.228856)\n",
            "Accuracy at step 4 - batch 19: 0.8988\n",
            "training loss at step 4 - batch 20: 0.34 (2019-08-04 13:35:42.241428)\n",
            "Accuracy at step 4 - batch 20: 0.908\n",
            "training loss at step 4 - batch 21: 0.35 (2019-08-04 13:35:42.360697)\n",
            "Accuracy at step 4 - batch 21: 0.8988\n",
            "training loss at step 4 - batch 22: 0.34 (2019-08-04 13:35:42.376172)\n",
            "Accuracy at step 4 - batch 22: 0.8956\n",
            "training loss at step 4 - batch 23: 0.31 (2019-08-04 13:35:42.390485)\n",
            "Accuracy at step 4 - batch 23: 0.9072\n",
            "training loss at step 4 - batch 24: 0.34 (2019-08-04 13:35:42.402620)\n",
            "Accuracy at step 4 - batch 24: 0.8968\n",
            "training loss at step 4 - batch 25: 0.34 (2019-08-04 13:35:42.415057)\n",
            "Accuracy at step 4 - batch 25: 0.9032\n",
            "training loss at step 4 - batch 26: 0.33 (2019-08-04 13:35:42.543912)\n",
            "Accuracy at step 4 - batch 26: 0.9032\n",
            "training loss at step 4 - batch 27: 0.34 (2019-08-04 13:35:42.562752)\n",
            "Accuracy at step 4 - batch 27: 0.9\n",
            "training loss at step 4 - batch 28: 0.32 (2019-08-04 13:35:42.575860)\n",
            "Accuracy at step 4 - batch 28: 0.9064\n",
            "training loss at step 4 - batch 29: 0.33 (2019-08-04 13:35:42.589223)\n",
            "Accuracy at step 4 - batch 29: 0.9068\n",
            "training loss at step 4 - batch 30: 0.37 (2019-08-04 13:35:42.602608)\n",
            "Accuracy at step 4 - batch 30: 0.8888\n",
            "training loss at step 4 - batch 31: 0.32 (2019-08-04 13:35:42.723669)\n",
            "Accuracy at step 4 - batch 31: 0.9052\n",
            "training loss at step 4 - batch 32: 0.34 (2019-08-04 13:35:42.737478)\n",
            "Accuracy at step 4 - batch 32: 0.8968\n",
            "training loss at step 4 - batch 33: 0.33 (2019-08-04 13:35:42.754301)\n",
            "Accuracy at step 4 - batch 33: 0.9016\n",
            "training loss at step 4 - batch 34: 0.34 (2019-08-04 13:35:42.767167)\n",
            "Accuracy at step 4 - batch 34: 0.8956\n",
            "training loss at step 4 - batch 35: 0.34 (2019-08-04 13:35:42.779522)\n",
            "Accuracy at step 4 - batch 35: 0.9032\n",
            "training loss at step 4 - batch 36: 0.33 (2019-08-04 13:35:42.906772)\n",
            "Accuracy at step 4 - batch 36: 0.8992\n",
            "training loss at step 4 - batch 37: 0.35 (2019-08-04 13:35:42.919547)\n",
            "Accuracy at step 4 - batch 37: 0.8944\n",
            "training loss at step 4 - batch 38: 0.34 (2019-08-04 13:35:42.931224)\n",
            "Accuracy at step 4 - batch 38: 0.9036\n",
            "training loss at step 4 - batch 39: 0.34 (2019-08-04 13:35:42.943597)\n",
            "Accuracy at step 4 - batch 39: 0.9016\n",
            "training loss at step 4 - batch 40: 0.34 (2019-08-04 13:35:42.959053)\n",
            "Accuracy at step 4 - batch 40: 0.9068\n",
            "training loss at step 4 - batch 41: 0.34 (2019-08-04 13:35:43.079216)\n",
            "Accuracy at step 4 - batch 41: 0.898\n",
            "training loss at step 4 - batch 42: 0.35 (2019-08-04 13:35:43.096055)\n",
            "Accuracy at step 4 - batch 42: 0.8948\n",
            "training loss at step 4 - batch 43: 0.36 (2019-08-04 13:35:43.110070)\n",
            "Accuracy at step 4 - batch 43: 0.8968\n",
            "training loss at step 4 - batch 44: 0.30 (2019-08-04 13:35:43.122544)\n",
            "Accuracy at step 4 - batch 44: 0.9084\n",
            "training loss at step 4 - batch 45: 0.31 (2019-08-04 13:35:43.135405)\n",
            "Accuracy at step 4 - batch 45: 0.9084\n",
            "training loss at step 4 - batch 46: 0.33 (2019-08-04 13:35:43.260092)\n",
            "Accuracy at step 4 - batch 46: 0.904\n",
            "training loss at step 4 - batch 47: 0.34 (2019-08-04 13:35:43.274441)\n",
            "Accuracy at step 4 - batch 47: 0.9024\n",
            "training loss at step 4 - batch 48: 0.32 (2019-08-04 13:35:43.287188)\n",
            "Accuracy at step 4 - batch 48: 0.9008\n",
            "training loss at step 4 - batch 49: 0.34 (2019-08-04 13:35:43.300420)\n",
            "Accuracy at step 4 - batch 49: 0.9016\n",
            "training loss at step 4 - batch 50: 0.36 (2019-08-04 13:35:43.313093)\n",
            "Accuracy at step 4 - batch 50: 0.8948\n",
            "training loss at step 4 - batch 51: 0.31 (2019-08-04 13:35:43.436007)\n",
            "Accuracy at step 4 - batch 51: 0.9064\n",
            "training loss at step 4 - batch 52: 0.34 (2019-08-04 13:35:43.449279)\n",
            "Accuracy at step 4 - batch 52: 0.8996\n",
            "training loss at step 4 - batch 53: 0.34 (2019-08-04 13:35:43.464971)\n",
            "Accuracy at step 4 - batch 53: 0.8964\n",
            "training loss at step 4 - batch 54: 0.33 (2019-08-04 13:35:43.479201)\n",
            "Accuracy at step 4 - batch 54: 0.9028\n",
            "training loss at step 4 - batch 55: 0.33 (2019-08-04 13:35:43.493177)\n",
            "Accuracy at step 4 - batch 55: 0.9052\n",
            "training loss at step 4 - batch 56: 0.33 (2019-08-04 13:35:43.613776)\n",
            "Accuracy at step 4 - batch 56: 0.8988\n",
            "training loss at step 4 - batch 57: 0.34 (2019-08-04 13:35:43.630631)\n",
            "Accuracy at step 4 - batch 57: 0.9032\n",
            "training loss at step 4 - batch 58: 0.33 (2019-08-04 13:35:43.646334)\n",
            "Accuracy at step 4 - batch 58: 0.9008\n",
            "training loss at step 4 - batch 59: 0.34 (2019-08-04 13:35:43.660002)\n",
            "Accuracy at step 4 - batch 59: 0.9004\n",
            "training loss at step 4 - batch 60: 0.33 (2019-08-04 13:35:43.675461)\n",
            "Accuracy at step 4 - batch 60: 0.9032\n",
            "training loss at step 4 - batch 61: 0.36 (2019-08-04 13:35:43.796643)\n",
            "Accuracy at step 4 - batch 61: 0.8916\n",
            "training loss at step 4 - batch 62: 0.34 (2019-08-04 13:35:43.811717)\n",
            "Accuracy at step 4 - batch 62: 0.8996\n",
            "training loss at step 4 - batch 63: 0.37 (2019-08-04 13:35:43.825782)\n",
            "Accuracy at step 4 - batch 63: 0.8896\n",
            "training loss at step 4 - batch 64: 0.34 (2019-08-04 13:35:43.843449)\n",
            "Accuracy at step 4 - batch 64: 0.8984\n",
            "training loss at step 4 - batch 65: 0.34 (2019-08-04 13:35:43.857233)\n",
            "Accuracy at step 4 - batch 65: 0.9016\n",
            "training loss at step 4 - batch 66: 0.34 (2019-08-04 13:35:43.977947)\n",
            "Accuracy at step 4 - batch 66: 0.9012\n",
            "training loss at step 4 - batch 67: 0.34 (2019-08-04 13:35:43.991308)\n",
            "Accuracy at step 4 - batch 67: 0.8972\n",
            "training loss at step 4 - batch 68: 0.31 (2019-08-04 13:35:44.003347)\n",
            "Accuracy at step 4 - batch 68: 0.9092\n",
            "training loss at step 4 - batch 69: 0.31 (2019-08-04 13:35:44.016063)\n",
            "Accuracy at step 4 - batch 69: 0.9036\n",
            "training loss at step 4 - batch 70: 0.29 (2019-08-04 13:35:44.028874)\n",
            "Accuracy at step 4 - batch 70: 0.9116\n",
            "training loss at step 4 - batch 71: 0.35 (2019-08-04 13:35:44.157600)\n",
            "Accuracy at step 4 - batch 71: 0.8992\n",
            "training loss at step 4 - batch 72: 0.33 (2019-08-04 13:35:44.171538)\n",
            "Accuracy at step 4 - batch 72: 0.906\n",
            "training loss at step 4 - batch 73: 0.34 (2019-08-04 13:35:44.185052)\n",
            "Accuracy at step 4 - batch 73: 0.8984\n",
            "training loss at step 4 - batch 74: 0.33 (2019-08-04 13:35:44.199928)\n",
            "Accuracy at step 4 - batch 74: 0.9072\n",
            "training loss at step 4 - batch 75: 0.35 (2019-08-04 13:35:44.213137)\n",
            "Accuracy at step 4 - batch 75: 0.9004\n",
            "training loss at step 4 - batch 76: 0.34 (2019-08-04 13:35:44.332780)\n",
            "Accuracy at step 4 - batch 76: 0.9016\n",
            "training loss at step 4 - batch 77: 0.34 (2019-08-04 13:35:44.349426)\n",
            "Accuracy at step 4 - batch 77: 0.8964\n",
            "training loss at step 4 - batch 78: 0.34 (2019-08-04 13:35:44.362096)\n",
            "Accuracy at step 4 - batch 78: 0.8988\n",
            "training loss at step 4 - batch 79: 0.35 (2019-08-04 13:35:44.375239)\n",
            "Accuracy at step 4 - batch 79: 0.898\n",
            "training loss at step 4 - batch 80: 0.33 (2019-08-04 13:35:44.389003)\n",
            "Accuracy at step 4 - batch 80: 0.9036\n",
            "training loss at step 4 - batch 81: 0.32 (2019-08-04 13:35:44.505643)\n",
            "Accuracy at step 4 - batch 81: 0.9048\n",
            "training loss at step 4 - batch 82: 0.34 (2019-08-04 13:35:44.520165)\n",
            "Accuracy at step 4 - batch 82: 0.8988\n",
            "training loss at step 4 - batch 83: 0.31 (2019-08-04 13:35:44.532698)\n",
            "Accuracy at step 4 - batch 83: 0.9032\n",
            "training loss at step 4 - batch 84: 0.34 (2019-08-04 13:35:44.544467)\n",
            "Accuracy at step 4 - batch 84: 0.9012\n",
            "training loss at step 4 - batch 85: 0.34 (2019-08-04 13:35:44.556902)\n",
            "Accuracy at step 4 - batch 85: 0.9\n",
            "training loss at step 4 - batch 86: 0.34 (2019-08-04 13:35:44.678745)\n",
            "Accuracy at step 4 - batch 86: 0.8996\n",
            "training loss at step 4 - batch 87: 0.34 (2019-08-04 13:35:44.696663)\n",
            "Accuracy at step 4 - batch 87: 0.9004\n",
            "training loss at step 4 - batch 88: 0.34 (2019-08-04 13:35:44.709203)\n",
            "Accuracy at step 4 - batch 88: 0.9\n",
            "training loss at step 4 - batch 89: 0.32 (2019-08-04 13:35:44.722087)\n",
            "Accuracy at step 4 - batch 89: 0.9032\n",
            "training loss at step 4 - batch 90: 0.30 (2019-08-04 13:35:44.736037)\n",
            "Accuracy at step 4 - batch 90: 0.9108\n",
            "training loss at step 4 - batch 91: 0.34 (2019-08-04 13:35:44.854795)\n",
            "Accuracy at step 4 - batch 91: 0.902\n",
            "training loss at step 4 - batch 92: 0.35 (2019-08-04 13:35:44.872778)\n",
            "Accuracy at step 4 - batch 92: 0.8996\n",
            "training loss at step 4 - batch 93: 0.35 (2019-08-04 13:35:44.889844)\n",
            "Accuracy at step 4 - batch 93: 0.9\n",
            "training loss at step 4 - batch 94: 0.33 (2019-08-04 13:35:44.903281)\n",
            "Accuracy at step 4 - batch 94: 0.8952\n",
            "training loss at step 4 - batch 95: 0.35 (2019-08-04 13:35:44.916223)\n",
            "Accuracy at step 4 - batch 95: 0.8964\n",
            "training loss at step 4 - batch 96: 0.33 (2019-08-04 13:35:45.037718)\n",
            "Accuracy at step 4 - batch 96: 0.8988\n",
            "training loss at step 4 - batch 97: 0.32 (2019-08-04 13:35:45.054675)\n",
            "Accuracy at step 4 - batch 97: 0.91\n",
            "training loss at step 4 - batch 98: 0.37 (2019-08-04 13:35:45.068516)\n",
            "Accuracy at step 4 - batch 98: 0.8948\n",
            "training loss at step 4 - batch 99: 0.33 (2019-08-04 13:35:45.082517)\n",
            "Accuracy at step 4 - batch 99: 0.9016\n",
            "training loss at step 4 - batch 100: 0.33 (2019-08-04 13:35:45.096632)\n",
            "Accuracy at step 4 - batch 100: 0.9028\n",
            "training loss at step 4 - batch 101: 0.34 (2019-08-04 13:35:45.218763)\n",
            "Accuracy at step 4 - batch 101: 0.898\n",
            "training loss at step 4 - batch 102: 0.34 (2019-08-04 13:35:45.234335)\n",
            "Accuracy at step 4 - batch 102: 0.898\n",
            "training loss at step 4 - batch 103: 0.34 (2019-08-04 13:35:45.246794)\n",
            "Accuracy at step 4 - batch 103: 0.896\n",
            "training loss at step 4 - batch 104: 0.31 (2019-08-04 13:35:45.260586)\n",
            "Accuracy at step 4 - batch 104: 0.9096\n",
            "training loss at step 4 - batch 105: 0.32 (2019-08-04 13:35:45.273067)\n",
            "Accuracy at step 4 - batch 105: 0.9072\n",
            "training loss at step 4 - batch 106: 0.34 (2019-08-04 13:35:45.392742)\n",
            "Accuracy at step 4 - batch 106: 0.8992\n",
            "training loss at step 4 - batch 107: 0.33 (2019-08-04 13:35:45.406174)\n",
            "Accuracy at step 4 - batch 107: 0.8936\n",
            "training loss at step 4 - batch 108: 0.35 (2019-08-04 13:35:45.419218)\n",
            "Accuracy at step 4 - batch 108: 0.9\n",
            "training loss at step 4 - batch 109: 0.33 (2019-08-04 13:35:45.431844)\n",
            "Accuracy at step 4 - batch 109: 0.9056\n",
            "training loss at step 4 - batch 110: 0.34 (2019-08-04 13:35:45.444276)\n",
            "Accuracy at step 4 - batch 110: 0.9028\n",
            "training loss at step 4 - batch 111: 0.32 (2019-08-04 13:35:45.561104)\n",
            "Accuracy at step 4 - batch 111: 0.904\n",
            "training loss at step 4 - batch 112: 0.34 (2019-08-04 13:35:45.574842)\n",
            "Accuracy at step 4 - batch 112: 0.8988\n",
            "training loss at step 4 - batch 113: 0.33 (2019-08-04 13:35:45.587426)\n",
            "Accuracy at step 4 - batch 113: 0.8996\n",
            "training loss at step 4 - batch 114: 0.33 (2019-08-04 13:35:45.602120)\n",
            "Accuracy at step 4 - batch 114: 0.906\n",
            "training loss at step 4 - batch 115: 0.34 (2019-08-04 13:35:45.615098)\n",
            "Accuracy at step 4 - batch 115: 0.904\n",
            "training loss at step 4 - batch 116: 0.34 (2019-08-04 13:35:45.752150)\n",
            "Accuracy at step 4 - batch 116: 0.9012\n",
            "training loss at step 4 - batch 117: 0.31 (2019-08-04 13:35:45.770667)\n",
            "Accuracy at step 4 - batch 117: 0.9096\n",
            "training loss at step 4 - batch 118: 0.33 (2019-08-04 13:35:45.783231)\n",
            "Accuracy at step 4 - batch 118: 0.904\n",
            "training loss at step 4 - batch 119: 0.34 (2019-08-04 13:35:45.797402)\n",
            "Accuracy at step 4 - batch 119: 0.8956\n",
            "training loss at step 4 - batch 120: 0.31 (2019-08-04 13:35:45.812480)\n",
            "Accuracy at step 4 - batch 120: 0.9076\n",
            "training loss at step 4 - batch 121: 0.34 (2019-08-04 13:35:45.946373)\n",
            "Accuracy at step 4 - batch 121: 0.8988\n",
            "training loss at step 4 - batch 122: 0.33 (2019-08-04 13:35:45.959375)\n",
            "Accuracy at step 4 - batch 122: 0.8984\n",
            "training loss at step 4 - batch 123: 0.34 (2019-08-04 13:35:45.972180)\n",
            "Accuracy at step 4 - batch 123: 0.896\n",
            "training loss at step 4 - batch 124: 0.32 (2019-08-04 13:35:45.985237)\n",
            "Accuracy at step 4 - batch 124: 0.9076\n",
            "training loss at step 4 - batch 125: 0.34 (2019-08-04 13:35:45.998358)\n",
            "Accuracy at step 4 - batch 125: 0.9\n",
            "training loss at step 4 - batch 126: 0.33 (2019-08-04 13:35:46.129032)\n",
            "Accuracy at step 4 - batch 126: 0.9024\n",
            "training loss at step 4 - batch 127: 0.35 (2019-08-04 13:35:46.142626)\n",
            "Accuracy at step 4 - batch 127: 0.896\n",
            "training loss at step 4 - batch 128: 0.34 (2019-08-04 13:35:46.154866)\n",
            "Accuracy at step 4 - batch 128: 0.8964\n",
            "training loss at step 4 - batch 129: 0.30 (2019-08-04 13:35:46.168832)\n",
            "Accuracy at step 4 - batch 129: 0.9164\n",
            "training loss at step 4 - batch 130: 0.35 (2019-08-04 13:35:46.184665)\n",
            "Accuracy at step 4 - batch 130: 0.9024\n",
            "training loss at step 4 - batch 131: 0.34 (2019-08-04 13:35:46.305646)\n",
            "Accuracy at step 4 - batch 131: 0.8944\n",
            "training loss at step 4 - batch 132: 0.34 (2019-08-04 13:35:46.318598)\n",
            "Accuracy at step 4 - batch 132: 0.9044\n",
            "training loss at step 4 - batch 133: 0.34 (2019-08-04 13:35:46.333039)\n",
            "Accuracy at step 4 - batch 133: 0.908\n",
            "training loss at step 4 - batch 134: 0.34 (2019-08-04 13:35:46.347118)\n",
            "Accuracy at step 4 - batch 134: 0.8956\n",
            "training loss at step 4 - batch 135: 0.33 (2019-08-04 13:35:46.359484)\n",
            "Accuracy at step 4 - batch 135: 0.898\n",
            "training loss at step 4 - batch 136: 0.37 (2019-08-04 13:35:46.476054)\n",
            "Accuracy at step 4 - batch 136: 0.8892\n",
            "training loss at step 4 - batch 137: 0.32 (2019-08-04 13:35:46.490943)\n",
            "Accuracy at step 4 - batch 137: 0.9032\n",
            "training loss at step 4 - batch 138: 0.32 (2019-08-04 13:35:46.503021)\n",
            "Accuracy at step 4 - batch 138: 0.9028\n",
            "training loss at step 4 - batch 139: 0.32 (2019-08-04 13:35:46.515645)\n",
            "Accuracy at step 4 - batch 139: 0.9056\n",
            "training loss at step 4 - batch 140: 0.36 (2019-08-04 13:35:46.529393)\n",
            "Accuracy at step 4 - batch 140: 0.8892\n",
            "training loss at step 4 - batch 141: 0.33 (2019-08-04 13:35:46.652733)\n",
            "Accuracy at step 4 - batch 141: 0.904\n",
            "training loss at step 4 - batch 142: 0.32 (2019-08-04 13:35:46.670690)\n",
            "Accuracy at step 4 - batch 142: 0.9072\n",
            "training loss at step 4 - batch 143: 0.34 (2019-08-04 13:35:46.683418)\n",
            "Accuracy at step 4 - batch 143: 0.8964\n",
            "training loss at step 4 - batch 144: 0.32 (2019-08-04 13:35:46.696927)\n",
            "Accuracy at step 4 - batch 144: 0.9004\n",
            "training loss at step 4 - batch 145: 0.33 (2019-08-04 13:35:46.709264)\n",
            "Accuracy at step 4 - batch 145: 0.9004\n",
            "training loss at step 4 - batch 146: 0.34 (2019-08-04 13:35:46.826293)\n",
            "Accuracy at step 4 - batch 146: 0.9056\n",
            "training loss at step 4 - batch 147: 0.30 (2019-08-04 13:35:46.843524)\n",
            "Accuracy at step 4 - batch 147: 0.912\n",
            "training loss at step 4 - batch 148: 0.33 (2019-08-04 13:35:46.857946)\n",
            "Accuracy at step 4 - batch 148: 0.9\n",
            "training loss at step 4 - batch 149: 0.35 (2019-08-04 13:35:46.872476)\n",
            "Accuracy at step 4 - batch 149: 0.9008\n",
            "training loss at step 4 - batch 150: 0.34 (2019-08-04 13:35:46.888522)\n",
            "Accuracy at step 4 - batch 150: 0.8988\n",
            "training loss at step 4 - batch 151: 0.35 (2019-08-04 13:35:47.034728)\n",
            "Accuracy at step 4 - batch 151: 0.8988\n",
            "training loss at step 4 - batch 152: 0.35 (2019-08-04 13:35:47.048867)\n",
            "Accuracy at step 4 - batch 152: 0.8968\n",
            "training loss at step 4 - batch 153: 0.32 (2019-08-04 13:35:47.072042)\n",
            "Accuracy at step 4 - batch 153: 0.9052\n",
            "training loss at step 4 - batch 154: 0.32 (2019-08-04 13:35:47.085704)\n",
            "Accuracy at step 4 - batch 154: 0.902\n",
            "training loss at step 4 - batch 155: 0.35 (2019-08-04 13:35:47.100033)\n",
            "Accuracy at step 4 - batch 155: 0.902\n",
            "training loss at step 4 - batch 156: 0.33 (2019-08-04 13:35:47.243087)\n",
            "Accuracy at step 4 - batch 156: 0.908\n",
            "training loss at step 4 - batch 157: 0.33 (2019-08-04 13:35:47.257905)\n",
            "Accuracy at step 4 - batch 157: 0.9068\n",
            "training loss at step 4 - batch 158: 0.33 (2019-08-04 13:35:47.271898)\n",
            "Accuracy at step 4 - batch 158: 0.896\n",
            "training loss at step 4 - batch 159: 0.33 (2019-08-04 13:35:47.290573)\n",
            "Accuracy at step 4 - batch 159: 0.902\n",
            "training loss at step 4 - batch 160: 0.35 (2019-08-04 13:35:47.302830)\n",
            "Accuracy at step 4 - batch 160: 0.902\n",
            "training loss at step 4 - batch 161: 0.32 (2019-08-04 13:35:47.433318)\n",
            "Accuracy at step 4 - batch 161: 0.9024\n",
            "training loss at step 4 - batch 162: 0.33 (2019-08-04 13:35:47.445504)\n",
            "Accuracy at step 4 - batch 162: 0.9004\n",
            "training loss at step 4 - batch 163: 0.35 (2019-08-04 13:35:47.458301)\n",
            "Accuracy at step 4 - batch 163: 0.8972\n",
            "training loss at step 4 - batch 164: 0.34 (2019-08-04 13:35:47.471663)\n",
            "Accuracy at step 4 - batch 164: 0.9024\n",
            "training loss at step 4 - batch 165: 0.34 (2019-08-04 13:35:47.484151)\n",
            "Accuracy at step 4 - batch 165: 0.8996\n",
            "training loss at step 4 - batch 166: 0.35 (2019-08-04 13:35:47.610871)\n",
            "Accuracy at step 4 - batch 166: 0.8932\n",
            "training loss at step 4 - batch 167: 0.33 (2019-08-04 13:35:47.625885)\n",
            "Accuracy at step 4 - batch 167: 0.902\n",
            "training loss at step 4 - batch 168: 0.35 (2019-08-04 13:35:47.638605)\n",
            "Accuracy at step 4 - batch 168: 0.9\n",
            "training loss at step 4 - batch 169: 0.33 (2019-08-04 13:35:47.650335)\n",
            "Accuracy at step 4 - batch 169: 0.9\n",
            "training loss at step 4 - batch 170: 0.34 (2019-08-04 13:35:47.664667)\n",
            "Accuracy at step 4 - batch 170: 0.9004\n",
            "training loss at step 4 - batch 171: 0.33 (2019-08-04 13:35:47.796110)\n",
            "Accuracy at step 4 - batch 171: 0.9032\n",
            "training loss at step 4 - batch 172: 0.32 (2019-08-04 13:35:47.812057)\n",
            "Accuracy at step 4 - batch 172: 0.9032\n",
            "training loss at step 4 - batch 173: 0.34 (2019-08-04 13:35:47.830072)\n",
            "Accuracy at step 4 - batch 173: 0.9004\n",
            "training loss at step 4 - batch 174: 0.31 (2019-08-04 13:35:47.844053)\n",
            "Accuracy at step 4 - batch 174: 0.9088\n",
            "training loss at step 4 - batch 175: 0.33 (2019-08-04 13:35:47.855829)\n",
            "Accuracy at step 4 - batch 175: 0.9068\n",
            "training loss at step 4 - batch 176: 0.34 (2019-08-04 13:35:47.990409)\n",
            "Accuracy at step 4 - batch 176: 0.9012\n",
            "training loss at step 4 - batch 177: 0.32 (2019-08-04 13:35:48.003338)\n",
            "Accuracy at step 4 - batch 177: 0.9032\n",
            "training loss at step 4 - batch 178: 0.32 (2019-08-04 13:35:48.018081)\n",
            "Accuracy at step 4 - batch 178: 0.9024\n",
            "training loss at step 4 - batch 179: 0.33 (2019-08-04 13:35:48.031767)\n",
            "Accuracy at step 4 - batch 179: 0.9\n",
            "training loss at step 4 - batch 180: 0.34 (2019-08-04 13:35:48.046313)\n",
            "Accuracy at step 4 - batch 180: 0.8988\n",
            "training loss at step 4 - batch 181: 0.34 (2019-08-04 13:35:48.196499)\n",
            "Accuracy at step 4 - batch 181: 0.9012\n",
            "training loss at step 4 - batch 182: 0.32 (2019-08-04 13:35:48.210194)\n",
            "Accuracy at step 4 - batch 182: 0.906\n",
            "training loss at step 4 - batch 183: 0.34 (2019-08-04 13:35:48.226772)\n",
            "Accuracy at step 4 - batch 183: 0.8996\n",
            "training loss at step 4 - batch 184: 0.32 (2019-08-04 13:35:48.240582)\n",
            "Accuracy at step 4 - batch 184: 0.9052\n",
            "training loss at step 4 - batch 185: 0.32 (2019-08-04 13:35:48.253029)\n",
            "Accuracy at step 4 - batch 185: 0.9052\n",
            "training loss at step 4 - batch 186: 0.34 (2019-08-04 13:35:48.382012)\n",
            "Accuracy at step 4 - batch 186: 0.894\n",
            "training loss at step 4 - batch 187: 0.35 (2019-08-04 13:35:48.398977)\n",
            "Accuracy at step 4 - batch 187: 0.8972\n",
            "training loss at step 4 - batch 188: 0.34 (2019-08-04 13:35:48.411012)\n",
            "Accuracy at step 4 - batch 188: 0.9032\n",
            "training loss at step 4 - batch 189: 0.35 (2019-08-04 13:35:48.423222)\n",
            "Accuracy at step 4 - batch 189: 0.9\n",
            "training loss at step 4 - batch 190: 0.33 (2019-08-04 13:35:48.438400)\n",
            "Accuracy at step 4 - batch 190: 0.9028\n",
            "training loss at step 4 - batch 191: 0.34 (2019-08-04 13:35:48.564375)\n",
            "Accuracy at step 4 - batch 191: 0.898\n",
            "training loss at step 4 - batch 192: 0.35 (2019-08-04 13:35:48.580715)\n",
            "Accuracy at step 4 - batch 192: 0.9024\n",
            "training loss at step 4 - batch 193: 0.33 (2019-08-04 13:35:48.593356)\n",
            "Accuracy at step 4 - batch 193: 0.8988\n",
            "training loss at step 4 - batch 194: 0.34 (2019-08-04 13:35:48.607010)\n",
            "Accuracy at step 4 - batch 194: 0.8992\n",
            "training loss at step 4 - batch 195: 0.33 (2019-08-04 13:35:48.622726)\n",
            "Accuracy at step 4 - batch 195: 0.91\n",
            "training loss at step 4 - batch 196: 0.32 (2019-08-04 13:35:48.751746)\n",
            "Accuracy at step 4 - batch 196: 0.9028\n",
            "training loss at step 4 - batch 197: 0.33 (2019-08-04 13:35:48.767244)\n",
            "Accuracy at step 4 - batch 197: 0.9004\n",
            "training loss at step 4 - batch 198: 0.34 (2019-08-04 13:35:48.781910)\n",
            "Accuracy at step 4 - batch 198: 0.8996\n",
            "training loss at step 4 - batch 199: 0.36 (2019-08-04 13:35:48.794399)\n",
            "Accuracy at step 4 - batch 199: 0.8932\n",
            "training loss at step 4 - batch 200: 0.33 (2019-08-04 13:35:48.807408)\n",
            "Accuracy at step 4 - batch 200: 0.9\n",
            "training loss at step 4 - batch 201: 0.36 (2019-08-04 13:35:48.934858)\n",
            "Accuracy at step 4 - batch 201: 0.8976\n",
            "training loss at step 4 - batch 202: 0.33 (2019-08-04 13:35:48.950063)\n",
            "Accuracy at step 4 - batch 202: 0.8992\n",
            "training loss at step 4 - batch 203: 0.34 (2019-08-04 13:35:48.969090)\n",
            "Accuracy at step 4 - batch 203: 0.8952\n",
            "training loss at step 4 - batch 204: 0.33 (2019-08-04 13:35:48.983156)\n",
            "Accuracy at step 4 - batch 204: 0.9024\n",
            "training loss at step 4 - batch 205: 0.34 (2019-08-04 13:35:48.995931)\n",
            "Accuracy at step 4 - batch 205: 0.8988\n",
            "training loss at step 4 - batch 206: 0.32 (2019-08-04 13:35:49.126201)\n",
            "Accuracy at step 4 - batch 206: 0.906\n",
            "training loss at step 4 - batch 207: 0.31 (2019-08-04 13:35:49.138512)\n",
            "Accuracy at step 4 - batch 207: 0.9052\n",
            "training loss at step 4 - batch 208: 0.33 (2019-08-04 13:35:49.150885)\n",
            "Accuracy at step 4 - batch 208: 0.9028\n",
            "training loss at step 4 - batch 209: 0.36 (2019-08-04 13:35:49.164213)\n",
            "Accuracy at step 4 - batch 209: 0.8948\n",
            "training loss at step 4 - batch 210: 0.31 (2019-08-04 13:35:49.179344)\n",
            "Accuracy at step 4 - batch 210: 0.9052\n",
            "training loss at step 4 - batch 211: 0.33 (2019-08-04 13:35:49.305480)\n",
            "Accuracy at step 4 - batch 211: 0.9012\n",
            "training loss at step 4 - batch 212: 0.31 (2019-08-04 13:35:49.319755)\n",
            "Accuracy at step 4 - batch 212: 0.9096\n",
            "training loss at step 4 - batch 213: 0.32 (2019-08-04 13:35:49.333107)\n",
            "Accuracy at step 4 - batch 213: 0.908\n",
            "training loss at step 4 - batch 214: 0.33 (2019-08-04 13:35:49.345923)\n",
            "Accuracy at step 4 - batch 214: 0.902\n",
            "training loss at step 4 - batch 215: 0.34 (2019-08-04 13:35:49.358614)\n",
            "Accuracy at step 4 - batch 215: 0.9024\n",
            "training loss at step 4 - batch 216: 0.33 (2019-08-04 13:35:49.484536)\n",
            "Accuracy at step 4 - batch 216: 0.9032\n",
            "training loss at step 4 - batch 217: 0.32 (2019-08-04 13:35:49.496976)\n",
            "Accuracy at step 4 - batch 217: 0.9072\n",
            "training loss at step 4 - batch 218: 0.33 (2019-08-04 13:35:49.509495)\n",
            "Accuracy at step 4 - batch 218: 0.9044\n",
            "training loss at step 4 - batch 219: 0.33 (2019-08-04 13:35:49.521714)\n",
            "Accuracy at step 4 - batch 219: 0.9\n",
            "training loss at step 4 - batch 220: 0.32 (2019-08-04 13:35:49.534728)\n",
            "Accuracy at step 4 - batch 220: 0.9028\n",
            "training loss at step 4 - batch 221: 0.34 (2019-08-04 13:35:49.662630)\n",
            "Accuracy at step 4 - batch 221: 0.9024\n",
            "training loss at step 4 - batch 222: 0.33 (2019-08-04 13:35:49.678920)\n",
            "Accuracy at step 4 - batch 222: 0.8996\n",
            "training loss at step 4 - batch 223: 0.34 (2019-08-04 13:35:49.693324)\n",
            "Accuracy at step 4 - batch 223: 0.8948\n",
            "training loss at step 4 - batch 224: 0.34 (2019-08-04 13:35:49.706075)\n",
            "Accuracy at step 4 - batch 224: 0.9012\n",
            "training loss at step 4 - batch 225: 0.33 (2019-08-04 13:35:49.719001)\n",
            "Accuracy at step 4 - batch 225: 0.8948\n",
            "training loss at step 4 - batch 226: 0.34 (2019-08-04 13:35:49.843339)\n",
            "Accuracy at step 4 - batch 226: 0.9052\n",
            "training loss at step 4 - batch 227: 0.34 (2019-08-04 13:35:49.858420)\n",
            "Accuracy at step 4 - batch 227: 0.8976\n",
            "training loss at step 4 - batch 228: 0.35 (2019-08-04 13:35:49.870399)\n",
            "Accuracy at step 4 - batch 228: 0.8976\n",
            "training loss at step 4 - batch 229: 0.33 (2019-08-04 13:35:49.883645)\n",
            "Accuracy at step 4 - batch 229: 0.9004\n",
            "training loss at step 4 - batch 230: 0.34 (2019-08-04 13:35:49.898897)\n",
            "Accuracy at step 4 - batch 230: 0.9012\n",
            "training loss at step 4 - batch 231: 0.33 (2019-08-04 13:35:50.035022)\n",
            "Accuracy at step 4 - batch 231: 0.8984\n",
            "training loss at step 4 - batch 232: 0.34 (2019-08-04 13:35:50.051468)\n",
            "Accuracy at step 4 - batch 232: 0.9068\n",
            "training loss at step 4 - batch 233: 0.34 (2019-08-04 13:35:50.065582)\n",
            "Accuracy at step 4 - batch 233: 0.9036\n",
            "training loss at step 4 - batch 234: 0.33 (2019-08-04 13:35:50.078331)\n",
            "Accuracy at step 4 - batch 234: 0.9128\n",
            "training loss at step 4 - batch 235: 0.34 (2019-08-04 13:35:50.091229)\n",
            "Accuracy at step 4 - batch 235: 0.9036\n",
            "training loss at step 4 - batch 236: 0.33 (2019-08-04 13:35:50.224098)\n",
            "Accuracy at step 4 - batch 236: 0.9004\n",
            "training loss at step 4 - batch 237: 0.33 (2019-08-04 13:35:50.238241)\n",
            "Accuracy at step 4 - batch 237: 0.8992\n",
            "training loss at step 4 - batch 238: 0.35 (2019-08-04 13:35:50.251190)\n",
            "Accuracy at step 4 - batch 238: 0.8972\n",
            "training loss at step 4 - batch 239: 0.31 (2019-08-04 13:35:50.264495)\n",
            "Accuracy at step 4 - batch 239: 0.9064\n",
            "training loss at step 4 - batch 240: 0.30 (2019-08-04 13:35:50.277451)\n",
            "Accuracy at step 4 - batch 240: 0.9144\n",
            "training loss at step 4 - batch 241: 0.32 (2019-08-04 13:35:50.531134)\n",
            "Accuracy at step 4 - batch 241: 0.9092\n",
            "training loss at step 4 - batch 242: 0.35 (2019-08-04 13:35:50.549546)\n",
            "Accuracy at step 4 - batch 242: 0.8968\n",
            "training loss at step 4 - batch 243: 0.33 (2019-08-04 13:35:50.562356)\n",
            "Accuracy at step 4 - batch 243: 0.9064\n",
            "training loss at step 4 - batch 244: 0.33 (2019-08-04 13:35:50.575601)\n",
            "Accuracy at step 4 - batch 244: 0.9044\n",
            "training loss at step 4 - batch 245: 0.32 (2019-08-04 13:35:50.588196)\n",
            "Accuracy at step 4 - batch 245: 0.908\n",
            "training loss at step 4 - batch 246: 0.34 (2019-08-04 13:35:50.721047)\n",
            "Accuracy at step 4 - batch 246: 0.8996\n",
            "training loss at step 4 - batch 247: 0.34 (2019-08-04 13:35:50.738608)\n",
            "Accuracy at step 4 - batch 247: 0.9028\n",
            "training loss at step 4 - batch 248: 0.36 (2019-08-04 13:35:50.753788)\n",
            "Accuracy at step 4 - batch 248: 0.8948\n",
            "training loss at step 4 - batch 249: 0.35 (2019-08-04 13:35:50.767195)\n",
            "Accuracy at step 4 - batch 249: 0.8992\n",
            "training loss at step 4 - batch 250: 0.32 (2019-08-04 13:35:50.782799)\n",
            "Accuracy at step 4 - batch 250: 0.9056\n",
            "training loss at step 4 - batch 251: 0.31 (2019-08-04 13:35:50.915025)\n",
            "Accuracy at step 4 - batch 251: 0.906\n",
            "training loss at step 4 - batch 252: 0.34 (2019-08-04 13:35:50.930624)\n",
            "Accuracy at step 4 - batch 252: 0.9016\n",
            "training loss at step 4 - batch 253: 0.34 (2019-08-04 13:35:50.944987)\n",
            "Accuracy at step 4 - batch 253: 0.9012\n",
            "training loss at step 4 - batch 254: 0.33 (2019-08-04 13:35:50.958051)\n",
            "Accuracy at step 4 - batch 254: 0.902\n",
            "training loss at step 4 - batch 255: 0.33 (2019-08-04 13:35:50.973063)\n",
            "Accuracy at step 4 - batch 255: 0.9044\n",
            "training loss at step 4 - batch 256: 0.32 (2019-08-04 13:35:51.104857)\n",
            "Accuracy at step 4 - batch 256: 0.9088\n",
            "training loss at step 4 - batch 257: 0.35 (2019-08-04 13:35:51.119750)\n",
            "Accuracy at step 4 - batch 257: 0.9036\n",
            "training loss at step 4 - batch 258: 0.31 (2019-08-04 13:35:51.134891)\n",
            "Accuracy at step 4 - batch 258: 0.9092\n",
            "training loss at step 4 - batch 259: 0.33 (2019-08-04 13:35:51.150298)\n",
            "Accuracy at step 4 - batch 259: 0.9\n",
            "training loss at step 4 - batch 260: 0.33 (2019-08-04 13:35:51.163240)\n",
            "Accuracy at step 4 - batch 260: 0.9028\n",
            "training loss at step 4 - batch 261: 0.34 (2019-08-04 13:35:51.292721)\n",
            "Accuracy at step 4 - batch 261: 0.8944\n",
            "training loss at step 4 - batch 262: 0.33 (2019-08-04 13:35:51.309835)\n",
            "Accuracy at step 4 - batch 262: 0.9064\n",
            "training loss at step 4 - batch 263: 0.33 (2019-08-04 13:35:51.322466)\n",
            "Accuracy at step 4 - batch 263: 0.9024\n",
            "training loss at step 4 - batch 264: 0.35 (2019-08-04 13:35:51.334578)\n",
            "Accuracy at step 4 - batch 264: 0.8948\n",
            "training loss at step 4 - batch 265: 0.34 (2019-08-04 13:35:51.346491)\n",
            "Accuracy at step 4 - batch 265: 0.898\n",
            "training loss at step 4 - batch 266: 0.31 (2019-08-04 13:35:51.467478)\n",
            "Accuracy at step 4 - batch 266: 0.9012\n",
            "training loss at step 4 - batch 267: 0.33 (2019-08-04 13:35:51.480487)\n",
            "Accuracy at step 4 - batch 267: 0.906\n",
            "training loss at step 4 - batch 268: 0.34 (2019-08-04 13:35:51.492572)\n",
            "Accuracy at step 4 - batch 268: 0.9032\n",
            "training loss at step 4 - batch 269: 0.33 (2019-08-04 13:35:51.505454)\n",
            "Accuracy at step 4 - batch 269: 0.9028\n",
            "training loss at step 4 - batch 270: 0.34 (2019-08-04 13:35:51.519722)\n",
            "Accuracy at step 4 - batch 270: 0.9004\n",
            "training loss at step 4 - batch 271: 0.34 (2019-08-04 13:35:51.642866)\n",
            "Accuracy at step 4 - batch 271: 0.9016\n",
            "training loss at step 4 - batch 272: 0.33 (2019-08-04 13:35:51.660093)\n",
            "Accuracy at step 4 - batch 272: 0.9044\n",
            "training loss at step 4 - batch 273: 0.33 (2019-08-04 13:35:51.675156)\n",
            "Accuracy at step 4 - batch 273: 0.9068\n",
            "training loss at step 4 - batch 274: 0.33 (2019-08-04 13:35:51.689289)\n",
            "Accuracy at step 4 - batch 274: 0.8996\n",
            "training loss at step 4 - batch 275: 0.33 (2019-08-04 13:35:51.701732)\n",
            "Accuracy at step 4 - batch 275: 0.9088\n",
            "training loss at step 4 - batch 276: 0.31 (2019-08-04 13:35:51.822423)\n",
            "Accuracy at step 4 - batch 276: 0.9108\n",
            "training loss at step 4 - batch 277: 0.34 (2019-08-04 13:35:51.835094)\n",
            "Accuracy at step 4 - batch 277: 0.9036\n",
            "training loss at step 4 - batch 278: 0.33 (2019-08-04 13:35:51.849943)\n",
            "Accuracy at step 4 - batch 278: 0.9028\n",
            "training loss at step 4 - batch 279: 0.36 (2019-08-04 13:35:51.865512)\n",
            "Accuracy at step 4 - batch 279: 0.8908\n",
            "training loss at step 4 - batch 280: 0.34 (2019-08-04 13:35:51.879294)\n",
            "Accuracy at step 4 - batch 280: 0.8992\n",
            "training loss at step 4 - batch 281: 0.34 (2019-08-04 13:35:52.006705)\n",
            "Accuracy at step 4 - batch 281: 0.8984\n",
            "training loss at step 4 - batch 282: 0.34 (2019-08-04 13:35:52.022446)\n",
            "Accuracy at step 4 - batch 282: 0.904\n",
            "training loss at step 4 - batch 283: 0.31 (2019-08-04 13:35:52.036053)\n",
            "Accuracy at step 4 - batch 283: 0.9096\n",
            "training loss at step 4 - batch 284: 0.32 (2019-08-04 13:35:52.049931)\n",
            "Accuracy at step 4 - batch 284: 0.9004\n",
            "training loss at step 4 - batch 285: 0.31 (2019-08-04 13:35:52.063107)\n",
            "Accuracy at step 4 - batch 285: 0.9048\n",
            "training loss at step 4 - batch 286: 0.33 (2019-08-04 13:35:52.198526)\n",
            "Accuracy at step 4 - batch 286: 0.9064\n",
            "training loss at step 4 - batch 287: 0.33 (2019-08-04 13:35:52.211495)\n",
            "Accuracy at step 4 - batch 287: 0.9032\n",
            "training loss at step 4 - batch 288: 0.35 (2019-08-04 13:35:52.224541)\n",
            "Accuracy at step 4 - batch 288: 0.8924\n",
            "training loss at step 4 - batch 289: 0.32 (2019-08-04 13:35:52.237820)\n",
            "Accuracy at step 4 - batch 289: 0.9068\n",
            "training loss at step 4 - batch 290: 0.33 (2019-08-04 13:35:52.253210)\n",
            "Accuracy at step 4 - batch 290: 0.9024\n",
            "training loss at step 4 - batch 291: 0.33 (2019-08-04 13:35:52.377230)\n",
            "Accuracy at step 4 - batch 291: 0.8996\n",
            "training loss at step 4 - batch 292: 0.33 (2019-08-04 13:35:52.389817)\n",
            "Accuracy at step 4 - batch 292: 0.9064\n",
            "training loss at step 4 - batch 293: 0.34 (2019-08-04 13:35:52.404207)\n",
            "Accuracy at step 4 - batch 293: 0.9\n",
            "training loss at step 4 - batch 294: 0.32 (2019-08-04 13:35:52.417524)\n",
            "Accuracy at step 4 - batch 294: 0.9072\n",
            "training loss at step 4 - batch 295: 0.35 (2019-08-04 13:35:52.432106)\n",
            "Accuracy at step 4 - batch 295: 0.8944\n",
            "training loss at step 4 - batch 296: 0.35 (2019-08-04 13:35:52.560385)\n",
            "Accuracy at step 4 - batch 296: 0.8948\n",
            "training loss at step 4 - batch 297: 0.33 (2019-08-04 13:35:52.577771)\n",
            "Accuracy at step 4 - batch 297: 0.908\n",
            "training loss at step 4 - batch 298: 0.32 (2019-08-04 13:35:52.589827)\n",
            "Accuracy at step 4 - batch 298: 0.9072\n",
            "training loss at step 4 - batch 299: 0.32 (2019-08-04 13:35:52.603149)\n",
            "Accuracy at step 4 - batch 299: 0.9064\n",
            "training loss at step 4 - batch 300: 0.34 (2019-08-04 13:35:52.619249)\n",
            "Accuracy at step 4 - batch 300: 0.9\n",
            "training loss at step 4 - batch 301: 0.33 (2019-08-04 13:35:52.738975)\n",
            "Accuracy at step 4 - batch 301: 0.8992\n",
            "training loss at step 4 - batch 302: 0.33 (2019-08-04 13:35:52.751577)\n",
            "Accuracy at step 4 - batch 302: 0.8992\n",
            "training loss at step 4 - batch 303: 0.33 (2019-08-04 13:35:52.765854)\n",
            "Accuracy at step 4 - batch 303: 0.9024\n",
            "training loss at step 4 - batch 304: 0.34 (2019-08-04 13:35:52.778183)\n",
            "Accuracy at step 4 - batch 304: 0.8992\n",
            "training loss at step 4 - batch 305: 0.32 (2019-08-04 13:35:52.791253)\n",
            "Accuracy at step 4 - batch 305: 0.9036\n",
            "training loss at step 4 - batch 306: 0.31 (2019-08-04 13:35:52.921580)\n",
            "Accuracy at step 4 - batch 306: 0.9108\n",
            "training loss at step 4 - batch 307: 0.33 (2019-08-04 13:35:52.934473)\n",
            "Accuracy at step 4 - batch 307: 0.9048\n",
            "training loss at step 4 - batch 308: 0.32 (2019-08-04 13:35:52.946437)\n",
            "Accuracy at step 4 - batch 308: 0.9032\n",
            "training loss at step 4 - batch 309: 0.33 (2019-08-04 13:35:52.959344)\n",
            "Accuracy at step 4 - batch 309: 0.9\n",
            "training loss at step 4 - batch 310: 0.32 (2019-08-04 13:35:52.973540)\n",
            "Accuracy at step 4 - batch 310: 0.908\n",
            "training loss at step 4 - batch 311: 0.31 (2019-08-04 13:35:53.108093)\n",
            "Accuracy at step 4 - batch 311: 0.9092\n",
            "training loss at step 4 - batch 312: 0.31 (2019-08-04 13:35:53.120936)\n",
            "Accuracy at step 4 - batch 312: 0.9104\n",
            "training loss at step 4 - batch 313: 0.34 (2019-08-04 13:35:53.136461)\n",
            "Accuracy at step 4 - batch 313: 0.8992\n",
            "training loss at step 4 - batch 314: 0.34 (2019-08-04 13:35:53.148869)\n",
            "Accuracy at step 4 - batch 314: 0.8952\n",
            "training loss at step 4 - batch 315: 0.33 (2019-08-04 13:35:53.162348)\n",
            "Accuracy at step 4 - batch 315: 0.898\n",
            "training loss at step 4 - batch 316: 0.33 (2019-08-04 13:35:53.287486)\n",
            "Accuracy at step 4 - batch 316: 0.8972\n",
            "training loss at step 4 - batch 317: 0.32 (2019-08-04 13:35:53.304055)\n",
            "Accuracy at step 4 - batch 317: 0.9036\n",
            "training loss at step 4 - batch 318: 0.35 (2019-08-04 13:35:53.316649)\n",
            "Accuracy at step 4 - batch 318: 0.8976\n",
            "training loss at step 4 - batch 319: 0.32 (2019-08-04 13:35:53.328710)\n",
            "Accuracy at step 4 - batch 319: 0.9016\n",
            "training loss at step 4 - batch 320: 0.34 (2019-08-04 13:35:53.342271)\n",
            "Accuracy at step 4 - batch 320: 0.898\n",
            "training loss at step 4 - batch 321: 0.34 (2019-08-04 13:35:53.469822)\n",
            "Accuracy at step 4 - batch 321: 0.9036\n",
            "training loss at step 4 - batch 322: 0.36 (2019-08-04 13:35:53.488399)\n",
            "Accuracy at step 4 - batch 322: 0.8984\n",
            "training loss at step 4 - batch 323: 0.34 (2019-08-04 13:35:53.501818)\n",
            "Accuracy at step 4 - batch 323: 0.898\n",
            "training loss at step 4 - batch 324: 0.32 (2019-08-04 13:35:53.516101)\n",
            "Accuracy at step 4 - batch 324: 0.8992\n",
            "training loss at step 4 - batch 325: 0.33 (2019-08-04 13:35:53.529333)\n",
            "Accuracy at step 4 - batch 325: 0.9008\n",
            "training loss at step 4 - batch 326: 0.32 (2019-08-04 13:35:53.659535)\n",
            "Accuracy at step 4 - batch 326: 0.9004\n",
            "training loss at step 4 - batch 327: 0.33 (2019-08-04 13:35:53.675832)\n",
            "Accuracy at step 4 - batch 327: 0.9048\n",
            "training loss at step 4 - batch 328: 0.33 (2019-08-04 13:35:53.689369)\n",
            "Accuracy at step 4 - batch 328: 0.9016\n",
            "training loss at step 4 - batch 329: 0.32 (2019-08-04 13:35:53.702020)\n",
            "Accuracy at step 4 - batch 329: 0.9076\n",
            "training loss at step 4 - batch 330: 0.34 (2019-08-04 13:35:53.714916)\n",
            "Accuracy at step 4 - batch 330: 0.8988\n",
            "training loss at step 4 - batch 331: 0.34 (2019-08-04 13:35:53.837619)\n",
            "Accuracy at step 4 - batch 331: 0.9032\n",
            "training loss at step 4 - batch 332: 0.34 (2019-08-04 13:35:53.853301)\n",
            "Accuracy at step 4 - batch 332: 0.898\n",
            "training loss at step 4 - batch 333: 0.32 (2019-08-04 13:35:53.867345)\n",
            "Accuracy at step 4 - batch 333: 0.904\n",
            "training loss at step 4 - batch 334: 0.33 (2019-08-04 13:35:53.880792)\n",
            "Accuracy at step 4 - batch 334: 0.9008\n",
            "training loss at step 4 - batch 335: 0.33 (2019-08-04 13:35:53.894367)\n",
            "Accuracy at step 4 - batch 335: 0.9028\n",
            "training loss at step 4 - batch 336: 0.32 (2019-08-04 13:35:54.019429)\n",
            "Accuracy at step 4 - batch 336: 0.9016\n",
            "training loss at step 4 - batch 337: 0.34 (2019-08-04 13:35:54.033273)\n",
            "Accuracy at step 4 - batch 337: 0.9\n",
            "training loss at step 4 - batch 338: 0.34 (2019-08-04 13:35:54.051851)\n",
            "Accuracy at step 4 - batch 338: 0.8992\n",
            "training loss at step 4 - batch 339: 0.33 (2019-08-04 13:35:54.065894)\n",
            "Accuracy at step 4 - batch 339: 0.9032\n",
            "training loss at step 4 - batch 340: 0.34 (2019-08-04 13:35:54.081454)\n",
            "Accuracy at step 4 - batch 340: 0.8976\n",
            "training loss at step 4 - batch 341: 0.32 (2019-08-04 13:35:54.204246)\n",
            "Accuracy at step 4 - batch 341: 0.9056\n",
            "training loss at step 4 - batch 342: 0.32 (2019-08-04 13:35:54.217087)\n",
            "Accuracy at step 4 - batch 342: 0.9072\n",
            "training loss at step 4 - batch 343: 0.34 (2019-08-04 13:35:54.230520)\n",
            "Accuracy at step 4 - batch 343: 0.8964\n",
            "training loss at step 4 - batch 344: 0.34 (2019-08-04 13:35:54.244229)\n",
            "Accuracy at step 4 - batch 344: 0.9012\n",
            "training loss at step 4 - batch 345: 0.34 (2019-08-04 13:35:54.257176)\n",
            "Accuracy at step 4 - batch 345: 0.8988\n",
            "training loss at step 4 - batch 346: 0.34 (2019-08-04 13:35:54.384215)\n",
            "Accuracy at step 4 - batch 346: 0.898\n",
            "training loss at step 4 - batch 347: 0.32 (2019-08-04 13:35:54.397119)\n",
            "Accuracy at step 4 - batch 347: 0.9008\n",
            "training loss at step 4 - batch 348: 0.31 (2019-08-04 13:35:54.410184)\n",
            "Accuracy at step 4 - batch 348: 0.904\n",
            "training loss at step 4 - batch 349: 0.33 (2019-08-04 13:35:54.426912)\n",
            "Accuracy at step 4 - batch 349: 0.902\n",
            "training loss at step 4 - batch 350: 0.33 (2019-08-04 13:35:54.442322)\n",
            "Accuracy at step 4 - batch 350: 0.902\n",
            "training loss at step 4 - batch 351: 0.34 (2019-08-04 13:35:54.563132)\n",
            "Accuracy at step 4 - batch 351: 0.8992\n",
            "training loss at step 4 - batch 352: 0.34 (2019-08-04 13:35:54.577035)\n",
            "Accuracy at step 4 - batch 352: 0.8968\n",
            "training loss at step 4 - batch 353: 0.33 (2019-08-04 13:35:54.591645)\n",
            "Accuracy at step 4 - batch 353: 0.9048\n",
            "training loss at step 4 - batch 354: 0.31 (2019-08-04 13:35:54.605006)\n",
            "Accuracy at step 4 - batch 354: 0.9088\n",
            "training loss at step 4 - batch 355: 0.32 (2019-08-04 13:35:54.617129)\n",
            "Accuracy at step 4 - batch 355: 0.9088\n",
            "training loss at step 4 - batch 356: 0.33 (2019-08-04 13:35:54.736568)\n",
            "Accuracy at step 4 - batch 356: 0.9024\n",
            "training loss at step 4 - batch 357: 0.30 (2019-08-04 13:35:54.749872)\n",
            "Accuracy at step 4 - batch 357: 0.912\n",
            "training loss at step 4 - batch 358: 0.35 (2019-08-04 13:35:54.764168)\n",
            "Accuracy at step 4 - batch 358: 0.9012\n",
            "training loss at step 4 - batch 359: 0.33 (2019-08-04 13:35:54.777672)\n",
            "Accuracy at step 4 - batch 359: 0.904\n",
            "training loss at step 4 - batch 360: 0.34 (2019-08-04 13:35:54.790356)\n",
            "Accuracy at step 4 - batch 360: 0.9024\n",
            "training loss at step 4 - batch 361: 0.31 (2019-08-04 13:35:54.917060)\n",
            "Accuracy at step 4 - batch 361: 0.9108\n",
            "training loss at step 4 - batch 362: 0.34 (2019-08-04 13:35:54.931528)\n",
            "Accuracy at step 4 - batch 362: 0.8964\n",
            "training loss at step 4 - batch 363: 0.35 (2019-08-04 13:35:54.946030)\n",
            "Accuracy at step 4 - batch 363: 0.8988\n",
            "training loss at step 4 - batch 364: 0.35 (2019-08-04 13:35:54.959117)\n",
            "Accuracy at step 4 - batch 364: 0.9016\n",
            "training loss at step 4 - batch 365: 0.32 (2019-08-04 13:35:54.972632)\n",
            "Accuracy at step 4 - batch 365: 0.91\n",
            "training loss at step 4 - batch 366: 0.35 (2019-08-04 13:35:55.104871)\n",
            "Accuracy at step 4 - batch 366: 0.8984\n",
            "training loss at step 4 - batch 367: 0.32 (2019-08-04 13:35:55.122392)\n",
            "Accuracy at step 4 - batch 367: 0.9044\n",
            "training loss at step 4 - batch 368: 0.33 (2019-08-04 13:35:55.137227)\n",
            "Accuracy at step 4 - batch 368: 0.8968\n",
            "training loss at step 4 - batch 369: 0.32 (2019-08-04 13:35:55.150690)\n",
            "Accuracy at step 4 - batch 369: 0.9028\n",
            "training loss at step 4 - batch 370: 0.32 (2019-08-04 13:35:55.163390)\n",
            "Accuracy at step 4 - batch 370: 0.9092\n",
            "training loss at step 4 - batch 371: 0.33 (2019-08-04 13:35:55.292148)\n",
            "Accuracy at step 4 - batch 371: 0.9048\n",
            "training loss at step 4 - batch 372: 0.34 (2019-08-04 13:35:55.309481)\n",
            "Accuracy at step 4 - batch 372: 0.8992\n",
            "training loss at step 4 - batch 373: 0.36 (2019-08-04 13:35:55.322409)\n",
            "Accuracy at step 4 - batch 373: 0.8964\n",
            "training loss at step 4 - batch 374: 0.33 (2019-08-04 13:35:55.337315)\n",
            "Accuracy at step 4 - batch 374: 0.902\n",
            "training loss at step 4 - batch 375: 0.32 (2019-08-04 13:35:55.350457)\n",
            "Accuracy at step 4 - batch 375: 0.9036\n",
            "training loss at step 4 - batch 376: 0.35 (2019-08-04 13:35:55.474921)\n",
            "Accuracy at step 4 - batch 376: 0.8908\n",
            "training loss at step 4 - batch 377: 0.33 (2019-08-04 13:35:55.492719)\n",
            "Accuracy at step 4 - batch 377: 0.9028\n",
            "training loss at step 4 - batch 378: 0.31 (2019-08-04 13:35:55.506675)\n",
            "Accuracy at step 4 - batch 378: 0.9088\n",
            "training loss at step 4 - batch 379: 0.33 (2019-08-04 13:35:55.519286)\n",
            "Accuracy at step 4 - batch 379: 0.9044\n",
            "training loss at step 4 - batch 380: 0.32 (2019-08-04 13:35:55.533608)\n",
            "Accuracy at step 4 - batch 380: 0.9052\n",
            "training loss at step 4 - batch 381: 0.32 (2019-08-04 13:35:55.662011)\n",
            "Accuracy at step 4 - batch 381: 0.9076\n",
            "training loss at step 4 - batch 382: 0.31 (2019-08-04 13:35:55.679483)\n",
            "Accuracy at step 4 - batch 382: 0.908\n",
            "training loss at step 4 - batch 383: 0.35 (2019-08-04 13:35:55.692933)\n",
            "Accuracy at step 4 - batch 383: 0.8984\n",
            "training loss at step 4 - batch 384: 0.33 (2019-08-04 13:35:55.706583)\n",
            "Accuracy at step 4 - batch 384: 0.9\n",
            "training loss at step 4 - batch 385: 0.32 (2019-08-04 13:35:55.720777)\n",
            "Accuracy at step 4 - batch 385: 0.9032\n",
            "training loss at step 4 - batch 386: 0.33 (2019-08-04 13:35:55.854370)\n",
            "Accuracy at step 4 - batch 386: 0.8984\n",
            "training loss at step 4 - batch 387: 0.36 (2019-08-04 13:35:55.872025)\n",
            "Accuracy at step 4 - batch 387: 0.8956\n",
            "training loss at step 4 - batch 388: 0.33 (2019-08-04 13:35:55.885962)\n",
            "Accuracy at step 4 - batch 388: 0.9004\n",
            "training loss at step 4 - batch 389: 0.36 (2019-08-04 13:35:55.898385)\n",
            "Accuracy at step 4 - batch 389: 0.8912\n",
            "training loss at step 4 - batch 390: 0.34 (2019-08-04 13:35:55.912643)\n",
            "Accuracy at step 4 - batch 390: 0.904\n",
            "training loss at step 4 - batch 391: 0.33 (2019-08-04 13:35:56.038895)\n",
            "Accuracy at step 4 - batch 391: 0.904\n",
            "training loss at step 4 - batch 392: 0.33 (2019-08-04 13:35:56.050975)\n",
            "Accuracy at step 4 - batch 392: 0.9052\n",
            "training loss at step 4 - batch 393: 0.32 (2019-08-04 13:35:56.063742)\n",
            "Accuracy at step 4 - batch 393: 0.9068\n",
            "training loss at step 4 - batch 394: 0.34 (2019-08-04 13:35:56.077549)\n",
            "Accuracy at step 4 - batch 394: 0.9056\n",
            "training loss at step 4 - batch 395: 0.34 (2019-08-04 13:35:56.100176)\n",
            "Accuracy at step 4 - batch 395: 0.8976\n",
            "training loss at step 4 - batch 396: 0.36 (2019-08-04 13:35:56.238415)\n",
            "Accuracy at step 4 - batch 396: 0.8936\n",
            "training loss at step 4 - batch 397: 0.34 (2019-08-04 13:35:56.252427)\n",
            "Accuracy at step 4 - batch 397: 0.8992\n",
            "training loss at step 4 - batch 398: 0.33 (2019-08-04 13:35:56.265343)\n",
            "Accuracy at step 4 - batch 398: 0.8976\n",
            "training loss at step 4 - batch 399: 0.34 (2019-08-04 13:35:56.278758)\n",
            "Accuracy at step 4 - batch 399: 0.8992\n",
            "training loss at step 4 - batch 400: 0.34 (2019-08-04 13:35:56.297576)\n",
            "Accuracy at step 4 - batch 400: 0.8948\n",
            "training loss at step 4 - batch 401: 0.36 (2019-08-04 13:35:56.424899)\n",
            "Accuracy at step 4 - batch 401: 0.8924\n",
            "training loss at step 4 - batch 402: 0.33 (2019-08-04 13:35:56.439500)\n",
            "Accuracy at step 4 - batch 402: 0.902\n",
            "training loss at step 4 - batch 403: 0.32 (2019-08-04 13:35:56.453483)\n",
            "Accuracy at step 4 - batch 403: 0.9036\n",
            "training loss at step 4 - batch 404: 0.31 (2019-08-04 13:35:56.465956)\n",
            "Accuracy at step 4 - batch 404: 0.9096\n",
            "training loss at step 4 - batch 405: 0.32 (2019-08-04 13:35:56.479780)\n",
            "Accuracy at step 4 - batch 405: 0.9092\n",
            "training loss at step 4 - batch 406: 0.32 (2019-08-04 13:35:56.608080)\n",
            "Accuracy at step 4 - batch 406: 0.9024\n",
            "training loss at step 4 - batch 407: 0.36 (2019-08-04 13:35:56.621389)\n",
            "Accuracy at step 4 - batch 407: 0.8924\n",
            "training loss at step 4 - batch 408: 0.31 (2019-08-04 13:35:56.635000)\n",
            "Accuracy at step 4 - batch 408: 0.9048\n",
            "training loss at step 4 - batch 409: 0.32 (2019-08-04 13:35:56.650162)\n",
            "Accuracy at step 4 - batch 409: 0.9072\n",
            "training loss at step 4 - batch 410: 0.34 (2019-08-04 13:35:56.663025)\n",
            "Accuracy at step 4 - batch 410: 0.8988\n",
            "training loss at step 4 - batch 411: 0.34 (2019-08-04 13:35:56.795855)\n",
            "Accuracy at step 4 - batch 411: 0.8992\n",
            "training loss at step 4 - batch 412: 0.33 (2019-08-04 13:35:56.811546)\n",
            "Accuracy at step 4 - batch 412: 0.9032\n",
            "training loss at step 4 - batch 413: 0.32 (2019-08-04 13:35:56.828661)\n",
            "Accuracy at step 4 - batch 413: 0.9068\n",
            "training loss at step 4 - batch 414: 0.33 (2019-08-04 13:35:56.841507)\n",
            "Accuracy at step 4 - batch 414: 0.9024\n",
            "training loss at step 4 - batch 415: 0.35 (2019-08-04 13:35:56.854584)\n",
            "Accuracy at step 4 - batch 415: 0.8972\n",
            "training loss at step 4 - batch 416: 0.33 (2019-08-04 13:35:56.978586)\n",
            "Accuracy at step 4 - batch 416: 0.9\n",
            "training loss at step 4 - batch 417: 0.34 (2019-08-04 13:35:56.992762)\n",
            "Accuracy at step 4 - batch 417: 0.9024\n",
            "training loss at step 4 - batch 418: 0.32 (2019-08-04 13:35:57.007983)\n",
            "Accuracy at step 4 - batch 418: 0.9044\n",
            "training loss at step 4 - batch 419: 0.32 (2019-08-04 13:35:57.021386)\n",
            "Accuracy at step 4 - batch 419: 0.902\n",
            "training loss at step 4 - batch 420: 0.33 (2019-08-04 13:35:57.034582)\n",
            "Accuracy at step 4 - batch 420: 0.8992\n",
            "training loss at step 4 - batch 421: 0.33 (2019-08-04 13:35:57.180010)\n",
            "Accuracy at step 4 - batch 421: 0.9004\n",
            "training loss at step 4 - batch 422: 0.32 (2019-08-04 13:35:57.193233)\n",
            "Accuracy at step 4 - batch 422: 0.906\n",
            "training loss at step 4 - batch 423: 0.31 (2019-08-04 13:35:57.206923)\n",
            "Accuracy at step 4 - batch 423: 0.9044\n",
            "training loss at step 4 - batch 424: 0.32 (2019-08-04 13:35:57.224310)\n",
            "Accuracy at step 4 - batch 424: 0.908\n",
            "training loss at step 4 - batch 425: 0.34 (2019-08-04 13:35:57.238022)\n",
            "Accuracy at step 4 - batch 425: 0.8988\n",
            "training loss at step 4 - batch 426: 0.35 (2019-08-04 13:35:57.358834)\n",
            "Accuracy at step 4 - batch 426: 0.8936\n",
            "training loss at step 4 - batch 427: 0.31 (2019-08-04 13:35:57.370998)\n",
            "Accuracy at step 4 - batch 427: 0.9052\n",
            "training loss at step 4 - batch 428: 0.32 (2019-08-04 13:35:57.385290)\n",
            "Accuracy at step 4 - batch 428: 0.902\n",
            "training loss at step 4 - batch 429: 0.33 (2019-08-04 13:35:57.397126)\n",
            "Accuracy at step 4 - batch 429: 0.8988\n",
            "training loss at step 4 - batch 430: 0.32 (2019-08-04 13:35:57.411894)\n",
            "Accuracy at step 4 - batch 430: 0.904\n",
            "training loss at step 4 - batch 431: 0.32 (2019-08-04 13:35:57.539710)\n",
            "Accuracy at step 4 - batch 431: 0.9032\n",
            "training loss at step 4 - batch 432: 0.28 (2019-08-04 13:35:57.552334)\n",
            "Accuracy at step 4 - batch 432: 0.916\n",
            "training loss at step 4 - batch 433: 0.35 (2019-08-04 13:35:57.564561)\n",
            "Accuracy at step 4 - batch 433: 0.8928\n",
            "training loss at step 4 - batch 434: 0.35 (2019-08-04 13:35:57.576837)\n",
            "Accuracy at step 4 - batch 434: 0.8984\n",
            "training loss at step 4 - batch 435: 0.33 (2019-08-04 13:35:57.589302)\n",
            "Accuracy at step 4 - batch 435: 0.9008\n",
            "training loss at step 4 - batch 436: 0.33 (2019-08-04 13:35:57.710023)\n",
            "Accuracy at step 4 - batch 436: 0.8984\n",
            "training loss at step 4 - batch 437: 0.32 (2019-08-04 13:35:57.725865)\n",
            "Accuracy at step 4 - batch 437: 0.9036\n",
            "training loss at step 4 - batch 438: 0.32 (2019-08-04 13:35:57.738262)\n",
            "Accuracy at step 4 - batch 438: 0.9024\n",
            "training loss at step 4 - batch 439: 0.34 (2019-08-04 13:35:57.754097)\n",
            "Accuracy at step 4 - batch 439: 0.9032\n",
            "training loss at step 4 - batch 440: 0.35 (2019-08-04 13:35:57.766796)\n",
            "Accuracy at step 4 - batch 440: 0.892\n",
            "training loss at step 4 - batch 441: 0.33 (2019-08-04 13:35:57.888883)\n",
            "Accuracy at step 4 - batch 441: 0.898\n",
            "training loss at step 4 - batch 442: 0.33 (2019-08-04 13:35:57.905683)\n",
            "Accuracy at step 4 - batch 442: 0.9004\n",
            "training loss at step 4 - batch 443: 0.34 (2019-08-04 13:35:57.919334)\n",
            "Accuracy at step 4 - batch 443: 0.8984\n",
            "training loss at step 4 - batch 444: 0.32 (2019-08-04 13:35:57.932982)\n",
            "Accuracy at step 4 - batch 444: 0.9024\n",
            "training loss at step 4 - batch 445: 0.30 (2019-08-04 13:35:57.945050)\n",
            "Accuracy at step 4 - batch 445: 0.9172\n",
            "training loss at step 4 - batch 446: 0.30 (2019-08-04 13:35:58.068704)\n",
            "Accuracy at step 4 - batch 446: 0.9092\n",
            "training loss at step 4 - batch 447: 0.36 (2019-08-04 13:35:58.085664)\n",
            "Accuracy at step 4 - batch 447: 0.8972\n",
            "training loss at step 4 - batch 448: 0.34 (2019-08-04 13:35:58.098205)\n",
            "Accuracy at step 4 - batch 448: 0.8976\n",
            "training loss at step 4 - batch 449: 0.34 (2019-08-04 13:35:58.110687)\n",
            "Accuracy at step 4 - batch 449: 0.9\n",
            "training loss at step 4 - batch 450: 0.30 (2019-08-04 13:35:58.124006)\n",
            "Accuracy at step 4 - batch 450: 0.9092\n",
            "training loss at step 4 - batch 451: 0.33 (2019-08-04 13:35:58.248369)\n",
            "Accuracy at step 4 - batch 451: 0.9016\n",
            "training loss at step 4 - batch 452: 0.33 (2019-08-04 13:35:58.261937)\n",
            "Accuracy at step 4 - batch 452: 0.902\n",
            "training loss at step 4 - batch 453: 0.32 (2019-08-04 13:35:58.276375)\n",
            "Accuracy at step 4 - batch 453: 0.9072\n",
            "training loss at step 4 - batch 454: 0.32 (2019-08-04 13:35:58.293530)\n",
            "Accuracy at step 4 - batch 454: 0.902\n",
            "training loss at step 4 - batch 455: 0.33 (2019-08-04 13:35:58.305739)\n",
            "Accuracy at step 4 - batch 455: 0.9076\n",
            "training loss at step 4 - batch 456: 0.32 (2019-08-04 13:35:58.430708)\n",
            "Accuracy at step 4 - batch 456: 0.9084\n",
            "training loss at step 4 - batch 457: 0.32 (2019-08-04 13:35:58.444716)\n",
            "Accuracy at step 4 - batch 457: 0.904\n",
            "training loss at step 4 - batch 458: 0.34 (2019-08-04 13:35:58.462936)\n",
            "Accuracy at step 4 - batch 458: 0.8988\n",
            "training loss at step 4 - batch 459: 0.33 (2019-08-04 13:35:58.477628)\n",
            "Accuracy at step 4 - batch 459: 0.9004\n",
            "training loss at step 4 - batch 460: 0.35 (2019-08-04 13:35:58.494197)\n",
            "Accuracy at step 4 - batch 460: 0.896\n",
            "training loss at step 4 - batch 461: 0.33 (2019-08-04 13:35:58.628729)\n",
            "Accuracy at step 4 - batch 461: 0.8992\n",
            "training loss at step 4 - batch 462: 0.31 (2019-08-04 13:35:58.644718)\n",
            "Accuracy at step 4 - batch 462: 0.906\n",
            "training loss at step 4 - batch 463: 0.36 (2019-08-04 13:35:58.657353)\n",
            "Accuracy at step 4 - batch 463: 0.8984\n",
            "training loss at step 4 - batch 464: 0.34 (2019-08-04 13:35:58.670421)\n",
            "Accuracy at step 4 - batch 464: 0.9016\n",
            "training loss at step 4 - batch 465: 0.34 (2019-08-04 13:35:58.683347)\n",
            "Accuracy at step 4 - batch 465: 0.8988\n",
            "training loss at step 4 - batch 466: 0.32 (2019-08-04 13:35:58.813646)\n",
            "Accuracy at step 4 - batch 466: 0.9024\n",
            "training loss at step 4 - batch 467: 0.35 (2019-08-04 13:35:58.826318)\n",
            "Accuracy at step 4 - batch 467: 0.8996\n",
            "training loss at step 4 - batch 468: 0.33 (2019-08-04 13:35:58.838705)\n",
            "Accuracy at step 4 - batch 468: 0.9\n",
            "training loss at step 4 - batch 469: 0.32 (2019-08-04 13:35:58.851609)\n",
            "Accuracy at step 4 - batch 469: 0.9064\n",
            "training loss at step 4 - batch 470: 0.36 (2019-08-04 13:35:58.865165)\n",
            "Accuracy at step 4 - batch 470: 0.8896\n",
            "training loss at step 4 - batch 471: 0.33 (2019-08-04 13:35:58.988009)\n",
            "Accuracy at step 4 - batch 471: 0.9052\n",
            "training loss at step 4 - batch 472: 0.35 (2019-08-04 13:35:59.003699)\n",
            "Accuracy at step 4 - batch 472: 0.9004\n",
            "training loss at step 4 - batch 473: 0.32 (2019-08-04 13:35:59.015650)\n",
            "Accuracy at step 4 - batch 473: 0.9052\n",
            "training loss at step 4 - batch 474: 0.32 (2019-08-04 13:35:59.028067)\n",
            "Accuracy at step 4 - batch 474: 0.9072\n",
            "training loss at step 4 - batch 475: 0.32 (2019-08-04 13:35:59.042324)\n",
            "Accuracy at step 4 - batch 475: 0.9008\n",
            "training loss at step 4 - batch 476: 0.31 (2019-08-04 13:35:59.166556)\n",
            "Accuracy at step 4 - batch 476: 0.912\n",
            "training loss at step 4 - batch 477: 0.34 (2019-08-04 13:35:59.181626)\n",
            "Accuracy at step 4 - batch 477: 0.9032\n",
            "training loss at step 4 - batch 478: 0.33 (2019-08-04 13:35:59.195504)\n",
            "Accuracy at step 4 - batch 478: 0.902\n",
            "training loss at step 4 - batch 479: 0.34 (2019-08-04 13:35:59.209254)\n",
            "Accuracy at step 4 - batch 479: 0.8988\n",
            "training loss at step 4 - batch 480: 0.32 (2019-08-04 13:35:59.221339)\n",
            "Accuracy at step 4 - batch 480: 0.9084\n",
            "training loss at step 4 - batch 481: 0.32 (2019-08-04 13:35:59.342877)\n",
            "Accuracy at step 4 - batch 481: 0.9092\n",
            "training loss at step 4 - batch 482: 0.32 (2019-08-04 13:35:59.355316)\n",
            "Accuracy at step 4 - batch 482: 0.9028\n",
            "training loss at step 4 - batch 483: 0.31 (2019-08-04 13:35:59.369522)\n",
            "Accuracy at step 4 - batch 483: 0.906\n",
            "training loss at step 4 - batch 484: 0.34 (2019-08-04 13:35:59.381879)\n",
            "Accuracy at step 4 - batch 484: 0.904\n",
            "training loss at step 4 - batch 485: 0.33 (2019-08-04 13:35:59.394339)\n",
            "Accuracy at step 4 - batch 485: 0.902\n",
            "training loss at step 4 - batch 486: 0.35 (2019-08-04 13:35:59.521589)\n",
            "Accuracy at step 4 - batch 486: 0.9008\n",
            "training loss at step 4 - batch 487: 0.33 (2019-08-04 13:35:59.533706)\n",
            "Accuracy at step 4 - batch 487: 0.9036\n",
            "training loss at step 4 - batch 488: 0.33 (2019-08-04 13:35:59.546481)\n",
            "Accuracy at step 4 - batch 488: 0.8992\n",
            "training loss at step 4 - batch 489: 0.32 (2019-08-04 13:35:59.558322)\n",
            "Accuracy at step 4 - batch 489: 0.9096\n",
            "training loss at step 4 - batch 490: 0.33 (2019-08-04 13:35:59.570230)\n",
            "Accuracy at step 4 - batch 490: 0.9044\n",
            "training loss at step 4 - batch 491: 0.34 (2019-08-04 13:35:59.688415)\n",
            "Accuracy at step 4 - batch 491: 0.8972\n",
            "training loss at step 4 - batch 492: 0.33 (2019-08-04 13:35:59.703628)\n",
            "Accuracy at step 4 - batch 492: 0.9004\n",
            "training loss at step 4 - batch 493: 0.34 (2019-08-04 13:35:59.716387)\n",
            "Accuracy at step 4 - batch 493: 0.9012\n",
            "training loss at step 4 - batch 494: 0.34 (2019-08-04 13:35:59.731009)\n",
            "Accuracy at step 4 - batch 494: 0.8992\n",
            "training loss at step 4 - batch 495: 0.34 (2019-08-04 13:35:59.743349)\n",
            "Accuracy at step 4 - batch 495: 0.8984\n",
            "training loss at step 4 - batch 496: 0.32 (2019-08-04 13:35:59.857885)\n",
            "Accuracy at step 4 - batch 496: 0.902\n",
            "training loss at step 4 - batch 497: 0.32 (2019-08-04 13:35:59.871214)\n",
            "Accuracy at step 4 - batch 497: 0.906\n",
            "training loss at step 4 - batch 498: 0.33 (2019-08-04 13:35:59.883098)\n",
            "Accuracy at step 4 - batch 498: 0.8964\n",
            "training loss at step 4 - batch 499: 0.31 (2019-08-04 13:35:59.896682)\n",
            "Accuracy at step 4 - batch 499: 0.9064\n",
            "training loss at step 4 - batch 500: 0.34 (2019-08-04 13:35:59.908950)\n",
            "Accuracy at step 4 - batch 500: 0.8992\n",
            "training loss at step 4 - batch 501: 0.33 (2019-08-04 13:36:00.031523)\n",
            "Accuracy at step 4 - batch 501: 0.9064\n",
            "training loss at step 4 - batch 502: 0.33 (2019-08-04 13:36:00.045599)\n",
            "Accuracy at step 4 - batch 502: 0.904\n",
            "training loss at step 4 - batch 503: 0.33 (2019-08-04 13:36:00.058207)\n",
            "Accuracy at step 4 - batch 503: 0.8976\n",
            "training loss at step 4 - batch 504: 0.35 (2019-08-04 13:36:00.070554)\n",
            "Accuracy at step 4 - batch 504: 0.8952\n",
            "training loss at step 4 - batch 505: 0.33 (2019-08-04 13:36:00.085553)\n",
            "Accuracy at step 4 - batch 505: 0.8992\n",
            "training loss at step 4 - batch 506: 0.30 (2019-08-04 13:36:00.218669)\n",
            "Accuracy at step 4 - batch 506: 0.9104\n",
            "training loss at step 4 - batch 507: 0.31 (2019-08-04 13:36:00.236525)\n",
            "Accuracy at step 4 - batch 507: 0.9068\n",
            "training loss at step 4 - batch 508: 0.34 (2019-08-04 13:36:00.253402)\n",
            "Accuracy at step 4 - batch 508: 0.9028\n",
            "training loss at step 4 - batch 509: 0.33 (2019-08-04 13:36:00.266994)\n",
            "Accuracy at step 4 - batch 509: 0.9044\n",
            "training loss at step 4 - batch 510: 0.34 (2019-08-04 13:36:00.281129)\n",
            "Accuracy at step 4 - batch 510: 0.8988\n",
            "training loss at step 4 - batch 511: 0.35 (2019-08-04 13:36:00.405545)\n",
            "Accuracy at step 4 - batch 511: 0.898\n",
            "training loss at step 4 - batch 512: 0.34 (2019-08-04 13:36:00.419502)\n",
            "Accuracy at step 4 - batch 512: 0.9004\n",
            "training loss at step 4 - batch 513: 0.35 (2019-08-04 13:36:00.431569)\n",
            "Accuracy at step 4 - batch 513: 0.8952\n",
            "training loss at step 4 - batch 514: 0.34 (2019-08-04 13:36:00.446262)\n",
            "Accuracy at step 4 - batch 514: 0.9012\n",
            "training loss at step 4 - batch 515: 0.34 (2019-08-04 13:36:00.458626)\n",
            "Accuracy at step 4 - batch 515: 0.8944\n",
            "training loss at step 4 - batch 516: 0.33 (2019-08-04 13:36:00.578565)\n",
            "Accuracy at step 4 - batch 516: 0.8964\n",
            "training loss at step 4 - batch 517: 0.32 (2019-08-04 13:36:00.594027)\n",
            "Accuracy at step 4 - batch 517: 0.8984\n",
            "training loss at step 4 - batch 518: 0.31 (2019-08-04 13:36:00.607649)\n",
            "Accuracy at step 4 - batch 518: 0.9064\n",
            "training loss at step 4 - batch 519: 0.31 (2019-08-04 13:36:00.623016)\n",
            "Accuracy at step 4 - batch 519: 0.9044\n",
            "training loss at step 4 - batch 520: 0.34 (2019-08-04 13:36:00.636821)\n",
            "Accuracy at step 4 - batch 520: 0.9044\n",
            "training loss at step 4 - batch 521: 0.31 (2019-08-04 13:36:00.768614)\n",
            "Accuracy at step 4 - batch 521: 0.9032\n",
            "training loss at step 4 - batch 522: 0.34 (2019-08-04 13:36:00.783051)\n",
            "Accuracy at step 4 - batch 522: 0.8956\n",
            "training loss at step 4 - batch 523: 0.32 (2019-08-04 13:36:00.796039)\n",
            "Accuracy at step 4 - batch 523: 0.9044\n",
            "training loss at step 4 - batch 524: 0.34 (2019-08-04 13:36:00.809523)\n",
            "Accuracy at step 4 - batch 524: 0.9044\n",
            "training loss at step 4 - batch 525: 0.32 (2019-08-04 13:36:00.822938)\n",
            "Accuracy at step 4 - batch 525: 0.9052\n",
            "training loss at step 4 - batch 526: 0.33 (2019-08-04 13:36:00.946026)\n",
            "Accuracy at step 4 - batch 526: 0.9028\n",
            "training loss at step 4 - batch 527: 0.33 (2019-08-04 13:36:00.958609)\n",
            "Accuracy at step 4 - batch 527: 0.9032\n",
            "training loss at step 4 - batch 528: 0.33 (2019-08-04 13:36:00.973518)\n",
            "Accuracy at step 4 - batch 528: 0.9\n",
            "training loss at step 4 - batch 529: 0.33 (2019-08-04 13:36:00.989978)\n",
            "Accuracy at step 4 - batch 529: 0.9028\n",
            "training loss at step 4 - batch 530: 0.30 (2019-08-04 13:36:01.003349)\n",
            "Accuracy at step 4 - batch 530: 0.9104\n",
            "training loss at step 4 - batch 531: 0.32 (2019-08-04 13:36:01.124739)\n",
            "Accuracy at step 4 - batch 531: 0.9024\n",
            "training loss at step 4 - batch 532: 0.34 (2019-08-04 13:36:01.137290)\n",
            "Accuracy at step 4 - batch 532: 0.8956\n",
            "training loss at step 4 - batch 533: 0.34 (2019-08-04 13:36:01.150068)\n",
            "Accuracy at step 4 - batch 533: 0.9012\n",
            "training loss at step 4 - batch 534: 0.32 (2019-08-04 13:36:01.163335)\n",
            "Accuracy at step 4 - batch 534: 0.908\n",
            "training loss at step 4 - batch 535: 0.32 (2019-08-04 13:36:01.178251)\n",
            "Accuracy at step 4 - batch 535: 0.9032\n",
            "training loss at step 4 - batch 536: 0.34 (2019-08-04 13:36:01.316744)\n",
            "Accuracy at step 4 - batch 536: 0.8956\n",
            "training loss at step 4 - batch 537: 0.31 (2019-08-04 13:36:01.329499)\n",
            "Accuracy at step 4 - batch 537: 0.9044\n",
            "training loss at step 4 - batch 538: 0.34 (2019-08-04 13:36:01.342322)\n",
            "Accuracy at step 4 - batch 538: 0.9012\n",
            "training loss at step 4 - batch 539: 0.30 (2019-08-04 13:36:01.355878)\n",
            "Accuracy at step 4 - batch 539: 0.9084\n",
            "training loss at step 4 - batch 540: 0.32 (2019-08-04 13:36:01.368524)\n",
            "Accuracy at step 4 - batch 540: 0.906\n",
            "training loss at step 4 - batch 541: 0.32 (2019-08-04 13:36:01.491913)\n",
            "Accuracy at step 4 - batch 541: 0.9036\n",
            "training loss at step 4 - batch 542: 0.33 (2019-08-04 13:36:01.508737)\n",
            "Accuracy at step 4 - batch 542: 0.9016\n",
            "training loss at step 4 - batch 543: 0.35 (2019-08-04 13:36:01.521258)\n",
            "Accuracy at step 4 - batch 543: 0.8964\n",
            "training loss at step 4 - batch 544: 0.32 (2019-08-04 13:36:01.534420)\n",
            "Accuracy at step 4 - batch 544: 0.9092\n",
            "training loss at step 4 - batch 545: 0.32 (2019-08-04 13:36:01.547377)\n",
            "Accuracy at step 4 - batch 545: 0.9028\n",
            "training loss at step 4 - batch 546: 0.32 (2019-08-04 13:36:01.672501)\n",
            "Accuracy at step 4 - batch 546: 0.9036\n",
            "training loss at step 4 - batch 547: 0.31 (2019-08-04 13:36:01.686247)\n",
            "Accuracy at step 4 - batch 547: 0.912\n",
            "training loss at step 4 - batch 548: 0.34 (2019-08-04 13:36:01.701321)\n",
            "Accuracy at step 4 - batch 548: 0.8944\n",
            "training loss at step 4 - batch 549: 0.32 (2019-08-04 13:36:01.714137)\n",
            "Accuracy at step 4 - batch 549: 0.9056\n",
            "training loss at step 4 - batch 550: 0.30 (2019-08-04 13:36:01.726575)\n",
            "Accuracy at step 4 - batch 550: 0.9084\n",
            "training loss at step 4 - batch 551: 0.35 (2019-08-04 13:36:01.846162)\n",
            "Accuracy at step 4 - batch 551: 0.8952\n",
            "training loss at step 4 - batch 552: 0.33 (2019-08-04 13:36:01.860617)\n",
            "Accuracy at step 4 - batch 552: 0.9028\n",
            "training loss at step 4 - batch 553: 0.34 (2019-08-04 13:36:01.873078)\n",
            "Accuracy at step 4 - batch 553: 0.9004\n",
            "training loss at step 4 - batch 554: 0.35 (2019-08-04 13:36:01.886717)\n",
            "Accuracy at step 4 - batch 554: 0.8996\n",
            "training loss at step 4 - batch 555: 0.31 (2019-08-04 13:36:01.898760)\n",
            "Accuracy at step 4 - batch 555: 0.9052\n",
            "training loss at step 4 - batch 556: 0.35 (2019-08-04 13:36:02.021196)\n",
            "Accuracy at step 4 - batch 556: 0.8928\n",
            "training loss at step 4 - batch 557: 0.30 (2019-08-04 13:36:02.033132)\n",
            "Accuracy at step 4 - batch 557: 0.9068\n",
            "training loss at step 4 - batch 558: 0.31 (2019-08-04 13:36:02.047072)\n",
            "Accuracy at step 4 - batch 558: 0.91\n",
            "training loss at step 4 - batch 559: 0.33 (2019-08-04 13:36:02.061123)\n",
            "Accuracy at step 4 - batch 559: 0.8992\n",
            "training loss at step 4 - batch 560: 0.32 (2019-08-04 13:36:02.075942)\n",
            "Accuracy at step 4 - batch 560: 0.9064\n",
            "training loss at step 4 - batch 561: 0.34 (2019-08-04 13:36:02.202440)\n",
            "Accuracy at step 4 - batch 561: 0.898\n",
            "training loss at step 4 - batch 562: 0.32 (2019-08-04 13:36:02.219233)\n",
            "Accuracy at step 4 - batch 562: 0.904\n",
            "training loss at step 4 - batch 563: 0.35 (2019-08-04 13:36:02.239025)\n",
            "Accuracy at step 4 - batch 563: 0.9\n",
            "training loss at step 4 - batch 564: 0.32 (2019-08-04 13:36:02.252252)\n",
            "Accuracy at step 4 - batch 564: 0.904\n",
            "training loss at step 4 - batch 565: 0.34 (2019-08-04 13:36:02.265677)\n",
            "Accuracy at step 4 - batch 565: 0.8996\n",
            "training loss at step 4 - batch 566: 0.32 (2019-08-04 13:36:02.388946)\n",
            "Accuracy at step 4 - batch 566: 0.9024\n",
            "training loss at step 4 - batch 567: 0.31 (2019-08-04 13:36:02.401244)\n",
            "Accuracy at step 4 - batch 567: 0.9076\n",
            "training loss at step 4 - batch 568: 0.31 (2019-08-04 13:36:02.413394)\n",
            "Accuracy at step 4 - batch 568: 0.9088\n",
            "training loss at step 4 - batch 569: 0.32 (2019-08-04 13:36:02.425634)\n",
            "Accuracy at step 4 - batch 569: 0.9064\n",
            "training loss at step 4 - batch 570: 0.32 (2019-08-04 13:36:02.439083)\n",
            "Accuracy at step 4 - batch 570: 0.9064\n",
            "training loss at step 4 - batch 571: 0.32 (2019-08-04 13:36:02.567798)\n",
            "Accuracy at step 4 - batch 571: 0.9024\n",
            "training loss at step 4 - batch 572: 0.31 (2019-08-04 13:36:02.582363)\n",
            "Accuracy at step 4 - batch 572: 0.906\n",
            "training loss at step 4 - batch 573: 0.33 (2019-08-04 13:36:02.595008)\n",
            "Accuracy at step 4 - batch 573: 0.902\n",
            "training loss at step 4 - batch 574: 0.33 (2019-08-04 13:36:02.607657)\n",
            "Accuracy at step 4 - batch 574: 0.896\n",
            "training loss at step 4 - batch 575: 0.31 (2019-08-04 13:36:02.621750)\n",
            "Accuracy at step 4 - batch 575: 0.9116\n",
            "training loss at step 4 - batch 576: 0.32 (2019-08-04 13:36:02.737932)\n",
            "Accuracy at step 4 - batch 576: 0.908\n",
            "training loss at step 4 - batch 577: 0.33 (2019-08-04 13:36:02.750825)\n",
            "Accuracy at step 4 - batch 577: 0.904\n",
            "training loss at step 4 - batch 578: 0.34 (2019-08-04 13:36:02.764762)\n",
            "Accuracy at step 4 - batch 578: 0.8984\n",
            "training loss at step 4 - batch 579: 0.32 (2019-08-04 13:36:02.779823)\n",
            "Accuracy at step 4 - batch 579: 0.9068\n",
            "training loss at step 4 - batch 580: 0.34 (2019-08-04 13:36:02.792406)\n",
            "Accuracy at step 4 - batch 580: 0.8956\n",
            "training loss at step 4 - batch 581: 0.33 (2019-08-04 13:36:02.913306)\n",
            "Accuracy at step 4 - batch 581: 0.9084\n",
            "training loss at step 4 - batch 582: 0.33 (2019-08-04 13:36:02.927607)\n",
            "Accuracy at step 4 - batch 582: 0.9028\n",
            "training loss at step 4 - batch 583: 0.32 (2019-08-04 13:36:02.940076)\n",
            "Accuracy at step 4 - batch 583: 0.9104\n",
            "training loss at step 4 - batch 584: 0.32 (2019-08-04 13:36:02.953272)\n",
            "Accuracy at step 4 - batch 584: 0.9048\n",
            "training loss at step 4 - batch 585: 0.33 (2019-08-04 13:36:02.966024)\n",
            "Accuracy at step 4 - batch 585: 0.9072\n",
            "training loss at step 4 - batch 586: 0.36 (2019-08-04 13:36:03.092498)\n",
            "Accuracy at step 4 - batch 586: 0.8968\n",
            "training loss at step 4 - batch 587: 0.35 (2019-08-04 13:36:03.109702)\n",
            "Accuracy at step 4 - batch 587: 0.8976\n",
            "training loss at step 4 - batch 588: 0.33 (2019-08-04 13:36:03.122344)\n",
            "Accuracy at step 4 - batch 588: 0.9068\n",
            "training loss at step 4 - batch 589: 0.36 (2019-08-04 13:36:03.134771)\n",
            "Accuracy at step 4 - batch 589: 0.9004\n",
            "training loss at step 4 - batch 590: 0.34 (2019-08-04 13:36:03.148349)\n",
            "Accuracy at step 4 - batch 590: 0.902\n",
            "training loss at step 4 - batch 591: 0.33 (2019-08-04 13:36:03.271828)\n",
            "Accuracy at step 4 - batch 591: 0.9032\n",
            "training loss at step 4 - batch 592: 0.31 (2019-08-04 13:36:03.286109)\n",
            "Accuracy at step 4 - batch 592: 0.9052\n",
            "training loss at step 4 - batch 593: 0.32 (2019-08-04 13:36:03.300512)\n",
            "Accuracy at step 4 - batch 593: 0.906\n",
            "training loss at step 4 - batch 594: 0.32 (2019-08-04 13:36:03.313165)\n",
            "Accuracy at step 4 - batch 594: 0.9056\n",
            "training loss at step 4 - batch 595: 0.32 (2019-08-04 13:36:03.327504)\n",
            "Accuracy at step 4 - batch 595: 0.9036\n",
            "training loss at step 4 - batch 596: 0.33 (2019-08-04 13:36:03.457878)\n",
            "Accuracy at step 4 - batch 596: 0.8992\n",
            "training loss at step 4 - batch 597: 0.33 (2019-08-04 13:36:03.470520)\n",
            "Accuracy at step 4 - batch 597: 0.8984\n",
            "training loss at step 4 - batch 598: 0.32 (2019-08-04 13:36:03.483518)\n",
            "Accuracy at step 4 - batch 598: 0.908\n",
            "training loss at step 4 - batch 599: 0.34 (2019-08-04 13:36:03.496135)\n",
            "Accuracy at step 4 - batch 599: 0.8988\n",
            "training loss at step 4 - batch 600: 0.34 (2019-08-04 13:36:03.510561)\n",
            "Accuracy at step 4 - batch 600: 0.8956\n",
            "training loss at step 4 - batch 601: 0.34 (2019-08-04 13:36:03.636486)\n",
            "Accuracy at step 4 - batch 601: 0.9008\n",
            "training loss at step 4 - batch 602: 0.31 (2019-08-04 13:36:03.648899)\n",
            "Accuracy at step 4 - batch 602: 0.906\n",
            "training loss at step 4 - batch 603: 0.33 (2019-08-04 13:36:03.661898)\n",
            "Accuracy at step 4 - batch 603: 0.8984\n",
            "training loss at step 4 - batch 604: 0.29 (2019-08-04 13:36:03.673943)\n",
            "Accuracy at step 4 - batch 604: 0.9092\n",
            "training loss at step 4 - batch 605: 0.35 (2019-08-04 13:36:03.688347)\n",
            "Accuracy at step 4 - batch 605: 0.898\n",
            "training loss at step 4 - batch 606: 0.33 (2019-08-04 13:36:03.812543)\n",
            "Accuracy at step 4 - batch 606: 0.9024\n",
            "training loss at step 4 - batch 607: 0.34 (2019-08-04 13:36:03.826761)\n",
            "Accuracy at step 4 - batch 607: 0.898\n",
            "training loss at step 4 - batch 608: 0.34 (2019-08-04 13:36:03.839565)\n",
            "Accuracy at step 4 - batch 608: 0.8988\n",
            "training loss at step 4 - batch 609: 0.34 (2019-08-04 13:36:03.853273)\n",
            "Accuracy at step 4 - batch 609: 0.9028\n",
            "training loss at step 4 - batch 610: 0.32 (2019-08-04 13:36:03.867858)\n",
            "Accuracy at step 4 - batch 610: 0.9048\n",
            "training loss at step 4 - batch 611: 0.34 (2019-08-04 13:36:03.992777)\n",
            "Accuracy at step 4 - batch 611: 0.8952\n",
            "training loss at step 4 - batch 612: 0.31 (2019-08-04 13:36:04.007663)\n",
            "Accuracy at step 4 - batch 612: 0.9096\n",
            "training loss at step 4 - batch 613: 0.33 (2019-08-04 13:36:04.024192)\n",
            "Accuracy at step 4 - batch 613: 0.9008\n",
            "training loss at step 4 - batch 614: 0.32 (2019-08-04 13:36:04.036607)\n",
            "Accuracy at step 4 - batch 614: 0.9068\n",
            "training loss at step 4 - batch 615: 0.33 (2019-08-04 13:36:04.048599)\n",
            "Accuracy at step 4 - batch 615: 0.898\n",
            "training loss at step 4 - batch 616: 0.32 (2019-08-04 13:36:04.169908)\n",
            "Accuracy at step 4 - batch 616: 0.9056\n",
            "training loss at step 4 - batch 617: 0.33 (2019-08-04 13:36:04.184460)\n",
            "Accuracy at step 4 - batch 617: 0.9044\n",
            "training loss at step 4 - batch 618: 0.33 (2019-08-04 13:36:04.197444)\n",
            "Accuracy at step 4 - batch 618: 0.9028\n",
            "training loss at step 4 - batch 619: 0.36 (2019-08-04 13:36:04.211306)\n",
            "Accuracy at step 4 - batch 619: 0.896\n",
            "training loss at step 4 - batch 620: 0.32 (2019-08-04 13:36:04.224260)\n",
            "Accuracy at step 4 - batch 620: 0.9052\n",
            "training loss at step 4 - batch 621: 0.33 (2019-08-04 13:36:04.367060)\n",
            "Accuracy at step 4 - batch 621: 0.8956\n",
            "training loss at step 4 - batch 622: 0.35 (2019-08-04 13:36:04.383932)\n",
            "Accuracy at step 4 - batch 622: 0.8976\n",
            "training loss at step 4 - batch 623: 0.31 (2019-08-04 13:36:04.396508)\n",
            "Accuracy at step 4 - batch 623: 0.9076\n",
            "training loss at step 4 - batch 624: 0.34 (2019-08-04 13:36:04.409490)\n",
            "Accuracy at step 4 - batch 624: 0.9\n",
            "training loss at step 4 - batch 625: 0.34 (2019-08-04 13:36:04.422057)\n",
            "Accuracy at step 4 - batch 625: 0.9004\n",
            "training loss at step 4 - batch 626: 0.32 (2019-08-04 13:36:04.546478)\n",
            "Accuracy at step 4 - batch 626: 0.9024\n",
            "training loss at step 4 - batch 627: 0.32 (2019-08-04 13:36:04.562100)\n",
            "Accuracy at step 4 - batch 627: 0.9028\n",
            "training loss at step 4 - batch 628: 0.34 (2019-08-04 13:36:04.576397)\n",
            "Accuracy at step 4 - batch 628: 0.8988\n",
            "training loss at step 4 - batch 629: 0.34 (2019-08-04 13:36:04.589004)\n",
            "Accuracy at step 4 - batch 629: 0.9024\n",
            "training loss at step 4 - batch 630: 0.33 (2019-08-04 13:36:04.602370)\n",
            "Accuracy at step 4 - batch 630: 0.9024\n",
            "training loss at step 4 - batch 631: 0.35 (2019-08-04 13:36:04.725910)\n",
            "Accuracy at step 4 - batch 631: 0.8988\n",
            "training loss at step 4 - batch 632: 0.32 (2019-08-04 13:36:04.742229)\n",
            "Accuracy at step 4 - batch 632: 0.904\n",
            "training loss at step 4 - batch 633: 0.34 (2019-08-04 13:36:04.757676)\n",
            "Accuracy at step 4 - batch 633: 0.8996\n",
            "training loss at step 4 - batch 634: 0.35 (2019-08-04 13:36:04.770249)\n",
            "Accuracy at step 4 - batch 634: 0.8956\n",
            "training loss at step 4 - batch 635: 0.32 (2019-08-04 13:36:04.782650)\n",
            "Accuracy at step 4 - batch 635: 0.9024\n",
            "training loss at step 4 - batch 636: 0.33 (2019-08-04 13:36:04.905604)\n",
            "Accuracy at step 4 - batch 636: 0.9088\n",
            "training loss at step 4 - batch 637: 0.31 (2019-08-04 13:36:04.921765)\n",
            "Accuracy at step 4 - batch 637: 0.9092\n",
            "training loss at step 4 - batch 638: 0.33 (2019-08-04 13:36:04.934707)\n",
            "Accuracy at step 4 - batch 638: 0.9016\n",
            "training loss at step 4 - batch 639: 0.34 (2019-08-04 13:36:04.947150)\n",
            "Accuracy at step 4 - batch 639: 0.898\n",
            "training loss at step 4 - batch 640: 0.33 (2019-08-04 13:36:04.960610)\n",
            "Accuracy at step 4 - batch 640: 0.906\n",
            "training loss at step 4 - batch 641: 0.35 (2019-08-04 13:36:05.083921)\n",
            "Accuracy at step 4 - batch 641: 0.9004\n",
            "training loss at step 4 - batch 642: 0.32 (2019-08-04 13:36:05.098240)\n",
            "Accuracy at step 4 - batch 642: 0.9048\n",
            "training loss at step 4 - batch 643: 0.32 (2019-08-04 13:36:05.110329)\n",
            "Accuracy at step 4 - batch 643: 0.9072\n",
            "training loss at step 4 - batch 644: 0.33 (2019-08-04 13:36:05.124442)\n",
            "Accuracy at step 4 - batch 644: 0.8996\n",
            "training loss at step 4 - batch 645: 0.33 (2019-08-04 13:36:05.137860)\n",
            "Accuracy at step 4 - batch 645: 0.8988\n",
            "training loss at step 4 - batch 646: 0.32 (2019-08-04 13:36:05.260514)\n",
            "Accuracy at step 4 - batch 646: 0.908\n",
            "training loss at step 4 - batch 647: 0.34 (2019-08-04 13:36:05.278473)\n",
            "Accuracy at step 4 - batch 647: 0.8924\n",
            "training loss at step 4 - batch 648: 0.33 (2019-08-04 13:36:05.293688)\n",
            "Accuracy at step 4 - batch 648: 0.9052\n",
            "training loss at step 4 - batch 649: 0.35 (2019-08-04 13:36:05.306223)\n",
            "Accuracy at step 4 - batch 649: 0.8968\n",
            "training loss at step 4 - batch 650: 0.32 (2019-08-04 13:36:05.318231)\n",
            "Accuracy at step 4 - batch 650: 0.908\n",
            "training loss at step 4 - batch 651: 0.33 (2019-08-04 13:36:05.444205)\n",
            "Accuracy at step 4 - batch 651: 0.9016\n",
            "training loss at step 4 - batch 652: 0.31 (2019-08-04 13:36:05.457099)\n",
            "Accuracy at step 4 - batch 652: 0.9084\n",
            "training loss at step 4 - batch 653: 0.34 (2019-08-04 13:36:05.471906)\n",
            "Accuracy at step 4 - batch 653: 0.8984\n",
            "training loss at step 4 - batch 654: 0.34 (2019-08-04 13:36:05.485284)\n",
            "Accuracy at step 4 - batch 654: 0.8988\n",
            "training loss at step 4 - batch 655: 0.34 (2019-08-04 13:36:05.498904)\n",
            "Accuracy at step 4 - batch 655: 0.9\n",
            "training loss at step 4 - batch 656: 0.32 (2019-08-04 13:36:05.626798)\n",
            "Accuracy at step 4 - batch 656: 0.8972\n",
            "training loss at step 4 - batch 657: 0.34 (2019-08-04 13:36:05.641689)\n",
            "Accuracy at step 4 - batch 657: 0.9012\n",
            "training loss at step 4 - batch 658: 0.33 (2019-08-04 13:36:05.655232)\n",
            "Accuracy at step 4 - batch 658: 0.8964\n",
            "training loss at step 4 - batch 659: 0.32 (2019-08-04 13:36:05.667217)\n",
            "Accuracy at step 4 - batch 659: 0.9032\n",
            "training loss at step 4 - batch 660: 0.31 (2019-08-04 13:36:05.682723)\n",
            "Accuracy at step 4 - batch 660: 0.9144\n",
            "training loss at step 4 - batch 661: 0.33 (2019-08-04 13:36:05.808007)\n",
            "Accuracy at step 4 - batch 661: 0.9036\n",
            "training loss at step 4 - batch 662: 0.32 (2019-08-04 13:36:05.821106)\n",
            "Accuracy at step 4 - batch 662: 0.9024\n",
            "training loss at step 4 - batch 663: 0.32 (2019-08-04 13:36:05.834924)\n",
            "Accuracy at step 4 - batch 663: 0.9064\n",
            "training loss at step 4 - batch 664: 0.34 (2019-08-04 13:36:05.848532)\n",
            "Accuracy at step 4 - batch 664: 0.9004\n",
            "training loss at step 4 - batch 665: 0.32 (2019-08-04 13:36:05.861667)\n",
            "Accuracy at step 4 - batch 665: 0.9052\n",
            "training loss at step 4 - batch 666: 0.31 (2019-08-04 13:36:05.991781)\n",
            "Accuracy at step 4 - batch 666: 0.9008\n",
            "training loss at step 4 - batch 667: 0.33 (2019-08-04 13:36:06.009711)\n",
            "Accuracy at step 4 - batch 667: 0.9044\n",
            "training loss at step 4 - batch 668: 0.33 (2019-08-04 13:36:06.025879)\n",
            "Accuracy at step 4 - batch 668: 0.8984\n",
            "training loss at step 4 - batch 669: 0.32 (2019-08-04 13:36:06.039693)\n",
            "Accuracy at step 4 - batch 669: 0.906\n",
            "training loss at step 4 - batch 670: 0.32 (2019-08-04 13:36:06.052421)\n",
            "Accuracy at step 4 - batch 670: 0.9036\n",
            "training loss at step 4 - batch 671: 0.34 (2019-08-04 13:36:06.175706)\n",
            "Accuracy at step 4 - batch 671: 0.8984\n",
            "training loss at step 4 - batch 672: 0.35 (2019-08-04 13:36:06.193993)\n",
            "Accuracy at step 4 - batch 672: 0.8956\n",
            "training loss at step 4 - batch 673: 0.33 (2019-08-04 13:36:06.208255)\n",
            "Accuracy at step 4 - batch 673: 0.9004\n",
            "training loss at step 4 - batch 674: 0.31 (2019-08-04 13:36:06.220226)\n",
            "Accuracy at step 4 - batch 674: 0.908\n",
            "training loss at step 4 - batch 675: 0.34 (2019-08-04 13:36:06.233411)\n",
            "Accuracy at step 4 - batch 675: 0.8968\n",
            "training loss at step 4 - batch 676: 0.32 (2019-08-04 13:36:06.367183)\n",
            "Accuracy at step 4 - batch 676: 0.9056\n",
            "training loss at step 4 - batch 677: 0.31 (2019-08-04 13:36:06.380061)\n",
            "Accuracy at step 4 - batch 677: 0.9048\n",
            "training loss at step 4 - batch 678: 0.33 (2019-08-04 13:36:06.392353)\n",
            "Accuracy at step 4 - batch 678: 0.9076\n",
            "training loss at step 4 - batch 679: 0.32 (2019-08-04 13:36:06.407579)\n",
            "Accuracy at step 4 - batch 679: 0.9044\n",
            "training loss at step 4 - batch 680: 0.34 (2019-08-04 13:36:06.419849)\n",
            "Accuracy at step 4 - batch 680: 0.9044\n",
            "training loss at step 4 - batch 681: 0.33 (2019-08-04 13:36:06.539998)\n",
            "Accuracy at step 4 - batch 681: 0.9064\n",
            "training loss at step 4 - batch 682: 0.32 (2019-08-04 13:36:06.552494)\n",
            "Accuracy at step 4 - batch 682: 0.9052\n",
            "training loss at step 4 - batch 683: 0.33 (2019-08-04 13:36:06.565864)\n",
            "Accuracy at step 4 - batch 683: 0.9016\n",
            "training loss at step 4 - batch 684: 0.33 (2019-08-04 13:36:06.578241)\n",
            "Accuracy at step 4 - batch 684: 0.9052\n",
            "training loss at step 4 - batch 685: 0.32 (2019-08-04 13:36:06.591108)\n",
            "Accuracy at step 4 - batch 685: 0.9044\n",
            "training loss at step 4 - batch 686: 0.36 (2019-08-04 13:36:06.721096)\n",
            "Accuracy at step 4 - batch 686: 0.8972\n",
            "training loss at step 4 - batch 687: 0.34 (2019-08-04 13:36:06.733290)\n",
            "Accuracy at step 4 - batch 687: 0.8988\n",
            "training loss at step 4 - batch 688: 0.34 (2019-08-04 13:36:06.746694)\n",
            "Accuracy at step 4 - batch 688: 0.9036\n",
            "training loss at step 4 - batch 689: 0.34 (2019-08-04 13:36:06.758940)\n",
            "Accuracy at step 4 - batch 689: 0.8972\n",
            "training loss at step 4 - batch 690: 0.35 (2019-08-04 13:36:06.772218)\n",
            "Accuracy at step 4 - batch 690: 0.8956\n",
            "training loss at step 4 - batch 691: 0.34 (2019-08-04 13:36:06.894218)\n",
            "Accuracy at step 4 - batch 691: 0.8992\n",
            "training loss at step 4 - batch 692: 0.32 (2019-08-04 13:36:06.911859)\n",
            "Accuracy at step 4 - batch 692: 0.9084\n",
            "training loss at step 4 - batch 693: 0.32 (2019-08-04 13:36:06.927177)\n",
            "Accuracy at step 4 - batch 693: 0.9012\n",
            "training loss at step 4 - batch 694: 0.33 (2019-08-04 13:36:06.940577)\n",
            "Accuracy at step 4 - batch 694: 0.902\n",
            "training loss at step 4 - batch 695: 0.31 (2019-08-04 13:36:06.952824)\n",
            "Accuracy at step 4 - batch 695: 0.9032\n",
            "training loss at step 4 - batch 696: 0.35 (2019-08-04 13:36:07.078547)\n",
            "Accuracy at step 4 - batch 696: 0.9\n",
            "training loss at step 4 - batch 697: 0.32 (2019-08-04 13:36:07.094257)\n",
            "Accuracy at step 4 - batch 697: 0.9076\n",
            "training loss at step 4 - batch 698: 0.31 (2019-08-04 13:36:07.107020)\n",
            "Accuracy at step 4 - batch 698: 0.9092\n",
            "training loss at step 4 - batch 699: 0.32 (2019-08-04 13:36:07.120741)\n",
            "Accuracy at step 4 - batch 699: 0.9004\n",
            "training loss at step 4 - batch 700: 0.32 (2019-08-04 13:36:07.135931)\n",
            "Accuracy at step 4 - batch 700: 0.9084\n",
            "training loss at step 4 - batch 701: 0.32 (2019-08-04 13:36:07.260038)\n",
            "Accuracy at step 4 - batch 701: 0.9012\n",
            "training loss at step 4 - batch 702: 0.32 (2019-08-04 13:36:07.277641)\n",
            "Accuracy at step 4 - batch 702: 0.904\n",
            "training loss at step 4 - batch 703: 0.32 (2019-08-04 13:36:07.290262)\n",
            "Accuracy at step 4 - batch 703: 0.9052\n",
            "training loss at step 4 - batch 704: 0.34 (2019-08-04 13:36:07.303979)\n",
            "Accuracy at step 4 - batch 704: 0.8984\n",
            "training loss at step 4 - batch 705: 0.33 (2019-08-04 13:36:07.317672)\n",
            "Accuracy at step 4 - batch 705: 0.9048\n",
            "training loss at step 4 - batch 706: 0.33 (2019-08-04 13:36:07.450562)\n",
            "Accuracy at step 4 - batch 706: 0.9032\n",
            "training loss at step 4 - batch 707: 0.32 (2019-08-04 13:36:07.463260)\n",
            "Accuracy at step 4 - batch 707: 0.9044\n",
            "training loss at step 4 - batch 708: 0.32 (2019-08-04 13:36:07.476694)\n",
            "Accuracy at step 4 - batch 708: 0.9048\n",
            "training loss at step 4 - batch 709: 0.33 (2019-08-04 13:36:07.489087)\n",
            "Accuracy at step 4 - batch 709: 0.9028\n",
            "training loss at step 4 - batch 710: 0.31 (2019-08-04 13:36:07.502543)\n",
            "Accuracy at step 4 - batch 710: 0.9064\n",
            "training loss at step 4 - batch 711: 0.34 (2019-08-04 13:36:07.626263)\n",
            "Accuracy at step 4 - batch 711: 0.8996\n",
            "training loss at step 4 - batch 712: 0.34 (2019-08-04 13:36:07.641010)\n",
            "Accuracy at step 4 - batch 712: 0.8984\n",
            "training loss at step 4 - batch 713: 0.35 (2019-08-04 13:36:07.655677)\n",
            "Accuracy at step 4 - batch 713: 0.9008\n",
            "training loss at step 4 - batch 714: 0.33 (2019-08-04 13:36:07.670075)\n",
            "Accuracy at step 4 - batch 714: 0.8992\n",
            "training loss at step 4 - batch 715: 0.33 (2019-08-04 13:36:07.683284)\n",
            "Accuracy at step 4 - batch 715: 0.9036\n",
            "training loss at step 4 - batch 716: 0.31 (2019-08-04 13:36:07.807193)\n",
            "Accuracy at step 4 - batch 716: 0.9044\n",
            "training loss at step 4 - batch 717: 0.34 (2019-08-04 13:36:07.821286)\n",
            "Accuracy at step 4 - batch 717: 0.9012\n",
            "training loss at step 4 - batch 718: 0.33 (2019-08-04 13:36:07.833272)\n",
            "Accuracy at step 4 - batch 718: 0.9036\n",
            "training loss at step 4 - batch 719: 0.33 (2019-08-04 13:36:07.846237)\n",
            "Accuracy at step 4 - batch 719: 0.9048\n",
            "training loss at step 4 - batch 720: 0.32 (2019-08-04 13:36:07.861445)\n",
            "Accuracy at step 4 - batch 720: 0.904\n",
            "training loss at step 4 - batch 721: 0.32 (2019-08-04 13:36:07.985996)\n",
            "Accuracy at step 4 - batch 721: 0.9092\n",
            "training loss at step 4 - batch 722: 0.31 (2019-08-04 13:36:07.999973)\n",
            "Accuracy at step 4 - batch 722: 0.9072\n",
            "training loss at step 4 - batch 723: 0.32 (2019-08-04 13:36:08.012223)\n",
            "Accuracy at step 4 - batch 723: 0.9016\n",
            "training loss at step 4 - batch 724: 0.33 (2019-08-04 13:36:08.024927)\n",
            "Accuracy at step 4 - batch 724: 0.9016\n",
            "training loss at step 4 - batch 725: 0.31 (2019-08-04 13:36:08.037452)\n",
            "Accuracy at step 4 - batch 725: 0.9108\n",
            "training loss at step 4 - batch 726: 0.34 (2019-08-04 13:36:08.163063)\n",
            "Accuracy at step 4 - batch 726: 0.9016\n",
            "training loss at step 4 - batch 727: 0.33 (2019-08-04 13:36:08.175565)\n",
            "Accuracy at step 4 - batch 727: 0.8996\n",
            "training loss at step 4 - batch 728: 0.32 (2019-08-04 13:36:08.187720)\n",
            "Accuracy at step 4 - batch 728: 0.904\n",
            "training loss at step 4 - batch 729: 0.35 (2019-08-04 13:36:08.200328)\n",
            "Accuracy at step 4 - batch 729: 0.8976\n",
            "training loss at step 4 - batch 730: 0.32 (2019-08-04 13:36:08.213454)\n",
            "Accuracy at step 4 - batch 730: 0.9028\n",
            "training loss at step 4 - batch 731: 0.34 (2019-08-04 13:36:08.333440)\n",
            "Accuracy at step 4 - batch 731: 0.896\n",
            "training loss at step 4 - batch 732: 0.34 (2019-08-04 13:36:08.350630)\n",
            "Accuracy at step 4 - batch 732: 0.8972\n",
            "training loss at step 4 - batch 733: 0.36 (2019-08-04 13:36:08.369818)\n",
            "Accuracy at step 4 - batch 733: 0.8916\n",
            "training loss at step 4 - batch 734: 0.34 (2019-08-04 13:36:08.382286)\n",
            "Accuracy at step 4 - batch 734: 0.9016\n",
            "training loss at step 4 - batch 735: 0.33 (2019-08-04 13:36:08.395477)\n",
            "Accuracy at step 4 - batch 735: 0.9048\n",
            "training loss at step 4 - batch 736: 0.33 (2019-08-04 13:36:08.516196)\n",
            "Accuracy at step 4 - batch 736: 0.8996\n",
            "training loss at step 4 - batch 737: 0.31 (2019-08-04 13:36:08.529151)\n",
            "Accuracy at step 4 - batch 737: 0.91\n",
            "training loss at step 4 - batch 738: 0.33 (2019-08-04 13:36:08.542066)\n",
            "Accuracy at step 4 - batch 738: 0.8984\n",
            "training loss at step 4 - batch 739: 0.34 (2019-08-04 13:36:08.556051)\n",
            "Accuracy at step 4 - batch 739: 0.8968\n",
            "training loss at step 4 - batch 740: 0.31 (2019-08-04 13:36:08.568842)\n",
            "Accuracy at step 4 - batch 740: 0.904\n",
            "training loss at step 4 - batch 741: 0.35 (2019-08-04 13:36:08.697866)\n",
            "Accuracy at step 4 - batch 741: 0.896\n",
            "training loss at step 4 - batch 742: 0.31 (2019-08-04 13:36:08.711718)\n",
            "Accuracy at step 4 - batch 742: 0.9108\n",
            "training loss at step 4 - batch 743: 0.32 (2019-08-04 13:36:08.725913)\n",
            "Accuracy at step 4 - batch 743: 0.8992\n",
            "training loss at step 4 - batch 744: 0.33 (2019-08-04 13:36:08.738650)\n",
            "Accuracy at step 4 - batch 744: 0.9016\n",
            "training loss at step 4 - batch 745: 0.32 (2019-08-04 13:36:08.750612)\n",
            "Accuracy at step 4 - batch 745: 0.9048\n",
            "training loss at step 4 - batch 746: 0.33 (2019-08-04 13:36:08.881330)\n",
            "Accuracy at step 4 - batch 746: 0.902\n",
            "training loss at step 4 - batch 747: 0.33 (2019-08-04 13:36:08.895237)\n",
            "Accuracy at step 4 - batch 747: 0.9008\n",
            "training loss at step 4 - batch 748: 0.33 (2019-08-04 13:36:08.910936)\n",
            "Accuracy at step 4 - batch 748: 0.8988\n",
            "training loss at step 4 - batch 749: 0.32 (2019-08-04 13:36:08.927934)\n",
            "Accuracy at step 4 - batch 749: 0.9056\n",
            "training loss at step 4 - batch 750: 0.34 (2019-08-04 13:36:08.942756)\n",
            "Accuracy at step 4 - batch 750: 0.9028\n",
            "training loss at step 4 - batch 751: 0.34 (2019-08-04 13:36:09.072493)\n",
            "Accuracy at step 4 - batch 751: 0.9008\n",
            "training loss at step 4 - batch 752: 0.30 (2019-08-04 13:36:09.088298)\n",
            "Accuracy at step 4 - batch 752: 0.9108\n",
            "training loss at step 4 - batch 753: 0.34 (2019-08-04 13:36:09.100995)\n",
            "Accuracy at step 4 - batch 753: 0.9032\n",
            "training loss at step 4 - batch 754: 0.32 (2019-08-04 13:36:09.116401)\n",
            "Accuracy at step 4 - batch 754: 0.908\n",
            "training loss at step 4 - batch 755: 0.33 (2019-08-04 13:36:09.129751)\n",
            "Accuracy at step 4 - batch 755: 0.9028\n",
            "training loss at step 4 - batch 756: 0.31 (2019-08-04 13:36:09.252065)\n",
            "Accuracy at step 4 - batch 756: 0.9092\n",
            "training loss at step 4 - batch 757: 0.34 (2019-08-04 13:36:09.266699)\n",
            "Accuracy at step 4 - batch 757: 0.8988\n",
            "training loss at step 4 - batch 758: 0.31 (2019-08-04 13:36:09.280530)\n",
            "Accuracy at step 4 - batch 758: 0.9032\n",
            "training loss at step 4 - batch 759: 0.34 (2019-08-04 13:36:09.293119)\n",
            "Accuracy at step 4 - batch 759: 0.9008\n",
            "training loss at step 4 - batch 760: 0.33 (2019-08-04 13:36:09.306208)\n",
            "Accuracy at step 4 - batch 760: 0.8968\n",
            "training loss at step 4 - batch 761: 0.32 (2019-08-04 13:36:09.438480)\n",
            "Accuracy at step 4 - batch 761: 0.9116\n",
            "training loss at step 4 - batch 762: 0.34 (2019-08-04 13:36:09.451483)\n",
            "Accuracy at step 4 - batch 762: 0.8988\n",
            "training loss at step 4 - batch 763: 0.32 (2019-08-04 13:36:09.465814)\n",
            "Accuracy at step 4 - batch 763: 0.9056\n",
            "training loss at step 4 - batch 764: 0.31 (2019-08-04 13:36:09.478599)\n",
            "Accuracy at step 4 - batch 764: 0.9084\n",
            "training loss at step 4 - batch 765: 0.33 (2019-08-04 13:36:09.492330)\n",
            "Accuracy at step 4 - batch 765: 0.9008\n",
            "training loss at step 4 - batch 766: 0.32 (2019-08-04 13:36:09.611188)\n",
            "Accuracy at step 4 - batch 766: 0.9048\n",
            "training loss at step 4 - batch 767: 0.32 (2019-08-04 13:36:09.628734)\n",
            "Accuracy at step 4 - batch 767: 0.9068\n",
            "training loss at step 4 - batch 768: 0.31 (2019-08-04 13:36:09.643396)\n",
            "Accuracy at step 4 - batch 768: 0.9036\n",
            "training loss at step 4 - batch 769: 0.32 (2019-08-04 13:36:09.656823)\n",
            "Accuracy at step 4 - batch 769: 0.9052\n",
            "training loss at step 4 - batch 770: 0.32 (2019-08-04 13:36:09.669132)\n",
            "Accuracy at step 4 - batch 770: 0.9016\n",
            "training loss at step 4 - batch 771: 0.32 (2019-08-04 13:36:09.796702)\n",
            "Accuracy at step 4 - batch 771: 0.908\n",
            "training loss at step 4 - batch 772: 0.36 (2019-08-04 13:36:09.810675)\n",
            "Accuracy at step 4 - batch 772: 0.8944\n",
            "training loss at step 4 - batch 773: 0.32 (2019-08-04 13:36:09.824891)\n",
            "Accuracy at step 4 - batch 773: 0.9028\n",
            "training loss at step 4 - batch 774: 0.33 (2019-08-04 13:36:09.838280)\n",
            "Accuracy at step 4 - batch 774: 0.906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 13:36:10.036100 140386591647616 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at step 4 - batch 775: 0.31 (2019-08-04 13:36:09.853616)\n",
            "Accuracy at step 4 - batch 775: 0.9104\n",
            "training loss at step 4 - batch 776: 0.33 (2019-08-04 13:36:09.974821)\n",
            "Accuracy at step 4 - batch 776: 0.908\n",
            "training loss at step 4 - batch 777: 0.32 (2019-08-04 13:36:09.989787)\n",
            "Accuracy at step 4 - batch 777: 0.9024\n",
            "training loss at step 4 - batch 778: 0.32 (2019-08-04 13:36:10.002552)\n",
            "Accuracy at step 4 - batch 778: 0.904\n",
            "training loss at step 4 - batch 779: 0.32 (2019-08-04 13:36:10.015345)\n",
            "Accuracy at step 4 - batch 779: 0.9032\n",
            "training loss at step 5 - batch 0: 0.33 (2019-08-04 13:36:10.031236)\n",
            "Accuracy at step 5 - batch 0: 0.9024\n",
            "training loss at step 5 - batch 1: 0.34 (2019-08-04 13:36:10.168518)\n",
            "Accuracy at step 5 - batch 1: 0.9036\n",
            "training loss at step 5 - batch 2: 0.33 (2019-08-04 13:36:10.182008)\n",
            "Accuracy at step 5 - batch 2: 0.9036\n",
            "training loss at step 5 - batch 3: 0.32 (2019-08-04 13:36:10.195097)\n",
            "Accuracy at step 5 - batch 3: 0.9048\n",
            "training loss at step 5 - batch 4: 0.30 (2019-08-04 13:36:10.207303)\n",
            "Accuracy at step 5 - batch 4: 0.9104\n",
            "training loss at step 5 - batch 5: 0.31 (2019-08-04 13:36:10.221897)\n",
            "Accuracy at step 5 - batch 5: 0.9064\n",
            "training loss at step 5 - batch 6: 0.34 (2019-08-04 13:36:10.346258)\n",
            "Accuracy at step 5 - batch 6: 0.8988\n",
            "training loss at step 5 - batch 7: 0.33 (2019-08-04 13:36:10.360210)\n",
            "Accuracy at step 5 - batch 7: 0.902\n",
            "training loss at step 5 - batch 8: 0.33 (2019-08-04 13:36:10.374864)\n",
            "Accuracy at step 5 - batch 8: 0.9056\n",
            "training loss at step 5 - batch 9: 0.33 (2019-08-04 13:36:10.393751)\n",
            "Accuracy at step 5 - batch 9: 0.8976\n",
            "training loss at step 5 - batch 10: 0.35 (2019-08-04 13:36:10.409044)\n",
            "Accuracy at step 5 - batch 10: 0.8936\n",
            "training loss at step 5 - batch 11: 0.33 (2019-08-04 13:36:10.533080)\n",
            "Accuracy at step 5 - batch 11: 0.9044\n",
            "training loss at step 5 - batch 12: 0.33 (2019-08-04 13:36:10.551340)\n",
            "Accuracy at step 5 - batch 12: 0.902\n",
            "training loss at step 5 - batch 13: 0.35 (2019-08-04 13:36:10.564108)\n",
            "Accuracy at step 5 - batch 13: 0.898\n",
            "training loss at step 5 - batch 14: 0.33 (2019-08-04 13:36:10.577587)\n",
            "Accuracy at step 5 - batch 14: 0.904\n",
            "training loss at step 5 - batch 15: 0.31 (2019-08-04 13:36:10.592967)\n",
            "Accuracy at step 5 - batch 15: 0.9096\n",
            "training loss at step 5 - batch 16: 0.33 (2019-08-04 13:36:10.717195)\n",
            "Accuracy at step 5 - batch 16: 0.9044\n",
            "training loss at step 5 - batch 17: 0.31 (2019-08-04 13:36:10.731116)\n",
            "Accuracy at step 5 - batch 17: 0.9064\n",
            "training loss at step 5 - batch 18: 0.33 (2019-08-04 13:36:10.744786)\n",
            "Accuracy at step 5 - batch 18: 0.9064\n",
            "training loss at step 5 - batch 19: 0.30 (2019-08-04 13:36:10.757656)\n",
            "Accuracy at step 5 - batch 19: 0.914\n",
            "training loss at step 5 - batch 20: 0.34 (2019-08-04 13:36:10.771318)\n",
            "Accuracy at step 5 - batch 20: 0.9004\n",
            "training loss at step 5 - batch 21: 0.33 (2019-08-04 13:36:10.907791)\n",
            "Accuracy at step 5 - batch 21: 0.9028\n",
            "training loss at step 5 - batch 22: 0.34 (2019-08-04 13:36:10.919781)\n",
            "Accuracy at step 5 - batch 22: 0.9004\n",
            "training loss at step 5 - batch 23: 0.32 (2019-08-04 13:36:10.933323)\n",
            "Accuracy at step 5 - batch 23: 0.9008\n",
            "training loss at step 5 - batch 24: 0.31 (2019-08-04 13:36:10.947447)\n",
            "Accuracy at step 5 - batch 24: 0.9068\n",
            "training loss at step 5 - batch 25: 0.34 (2019-08-04 13:36:10.960304)\n",
            "Accuracy at step 5 - batch 25: 0.8976\n",
            "training loss at step 5 - batch 26: 0.34 (2019-08-04 13:36:11.077983)\n",
            "Accuracy at step 5 - batch 26: 0.9052\n",
            "training loss at step 5 - batch 27: 0.31 (2019-08-04 13:36:11.091819)\n",
            "Accuracy at step 5 - batch 27: 0.902\n",
            "training loss at step 5 - batch 28: 0.33 (2019-08-04 13:36:11.103963)\n",
            "Accuracy at step 5 - batch 28: 0.9028\n",
            "training loss at step 5 - batch 29: 0.31 (2019-08-04 13:36:11.122137)\n",
            "Accuracy at step 5 - batch 29: 0.9084\n",
            "training loss at step 5 - batch 30: 0.33 (2019-08-04 13:36:11.134896)\n",
            "Accuracy at step 5 - batch 30: 0.9044\n",
            "training loss at step 5 - batch 31: 0.34 (2019-08-04 13:36:11.254383)\n",
            "Accuracy at step 5 - batch 31: 0.8944\n",
            "training loss at step 5 - batch 32: 0.31 (2019-08-04 13:36:11.266987)\n",
            "Accuracy at step 5 - batch 32: 0.9024\n",
            "training loss at step 5 - batch 33: 0.31 (2019-08-04 13:36:11.280745)\n",
            "Accuracy at step 5 - batch 33: 0.902\n",
            "training loss at step 5 - batch 34: 0.34 (2019-08-04 13:36:11.292687)\n",
            "Accuracy at step 5 - batch 34: 0.8992\n",
            "training loss at step 5 - batch 35: 0.32 (2019-08-04 13:36:11.304915)\n",
            "Accuracy at step 5 - batch 35: 0.906\n",
            "training loss at step 5 - batch 36: 0.33 (2019-08-04 13:36:11.433521)\n",
            "Accuracy at step 5 - batch 36: 0.8988\n",
            "training loss at step 5 - batch 37: 0.32 (2019-08-04 13:36:11.446734)\n",
            "Accuracy at step 5 - batch 37: 0.902\n",
            "training loss at step 5 - batch 38: 0.35 (2019-08-04 13:36:11.459390)\n",
            "Accuracy at step 5 - batch 38: 0.8992\n",
            "training loss at step 5 - batch 39: 0.32 (2019-08-04 13:36:11.471704)\n",
            "Accuracy at step 5 - batch 39: 0.9052\n",
            "training loss at step 5 - batch 40: 0.33 (2019-08-04 13:36:11.483932)\n",
            "Accuracy at step 5 - batch 40: 0.9032\n",
            "training loss at step 5 - batch 41: 0.34 (2019-08-04 13:36:11.611074)\n",
            "Accuracy at step 5 - batch 41: 0.9048\n",
            "training loss at step 5 - batch 42: 0.33 (2019-08-04 13:36:11.625471)\n",
            "Accuracy at step 5 - batch 42: 0.9016\n",
            "training loss at step 5 - batch 43: 0.35 (2019-08-04 13:36:11.638956)\n",
            "Accuracy at step 5 - batch 43: 0.8928\n",
            "training loss at step 5 - batch 44: 0.33 (2019-08-04 13:36:11.652340)\n",
            "Accuracy at step 5 - batch 44: 0.9064\n",
            "training loss at step 5 - batch 45: 0.29 (2019-08-04 13:36:11.665407)\n",
            "Accuracy at step 5 - batch 45: 0.9104\n",
            "training loss at step 5 - batch 46: 0.31 (2019-08-04 13:36:11.788840)\n",
            "Accuracy at step 5 - batch 46: 0.908\n",
            "training loss at step 5 - batch 47: 0.31 (2019-08-04 13:36:11.805963)\n",
            "Accuracy at step 5 - batch 47: 0.908\n",
            "training loss at step 5 - batch 48: 0.33 (2019-08-04 13:36:11.821023)\n",
            "Accuracy at step 5 - batch 48: 0.8988\n",
            "training loss at step 5 - batch 49: 0.34 (2019-08-04 13:36:11.833362)\n",
            "Accuracy at step 5 - batch 49: 0.8996\n",
            "training loss at step 5 - batch 50: 0.33 (2019-08-04 13:36:11.848396)\n",
            "Accuracy at step 5 - batch 50: 0.9032\n",
            "training loss at step 5 - batch 51: 0.35 (2019-08-04 13:36:11.966985)\n",
            "Accuracy at step 5 - batch 51: 0.9008\n",
            "training loss at step 5 - batch 52: 0.30 (2019-08-04 13:36:11.981915)\n",
            "Accuracy at step 5 - batch 52: 0.908\n",
            "training loss at step 5 - batch 53: 0.33 (2019-08-04 13:36:11.994244)\n",
            "Accuracy at step 5 - batch 53: 0.9024\n",
            "training loss at step 5 - batch 54: 0.33 (2019-08-04 13:36:12.006939)\n",
            "Accuracy at step 5 - batch 54: 0.8988\n",
            "training loss at step 5 - batch 55: 0.32 (2019-08-04 13:36:12.019385)\n",
            "Accuracy at step 5 - batch 55: 0.904\n",
            "training loss at step 5 - batch 56: 0.31 (2019-08-04 13:36:12.148248)\n",
            "Accuracy at step 5 - batch 56: 0.9068\n",
            "training loss at step 5 - batch 57: 0.32 (2019-08-04 13:36:12.162590)\n",
            "Accuracy at step 5 - batch 57: 0.906\n",
            "training loss at step 5 - batch 58: 0.33 (2019-08-04 13:36:12.176137)\n",
            "Accuracy at step 5 - batch 58: 0.9024\n",
            "training loss at step 5 - batch 59: 0.32 (2019-08-04 13:36:12.188459)\n",
            "Accuracy at step 5 - batch 59: 0.9072\n",
            "training loss at step 5 - batch 60: 0.34 (2019-08-04 13:36:12.201341)\n",
            "Accuracy at step 5 - batch 60: 0.896\n",
            "training loss at step 5 - batch 61: 0.33 (2019-08-04 13:36:12.325228)\n",
            "Accuracy at step 5 - batch 61: 0.9084\n",
            "training loss at step 5 - batch 62: 0.35 (2019-08-04 13:36:12.337889)\n",
            "Accuracy at step 5 - batch 62: 0.896\n",
            "training loss at step 5 - batch 63: 0.34 (2019-08-04 13:36:12.351575)\n",
            "Accuracy at step 5 - batch 63: 0.8976\n",
            "training loss at step 5 - batch 64: 0.35 (2019-08-04 13:36:12.367695)\n",
            "Accuracy at step 5 - batch 64: 0.894\n",
            "training loss at step 5 - batch 65: 0.33 (2019-08-04 13:36:12.379856)\n",
            "Accuracy at step 5 - batch 65: 0.8992\n",
            "training loss at step 5 - batch 66: 0.34 (2019-08-04 13:36:12.506485)\n",
            "Accuracy at step 5 - batch 66: 0.9004\n",
            "training loss at step 5 - batch 67: 0.33 (2019-08-04 13:36:12.519610)\n",
            "Accuracy at step 5 - batch 67: 0.8988\n",
            "training loss at step 5 - batch 68: 0.31 (2019-08-04 13:36:12.532367)\n",
            "Accuracy at step 5 - batch 68: 0.9024\n",
            "training loss at step 5 - batch 69: 0.32 (2019-08-04 13:36:12.546876)\n",
            "Accuracy at step 5 - batch 69: 0.9048\n",
            "training loss at step 5 - batch 70: 0.29 (2019-08-04 13:36:12.561921)\n",
            "Accuracy at step 5 - batch 70: 0.9132\n",
            "training loss at step 5 - batch 71: 0.29 (2019-08-04 13:36:12.685749)\n",
            "Accuracy at step 5 - batch 71: 0.908\n",
            "training loss at step 5 - batch 72: 0.34 (2019-08-04 13:36:12.699263)\n",
            "Accuracy at step 5 - batch 72: 0.8992\n",
            "training loss at step 5 - batch 73: 0.33 (2019-08-04 13:36:12.712337)\n",
            "Accuracy at step 5 - batch 73: 0.904\n",
            "training loss at step 5 - batch 74: 0.34 (2019-08-04 13:36:12.724127)\n",
            "Accuracy at step 5 - batch 74: 0.9012\n",
            "training loss at step 5 - batch 75: 0.32 (2019-08-04 13:36:12.737439)\n",
            "Accuracy at step 5 - batch 75: 0.908\n",
            "training loss at step 5 - batch 76: 0.34 (2019-08-04 13:36:12.864406)\n",
            "Accuracy at step 5 - batch 76: 0.904\n",
            "training loss at step 5 - batch 77: 0.33 (2019-08-04 13:36:12.876958)\n",
            "Accuracy at step 5 - batch 77: 0.9032\n",
            "training loss at step 5 - batch 78: 0.33 (2019-08-04 13:36:12.889896)\n",
            "Accuracy at step 5 - batch 78: 0.904\n",
            "training loss at step 5 - batch 79: 0.34 (2019-08-04 13:36:12.902976)\n",
            "Accuracy at step 5 - batch 79: 0.9012\n",
            "training loss at step 5 - batch 80: 0.33 (2019-08-04 13:36:12.915001)\n",
            "Accuracy at step 5 - batch 80: 0.9036\n",
            "training loss at step 5 - batch 81: 0.32 (2019-08-04 13:36:13.034468)\n",
            "Accuracy at step 5 - batch 81: 0.904\n",
            "training loss at step 5 - batch 82: 0.31 (2019-08-04 13:36:13.046927)\n",
            "Accuracy at step 5 - batch 82: 0.9084\n",
            "training loss at step 5 - batch 83: 0.34 (2019-08-04 13:36:13.058865)\n",
            "Accuracy at step 5 - batch 83: 0.8976\n",
            "training loss at step 5 - batch 84: 0.30 (2019-08-04 13:36:13.075190)\n",
            "Accuracy at step 5 - batch 84: 0.9084\n",
            "training loss at step 5 - batch 85: 0.33 (2019-08-04 13:36:13.088782)\n",
            "Accuracy at step 5 - batch 85: 0.9016\n",
            "training loss at step 5 - batch 86: 0.34 (2019-08-04 13:36:13.209656)\n",
            "Accuracy at step 5 - batch 86: 0.8976\n",
            "training loss at step 5 - batch 87: 0.32 (2019-08-04 13:36:13.225508)\n",
            "Accuracy at step 5 - batch 87: 0.908\n",
            "training loss at step 5 - batch 88: 0.34 (2019-08-04 13:36:13.238707)\n",
            "Accuracy at step 5 - batch 88: 0.9008\n",
            "training loss at step 5 - batch 89: 0.33 (2019-08-04 13:36:13.251239)\n",
            "Accuracy at step 5 - batch 89: 0.9028\n",
            "training loss at step 5 - batch 90: 0.30 (2019-08-04 13:36:13.264253)\n",
            "Accuracy at step 5 - batch 90: 0.9116\n",
            "training loss at step 5 - batch 91: 0.29 (2019-08-04 13:36:13.388529)\n",
            "Accuracy at step 5 - batch 91: 0.9112\n",
            "training loss at step 5 - batch 92: 0.33 (2019-08-04 13:36:13.401144)\n",
            "Accuracy at step 5 - batch 92: 0.9036\n",
            "training loss at step 5 - batch 93: 0.35 (2019-08-04 13:36:13.414177)\n",
            "Accuracy at step 5 - batch 93: 0.9\n",
            "training loss at step 5 - batch 94: 0.33 (2019-08-04 13:36:13.427406)\n",
            "Accuracy at step 5 - batch 94: 0.9044\n",
            "training loss at step 5 - batch 95: 0.34 (2019-08-04 13:36:13.440263)\n",
            "Accuracy at step 5 - batch 95: 0.8916\n",
            "training loss at step 5 - batch 96: 0.32 (2019-08-04 13:36:13.565900)\n",
            "Accuracy at step 5 - batch 96: 0.906\n",
            "training loss at step 5 - batch 97: 0.32 (2019-08-04 13:36:13.579261)\n",
            "Accuracy at step 5 - batch 97: 0.9028\n",
            "training loss at step 5 - batch 98: 0.31 (2019-08-04 13:36:13.594215)\n",
            "Accuracy at step 5 - batch 98: 0.9068\n",
            "training loss at step 5 - batch 99: 0.35 (2019-08-04 13:36:13.606561)\n",
            "Accuracy at step 5 - batch 99: 0.8976\n",
            "training loss at step 5 - batch 100: 0.34 (2019-08-04 13:36:13.618789)\n",
            "Accuracy at step 5 - batch 100: 0.8992\n",
            "training loss at step 5 - batch 101: 0.32 (2019-08-04 13:36:13.738060)\n",
            "Accuracy at step 5 - batch 101: 0.9052\n",
            "training loss at step 5 - batch 102: 0.32 (2019-08-04 13:36:13.751706)\n",
            "Accuracy at step 5 - batch 102: 0.9044\n",
            "training loss at step 5 - batch 103: 0.34 (2019-08-04 13:36:13.764423)\n",
            "Accuracy at step 5 - batch 103: 0.9\n",
            "training loss at step 5 - batch 104: 0.31 (2019-08-04 13:36:13.777053)\n",
            "Accuracy at step 5 - batch 104: 0.908\n",
            "training loss at step 5 - batch 105: 0.32 (2019-08-04 13:36:13.788776)\n",
            "Accuracy at step 5 - batch 105: 0.9056\n",
            "training loss at step 5 - batch 106: 0.32 (2019-08-04 13:36:13.915973)\n",
            "Accuracy at step 5 - batch 106: 0.9028\n",
            "training loss at step 5 - batch 107: 0.32 (2019-08-04 13:36:13.934164)\n",
            "Accuracy at step 5 - batch 107: 0.9036\n",
            "training loss at step 5 - batch 108: 0.32 (2019-08-04 13:36:13.947080)\n",
            "Accuracy at step 5 - batch 108: 0.9024\n",
            "training loss at step 5 - batch 109: 0.35 (2019-08-04 13:36:13.959596)\n",
            "Accuracy at step 5 - batch 109: 0.8964\n",
            "training loss at step 5 - batch 110: 0.33 (2019-08-04 13:36:13.973054)\n",
            "Accuracy at step 5 - batch 110: 0.9036\n",
            "training loss at step 5 - batch 111: 0.31 (2019-08-04 13:36:14.096518)\n",
            "Accuracy at step 5 - batch 111: 0.908\n",
            "training loss at step 5 - batch 112: 0.33 (2019-08-04 13:36:14.108957)\n",
            "Accuracy at step 5 - batch 112: 0.9024\n",
            "training loss at step 5 - batch 113: 0.33 (2019-08-04 13:36:14.124298)\n",
            "Accuracy at step 5 - batch 113: 0.8972\n",
            "training loss at step 5 - batch 114: 0.32 (2019-08-04 13:36:14.138927)\n",
            "Accuracy at step 5 - batch 114: 0.9044\n",
            "training loss at step 5 - batch 115: 0.32 (2019-08-04 13:36:14.151164)\n",
            "Accuracy at step 5 - batch 115: 0.9056\n",
            "training loss at step 5 - batch 116: 0.33 (2019-08-04 13:36:14.270318)\n",
            "Accuracy at step 5 - batch 116: 0.906\n",
            "training loss at step 5 - batch 117: 0.32 (2019-08-04 13:36:14.287014)\n",
            "Accuracy at step 5 - batch 117: 0.9072\n",
            "training loss at step 5 - batch 118: 0.32 (2019-08-04 13:36:14.300841)\n",
            "Accuracy at step 5 - batch 118: 0.9076\n",
            "training loss at step 5 - batch 119: 0.32 (2019-08-04 13:36:14.314418)\n",
            "Accuracy at step 5 - batch 119: 0.9072\n",
            "training loss at step 5 - batch 120: 0.32 (2019-08-04 13:36:14.330990)\n",
            "Accuracy at step 5 - batch 120: 0.9008\n",
            "training loss at step 5 - batch 121: 0.32 (2019-08-04 13:36:14.448859)\n",
            "Accuracy at step 5 - batch 121: 0.906\n",
            "training loss at step 5 - batch 122: 0.32 (2019-08-04 13:36:14.465579)\n",
            "Accuracy at step 5 - batch 122: 0.9044\n",
            "training loss at step 5 - batch 123: 0.34 (2019-08-04 13:36:14.482449)\n",
            "Accuracy at step 5 - batch 123: 0.8984\n",
            "training loss at step 5 - batch 124: 0.32 (2019-08-04 13:36:14.497637)\n",
            "Accuracy at step 5 - batch 124: 0.9052\n",
            "training loss at step 5 - batch 125: 0.31 (2019-08-04 13:36:14.510409)\n",
            "Accuracy at step 5 - batch 125: 0.908\n",
            "training loss at step 5 - batch 126: 0.32 (2019-08-04 13:36:14.635506)\n",
            "Accuracy at step 5 - batch 126: 0.9056\n",
            "training loss at step 5 - batch 127: 0.33 (2019-08-04 13:36:14.651796)\n",
            "Accuracy at step 5 - batch 127: 0.906\n",
            "training loss at step 5 - batch 128: 0.34 (2019-08-04 13:36:14.664411)\n",
            "Accuracy at step 5 - batch 128: 0.896\n",
            "training loss at step 5 - batch 129: 0.32 (2019-08-04 13:36:14.676266)\n",
            "Accuracy at step 5 - batch 129: 0.8932\n",
            "training loss at step 5 - batch 130: 0.31 (2019-08-04 13:36:14.689230)\n",
            "Accuracy at step 5 - batch 130: 0.9116\n",
            "training loss at step 5 - batch 131: 0.33 (2019-08-04 13:36:14.812917)\n",
            "Accuracy at step 5 - batch 131: 0.9064\n",
            "training loss at step 5 - batch 132: 0.34 (2019-08-04 13:36:14.826978)\n",
            "Accuracy at step 5 - batch 132: 0.896\n",
            "training loss at step 5 - batch 133: 0.33 (2019-08-04 13:36:14.843784)\n",
            "Accuracy at step 5 - batch 133: 0.9052\n",
            "training loss at step 5 - batch 134: 0.33 (2019-08-04 13:36:14.857741)\n",
            "Accuracy at step 5 - batch 134: 0.904\n",
            "training loss at step 5 - batch 135: 0.34 (2019-08-04 13:36:14.869711)\n",
            "Accuracy at step 5 - batch 135: 0.8992\n",
            "training loss at step 5 - batch 136: 0.33 (2019-08-04 13:36:14.987778)\n",
            "Accuracy at step 5 - batch 136: 0.8992\n",
            "training loss at step 5 - batch 137: 0.35 (2019-08-04 13:36:15.003729)\n",
            "Accuracy at step 5 - batch 137: 0.8988\n",
            "training loss at step 5 - batch 138: 0.32 (2019-08-04 13:36:15.016316)\n",
            "Accuracy at step 5 - batch 138: 0.9036\n",
            "training loss at step 5 - batch 139: 0.32 (2019-08-04 13:36:15.029528)\n",
            "Accuracy at step 5 - batch 139: 0.9064\n",
            "training loss at step 5 - batch 140: 0.34 (2019-08-04 13:36:15.041234)\n",
            "Accuracy at step 5 - batch 140: 0.9004\n",
            "training loss at step 5 - batch 141: 0.33 (2019-08-04 13:36:15.165965)\n",
            "Accuracy at step 5 - batch 141: 0.8984\n",
            "training loss at step 5 - batch 142: 0.32 (2019-08-04 13:36:15.178261)\n",
            "Accuracy at step 5 - batch 142: 0.902\n",
            "training loss at step 5 - batch 143: 0.32 (2019-08-04 13:36:15.191867)\n",
            "Accuracy at step 5 - batch 143: 0.9072\n",
            "training loss at step 5 - batch 144: 0.34 (2019-08-04 13:36:15.204657)\n",
            "Accuracy at step 5 - batch 144: 0.898\n",
            "training loss at step 5 - batch 145: 0.31 (2019-08-04 13:36:15.218521)\n",
            "Accuracy at step 5 - batch 145: 0.904\n",
            "training loss at step 5 - batch 146: 0.32 (2019-08-04 13:36:15.338593)\n",
            "Accuracy at step 5 - batch 146: 0.9028\n",
            "training loss at step 5 - batch 147: 0.33 (2019-08-04 13:36:15.355019)\n",
            "Accuracy at step 5 - batch 147: 0.9072\n",
            "training loss at step 5 - batch 148: 0.31 (2019-08-04 13:36:15.370624)\n",
            "Accuracy at step 5 - batch 148: 0.9076\n",
            "training loss at step 5 - batch 149: 0.31 (2019-08-04 13:36:15.384308)\n",
            "Accuracy at step 5 - batch 149: 0.9048\n",
            "training loss at step 5 - batch 150: 0.33 (2019-08-04 13:36:15.397074)\n",
            "Accuracy at step 5 - batch 150: 0.906\n",
            "training loss at step 5 - batch 151: 0.33 (2019-08-04 13:36:15.518131)\n",
            "Accuracy at step 5 - batch 151: 0.902\n",
            "training loss at step 5 - batch 152: 0.35 (2019-08-04 13:36:15.532937)\n",
            "Accuracy at step 5 - batch 152: 0.9032\n",
            "training loss at step 5 - batch 153: 0.34 (2019-08-04 13:36:15.546022)\n",
            "Accuracy at step 5 - batch 153: 0.898\n",
            "training loss at step 5 - batch 154: 0.31 (2019-08-04 13:36:15.557790)\n",
            "Accuracy at step 5 - batch 154: 0.9096\n",
            "training loss at step 5 - batch 155: 0.32 (2019-08-04 13:36:15.570247)\n",
            "Accuracy at step 5 - batch 155: 0.9052\n",
            "training loss at step 5 - batch 156: 0.33 (2019-08-04 13:36:15.703979)\n",
            "Accuracy at step 5 - batch 156: 0.904\n",
            "training loss at step 5 - batch 157: 0.32 (2019-08-04 13:36:15.717443)\n",
            "Accuracy at step 5 - batch 157: 0.91\n",
            "training loss at step 5 - batch 158: 0.32 (2019-08-04 13:36:15.729749)\n",
            "Accuracy at step 5 - batch 158: 0.9048\n",
            "training loss at step 5 - batch 159: 0.33 (2019-08-04 13:36:15.742449)\n",
            "Accuracy at step 5 - batch 159: 0.898\n",
            "training loss at step 5 - batch 160: 0.32 (2019-08-04 13:36:15.754656)\n",
            "Accuracy at step 5 - batch 160: 0.9068\n",
            "training loss at step 5 - batch 161: 0.35 (2019-08-04 13:36:15.881392)\n",
            "Accuracy at step 5 - batch 161: 0.9\n",
            "training loss at step 5 - batch 162: 0.31 (2019-08-04 13:36:15.898599)\n",
            "Accuracy at step 5 - batch 162: 0.904\n",
            "training loss at step 5 - batch 163: 0.32 (2019-08-04 13:36:15.914915)\n",
            "Accuracy at step 5 - batch 163: 0.906\n",
            "training loss at step 5 - batch 164: 0.32 (2019-08-04 13:36:15.931299)\n",
            "Accuracy at step 5 - batch 164: 0.9076\n",
            "training loss at step 5 - batch 165: 0.35 (2019-08-04 13:36:15.943562)\n",
            "Accuracy at step 5 - batch 165: 0.8956\n",
            "training loss at step 5 - batch 166: 0.32 (2019-08-04 13:36:16.073817)\n",
            "Accuracy at step 5 - batch 166: 0.9076\n",
            "training loss at step 5 - batch 167: 0.35 (2019-08-04 13:36:16.087216)\n",
            "Accuracy at step 5 - batch 167: 0.8928\n",
            "training loss at step 5 - batch 168: 0.33 (2019-08-04 13:36:16.100331)\n",
            "Accuracy at step 5 - batch 168: 0.9036\n",
            "training loss at step 5 - batch 169: 0.32 (2019-08-04 13:36:16.113317)\n",
            "Accuracy at step 5 - batch 169: 0.9024\n",
            "training loss at step 5 - batch 170: 0.33 (2019-08-04 13:36:16.129004)\n",
            "Accuracy at step 5 - batch 170: 0.9032\n",
            "training loss at step 5 - batch 171: 0.33 (2019-08-04 13:36:16.250430)\n",
            "Accuracy at step 5 - batch 171: 0.9016\n",
            "training loss at step 5 - batch 172: 0.32 (2019-08-04 13:36:16.264750)\n",
            "Accuracy at step 5 - batch 172: 0.9036\n",
            "training loss at step 5 - batch 173: 0.31 (2019-08-04 13:36:16.277439)\n",
            "Accuracy at step 5 - batch 173: 0.9128\n",
            "training loss at step 5 - batch 174: 0.33 (2019-08-04 13:36:16.291186)\n",
            "Accuracy at step 5 - batch 174: 0.8988\n",
            "training loss at step 5 - batch 175: 0.32 (2019-08-04 13:36:16.305178)\n",
            "Accuracy at step 5 - batch 175: 0.9032\n",
            "training loss at step 5 - batch 176: 0.33 (2019-08-04 13:36:16.429185)\n",
            "Accuracy at step 5 - batch 176: 0.9056\n",
            "training loss at step 5 - batch 177: 0.33 (2019-08-04 13:36:16.442275)\n",
            "Accuracy at step 5 - batch 177: 0.9016\n",
            "training loss at step 5 - batch 178: 0.32 (2019-08-04 13:36:16.455649)\n",
            "Accuracy at step 5 - batch 178: 0.8992\n",
            "training loss at step 5 - batch 179: 0.31 (2019-08-04 13:36:16.469944)\n",
            "Accuracy at step 5 - batch 179: 0.9052\n",
            "training loss at step 5 - batch 180: 0.32 (2019-08-04 13:36:16.483549)\n",
            "Accuracy at step 5 - batch 180: 0.9028\n",
            "training loss at step 5 - batch 181: 0.33 (2019-08-04 13:36:16.609865)\n",
            "Accuracy at step 5 - batch 181: 0.9008\n",
            "training loss at step 5 - batch 182: 0.33 (2019-08-04 13:36:16.623710)\n",
            "Accuracy at step 5 - batch 182: 0.9052\n",
            "training loss at step 5 - batch 183: 0.33 (2019-08-04 13:36:16.638487)\n",
            "Accuracy at step 5 - batch 183: 0.906\n",
            "training loss at step 5 - batch 184: 0.32 (2019-08-04 13:36:16.651213)\n",
            "Accuracy at step 5 - batch 184: 0.9044\n",
            "training loss at step 5 - batch 185: 0.31 (2019-08-04 13:36:16.663469)\n",
            "Accuracy at step 5 - batch 185: 0.908\n",
            "training loss at step 5 - batch 186: 0.31 (2019-08-04 13:36:16.782235)\n",
            "Accuracy at step 5 - batch 186: 0.904\n",
            "training loss at step 5 - batch 187: 0.34 (2019-08-04 13:36:16.795826)\n",
            "Accuracy at step 5 - batch 187: 0.894\n",
            "training loss at step 5 - batch 188: 0.33 (2019-08-04 13:36:16.808480)\n",
            "Accuracy at step 5 - batch 188: 0.9056\n",
            "training loss at step 5 - batch 189: 0.34 (2019-08-04 13:36:16.821701)\n",
            "Accuracy at step 5 - batch 189: 0.8976\n",
            "training loss at step 5 - batch 190: 0.34 (2019-08-04 13:36:16.835272)\n",
            "Accuracy at step 5 - batch 190: 0.9024\n",
            "training loss at step 5 - batch 191: 0.32 (2019-08-04 13:36:16.959484)\n",
            "Accuracy at step 5 - batch 191: 0.9012\n",
            "training loss at step 5 - batch 192: 0.32 (2019-08-04 13:36:16.975532)\n",
            "Accuracy at step 5 - batch 192: 0.9044\n",
            "training loss at step 5 - batch 193: 0.34 (2019-08-04 13:36:16.989205)\n",
            "Accuracy at step 5 - batch 193: 0.9028\n",
            "training loss at step 5 - batch 194: 0.33 (2019-08-04 13:36:17.002389)\n",
            "Accuracy at step 5 - batch 194: 0.9004\n",
            "training loss at step 5 - batch 195: 0.33 (2019-08-04 13:36:17.015324)\n",
            "Accuracy at step 5 - batch 195: 0.9024\n",
            "training loss at step 5 - batch 196: 0.31 (2019-08-04 13:36:17.139757)\n",
            "Accuracy at step 5 - batch 196: 0.912\n",
            "training loss at step 5 - batch 197: 0.32 (2019-08-04 13:36:17.156957)\n",
            "Accuracy at step 5 - batch 197: 0.9056\n",
            "training loss at step 5 - batch 198: 0.30 (2019-08-04 13:36:17.172876)\n",
            "Accuracy at step 5 - batch 198: 0.9056\n",
            "training loss at step 5 - batch 199: 0.35 (2019-08-04 13:36:17.185787)\n",
            "Accuracy at step 5 - batch 199: 0.8984\n",
            "training loss at step 5 - batch 200: 0.33 (2019-08-04 13:36:17.198327)\n",
            "Accuracy at step 5 - batch 200: 0.9012\n",
            "training loss at step 5 - batch 201: 0.34 (2019-08-04 13:36:17.318493)\n",
            "Accuracy at step 5 - batch 201: 0.8992\n",
            "training loss at step 5 - batch 202: 0.34 (2019-08-04 13:36:17.334628)\n",
            "Accuracy at step 5 - batch 202: 0.904\n",
            "training loss at step 5 - batch 203: 0.33 (2019-08-04 13:36:17.347046)\n",
            "Accuracy at step 5 - batch 203: 0.904\n",
            "training loss at step 5 - batch 204: 0.34 (2019-08-04 13:36:17.360299)\n",
            "Accuracy at step 5 - batch 204: 0.8956\n",
            "training loss at step 5 - batch 205: 0.33 (2019-08-04 13:36:17.373369)\n",
            "Accuracy at step 5 - batch 205: 0.8988\n",
            "training loss at step 5 - batch 206: 0.32 (2019-08-04 13:36:17.494061)\n",
            "Accuracy at step 5 - batch 206: 0.9056\n",
            "training loss at step 5 - batch 207: 0.31 (2019-08-04 13:36:17.511516)\n",
            "Accuracy at step 5 - batch 207: 0.9052\n",
            "training loss at step 5 - batch 208: 0.31 (2019-08-04 13:36:17.523879)\n",
            "Accuracy at step 5 - batch 208: 0.9088\n",
            "training loss at step 5 - batch 209: 0.33 (2019-08-04 13:36:17.540047)\n",
            "Accuracy at step 5 - batch 209: 0.9004\n",
            "training loss at step 5 - batch 210: 0.34 (2019-08-04 13:36:17.556787)\n",
            "Accuracy at step 5 - batch 210: 0.8972\n",
            "training loss at step 5 - batch 211: 0.30 (2019-08-04 13:36:17.685861)\n",
            "Accuracy at step 5 - batch 211: 0.9124\n",
            "training loss at step 5 - batch 212: 0.32 (2019-08-04 13:36:17.698007)\n",
            "Accuracy at step 5 - batch 212: 0.8968\n",
            "training loss at step 5 - batch 213: 0.30 (2019-08-04 13:36:17.711272)\n",
            "Accuracy at step 5 - batch 213: 0.9136\n",
            "training loss at step 5 - batch 214: 0.32 (2019-08-04 13:36:17.723757)\n",
            "Accuracy at step 5 - batch 214: 0.9044\n",
            "training loss at step 5 - batch 215: 0.33 (2019-08-04 13:36:17.736289)\n",
            "Accuracy at step 5 - batch 215: 0.904\n",
            "training loss at step 5 - batch 216: 0.30 (2019-08-04 13:36:17.855503)\n",
            "Accuracy at step 5 - batch 216: 0.9112\n",
            "training loss at step 5 - batch 217: 0.32 (2019-08-04 13:36:17.872456)\n",
            "Accuracy at step 5 - batch 217: 0.908\n",
            "training loss at step 5 - batch 218: 0.33 (2019-08-04 13:36:17.884671)\n",
            "Accuracy at step 5 - batch 218: 0.9032\n",
            "training loss at step 5 - batch 219: 0.31 (2019-08-04 13:36:17.899733)\n",
            "Accuracy at step 5 - batch 219: 0.9116\n",
            "training loss at step 5 - batch 220: 0.32 (2019-08-04 13:36:17.913408)\n",
            "Accuracy at step 5 - batch 220: 0.9\n",
            "training loss at step 5 - batch 221: 0.31 (2019-08-04 13:36:18.034658)\n",
            "Accuracy at step 5 - batch 221: 0.9052\n",
            "training loss at step 5 - batch 222: 0.34 (2019-08-04 13:36:18.051717)\n",
            "Accuracy at step 5 - batch 222: 0.9016\n",
            "training loss at step 5 - batch 223: 0.32 (2019-08-04 13:36:18.064441)\n",
            "Accuracy at step 5 - batch 223: 0.902\n",
            "training loss at step 5 - batch 224: 0.33 (2019-08-04 13:36:18.077183)\n",
            "Accuracy at step 5 - batch 224: 0.8996\n",
            "training loss at step 5 - batch 225: 0.35 (2019-08-04 13:36:18.090315)\n",
            "Accuracy at step 5 - batch 225: 0.8984\n",
            "training loss at step 5 - batch 226: 0.31 (2019-08-04 13:36:18.215107)\n",
            "Accuracy at step 5 - batch 226: 0.9052\n",
            "training loss at step 5 - batch 227: 0.34 (2019-08-04 13:36:18.229151)\n",
            "Accuracy at step 5 - batch 227: 0.9032\n",
            "training loss at step 5 - batch 228: 0.33 (2019-08-04 13:36:18.242930)\n",
            "Accuracy at step 5 - batch 228: 0.8984\n",
            "training loss at step 5 - batch 229: 0.34 (2019-08-04 13:36:18.256292)\n",
            "Accuracy at step 5 - batch 229: 0.9012\n",
            "training loss at step 5 - batch 230: 0.35 (2019-08-04 13:36:18.271539)\n",
            "Accuracy at step 5 - batch 230: 0.8992\n",
            "training loss at step 5 - batch 231: 0.32 (2019-08-04 13:36:18.397857)\n",
            "Accuracy at step 5 - batch 231: 0.9064\n",
            "training loss at step 5 - batch 232: 0.32 (2019-08-04 13:36:18.414720)\n",
            "Accuracy at step 5 - batch 232: 0.9064\n",
            "training loss at step 5 - batch 233: 0.33 (2019-08-04 13:36:18.430679)\n",
            "Accuracy at step 5 - batch 233: 0.9072\n",
            "training loss at step 5 - batch 234: 0.34 (2019-08-04 13:36:18.443227)\n",
            "Accuracy at step 5 - batch 234: 0.9028\n",
            "training loss at step 5 - batch 235: 0.33 (2019-08-04 13:36:18.456827)\n",
            "Accuracy at step 5 - batch 235: 0.9108\n",
            "training loss at step 5 - batch 236: 0.32 (2019-08-04 13:36:18.577316)\n",
            "Accuracy at step 5 - batch 236: 0.9072\n",
            "training loss at step 5 - batch 237: 0.32 (2019-08-04 13:36:18.590948)\n",
            "Accuracy at step 5 - batch 237: 0.9056\n",
            "training loss at step 5 - batch 238: 0.33 (2019-08-04 13:36:18.604245)\n",
            "Accuracy at step 5 - batch 238: 0.8996\n",
            "training loss at step 5 - batch 239: 0.32 (2019-08-04 13:36:18.617181)\n",
            "Accuracy at step 5 - batch 239: 0.9052\n",
            "training loss at step 5 - batch 240: 0.30 (2019-08-04 13:36:18.630008)\n",
            "Accuracy at step 5 - batch 240: 0.9088\n",
            "training loss at step 5 - batch 241: 0.31 (2019-08-04 13:36:18.753897)\n",
            "Accuracy at step 5 - batch 241: 0.9084\n",
            "training loss at step 5 - batch 242: 0.31 (2019-08-04 13:36:18.768778)\n",
            "Accuracy at step 5 - batch 242: 0.906\n",
            "training loss at step 5 - batch 243: 0.33 (2019-08-04 13:36:18.781677)\n",
            "Accuracy at step 5 - batch 243: 0.908\n",
            "training loss at step 5 - batch 244: 0.31 (2019-08-04 13:36:18.795219)\n",
            "Accuracy at step 5 - batch 244: 0.9088\n",
            "training loss at step 5 - batch 245: 0.32 (2019-08-04 13:36:18.807170)\n",
            "Accuracy at step 5 - batch 245: 0.9032\n",
            "training loss at step 5 - batch 246: 0.30 (2019-08-04 13:36:18.930267)\n",
            "Accuracy at step 5 - batch 246: 0.9144\n",
            "training loss at step 5 - batch 247: 0.35 (2019-08-04 13:36:18.946111)\n",
            "Accuracy at step 5 - batch 247: 0.8992\n",
            "training loss at step 5 - batch 248: 0.32 (2019-08-04 13:36:18.962388)\n",
            "Accuracy at step 5 - batch 248: 0.9068\n",
            "training loss at step 5 - batch 249: 0.35 (2019-08-04 13:36:18.976383)\n",
            "Accuracy at step 5 - batch 249: 0.9008\n",
            "training loss at step 5 - batch 250: 0.34 (2019-08-04 13:36:18.989426)\n",
            "Accuracy at step 5 - batch 250: 0.902\n",
            "training loss at step 5 - batch 251: 0.31 (2019-08-04 13:36:19.112315)\n",
            "Accuracy at step 5 - batch 251: 0.904\n",
            "training loss at step 5 - batch 252: 0.31 (2019-08-04 13:36:19.129724)\n",
            "Accuracy at step 5 - batch 252: 0.9104\n",
            "training loss at step 5 - batch 253: 0.33 (2019-08-04 13:36:19.142333)\n",
            "Accuracy at step 5 - batch 253: 0.9012\n",
            "training loss at step 5 - batch 254: 0.33 (2019-08-04 13:36:19.156095)\n",
            "Accuracy at step 5 - batch 254: 0.906\n",
            "training loss at step 5 - batch 255: 0.33 (2019-08-04 13:36:19.170432)\n",
            "Accuracy at step 5 - batch 255: 0.9016\n",
            "training loss at step 5 - batch 256: 0.31 (2019-08-04 13:36:19.292425)\n",
            "Accuracy at step 5 - batch 256: 0.9096\n",
            "training loss at step 5 - batch 257: 0.31 (2019-08-04 13:36:19.312977)\n",
            "Accuracy at step 5 - batch 257: 0.9124\n",
            "training loss at step 5 - batch 258: 0.33 (2019-08-04 13:36:19.332090)\n",
            "Accuracy at step 5 - batch 258: 0.9068\n",
            "training loss at step 5 - batch 259: 0.31 (2019-08-04 13:36:19.345540)\n",
            "Accuracy at step 5 - batch 259: 0.91\n",
            "training loss at step 5 - batch 260: 0.32 (2019-08-04 13:36:19.358540)\n",
            "Accuracy at step 5 - batch 260: 0.9032\n",
            "training loss at step 5 - batch 261: 0.33 (2019-08-04 13:36:19.490199)\n",
            "Accuracy at step 5 - batch 261: 0.9048\n",
            "training loss at step 5 - batch 262: 0.32 (2019-08-04 13:36:19.506393)\n",
            "Accuracy at step 5 - batch 262: 0.904\n",
            "training loss at step 5 - batch 263: 0.33 (2019-08-04 13:36:19.519012)\n",
            "Accuracy at step 5 - batch 263: 0.9052\n",
            "training loss at step 5 - batch 264: 0.32 (2019-08-04 13:36:19.533310)\n",
            "Accuracy at step 5 - batch 264: 0.9036\n",
            "training loss at step 5 - batch 265: 0.35 (2019-08-04 13:36:19.545905)\n",
            "Accuracy at step 5 - batch 265: 0.8968\n",
            "training loss at step 5 - batch 266: 0.33 (2019-08-04 13:36:19.667792)\n",
            "Accuracy at step 5 - batch 266: 0.9004\n",
            "training loss at step 5 - batch 267: 0.29 (2019-08-04 13:36:19.680363)\n",
            "Accuracy at step 5 - batch 267: 0.9124\n",
            "training loss at step 5 - batch 268: 0.32 (2019-08-04 13:36:19.695389)\n",
            "Accuracy at step 5 - batch 268: 0.9056\n",
            "training loss at step 5 - batch 269: 0.35 (2019-08-04 13:36:19.708701)\n",
            "Accuracy at step 5 - batch 269: 0.8992\n",
            "training loss at step 5 - batch 270: 0.32 (2019-08-04 13:36:19.720874)\n",
            "Accuracy at step 5 - batch 270: 0.9052\n",
            "training loss at step 5 - batch 271: 0.33 (2019-08-04 13:36:19.837682)\n",
            "Accuracy at step 5 - batch 271: 0.9\n",
            "training loss at step 5 - batch 272: 0.32 (2019-08-04 13:36:19.852365)\n",
            "Accuracy at step 5 - batch 272: 0.9044\n",
            "training loss at step 5 - batch 273: 0.33 (2019-08-04 13:36:19.865482)\n",
            "Accuracy at step 5 - batch 273: 0.9056\n",
            "training loss at step 5 - batch 274: 0.32 (2019-08-04 13:36:19.877497)\n",
            "Accuracy at step 5 - batch 274: 0.904\n",
            "training loss at step 5 - batch 275: 0.32 (2019-08-04 13:36:19.890010)\n",
            "Accuracy at step 5 - batch 275: 0.9048\n",
            "training loss at step 5 - batch 276: 0.33 (2019-08-04 13:36:20.017582)\n",
            "Accuracy at step 5 - batch 276: 0.9084\n",
            "training loss at step 5 - batch 277: 0.31 (2019-08-04 13:36:20.032796)\n",
            "Accuracy at step 5 - batch 277: 0.908\n",
            "training loss at step 5 - batch 278: 0.32 (2019-08-04 13:36:20.045631)\n",
            "Accuracy at step 5 - batch 278: 0.9048\n",
            "training loss at step 5 - batch 279: 0.33 (2019-08-04 13:36:20.059318)\n",
            "Accuracy at step 5 - batch 279: 0.902\n",
            "training loss at step 5 - batch 280: 0.34 (2019-08-04 13:36:20.072464)\n",
            "Accuracy at step 5 - batch 280: 0.8976\n",
            "training loss at step 5 - batch 281: 0.33 (2019-08-04 13:36:20.194130)\n",
            "Accuracy at step 5 - batch 281: 0.9\n",
            "training loss at step 5 - batch 282: 0.33 (2019-08-04 13:36:20.208582)\n",
            "Accuracy at step 5 - batch 282: 0.9012\n",
            "training loss at step 5 - batch 283: 0.33 (2019-08-04 13:36:20.223694)\n",
            "Accuracy at step 5 - batch 283: 0.9072\n",
            "training loss at step 5 - batch 284: 0.30 (2019-08-04 13:36:20.238161)\n",
            "Accuracy at step 5 - batch 284: 0.9132\n",
            "training loss at step 5 - batch 285: 0.31 (2019-08-04 13:36:20.250092)\n",
            "Accuracy at step 5 - batch 285: 0.9064\n",
            "training loss at step 5 - batch 286: 0.30 (2019-08-04 13:36:20.367840)\n",
            "Accuracy at step 5 - batch 286: 0.9052\n",
            "training loss at step 5 - batch 287: 0.34 (2019-08-04 13:36:20.380977)\n",
            "Accuracy at step 5 - batch 287: 0.9016\n",
            "training loss at step 5 - batch 288: 0.33 (2019-08-04 13:36:20.394819)\n",
            "Accuracy at step 5 - batch 288: 0.9028\n",
            "training loss at step 5 - batch 289: 0.34 (2019-08-04 13:36:20.406944)\n",
            "Accuracy at step 5 - batch 289: 0.8932\n",
            "training loss at step 5 - batch 290: 0.32 (2019-08-04 13:36:20.421548)\n",
            "Accuracy at step 5 - batch 290: 0.9056\n",
            "training loss at step 5 - batch 291: 0.30 (2019-08-04 13:36:20.543653)\n",
            "Accuracy at step 5 - batch 291: 0.9092\n",
            "training loss at step 5 - batch 292: 0.31 (2019-08-04 13:36:20.556675)\n",
            "Accuracy at step 5 - batch 292: 0.9084\n",
            "training loss at step 5 - batch 293: 0.33 (2019-08-04 13:36:20.569447)\n",
            "Accuracy at step 5 - batch 293: 0.904\n",
            "training loss at step 5 - batch 294: 0.34 (2019-08-04 13:36:20.582163)\n",
            "Accuracy at step 5 - batch 294: 0.9012\n",
            "training loss at step 5 - batch 295: 0.31 (2019-08-04 13:36:20.595891)\n",
            "Accuracy at step 5 - batch 295: 0.908\n",
            "training loss at step 5 - batch 296: 0.34 (2019-08-04 13:36:20.718143)\n",
            "Accuracy at step 5 - batch 296: 0.8996\n",
            "training loss at step 5 - batch 297: 0.33 (2019-08-04 13:36:20.731255)\n",
            "Accuracy at step 5 - batch 297: 0.9008\n",
            "training loss at step 5 - batch 298: 0.32 (2019-08-04 13:36:20.746039)\n",
            "Accuracy at step 5 - batch 298: 0.9112\n",
            "training loss at step 5 - batch 299: 0.32 (2019-08-04 13:36:20.762077)\n",
            "Accuracy at step 5 - batch 299: 0.91\n",
            "training loss at step 5 - batch 300: 0.31 (2019-08-04 13:36:20.774454)\n",
            "Accuracy at step 5 - batch 300: 0.906\n",
            "training loss at step 5 - batch 301: 0.34 (2019-08-04 13:36:20.900170)\n",
            "Accuracy at step 5 - batch 301: 0.8968\n",
            "training loss at step 5 - batch 302: 0.33 (2019-08-04 13:36:20.917631)\n",
            "Accuracy at step 5 - batch 302: 0.8988\n",
            "training loss at step 5 - batch 303: 0.31 (2019-08-04 13:36:20.930212)\n",
            "Accuracy at step 5 - batch 303: 0.9064\n",
            "training loss at step 5 - batch 304: 0.34 (2019-08-04 13:36:20.944280)\n",
            "Accuracy at step 5 - batch 304: 0.9\n",
            "training loss at step 5 - batch 305: 0.32 (2019-08-04 13:36:20.960664)\n",
            "Accuracy at step 5 - batch 305: 0.9\n",
            "training loss at step 5 - batch 306: 0.32 (2019-08-04 13:36:21.086011)\n",
            "Accuracy at step 5 - batch 306: 0.908\n",
            "training loss at step 5 - batch 307: 0.31 (2019-08-04 13:36:21.098878)\n",
            "Accuracy at step 5 - batch 307: 0.908\n",
            "training loss at step 5 - batch 308: 0.31 (2019-08-04 13:36:21.111275)\n",
            "Accuracy at step 5 - batch 308: 0.9104\n",
            "training loss at step 5 - batch 309: 0.32 (2019-08-04 13:36:21.124332)\n",
            "Accuracy at step 5 - batch 309: 0.9064\n",
            "training loss at step 5 - batch 310: 0.30 (2019-08-04 13:36:21.137256)\n",
            "Accuracy at step 5 - batch 310: 0.9068\n",
            "training loss at step 5 - batch 311: 0.32 (2019-08-04 13:36:21.263938)\n",
            "Accuracy at step 5 - batch 311: 0.906\n",
            "training loss at step 5 - batch 312: 0.31 (2019-08-04 13:36:21.281902)\n",
            "Accuracy at step 5 - batch 312: 0.9136\n",
            "training loss at step 5 - batch 313: 0.30 (2019-08-04 13:36:21.293916)\n",
            "Accuracy at step 5 - batch 313: 0.9096\n",
            "training loss at step 5 - batch 314: 0.33 (2019-08-04 13:36:21.307357)\n",
            "Accuracy at step 5 - batch 314: 0.8976\n",
            "training loss at step 5 - batch 315: 0.32 (2019-08-04 13:36:21.319696)\n",
            "Accuracy at step 5 - batch 315: 0.8992\n",
            "training loss at step 5 - batch 316: 0.33 (2019-08-04 13:36:21.436717)\n",
            "Accuracy at step 5 - batch 316: 0.9008\n",
            "training loss at step 5 - batch 317: 0.33 (2019-08-04 13:36:21.453669)\n",
            "Accuracy at step 5 - batch 317: 0.8928\n",
            "training loss at step 5 - batch 318: 0.32 (2019-08-04 13:36:21.468854)\n",
            "Accuracy at step 5 - batch 318: 0.9028\n",
            "training loss at step 5 - batch 319: 0.35 (2019-08-04 13:36:21.484152)\n",
            "Accuracy at step 5 - batch 319: 0.8988\n",
            "training loss at step 5 - batch 320: 0.31 (2019-08-04 13:36:21.496136)\n",
            "Accuracy at step 5 - batch 320: 0.9104\n",
            "training loss at step 5 - batch 321: 0.34 (2019-08-04 13:36:21.617002)\n",
            "Accuracy at step 5 - batch 321: 0.8988\n",
            "training loss at step 5 - batch 322: 0.34 (2019-08-04 13:36:21.637733)\n",
            "Accuracy at step 5 - batch 322: 0.9052\n",
            "training loss at step 5 - batch 323: 0.34 (2019-08-04 13:36:21.650128)\n",
            "Accuracy at step 5 - batch 323: 0.9012\n",
            "training loss at step 5 - batch 324: 0.32 (2019-08-04 13:36:21.663018)\n",
            "Accuracy at step 5 - batch 324: 0.8992\n",
            "training loss at step 5 - batch 325: 0.31 (2019-08-04 13:36:21.677535)\n",
            "Accuracy at step 5 - batch 325: 0.9044\n",
            "training loss at step 5 - batch 326: 0.33 (2019-08-04 13:36:21.800124)\n",
            "Accuracy at step 5 - batch 326: 0.8992\n",
            "training loss at step 5 - batch 327: 0.33 (2019-08-04 13:36:21.813871)\n",
            "Accuracy at step 5 - batch 327: 0.9012\n",
            "training loss at step 5 - batch 328: 0.33 (2019-08-04 13:36:21.827234)\n",
            "Accuracy at step 5 - batch 328: 0.9016\n",
            "training loss at step 5 - batch 329: 0.31 (2019-08-04 13:36:21.840218)\n",
            "Accuracy at step 5 - batch 329: 0.9076\n",
            "training loss at step 5 - batch 330: 0.32 (2019-08-04 13:36:21.853260)\n",
            "Accuracy at step 5 - batch 330: 0.9084\n",
            "training loss at step 5 - batch 331: 0.34 (2019-08-04 13:36:21.973181)\n",
            "Accuracy at step 5 - batch 331: 0.898\n",
            "training loss at step 5 - batch 332: 0.32 (2019-08-04 13:36:21.990832)\n",
            "Accuracy at step 5 - batch 332: 0.9076\n",
            "training loss at step 5 - batch 333: 0.34 (2019-08-04 13:36:22.003240)\n",
            "Accuracy at step 5 - batch 333: 0.8976\n",
            "training loss at step 5 - batch 334: 0.31 (2019-08-04 13:36:22.016168)\n",
            "Accuracy at step 5 - batch 334: 0.9096\n",
            "training loss at step 5 - batch 335: 0.33 (2019-08-04 13:36:22.029179)\n",
            "Accuracy at step 5 - batch 335: 0.9036\n",
            "training loss at step 5 - batch 336: 0.33 (2019-08-04 13:36:22.161335)\n",
            "Accuracy at step 5 - batch 336: 0.9056\n",
            "training loss at step 5 - batch 337: 0.30 (2019-08-04 13:36:22.173722)\n",
            "Accuracy at step 5 - batch 337: 0.9092\n",
            "training loss at step 5 - batch 338: 0.34 (2019-08-04 13:36:22.189740)\n",
            "Accuracy at step 5 - batch 338: 0.8996\n",
            "training loss at step 5 - batch 339: 0.32 (2019-08-04 13:36:22.202194)\n",
            "Accuracy at step 5 - batch 339: 0.9056\n",
            "training loss at step 5 - batch 340: 0.34 (2019-08-04 13:36:22.215086)\n",
            "Accuracy at step 5 - batch 340: 0.8972\n",
            "training loss at step 5 - batch 341: 0.34 (2019-08-04 13:36:22.332368)\n",
            "Accuracy at step 5 - batch 341: 0.9008\n",
            "training loss at step 5 - batch 342: 0.30 (2019-08-04 13:36:22.347525)\n",
            "Accuracy at step 5 - batch 342: 0.9108\n",
            "training loss at step 5 - batch 343: 0.32 (2019-08-04 13:36:22.360190)\n",
            "Accuracy at step 5 - batch 343: 0.9072\n",
            "training loss at step 5 - batch 344: 0.33 (2019-08-04 13:36:22.374545)\n",
            "Accuracy at step 5 - batch 344: 0.8984\n",
            "training loss at step 5 - batch 345: 0.34 (2019-08-04 13:36:22.391619)\n",
            "Accuracy at step 5 - batch 345: 0.8988\n",
            "training loss at step 5 - batch 346: 0.33 (2019-08-04 13:36:22.514306)\n",
            "Accuracy at step 5 - batch 346: 0.8992\n",
            "training loss at step 5 - batch 347: 0.31 (2019-08-04 13:36:22.530634)\n",
            "Accuracy at step 5 - batch 347: 0.9056\n",
            "training loss at step 5 - batch 348: 0.32 (2019-08-04 13:36:22.544861)\n",
            "Accuracy at step 5 - batch 348: 0.9068\n",
            "training loss at step 5 - batch 349: 0.30 (2019-08-04 13:36:22.557746)\n",
            "Accuracy at step 5 - batch 349: 0.904\n",
            "training loss at step 5 - batch 350: 0.33 (2019-08-04 13:36:22.569632)\n",
            "Accuracy at step 5 - batch 350: 0.9024\n",
            "training loss at step 5 - batch 351: 0.32 (2019-08-04 13:36:22.699670)\n",
            "Accuracy at step 5 - batch 351: 0.9008\n",
            "training loss at step 5 - batch 352: 0.32 (2019-08-04 13:36:22.713217)\n",
            "Accuracy at step 5 - batch 352: 0.9044\n",
            "training loss at step 5 - batch 353: 0.34 (2019-08-04 13:36:22.726127)\n",
            "Accuracy at step 5 - batch 353: 0.8956\n",
            "training loss at step 5 - batch 354: 0.33 (2019-08-04 13:36:22.739196)\n",
            "Accuracy at step 5 - batch 354: 0.9024\n",
            "training loss at step 5 - batch 355: 0.29 (2019-08-04 13:36:22.752109)\n",
            "Accuracy at step 5 - batch 355: 0.9212\n",
            "training loss at step 5 - batch 356: 0.32 (2019-08-04 13:36:22.872364)\n",
            "Accuracy at step 5 - batch 356: 0.9064\n",
            "training loss at step 5 - batch 357: 0.33 (2019-08-04 13:36:22.884622)\n",
            "Accuracy at step 5 - batch 357: 0.9044\n",
            "training loss at step 5 - batch 358: 0.30 (2019-08-04 13:36:22.896884)\n",
            "Accuracy at step 5 - batch 358: 0.9152\n",
            "training loss at step 5 - batch 359: 0.33 (2019-08-04 13:36:22.911718)\n",
            "Accuracy at step 5 - batch 359: 0.9036\n",
            "training loss at step 5 - batch 360: 0.33 (2019-08-04 13:36:22.925856)\n",
            "Accuracy at step 5 - batch 360: 0.9024\n",
            "training loss at step 5 - batch 361: 0.32 (2019-08-04 13:36:23.044614)\n",
            "Accuracy at step 5 - batch 361: 0.9072\n",
            "training loss at step 5 - batch 362: 0.31 (2019-08-04 13:36:23.058599)\n",
            "Accuracy at step 5 - batch 362: 0.9124\n",
            "training loss at step 5 - batch 363: 0.35 (2019-08-04 13:36:23.070896)\n",
            "Accuracy at step 5 - batch 363: 0.8996\n",
            "training loss at step 5 - batch 364: 0.33 (2019-08-04 13:36:23.083369)\n",
            "Accuracy at step 5 - batch 364: 0.902\n",
            "training loss at step 5 - batch 365: 0.33 (2019-08-04 13:36:23.095649)\n",
            "Accuracy at step 5 - batch 365: 0.9056\n",
            "training loss at step 5 - batch 366: 0.32 (2019-08-04 13:36:23.218032)\n",
            "Accuracy at step 5 - batch 366: 0.9108\n",
            "training loss at step 5 - batch 367: 0.34 (2019-08-04 13:36:23.235179)\n",
            "Accuracy at step 5 - batch 367: 0.9028\n",
            "training loss at step 5 - batch 368: 0.31 (2019-08-04 13:36:23.248400)\n",
            "Accuracy at step 5 - batch 368: 0.9068\n",
            "training loss at step 5 - batch 369: 0.33 (2019-08-04 13:36:23.261434)\n",
            "Accuracy at step 5 - batch 369: 0.8996\n",
            "training loss at step 5 - batch 370: 0.32 (2019-08-04 13:36:23.273752)\n",
            "Accuracy at step 5 - batch 370: 0.904\n",
            "training loss at step 5 - batch 371: 0.32 (2019-08-04 13:36:23.394371)\n",
            "Accuracy at step 5 - batch 371: 0.9124\n",
            "training loss at step 5 - batch 372: 0.32 (2019-08-04 13:36:23.407901)\n",
            "Accuracy at step 5 - batch 372: 0.9064\n",
            "training loss at step 5 - batch 373: 0.35 (2019-08-04 13:36:23.423058)\n",
            "Accuracy at step 5 - batch 373: 0.8992\n",
            "training loss at step 5 - batch 374: 0.35 (2019-08-04 13:36:23.438686)\n",
            "Accuracy at step 5 - batch 374: 0.9024\n",
            "training loss at step 5 - batch 375: 0.33 (2019-08-04 13:36:23.451308)\n",
            "Accuracy at step 5 - batch 375: 0.9048\n",
            "training loss at step 5 - batch 376: 0.33 (2019-08-04 13:36:23.568114)\n",
            "Accuracy at step 5 - batch 376: 0.8968\n",
            "training loss at step 5 - batch 377: 0.31 (2019-08-04 13:36:23.581641)\n",
            "Accuracy at step 5 - batch 377: 0.9052\n",
            "training loss at step 5 - batch 378: 0.32 (2019-08-04 13:36:23.594986)\n",
            "Accuracy at step 5 - batch 378: 0.9096\n",
            "training loss at step 5 - batch 379: 0.32 (2019-08-04 13:36:23.607346)\n",
            "Accuracy at step 5 - batch 379: 0.9044\n",
            "training loss at step 5 - batch 380: 0.31 (2019-08-04 13:36:23.620184)\n",
            "Accuracy at step 5 - batch 380: 0.91\n",
            "training loss at step 5 - batch 381: 0.31 (2019-08-04 13:36:23.762414)\n",
            "Accuracy at step 5 - batch 381: 0.9076\n",
            "training loss at step 5 - batch 382: 0.31 (2019-08-04 13:36:23.776877)\n",
            "Accuracy at step 5 - batch 382: 0.912\n",
            "training loss at step 5 - batch 383: 0.33 (2019-08-04 13:36:23.789625)\n",
            "Accuracy at step 5 - batch 383: 0.9\n",
            "training loss at step 5 - batch 384: 0.33 (2019-08-04 13:36:23.802029)\n",
            "Accuracy at step 5 - batch 384: 0.9024\n",
            "training loss at step 5 - batch 385: 0.33 (2019-08-04 13:36:23.814558)\n",
            "Accuracy at step 5 - batch 385: 0.9004\n",
            "training loss at step 5 - batch 386: 0.31 (2019-08-04 13:36:23.934505)\n",
            "Accuracy at step 5 - batch 386: 0.9088\n",
            "training loss at step 5 - batch 387: 0.33 (2019-08-04 13:36:23.946783)\n",
            "Accuracy at step 5 - batch 387: 0.8996\n",
            "training loss at step 5 - batch 388: 0.35 (2019-08-04 13:36:23.959784)\n",
            "Accuracy at step 5 - batch 388: 0.9016\n",
            "training loss at step 5 - batch 389: 0.32 (2019-08-04 13:36:23.976171)\n",
            "Accuracy at step 5 - batch 389: 0.9028\n",
            "training loss at step 5 - batch 390: 0.34 (2019-08-04 13:36:23.989734)\n",
            "Accuracy at step 5 - batch 390: 0.8972\n",
            "training loss at step 5 - batch 391: 0.34 (2019-08-04 13:36:24.115300)\n",
            "Accuracy at step 5 - batch 391: 0.9016\n",
            "training loss at step 5 - batch 392: 0.31 (2019-08-04 13:36:24.128088)\n",
            "Accuracy at step 5 - batch 392: 0.9068\n",
            "training loss at step 5 - batch 393: 0.33 (2019-08-04 13:36:24.140380)\n",
            "Accuracy at step 5 - batch 393: 0.9032\n",
            "training loss at step 5 - batch 394: 0.32 (2019-08-04 13:36:24.154065)\n",
            "Accuracy at step 5 - batch 394: 0.9076\n",
            "training loss at step 5 - batch 395: 0.32 (2019-08-04 13:36:24.167965)\n",
            "Accuracy at step 5 - batch 395: 0.9112\n",
            "training loss at step 5 - batch 396: 0.35 (2019-08-04 13:36:24.293708)\n",
            "Accuracy at step 5 - batch 396: 0.8968\n",
            "training loss at step 5 - batch 397: 0.35 (2019-08-04 13:36:24.312006)\n",
            "Accuracy at step 5 - batch 397: 0.894\n",
            "training loss at step 5 - batch 398: 0.31 (2019-08-04 13:36:24.325905)\n",
            "Accuracy at step 5 - batch 398: 0.9048\n",
            "training loss at step 5 - batch 399: 0.33 (2019-08-04 13:36:24.338038)\n",
            "Accuracy at step 5 - batch 399: 0.9\n",
            "training loss at step 5 - batch 400: 0.33 (2019-08-04 13:36:24.351998)\n",
            "Accuracy at step 5 - batch 400: 0.8988\n",
            "training loss at step 5 - batch 401: 0.33 (2019-08-04 13:36:24.475286)\n",
            "Accuracy at step 5 - batch 401: 0.8984\n",
            "training loss at step 5 - batch 402: 0.34 (2019-08-04 13:36:24.489408)\n",
            "Accuracy at step 5 - batch 402: 0.9016\n",
            "training loss at step 5 - batch 403: 0.33 (2019-08-04 13:36:24.508005)\n",
            "Accuracy at step 5 - batch 403: 0.9008\n",
            "training loss at step 5 - batch 404: 0.31 (2019-08-04 13:36:24.520340)\n",
            "Accuracy at step 5 - batch 404: 0.9068\n",
            "training loss at step 5 - batch 405: 0.30 (2019-08-04 13:36:24.532400)\n",
            "Accuracy at step 5 - batch 405: 0.908\n",
            "training loss at step 5 - batch 406: 0.30 (2019-08-04 13:36:24.790563)\n",
            "Accuracy at step 5 - batch 406: 0.9064\n",
            "training loss at step 5 - batch 407: 0.33 (2019-08-04 13:36:24.807709)\n",
            "Accuracy at step 5 - batch 407: 0.904\n",
            "training loss at step 5 - batch 408: 0.35 (2019-08-04 13:36:24.820927)\n",
            "Accuracy at step 5 - batch 408: 0.898\n",
            "training loss at step 5 - batch 409: 0.30 (2019-08-04 13:36:24.832781)\n",
            "Accuracy at step 5 - batch 409: 0.9084\n",
            "training loss at step 5 - batch 410: 0.33 (2019-08-04 13:36:24.845350)\n",
            "Accuracy at step 5 - batch 410: 0.9008\n",
            "training loss at step 5 - batch 411: 0.34 (2019-08-04 13:36:24.961826)\n",
            "Accuracy at step 5 - batch 411: 0.9052\n",
            "training loss at step 5 - batch 412: 0.34 (2019-08-04 13:36:24.975611)\n",
            "Accuracy at step 5 - batch 412: 0.8952\n",
            "training loss at step 5 - batch 413: 0.33 (2019-08-04 13:36:24.987242)\n",
            "Accuracy at step 5 - batch 413: 0.9048\n",
            "training loss at step 5 - batch 414: 0.32 (2019-08-04 13:36:25.003249)\n",
            "Accuracy at step 5 - batch 414: 0.9056\n",
            "training loss at step 5 - batch 415: 0.33 (2019-08-04 13:36:25.016421)\n",
            "Accuracy at step 5 - batch 415: 0.9016\n",
            "training loss at step 5 - batch 416: 0.34 (2019-08-04 13:36:25.140526)\n",
            "Accuracy at step 5 - batch 416: 0.8992\n",
            "training loss at step 5 - batch 417: 0.31 (2019-08-04 13:36:25.156074)\n",
            "Accuracy at step 5 - batch 417: 0.906\n",
            "training loss at step 5 - batch 418: 0.34 (2019-08-04 13:36:25.169786)\n",
            "Accuracy at step 5 - batch 418: 0.8996\n",
            "training loss at step 5 - batch 419: 0.32 (2019-08-04 13:36:25.182386)\n",
            "Accuracy at step 5 - batch 419: 0.9072\n",
            "training loss at step 5 - batch 420: 0.32 (2019-08-04 13:36:25.195312)\n",
            "Accuracy at step 5 - batch 420: 0.9004\n",
            "training loss at step 5 - batch 421: 0.32 (2019-08-04 13:36:25.320255)\n",
            "Accuracy at step 5 - batch 421: 0.9004\n",
            "training loss at step 5 - batch 422: 0.33 (2019-08-04 13:36:25.334637)\n",
            "Accuracy at step 5 - batch 422: 0.902\n",
            "training loss at step 5 - batch 423: 0.31 (2019-08-04 13:36:25.347500)\n",
            "Accuracy at step 5 - batch 423: 0.9084\n",
            "training loss at step 5 - batch 424: 0.32 (2019-08-04 13:36:25.359428)\n",
            "Accuracy at step 5 - batch 424: 0.9024\n",
            "training loss at step 5 - batch 425: 0.31 (2019-08-04 13:36:25.372437)\n",
            "Accuracy at step 5 - batch 425: 0.9096\n",
            "training loss at step 5 - batch 426: 0.33 (2019-08-04 13:36:25.485508)\n",
            "Accuracy at step 5 - batch 426: 0.8992\n",
            "training loss at step 5 - batch 427: 0.35 (2019-08-04 13:36:25.498782)\n",
            "Accuracy at step 5 - batch 427: 0.8956\n",
            "training loss at step 5 - batch 428: 0.30 (2019-08-04 13:36:25.513433)\n",
            "Accuracy at step 5 - batch 428: 0.908\n",
            "training loss at step 5 - batch 429: 0.30 (2019-08-04 13:36:25.529294)\n",
            "Accuracy at step 5 - batch 429: 0.9036\n",
            "training loss at step 5 - batch 430: 0.33 (2019-08-04 13:36:25.542041)\n",
            "Accuracy at step 5 - batch 430: 0.9012\n",
            "training loss at step 5 - batch 431: 0.31 (2019-08-04 13:36:25.658151)\n",
            "Accuracy at step 5 - batch 431: 0.904\n",
            "training loss at step 5 - batch 432: 0.31 (2019-08-04 13:36:25.671918)\n",
            "Accuracy at step 5 - batch 432: 0.9072\n",
            "training loss at step 5 - batch 433: 0.31 (2019-08-04 13:36:25.685225)\n",
            "Accuracy at step 5 - batch 433: 0.9144\n",
            "training loss at step 5 - batch 434: 0.34 (2019-08-04 13:36:25.700944)\n",
            "Accuracy at step 5 - batch 434: 0.8984\n",
            "training loss at step 5 - batch 435: 0.33 (2019-08-04 13:36:25.716032)\n",
            "Accuracy at step 5 - batch 435: 0.902\n",
            "training loss at step 5 - batch 436: 0.32 (2019-08-04 13:36:25.838421)\n",
            "Accuracy at step 5 - batch 436: 0.9016\n",
            "training loss at step 5 - batch 437: 0.31 (2019-08-04 13:36:25.851397)\n",
            "Accuracy at step 5 - batch 437: 0.904\n",
            "training loss at step 5 - batch 438: 0.31 (2019-08-04 13:36:25.865466)\n",
            "Accuracy at step 5 - batch 438: 0.9036\n",
            "training loss at step 5 - batch 439: 0.32 (2019-08-04 13:36:25.877847)\n",
            "Accuracy at step 5 - batch 439: 0.9028\n",
            "training loss at step 5 - batch 440: 0.33 (2019-08-04 13:36:25.890356)\n",
            "Accuracy at step 5 - batch 440: 0.9028\n",
            "training loss at step 5 - batch 441: 0.34 (2019-08-04 13:36:26.024586)\n",
            "Accuracy at step 5 - batch 441: 0.8972\n",
            "training loss at step 5 - batch 442: 0.33 (2019-08-04 13:36:26.039319)\n",
            "Accuracy at step 5 - batch 442: 0.8984\n",
            "training loss at step 5 - batch 443: 0.32 (2019-08-04 13:36:26.055146)\n",
            "Accuracy at step 5 - batch 443: 0.9032\n",
            "training loss at step 5 - batch 444: 0.34 (2019-08-04 13:36:26.069653)\n",
            "Accuracy at step 5 - batch 444: 0.8956\n",
            "training loss at step 5 - batch 445: 0.30 (2019-08-04 13:36:26.082314)\n",
            "Accuracy at step 5 - batch 445: 0.9096\n",
            "training loss at step 5 - batch 446: 0.30 (2019-08-04 13:36:26.202515)\n",
            "Accuracy at step 5 - batch 446: 0.9116\n",
            "training loss at step 5 - batch 447: 0.31 (2019-08-04 13:36:26.214916)\n",
            "Accuracy at step 5 - batch 447: 0.9104\n",
            "training loss at step 5 - batch 448: 0.34 (2019-08-04 13:36:26.227654)\n",
            "Accuracy at step 5 - batch 448: 0.9004\n",
            "training loss at step 5 - batch 449: 0.33 (2019-08-04 13:36:26.244254)\n",
            "Accuracy at step 5 - batch 449: 0.9004\n",
            "training loss at step 5 - batch 450: 0.33 (2019-08-04 13:36:26.259116)\n",
            "Accuracy at step 5 - batch 450: 0.9044\n",
            "training loss at step 5 - batch 451: 0.31 (2019-08-04 13:36:26.387778)\n",
            "Accuracy at step 5 - batch 451: 0.906\n",
            "training loss at step 5 - batch 452: 0.32 (2019-08-04 13:36:26.401275)\n",
            "Accuracy at step 5 - batch 452: 0.9044\n",
            "training loss at step 5 - batch 453: 0.32 (2019-08-04 13:36:26.413351)\n",
            "Accuracy at step 5 - batch 453: 0.906\n",
            "training loss at step 5 - batch 454: 0.32 (2019-08-04 13:36:26.427199)\n",
            "Accuracy at step 5 - batch 454: 0.9056\n",
            "training loss at step 5 - batch 455: 0.31 (2019-08-04 13:36:26.440377)\n",
            "Accuracy at step 5 - batch 455: 0.9084\n",
            "training loss at step 5 - batch 456: 0.32 (2019-08-04 13:36:26.557590)\n",
            "Accuracy at step 5 - batch 456: 0.9072\n",
            "training loss at step 5 - batch 457: 0.32 (2019-08-04 13:36:26.570708)\n",
            "Accuracy at step 5 - batch 457: 0.91\n",
            "training loss at step 5 - batch 458: 0.30 (2019-08-04 13:36:26.583319)\n",
            "Accuracy at step 5 - batch 458: 0.9068\n",
            "training loss at step 5 - batch 459: 0.34 (2019-08-04 13:36:26.596277)\n",
            "Accuracy at step 5 - batch 459: 0.8972\n",
            "training loss at step 5 - batch 460: 0.32 (2019-08-04 13:36:26.609309)\n",
            "Accuracy at step 5 - batch 460: 0.9052\n",
            "training loss at step 5 - batch 461: 0.34 (2019-08-04 13:36:26.736551)\n",
            "Accuracy at step 5 - batch 461: 0.8968\n",
            "training loss at step 5 - batch 462: 0.32 (2019-08-04 13:36:26.752457)\n",
            "Accuracy at step 5 - batch 462: 0.906\n",
            "training loss at step 5 - batch 463: 0.31 (2019-08-04 13:36:26.770981)\n",
            "Accuracy at step 5 - batch 463: 0.9088\n",
            "training loss at step 5 - batch 464: 0.35 (2019-08-04 13:36:26.783598)\n",
            "Accuracy at step 5 - batch 464: 0.9016\n",
            "training loss at step 5 - batch 465: 0.33 (2019-08-04 13:36:26.796725)\n",
            "Accuracy at step 5 - batch 465: 0.9068\n",
            "training loss at step 5 - batch 466: 0.34 (2019-08-04 13:36:26.917527)\n",
            "Accuracy at step 5 - batch 466: 0.8984\n",
            "training loss at step 5 - batch 467: 0.32 (2019-08-04 13:36:26.931043)\n",
            "Accuracy at step 5 - batch 467: 0.9084\n",
            "training loss at step 5 - batch 468: 0.33 (2019-08-04 13:36:26.943677)\n",
            "Accuracy at step 5 - batch 468: 0.902\n",
            "training loss at step 5 - batch 469: 0.34 (2019-08-04 13:36:26.956048)\n",
            "Accuracy at step 5 - batch 469: 0.9004\n",
            "training loss at step 5 - batch 470: 0.30 (2019-08-04 13:36:26.968580)\n",
            "Accuracy at step 5 - batch 470: 0.9084\n",
            "training loss at step 5 - batch 471: 0.35 (2019-08-04 13:36:27.095097)\n",
            "Accuracy at step 5 - batch 471: 0.8924\n",
            "training loss at step 5 - batch 472: 0.32 (2019-08-04 13:36:27.107728)\n",
            "Accuracy at step 5 - batch 472: 0.9036\n",
            "training loss at step 5 - batch 473: 0.32 (2019-08-04 13:36:27.122283)\n",
            "Accuracy at step 5 - batch 473: 0.906\n",
            "training loss at step 5 - batch 474: 0.32 (2019-08-04 13:36:27.135330)\n",
            "Accuracy at step 5 - batch 474: 0.9\n",
            "training loss at step 5 - batch 475: 0.31 (2019-08-04 13:36:27.148408)\n",
            "Accuracy at step 5 - batch 475: 0.9084\n",
            "training loss at step 5 - batch 476: 0.32 (2019-08-04 13:36:27.268965)\n",
            "Accuracy at step 5 - batch 476: 0.9028\n",
            "training loss at step 5 - batch 477: 0.30 (2019-08-04 13:36:27.282244)\n",
            "Accuracy at step 5 - batch 477: 0.914\n",
            "training loss at step 5 - batch 478: 0.33 (2019-08-04 13:36:27.294861)\n",
            "Accuracy at step 5 - batch 478: 0.9048\n",
            "training loss at step 5 - batch 479: 0.32 (2019-08-04 13:36:27.310557)\n",
            "Accuracy at step 5 - batch 479: 0.9024\n",
            "training loss at step 5 - batch 480: 0.32 (2019-08-04 13:36:27.323374)\n",
            "Accuracy at step 5 - batch 480: 0.9052\n",
            "training loss at step 5 - batch 481: 0.31 (2019-08-04 13:36:27.438567)\n",
            "Accuracy at step 5 - batch 481: 0.91\n",
            "training loss at step 5 - batch 482: 0.32 (2019-08-04 13:36:27.451067)\n",
            "Accuracy at step 5 - batch 482: 0.9036\n",
            "training loss at step 5 - batch 483: 0.30 (2019-08-04 13:36:27.463458)\n",
            "Accuracy at step 5 - batch 483: 0.91\n",
            "training loss at step 5 - batch 484: 0.31 (2019-08-04 13:36:27.475737)\n",
            "Accuracy at step 5 - batch 484: 0.908\n",
            "training loss at step 5 - batch 485: 0.34 (2019-08-04 13:36:27.488166)\n",
            "Accuracy at step 5 - batch 485: 0.9016\n",
            "training loss at step 5 - batch 486: 0.31 (2019-08-04 13:36:27.610388)\n",
            "Accuracy at step 5 - batch 486: 0.9032\n",
            "training loss at step 5 - batch 487: 0.34 (2019-08-04 13:36:27.624939)\n",
            "Accuracy at step 5 - batch 487: 0.9028\n",
            "training loss at step 5 - batch 488: 0.33 (2019-08-04 13:36:27.638756)\n",
            "Accuracy at step 5 - batch 488: 0.9028\n",
            "training loss at step 5 - batch 489: 0.31 (2019-08-04 13:36:27.651196)\n",
            "Accuracy at step 5 - batch 489: 0.906\n",
            "training loss at step 5 - batch 490: 0.31 (2019-08-04 13:36:27.664288)\n",
            "Accuracy at step 5 - batch 490: 0.9076\n",
            "training loss at step 5 - batch 491: 0.32 (2019-08-04 13:36:27.790588)\n",
            "Accuracy at step 5 - batch 491: 0.9048\n",
            "training loss at step 5 - batch 492: 0.33 (2019-08-04 13:36:27.805039)\n",
            "Accuracy at step 5 - batch 492: 0.9\n",
            "training loss at step 5 - batch 493: 0.34 (2019-08-04 13:36:27.821541)\n",
            "Accuracy at step 5 - batch 493: 0.9\n",
            "training loss at step 5 - batch 494: 0.32 (2019-08-04 13:36:27.834256)\n",
            "Accuracy at step 5 - batch 494: 0.9064\n",
            "training loss at step 5 - batch 495: 0.33 (2019-08-04 13:36:27.847289)\n",
            "Accuracy at step 5 - batch 495: 0.8972\n",
            "training loss at step 5 - batch 496: 0.33 (2019-08-04 13:36:27.965151)\n",
            "Accuracy at step 5 - batch 496: 0.9032\n",
            "training loss at step 5 - batch 497: 0.31 (2019-08-04 13:36:27.981180)\n",
            "Accuracy at step 5 - batch 497: 0.9064\n",
            "training loss at step 5 - batch 498: 0.32 (2019-08-04 13:36:27.993993)\n",
            "Accuracy at step 5 - batch 498: 0.9068\n",
            "training loss at step 5 - batch 499: 0.31 (2019-08-04 13:36:28.008098)\n",
            "Accuracy at step 5 - batch 499: 0.9012\n",
            "training loss at step 5 - batch 500: 0.32 (2019-08-04 13:36:28.021268)\n",
            "Accuracy at step 5 - batch 500: 0.906\n",
            "training loss at step 5 - batch 501: 0.33 (2019-08-04 13:36:28.144956)\n",
            "Accuracy at step 5 - batch 501: 0.9\n",
            "training loss at step 5 - batch 502: 0.34 (2019-08-04 13:36:28.158619)\n",
            "Accuracy at step 5 - batch 502: 0.902\n",
            "training loss at step 5 - batch 503: 0.31 (2019-08-04 13:36:28.172301)\n",
            "Accuracy at step 5 - batch 503: 0.908\n",
            "training loss at step 5 - batch 504: 0.32 (2019-08-04 13:36:28.185352)\n",
            "Accuracy at step 5 - batch 504: 0.902\n",
            "training loss at step 5 - batch 505: 0.33 (2019-08-04 13:36:28.198203)\n",
            "Accuracy at step 5 - batch 505: 0.8988\n",
            "training loss at step 5 - batch 506: 0.31 (2019-08-04 13:36:28.324606)\n",
            "Accuracy at step 5 - batch 506: 0.9056\n",
            "training loss at step 5 - batch 507: 0.30 (2019-08-04 13:36:28.336975)\n",
            "Accuracy at step 5 - batch 507: 0.908\n",
            "training loss at step 5 - batch 508: 0.31 (2019-08-04 13:36:28.351192)\n",
            "Accuracy at step 5 - batch 508: 0.9068\n",
            "training loss at step 5 - batch 509: 0.34 (2019-08-04 13:36:28.365052)\n",
            "Accuracy at step 5 - batch 509: 0.9\n",
            "training loss at step 5 - batch 510: 0.32 (2019-08-04 13:36:28.377691)\n",
            "Accuracy at step 5 - batch 510: 0.9056\n",
            "training loss at step 5 - batch 511: 0.33 (2019-08-04 13:36:28.494694)\n",
            "Accuracy at step 5 - batch 511: 0.902\n",
            "training loss at step 5 - batch 512: 0.35 (2019-08-04 13:36:28.511164)\n",
            "Accuracy at step 5 - batch 512: 0.902\n",
            "training loss at step 5 - batch 513: 0.33 (2019-08-04 13:36:28.523327)\n",
            "Accuracy at step 5 - batch 513: 0.8996\n",
            "training loss at step 5 - batch 514: 0.33 (2019-08-04 13:36:28.535621)\n",
            "Accuracy at step 5 - batch 514: 0.9056\n",
            "training loss at step 5 - batch 515: 0.33 (2019-08-04 13:36:28.548086)\n",
            "Accuracy at step 5 - batch 515: 0.9\n",
            "training loss at step 5 - batch 516: 0.33 (2019-08-04 13:36:28.673457)\n",
            "Accuracy at step 5 - batch 516: 0.8944\n",
            "training loss at step 5 - batch 517: 0.32 (2019-08-04 13:36:28.689279)\n",
            "Accuracy at step 5 - batch 517: 0.9\n",
            "training loss at step 5 - batch 518: 0.30 (2019-08-04 13:36:28.702278)\n",
            "Accuracy at step 5 - batch 518: 0.9076\n",
            "training loss at step 5 - batch 519: 0.31 (2019-08-04 13:36:28.715427)\n",
            "Accuracy at step 5 - batch 519: 0.9064\n",
            "training loss at step 5 - batch 520: 0.31 (2019-08-04 13:36:28.728187)\n",
            "Accuracy at step 5 - batch 520: 0.9092\n",
            "training loss at step 5 - batch 521: 0.32 (2019-08-04 13:36:28.853773)\n",
            "Accuracy at step 5 - batch 521: 0.9072\n",
            "training loss at step 5 - batch 522: 0.31 (2019-08-04 13:36:28.867203)\n",
            "Accuracy at step 5 - batch 522: 0.9012\n",
            "training loss at step 5 - batch 523: 0.32 (2019-08-04 13:36:28.889701)\n",
            "Accuracy at step 5 - batch 523: 0.904\n",
            "training loss at step 5 - batch 524: 0.32 (2019-08-04 13:36:28.902098)\n",
            "Accuracy at step 5 - batch 524: 0.9\n",
            "training loss at step 5 - batch 525: 0.33 (2019-08-04 13:36:28.915174)\n",
            "Accuracy at step 5 - batch 525: 0.906\n",
            "training loss at step 5 - batch 526: 0.31 (2019-08-04 13:36:29.034187)\n",
            "Accuracy at step 5 - batch 526: 0.9096\n",
            "training loss at step 5 - batch 527: 0.32 (2019-08-04 13:36:29.048997)\n",
            "Accuracy at step 5 - batch 527: 0.9024\n",
            "training loss at step 5 - batch 528: 0.33 (2019-08-04 13:36:29.063045)\n",
            "Accuracy at step 5 - batch 528: 0.9016\n",
            "training loss at step 5 - batch 529: 0.33 (2019-08-04 13:36:29.077074)\n",
            "Accuracy at step 5 - batch 529: 0.9016\n",
            "training loss at step 5 - batch 530: 0.32 (2019-08-04 13:36:29.090233)\n",
            "Accuracy at step 5 - batch 530: 0.9008\n",
            "training loss at step 5 - batch 531: 0.28 (2019-08-04 13:36:29.212499)\n",
            "Accuracy at step 5 - batch 531: 0.9156\n",
            "training loss at step 5 - batch 532: 0.32 (2019-08-04 13:36:29.226700)\n",
            "Accuracy at step 5 - batch 532: 0.9028\n",
            "training loss at step 5 - batch 533: 0.33 (2019-08-04 13:36:29.239242)\n",
            "Accuracy at step 5 - batch 533: 0.8972\n",
            "training loss at step 5 - batch 534: 0.33 (2019-08-04 13:36:29.252587)\n",
            "Accuracy at step 5 - batch 534: 0.9044\n",
            "training loss at step 5 - batch 535: 0.32 (2019-08-04 13:36:29.265125)\n",
            "Accuracy at step 5 - batch 535: 0.9056\n",
            "training loss at step 5 - batch 536: 0.30 (2019-08-04 13:36:29.387019)\n",
            "Accuracy at step 5 - batch 536: 0.9036\n",
            "training loss at step 5 - batch 537: 0.32 (2019-08-04 13:36:29.400823)\n",
            "Accuracy at step 5 - batch 537: 0.8992\n",
            "training loss at step 5 - batch 538: 0.30 (2019-08-04 13:36:29.414108)\n",
            "Accuracy at step 5 - batch 538: 0.9128\n",
            "training loss at step 5 - batch 539: 0.33 (2019-08-04 13:36:29.431686)\n",
            "Accuracy at step 5 - batch 539: 0.9064\n",
            "training loss at step 5 - batch 540: 0.30 (2019-08-04 13:36:29.444382)\n",
            "Accuracy at step 5 - batch 540: 0.9104\n",
            "training loss at step 5 - batch 541: 0.32 (2019-08-04 13:36:29.566832)\n",
            "Accuracy at step 5 - batch 541: 0.9056\n",
            "training loss at step 5 - batch 542: 0.32 (2019-08-04 13:36:29.579030)\n",
            "Accuracy at step 5 - batch 542: 0.9052\n",
            "training loss at step 5 - batch 543: 0.33 (2019-08-04 13:36:29.591523)\n",
            "Accuracy at step 5 - batch 543: 0.9\n",
            "training loss at step 5 - batch 544: 0.34 (2019-08-04 13:36:29.603849)\n",
            "Accuracy at step 5 - batch 544: 0.8972\n",
            "training loss at step 5 - batch 545: 0.31 (2019-08-04 13:36:29.619490)\n",
            "Accuracy at step 5 - batch 545: 0.9108\n",
            "training loss at step 5 - batch 546: 0.32 (2019-08-04 13:36:29.741991)\n",
            "Accuracy at step 5 - batch 546: 0.9036\n",
            "training loss at step 5 - batch 547: 0.30 (2019-08-04 13:36:29.754781)\n",
            "Accuracy at step 5 - batch 547: 0.9088\n",
            "training loss at step 5 - batch 548: 0.31 (2019-08-04 13:36:29.767577)\n",
            "Accuracy at step 5 - batch 548: 0.9096\n",
            "training loss at step 5 - batch 549: 0.32 (2019-08-04 13:36:29.782309)\n",
            "Accuracy at step 5 - batch 549: 0.902\n",
            "training loss at step 5 - batch 550: 0.31 (2019-08-04 13:36:29.800702)\n",
            "Accuracy at step 5 - batch 550: 0.91\n",
            "training loss at step 5 - batch 551: 0.30 (2019-08-04 13:36:29.930228)\n",
            "Accuracy at step 5 - batch 551: 0.9092\n",
            "training loss at step 5 - batch 552: 0.34 (2019-08-04 13:36:29.942700)\n",
            "Accuracy at step 5 - batch 552: 0.9\n",
            "training loss at step 5 - batch 553: 0.32 (2019-08-04 13:36:29.956257)\n",
            "Accuracy at step 5 - batch 553: 0.902\n",
            "training loss at step 5 - batch 554: 0.32 (2019-08-04 13:36:29.969297)\n",
            "Accuracy at step 5 - batch 554: 0.9064\n",
            "training loss at step 5 - batch 555: 0.33 (2019-08-04 13:36:29.981520)\n",
            "Accuracy at step 5 - batch 555: 0.9024\n",
            "training loss at step 5 - batch 556: 0.31 (2019-08-04 13:36:30.115360)\n",
            "Accuracy at step 5 - batch 556: 0.9036\n",
            "training loss at step 5 - batch 557: 0.34 (2019-08-04 13:36:30.131671)\n",
            "Accuracy at step 5 - batch 557: 0.8984\n",
            "training loss at step 5 - batch 558: 0.30 (2019-08-04 13:36:30.145327)\n",
            "Accuracy at step 5 - batch 558: 0.9084\n",
            "training loss at step 5 - batch 559: 0.31 (2019-08-04 13:36:30.158845)\n",
            "Accuracy at step 5 - batch 559: 0.9068\n",
            "training loss at step 5 - batch 560: 0.30 (2019-08-04 13:36:30.171745)\n",
            "Accuracy at step 5 - batch 560: 0.9056\n",
            "training loss at step 5 - batch 561: 0.31 (2019-08-04 13:36:30.300014)\n",
            "Accuracy at step 5 - batch 561: 0.9048\n",
            "training loss at step 5 - batch 562: 0.32 (2019-08-04 13:36:30.312245)\n",
            "Accuracy at step 5 - batch 562: 0.9024\n",
            "training loss at step 5 - batch 563: 0.30 (2019-08-04 13:36:30.325048)\n",
            "Accuracy at step 5 - batch 563: 0.9096\n",
            "training loss at step 5 - batch 564: 0.35 (2019-08-04 13:36:30.338871)\n",
            "Accuracy at step 5 - batch 564: 0.898\n",
            "training loss at step 5 - batch 565: 0.32 (2019-08-04 13:36:30.352455)\n",
            "Accuracy at step 5 - batch 565: 0.9032\n",
            "training loss at step 5 - batch 566: 0.34 (2019-08-04 13:36:30.472964)\n",
            "Accuracy at step 5 - batch 566: 0.9032\n",
            "training loss at step 5 - batch 567: 0.31 (2019-08-04 13:36:30.485209)\n",
            "Accuracy at step 5 - batch 567: 0.9052\n",
            "training loss at step 5 - batch 568: 0.30 (2019-08-04 13:36:30.497277)\n",
            "Accuracy at step 5 - batch 568: 0.912\n",
            "training loss at step 5 - batch 569: 0.30 (2019-08-04 13:36:30.511090)\n",
            "Accuracy at step 5 - batch 569: 0.9112\n",
            "training loss at step 5 - batch 570: 0.32 (2019-08-04 13:36:30.524215)\n",
            "Accuracy at step 5 - batch 570: 0.9032\n",
            "training loss at step 5 - batch 571: 0.30 (2019-08-04 13:36:30.640643)\n",
            "Accuracy at step 5 - batch 571: 0.9072\n",
            "training loss at step 5 - batch 572: 0.33 (2019-08-04 13:36:30.653698)\n",
            "Accuracy at step 5 - batch 572: 0.9016\n",
            "training loss at step 5 - batch 573: 0.30 (2019-08-04 13:36:30.665855)\n",
            "Accuracy at step 5 - batch 573: 0.9068\n",
            "training loss at step 5 - batch 574: 0.31 (2019-08-04 13:36:30.678170)\n",
            "Accuracy at step 5 - batch 574: 0.9052\n",
            "training loss at step 5 - batch 575: 0.33 (2019-08-04 13:36:30.691603)\n",
            "Accuracy at step 5 - batch 575: 0.9024\n",
            "training loss at step 5 - batch 576: 0.30 (2019-08-04 13:36:30.816127)\n",
            "Accuracy at step 5 - batch 576: 0.9132\n",
            "training loss at step 5 - batch 577: 0.32 (2019-08-04 13:36:30.829696)\n",
            "Accuracy at step 5 - batch 577: 0.9024\n",
            "training loss at step 5 - batch 578: 0.32 (2019-08-04 13:36:30.845507)\n",
            "Accuracy at step 5 - batch 578: 0.9056\n",
            "training loss at step 5 - batch 579: 0.31 (2019-08-04 13:36:30.858416)\n",
            "Accuracy at step 5 - batch 579: 0.9092\n",
            "training loss at step 5 - batch 580: 0.32 (2019-08-04 13:36:30.871191)\n",
            "Accuracy at step 5 - batch 580: 0.9068\n",
            "training loss at step 5 - batch 581: 0.33 (2019-08-04 13:36:30.996609)\n",
            "Accuracy at step 5 - batch 581: 0.9048\n",
            "training loss at step 5 - batch 582: 0.33 (2019-08-04 13:36:31.010356)\n",
            "Accuracy at step 5 - batch 582: 0.9072\n",
            "training loss at step 5 - batch 583: 0.32 (2019-08-04 13:36:31.026524)\n",
            "Accuracy at step 5 - batch 583: 0.9032\n",
            "training loss at step 5 - batch 584: 0.31 (2019-08-04 13:36:31.041367)\n",
            "Accuracy at step 5 - batch 584: 0.9108\n",
            "training loss at step 5 - batch 585: 0.31 (2019-08-04 13:36:31.057726)\n",
            "Accuracy at step 5 - batch 585: 0.9044\n",
            "training loss at step 5 - batch 586: 0.34 (2019-08-04 13:36:31.185795)\n",
            "Accuracy at step 5 - batch 586: 0.8996\n",
            "training loss at step 5 - batch 587: 0.33 (2019-08-04 13:36:31.202266)\n",
            "Accuracy at step 5 - batch 587: 0.904\n",
            "training loss at step 5 - batch 588: 0.34 (2019-08-04 13:36:31.214703)\n",
            "Accuracy at step 5 - batch 588: 0.8976\n",
            "training loss at step 5 - batch 589: 0.33 (2019-08-04 13:36:31.227105)\n",
            "Accuracy at step 5 - batch 589: 0.9044\n",
            "training loss at step 5 - batch 590: 0.35 (2019-08-04 13:36:31.239449)\n",
            "Accuracy at step 5 - batch 590: 0.8992\n",
            "training loss at step 5 - batch 591: 0.33 (2019-08-04 13:36:31.369732)\n",
            "Accuracy at step 5 - batch 591: 0.9008\n",
            "training loss at step 5 - batch 592: 0.31 (2019-08-04 13:36:31.387491)\n",
            "Accuracy at step 5 - batch 592: 0.9068\n",
            "training loss at step 5 - batch 593: 0.32 (2019-08-04 13:36:31.399567)\n",
            "Accuracy at step 5 - batch 593: 0.9032\n",
            "training loss at step 5 - batch 594: 0.31 (2019-08-04 13:36:31.411674)\n",
            "Accuracy at step 5 - batch 594: 0.9108\n",
            "training loss at step 5 - batch 595: 0.31 (2019-08-04 13:36:31.423876)\n",
            "Accuracy at step 5 - batch 595: 0.9064\n",
            "training loss at step 5 - batch 596: 0.31 (2019-08-04 13:36:31.538443)\n",
            "Accuracy at step 5 - batch 596: 0.8992\n",
            "training loss at step 5 - batch 597: 0.33 (2019-08-04 13:36:31.551198)\n",
            "Accuracy at step 5 - batch 597: 0.9036\n",
            "training loss at step 5 - batch 598: 0.32 (2019-08-04 13:36:31.564487)\n",
            "Accuracy at step 5 - batch 598: 0.9032\n",
            "training loss at step 5 - batch 599: 0.31 (2019-08-04 13:36:31.578711)\n",
            "Accuracy at step 5 - batch 599: 0.9112\n",
            "training loss at step 5 - batch 600: 0.34 (2019-08-04 13:36:31.591113)\n",
            "Accuracy at step 5 - batch 600: 0.9012\n",
            "training loss at step 5 - batch 601: 0.33 (2019-08-04 13:36:31.712498)\n",
            "Accuracy at step 5 - batch 601: 0.8948\n",
            "training loss at step 5 - batch 602: 0.34 (2019-08-04 13:36:31.728123)\n",
            "Accuracy at step 5 - batch 602: 0.902\n",
            "training loss at step 5 - batch 603: 0.31 (2019-08-04 13:36:31.741288)\n",
            "Accuracy at step 5 - batch 603: 0.9036\n",
            "training loss at step 5 - batch 604: 0.31 (2019-08-04 13:36:31.755747)\n",
            "Accuracy at step 5 - batch 604: 0.9068\n",
            "training loss at step 5 - batch 605: 0.31 (2019-08-04 13:36:31.768504)\n",
            "Accuracy at step 5 - batch 605: 0.906\n",
            "training loss at step 5 - batch 606: 0.33 (2019-08-04 13:36:31.895064)\n",
            "Accuracy at step 5 - batch 606: 0.9044\n",
            "training loss at step 5 - batch 607: 0.33 (2019-08-04 13:36:31.906993)\n",
            "Accuracy at step 5 - batch 607: 0.9016\n",
            "training loss at step 5 - batch 608: 0.33 (2019-08-04 13:36:31.918776)\n",
            "Accuracy at step 5 - batch 608: 0.9\n",
            "training loss at step 5 - batch 609: 0.34 (2019-08-04 13:36:31.932413)\n",
            "Accuracy at step 5 - batch 609: 0.8964\n",
            "training loss at step 5 - batch 610: 0.32 (2019-08-04 13:36:31.945372)\n",
            "Accuracy at step 5 - batch 610: 0.9028\n",
            "training loss at step 5 - batch 611: 0.32 (2019-08-04 13:36:32.060993)\n",
            "Accuracy at step 5 - batch 611: 0.906\n",
            "training loss at step 5 - batch 612: 0.33 (2019-08-04 13:36:32.074386)\n",
            "Accuracy at step 5 - batch 612: 0.9004\n",
            "training loss at step 5 - batch 613: 0.30 (2019-08-04 13:36:32.088649)\n",
            "Accuracy at step 5 - batch 613: 0.9132\n",
            "training loss at step 5 - batch 614: 0.33 (2019-08-04 13:36:32.102372)\n",
            "Accuracy at step 5 - batch 614: 0.8972\n",
            "training loss at step 5 - batch 615: 0.31 (2019-08-04 13:36:32.115121)\n",
            "Accuracy at step 5 - batch 615: 0.9088\n",
            "training loss at step 5 - batch 616: 0.33 (2019-08-04 13:36:32.242749)\n",
            "Accuracy at step 5 - batch 616: 0.9036\n",
            "training loss at step 5 - batch 617: 0.31 (2019-08-04 13:36:32.258782)\n",
            "Accuracy at step 5 - batch 617: 0.9092\n",
            "training loss at step 5 - batch 618: 0.33 (2019-08-04 13:36:32.273165)\n",
            "Accuracy at step 5 - batch 618: 0.902\n",
            "training loss at step 5 - batch 619: 0.33 (2019-08-04 13:36:32.286354)\n",
            "Accuracy at step 5 - batch 619: 0.9044\n",
            "training loss at step 5 - batch 620: 0.34 (2019-08-04 13:36:32.299430)\n",
            "Accuracy at step 5 - batch 620: 0.8988\n",
            "training loss at step 5 - batch 621: 0.32 (2019-08-04 13:36:32.425253)\n",
            "Accuracy at step 5 - batch 621: 0.9052\n",
            "training loss at step 5 - batch 622: 0.32 (2019-08-04 13:36:32.443710)\n",
            "Accuracy at step 5 - batch 622: 0.8988\n",
            "training loss at step 5 - batch 623: 0.33 (2019-08-04 13:36:32.456224)\n",
            "Accuracy at step 5 - batch 623: 0.9024\n",
            "training loss at step 5 - batch 624: 0.30 (2019-08-04 13:36:32.469376)\n",
            "Accuracy at step 5 - batch 624: 0.9052\n",
            "training loss at step 5 - batch 625: 0.33 (2019-08-04 13:36:32.481268)\n",
            "Accuracy at step 5 - batch 625: 0.9044\n",
            "training loss at step 5 - batch 626: 0.32 (2019-08-04 13:36:32.599841)\n",
            "Accuracy at step 5 - batch 626: 0.9024\n",
            "training loss at step 5 - batch 627: 0.32 (2019-08-04 13:36:32.617972)\n",
            "Accuracy at step 5 - batch 627: 0.9028\n",
            "training loss at step 5 - batch 628: 0.33 (2019-08-04 13:36:32.631585)\n",
            "Accuracy at step 5 - batch 628: 0.9048\n",
            "training loss at step 5 - batch 629: 0.33 (2019-08-04 13:36:32.646239)\n",
            "Accuracy at step 5 - batch 629: 0.9012\n",
            "training loss at step 5 - batch 630: 0.34 (2019-08-04 13:36:32.658536)\n",
            "Accuracy at step 5 - batch 630: 0.9004\n",
            "training loss at step 5 - batch 631: 0.32 (2019-08-04 13:36:32.782902)\n",
            "Accuracy at step 5 - batch 631: 0.9036\n",
            "training loss at step 5 - batch 632: 0.33 (2019-08-04 13:36:32.800394)\n",
            "Accuracy at step 5 - batch 632: 0.9028\n",
            "training loss at step 5 - batch 633: 0.33 (2019-08-04 13:36:32.814522)\n",
            "Accuracy at step 5 - batch 633: 0.9028\n",
            "training loss at step 5 - batch 634: 0.33 (2019-08-04 13:36:32.827457)\n",
            "Accuracy at step 5 - batch 634: 0.9048\n",
            "training loss at step 5 - batch 635: 0.34 (2019-08-04 13:36:32.843088)\n",
            "Accuracy at step 5 - batch 635: 0.896\n",
            "training loss at step 5 - batch 636: 0.31 (2019-08-04 13:36:32.965829)\n",
            "Accuracy at step 5 - batch 636: 0.9072\n",
            "training loss at step 5 - batch 637: 0.31 (2019-08-04 13:36:32.979790)\n",
            "Accuracy at step 5 - batch 637: 0.9124\n",
            "training loss at step 5 - batch 638: 0.31 (2019-08-04 13:36:32.992368)\n",
            "Accuracy at step 5 - batch 638: 0.9068\n",
            "training loss at step 5 - batch 639: 0.33 (2019-08-04 13:36:33.004623)\n",
            "Accuracy at step 5 - batch 639: 0.9024\n",
            "training loss at step 5 - batch 640: 0.32 (2019-08-04 13:36:33.016774)\n",
            "Accuracy at step 5 - batch 640: 0.9044\n",
            "training loss at step 5 - batch 641: 0.33 (2019-08-04 13:36:33.139928)\n",
            "Accuracy at step 5 - batch 641: 0.9064\n",
            "training loss at step 5 - batch 642: 0.31 (2019-08-04 13:36:33.152634)\n",
            "Accuracy at step 5 - batch 642: 0.9084\n",
            "training loss at step 5 - batch 643: 0.31 (2019-08-04 13:36:33.165273)\n",
            "Accuracy at step 5 - batch 643: 0.9084\n",
            "training loss at step 5 - batch 644: 0.31 (2019-08-04 13:36:33.177203)\n",
            "Accuracy at step 5 - batch 644: 0.9056\n",
            "training loss at step 5 - batch 645: 0.33 (2019-08-04 13:36:33.190763)\n",
            "Accuracy at step 5 - batch 645: 0.906\n",
            "training loss at step 5 - batch 646: 0.33 (2019-08-04 13:36:33.307468)\n",
            "Accuracy at step 5 - batch 646: 0.9036\n",
            "training loss at step 5 - batch 647: 0.32 (2019-08-04 13:36:33.321045)\n",
            "Accuracy at step 5 - batch 647: 0.9056\n",
            "training loss at step 5 - batch 648: 0.33 (2019-08-04 13:36:33.333010)\n",
            "Accuracy at step 5 - batch 648: 0.8992\n",
            "training loss at step 5 - batch 649: 0.33 (2019-08-04 13:36:33.349027)\n",
            "Accuracy at step 5 - batch 649: 0.904\n",
            "training loss at step 5 - batch 650: 0.34 (2019-08-04 13:36:33.362402)\n",
            "Accuracy at step 5 - batch 650: 0.8956\n",
            "training loss at step 5 - batch 651: 0.30 (2019-08-04 13:36:33.484090)\n",
            "Accuracy at step 5 - batch 651: 0.912\n",
            "training loss at step 5 - batch 652: 0.32 (2019-08-04 13:36:33.497507)\n",
            "Accuracy at step 5 - batch 652: 0.9028\n",
            "training loss at step 5 - batch 653: 0.30 (2019-08-04 13:36:33.510641)\n",
            "Accuracy at step 5 - batch 653: 0.9076\n",
            "training loss at step 5 - batch 654: 0.32 (2019-08-04 13:36:33.525075)\n",
            "Accuracy at step 5 - batch 654: 0.9044\n",
            "training loss at step 5 - batch 655: 0.34 (2019-08-04 13:36:33.537260)\n",
            "Accuracy at step 5 - batch 655: 0.8996\n",
            "training loss at step 5 - batch 656: 0.33 (2019-08-04 13:36:33.664030)\n",
            "Accuracy at step 5 - batch 656: 0.9024\n",
            "training loss at step 5 - batch 657: 0.32 (2019-08-04 13:36:33.676515)\n",
            "Accuracy at step 5 - batch 657: 0.9004\n",
            "training loss at step 5 - batch 658: 0.32 (2019-08-04 13:36:33.688240)\n",
            "Accuracy at step 5 - batch 658: 0.9048\n",
            "training loss at step 5 - batch 659: 0.33 (2019-08-04 13:36:33.700737)\n",
            "Accuracy at step 5 - batch 659: 0.8968\n",
            "training loss at step 5 - batch 660: 0.32 (2019-08-04 13:36:33.713496)\n",
            "Accuracy at step 5 - batch 660: 0.9056\n",
            "training loss at step 5 - batch 661: 0.30 (2019-08-04 13:36:33.835468)\n",
            "Accuracy at step 5 - batch 661: 0.9144\n",
            "training loss at step 5 - batch 662: 0.33 (2019-08-04 13:36:33.849716)\n",
            "Accuracy at step 5 - batch 662: 0.9064\n",
            "training loss at step 5 - batch 663: 0.30 (2019-08-04 13:36:33.862749)\n",
            "Accuracy at step 5 - batch 663: 0.9044\n",
            "training loss at step 5 - batch 664: 0.32 (2019-08-04 13:36:33.880529)\n",
            "Accuracy at step 5 - batch 664: 0.9072\n",
            "training loss at step 5 - batch 665: 0.33 (2019-08-04 13:36:33.895381)\n",
            "Accuracy at step 5 - batch 665: 0.9032\n",
            "training loss at step 5 - batch 666: 0.31 (2019-08-04 13:36:34.016887)\n",
            "Accuracy at step 5 - batch 666: 0.9076\n",
            "training loss at step 5 - batch 667: 0.31 (2019-08-04 13:36:34.030578)\n",
            "Accuracy at step 5 - batch 667: 0.9048\n",
            "training loss at step 5 - batch 668: 0.33 (2019-08-04 13:36:34.043663)\n",
            "Accuracy at step 5 - batch 668: 0.9024\n",
            "training loss at step 5 - batch 669: 0.32 (2019-08-04 13:36:34.055690)\n",
            "Accuracy at step 5 - batch 669: 0.9036\n",
            "training loss at step 5 - batch 670: 0.31 (2019-08-04 13:36:34.069789)\n",
            "Accuracy at step 5 - batch 670: 0.9096\n",
            "training loss at step 5 - batch 671: 0.32 (2019-08-04 13:36:34.196516)\n",
            "Accuracy at step 5 - batch 671: 0.9036\n",
            "training loss at step 5 - batch 672: 0.34 (2019-08-04 13:36:34.209322)\n",
            "Accuracy at step 5 - batch 672: 0.8936\n",
            "training loss at step 5 - batch 673: 0.32 (2019-08-04 13:36:34.222451)\n",
            "Accuracy at step 5 - batch 673: 0.9048\n",
            "training loss at step 5 - batch 674: 0.32 (2019-08-04 13:36:34.235390)\n",
            "Accuracy at step 5 - batch 674: 0.9044\n",
            "training loss at step 5 - batch 675: 0.31 (2019-08-04 13:36:34.247116)\n",
            "Accuracy at step 5 - batch 675: 0.9052\n",
            "training loss at step 5 - batch 676: 0.34 (2019-08-04 13:36:34.371025)\n",
            "Accuracy at step 5 - batch 676: 0.898\n",
            "training loss at step 5 - batch 677: 0.30 (2019-08-04 13:36:34.383782)\n",
            "Accuracy at step 5 - batch 677: 0.9112\n",
            "training loss at step 5 - batch 678: 0.32 (2019-08-04 13:36:34.395704)\n",
            "Accuracy at step 5 - batch 678: 0.904\n",
            "training loss at step 5 - batch 679: 0.32 (2019-08-04 13:36:34.413313)\n",
            "Accuracy at step 5 - batch 679: 0.908\n",
            "training loss at step 5 - batch 680: 0.31 (2019-08-04 13:36:34.426819)\n",
            "Accuracy at step 5 - batch 680: 0.9096\n",
            "training loss at step 5 - batch 681: 0.34 (2019-08-04 13:36:34.549492)\n",
            "Accuracy at step 5 - batch 681: 0.9012\n",
            "training loss at step 5 - batch 682: 0.32 (2019-08-04 13:36:34.563546)\n",
            "Accuracy at step 5 - batch 682: 0.9088\n",
            "training loss at step 5 - batch 683: 0.32 (2019-08-04 13:36:34.575927)\n",
            "Accuracy at step 5 - batch 683: 0.9024\n",
            "training loss at step 5 - batch 684: 0.32 (2019-08-04 13:36:34.588264)\n",
            "Accuracy at step 5 - batch 684: 0.9032\n",
            "training loss at step 5 - batch 685: 0.31 (2019-08-04 13:36:34.602228)\n",
            "Accuracy at step 5 - batch 685: 0.9116\n",
            "training loss at step 5 - batch 686: 0.31 (2019-08-04 13:36:34.726713)\n",
            "Accuracy at step 5 - batch 686: 0.9076\n",
            "training loss at step 5 - batch 687: 0.35 (2019-08-04 13:36:34.743639)\n",
            "Accuracy at step 5 - batch 687: 0.8996\n",
            "training loss at step 5 - batch 688: 0.34 (2019-08-04 13:36:34.756251)\n",
            "Accuracy at step 5 - batch 688: 0.9016\n",
            "training loss at step 5 - batch 689: 0.34 (2019-08-04 13:36:34.769012)\n",
            "Accuracy at step 5 - batch 689: 0.9004\n",
            "training loss at step 5 - batch 690: 0.32 (2019-08-04 13:36:34.782014)\n",
            "Accuracy at step 5 - batch 690: 0.904\n",
            "training loss at step 5 - batch 691: 0.34 (2019-08-04 13:36:34.909252)\n",
            "Accuracy at step 5 - batch 691: 0.898\n",
            "training loss at step 5 - batch 692: 0.33 (2019-08-04 13:36:34.925739)\n",
            "Accuracy at step 5 - batch 692: 0.8984\n",
            "training loss at step 5 - batch 693: 0.32 (2019-08-04 13:36:34.943674)\n",
            "Accuracy at step 5 - batch 693: 0.9076\n",
            "training loss at step 5 - batch 694: 0.31 (2019-08-04 13:36:34.956499)\n",
            "Accuracy at step 5 - batch 694: 0.9088\n",
            "training loss at step 5 - batch 695: 0.31 (2019-08-04 13:36:34.968547)\n",
            "Accuracy at step 5 - batch 695: 0.9048\n",
            "training loss at step 5 - batch 696: 0.31 (2019-08-04 13:36:35.091478)\n",
            "Accuracy at step 5 - batch 696: 0.9024\n",
            "training loss at step 5 - batch 697: 0.33 (2019-08-04 13:36:35.105441)\n",
            "Accuracy at step 5 - batch 697: 0.9036\n",
            "training loss at step 5 - batch 698: 0.31 (2019-08-04 13:36:35.117710)\n",
            "Accuracy at step 5 - batch 698: 0.906\n",
            "training loss at step 5 - batch 699: 0.31 (2019-08-04 13:36:35.130565)\n",
            "Accuracy at step 5 - batch 699: 0.9088\n",
            "training loss at step 5 - batch 700: 0.32 (2019-08-04 13:36:35.143481)\n",
            "Accuracy at step 5 - batch 700: 0.8992\n",
            "training loss at step 5 - batch 701: 0.31 (2019-08-04 13:36:35.268436)\n",
            "Accuracy at step 5 - batch 701: 0.906\n",
            "training loss at step 5 - batch 702: 0.32 (2019-08-04 13:36:35.282185)\n",
            "Accuracy at step 5 - batch 702: 0.9044\n",
            "training loss at step 5 - batch 703: 0.31 (2019-08-04 13:36:35.293920)\n",
            "Accuracy at step 5 - batch 703: 0.9068\n",
            "training loss at step 5 - batch 704: 0.32 (2019-08-04 13:36:35.305846)\n",
            "Accuracy at step 5 - batch 704: 0.9064\n",
            "training loss at step 5 - batch 705: 0.34 (2019-08-04 13:36:35.318406)\n",
            "Accuracy at step 5 - batch 705: 0.902\n",
            "training loss at step 5 - batch 706: 0.32 (2019-08-04 13:36:35.437482)\n",
            "Accuracy at step 5 - batch 706: 0.908\n",
            "training loss at step 5 - batch 707: 0.32 (2019-08-04 13:36:35.450260)\n",
            "Accuracy at step 5 - batch 707: 0.9052\n",
            "training loss at step 5 - batch 708: 0.32 (2019-08-04 13:36:35.463180)\n",
            "Accuracy at step 5 - batch 708: 0.9092\n",
            "training loss at step 5 - batch 709: 0.32 (2019-08-04 13:36:35.478353)\n",
            "Accuracy at step 5 - batch 709: 0.9028\n",
            "training loss at step 5 - batch 710: 0.32 (2019-08-04 13:36:35.490907)\n",
            "Accuracy at step 5 - batch 710: 0.9032\n",
            "training loss at step 5 - batch 711: 0.31 (2019-08-04 13:36:35.611543)\n",
            "Accuracy at step 5 - batch 711: 0.904\n",
            "training loss at step 5 - batch 712: 0.34 (2019-08-04 13:36:35.631253)\n",
            "Accuracy at step 5 - batch 712: 0.8976\n",
            "training loss at step 5 - batch 713: 0.34 (2019-08-04 13:36:35.643701)\n",
            "Accuracy at step 5 - batch 713: 0.902\n",
            "training loss at step 5 - batch 714: 0.33 (2019-08-04 13:36:35.656147)\n",
            "Accuracy at step 5 - batch 714: 0.9048\n",
            "training loss at step 5 - batch 715: 0.33 (2019-08-04 13:36:35.669418)\n",
            "Accuracy at step 5 - batch 715: 0.9016\n",
            "training loss at step 5 - batch 716: 0.32 (2019-08-04 13:36:35.789638)\n",
            "Accuracy at step 5 - batch 716: 0.9088\n",
            "training loss at step 5 - batch 717: 0.31 (2019-08-04 13:36:35.804398)\n",
            "Accuracy at step 5 - batch 717: 0.908\n",
            "training loss at step 5 - batch 718: 0.32 (2019-08-04 13:36:35.818999)\n",
            "Accuracy at step 5 - batch 718: 0.9036\n",
            "training loss at step 5 - batch 719: 0.33 (2019-08-04 13:36:35.831492)\n",
            "Accuracy at step 5 - batch 719: 0.9036\n",
            "training loss at step 5 - batch 720: 0.31 (2019-08-04 13:36:35.843962)\n",
            "Accuracy at step 5 - batch 720: 0.9096\n",
            "training loss at step 5 - batch 721: 0.32 (2019-08-04 13:36:35.964641)\n",
            "Accuracy at step 5 - batch 721: 0.9012\n",
            "training loss at step 5 - batch 722: 0.31 (2019-08-04 13:36:35.977328)\n",
            "Accuracy at step 5 - batch 722: 0.9076\n",
            "training loss at step 5 - batch 723: 0.31 (2019-08-04 13:36:35.989996)\n",
            "Accuracy at step 5 - batch 723: 0.9052\n",
            "training loss at step 5 - batch 724: 0.30 (2019-08-04 13:36:36.005766)\n",
            "Accuracy at step 5 - batch 724: 0.9056\n",
            "training loss at step 5 - batch 725: 0.32 (2019-08-04 13:36:36.018135)\n",
            "Accuracy at step 5 - batch 725: 0.9044\n",
            "training loss at step 5 - batch 726: 0.30 (2019-08-04 13:36:36.152942)\n",
            "Accuracy at step 5 - batch 726: 0.9084\n",
            "training loss at step 5 - batch 727: 0.34 (2019-08-04 13:36:36.168465)\n",
            "Accuracy at step 5 - batch 727: 0.8984\n",
            "training loss at step 5 - batch 728: 0.32 (2019-08-04 13:36:36.180628)\n",
            "Accuracy at step 5 - batch 728: 0.9008\n",
            "training loss at step 5 - batch 729: 0.32 (2019-08-04 13:36:36.194926)\n",
            "Accuracy at step 5 - batch 729: 0.9072\n",
            "training loss at step 5 - batch 730: 0.35 (2019-08-04 13:36:36.209498)\n",
            "Accuracy at step 5 - batch 730: 0.8996\n",
            "training loss at step 5 - batch 731: 0.31 (2019-08-04 13:36:36.330254)\n",
            "Accuracy at step 5 - batch 731: 0.8992\n",
            "training loss at step 5 - batch 732: 0.33 (2019-08-04 13:36:36.347442)\n",
            "Accuracy at step 5 - batch 732: 0.9016\n",
            "training loss at step 5 - batch 733: 0.34 (2019-08-04 13:36:36.359639)\n",
            "Accuracy at step 5 - batch 733: 0.9012\n",
            "training loss at step 5 - batch 734: 0.35 (2019-08-04 13:36:36.372889)\n",
            "Accuracy at step 5 - batch 734: 0.8944\n",
            "training loss at step 5 - batch 735: 0.33 (2019-08-04 13:36:36.385348)\n",
            "Accuracy at step 5 - batch 735: 0.902\n",
            "training loss at step 5 - batch 736: 0.32 (2019-08-04 13:36:36.513291)\n",
            "Accuracy at step 5 - batch 736: 0.9052\n",
            "training loss at step 5 - batch 737: 0.33 (2019-08-04 13:36:36.530743)\n",
            "Accuracy at step 5 - batch 737: 0.8988\n",
            "training loss at step 5 - batch 738: 0.32 (2019-08-04 13:36:36.542964)\n",
            "Accuracy at step 5 - batch 738: 0.9084\n",
            "training loss at step 5 - batch 739: 0.32 (2019-08-04 13:36:36.555267)\n",
            "Accuracy at step 5 - batch 739: 0.9008\n",
            "training loss at step 5 - batch 740: 0.33 (2019-08-04 13:36:36.568479)\n",
            "Accuracy at step 5 - batch 740: 0.8988\n",
            "training loss at step 5 - batch 741: 0.32 (2019-08-04 13:36:36.687178)\n",
            "Accuracy at step 5 - batch 741: 0.9\n",
            "training loss at step 5 - batch 742: 0.34 (2019-08-04 13:36:36.699905)\n",
            "Accuracy at step 5 - batch 742: 0.8992\n",
            "training loss at step 5 - batch 743: 0.30 (2019-08-04 13:36:36.713301)\n",
            "Accuracy at step 5 - batch 743: 0.9116\n",
            "training loss at step 5 - batch 744: 0.31 (2019-08-04 13:36:36.732589)\n",
            "Accuracy at step 5 - batch 744: 0.904\n",
            "training loss at step 5 - batch 745: 0.31 (2019-08-04 13:36:36.745907)\n",
            "Accuracy at step 5 - batch 745: 0.9004\n",
            "training loss at step 5 - batch 746: 0.33 (2019-08-04 13:36:36.869554)\n",
            "Accuracy at step 5 - batch 746: 0.9032\n",
            "training loss at step 5 - batch 747: 0.33 (2019-08-04 13:36:36.887785)\n",
            "Accuracy at step 5 - batch 747: 0.902\n",
            "training loss at step 5 - batch 748: 0.32 (2019-08-04 13:36:36.901000)\n",
            "Accuracy at step 5 - batch 748: 0.904\n",
            "training loss at step 5 - batch 749: 0.31 (2019-08-04 13:36:36.914242)\n",
            "Accuracy at step 5 - batch 749: 0.906\n",
            "training loss at step 5 - batch 750: 0.32 (2019-08-04 13:36:36.931042)\n",
            "Accuracy at step 5 - batch 750: 0.9072\n",
            "training loss at step 5 - batch 751: 0.34 (2019-08-04 13:36:37.057996)\n",
            "Accuracy at step 5 - batch 751: 0.9\n",
            "training loss at step 5 - batch 752: 0.31 (2019-08-04 13:36:37.071438)\n",
            "Accuracy at step 5 - batch 752: 0.9068\n",
            "training loss at step 5 - batch 753: 0.30 (2019-08-04 13:36:37.085901)\n",
            "Accuracy at step 5 - batch 753: 0.9124\n",
            "training loss at step 5 - batch 754: 0.33 (2019-08-04 13:36:37.099108)\n",
            "Accuracy at step 5 - batch 754: 0.9044\n",
            "training loss at step 5 - batch 755: 0.33 (2019-08-04 13:36:37.111052)\n",
            "Accuracy at step 5 - batch 755: 0.9076\n",
            "training loss at step 5 - batch 756: 0.32 (2019-08-04 13:36:37.229688)\n",
            "Accuracy at step 5 - batch 756: 0.9044\n",
            "training loss at step 5 - batch 757: 0.31 (2019-08-04 13:36:37.242934)\n",
            "Accuracy at step 5 - batch 757: 0.9064\n",
            "training loss at step 5 - batch 758: 0.33 (2019-08-04 13:36:37.255076)\n",
            "Accuracy at step 5 - batch 758: 0.9056\n",
            "training loss at step 5 - batch 759: 0.32 (2019-08-04 13:36:37.268396)\n",
            "Accuracy at step 5 - batch 759: 0.9052\n",
            "training loss at step 5 - batch 760: 0.32 (2019-08-04 13:36:37.280471)\n",
            "Accuracy at step 5 - batch 760: 0.9048\n",
            "training loss at step 5 - batch 761: 0.32 (2019-08-04 13:36:37.406106)\n",
            "Accuracy at step 5 - batch 761: 0.9024\n",
            "training loss at step 5 - batch 762: 0.33 (2019-08-04 13:36:37.423380)\n",
            "Accuracy at step 5 - batch 762: 0.9036\n",
            "training loss at step 5 - batch 763: 0.33 (2019-08-04 13:36:37.438894)\n",
            "Accuracy at step 5 - batch 763: 0.8972\n",
            "training loss at step 5 - batch 764: 0.30 (2019-08-04 13:36:37.452461)\n",
            "Accuracy at step 5 - batch 764: 0.9132\n",
            "training loss at step 5 - batch 765: 0.32 (2019-08-04 13:36:37.465161)\n",
            "Accuracy at step 5 - batch 765: 0.9056\n",
            "training loss at step 5 - batch 766: 0.33 (2019-08-04 13:36:37.586091)\n",
            "Accuracy at step 5 - batch 766: 0.9024\n",
            "training loss at step 5 - batch 767: 0.32 (2019-08-04 13:36:37.603740)\n",
            "Accuracy at step 5 - batch 767: 0.9036\n",
            "training loss at step 5 - batch 768: 0.30 (2019-08-04 13:36:37.615690)\n",
            "Accuracy at step 5 - batch 768: 0.912\n",
            "training loss at step 5 - batch 769: 0.31 (2019-08-04 13:36:37.627939)\n",
            "Accuracy at step 5 - batch 769: 0.9052\n",
            "training loss at step 5 - batch 770: 0.31 (2019-08-04 13:36:37.640992)\n",
            "Accuracy at step 5 - batch 770: 0.9108\n",
            "training loss at step 5 - batch 771: 0.32 (2019-08-04 13:36:37.762601)\n",
            "Accuracy at step 5 - batch 771: 0.9072\n",
            "training loss at step 5 - batch 772: 0.32 (2019-08-04 13:36:37.778835)\n",
            "Accuracy at step 5 - batch 772: 0.9072\n",
            "training loss at step 5 - batch 773: 0.34 (2019-08-04 13:36:37.791965)\n",
            "Accuracy at step 5 - batch 773: 0.8972\n",
            "training loss at step 5 - batch 774: 0.33 (2019-08-04 13:36:37.804655)\n",
            "Accuracy at step 5 - batch 774: 0.9048\n",
            "training loss at step 5 - batch 775: 0.30 (2019-08-04 13:36:37.817272)\n",
            "Accuracy at step 5 - batch 775: 0.9092\n",
            "training loss at step 5 - batch 776: 0.32 (2019-08-04 13:36:37.946756)\n",
            "Accuracy at step 5 - batch 776: 0.9076\n",
            "training loss at step 5 - batch 777: 0.32 (2019-08-04 13:36:37.958774)\n",
            "Accuracy at step 5 - batch 777: 0.9064\n",
            "training loss at step 5 - batch 778: 0.31 (2019-08-04 13:36:37.974137)\n",
            "Accuracy at step 5 - batch 778: 0.9036\n",
            "training loss at step 5 - batch 779: 0.32 (2019-08-04 13:36:37.988082)\n",
            "Accuracy at step 5 - batch 779: 0.9036\n",
            "training loss at step 6 - batch 0: 0.30 (2019-08-04 13:36:38.001091)\n",
            "Accuracy at step 6 - batch 0: 0.9072\n",
            "training loss at step 6 - batch 1: 0.33 (2019-08-04 13:36:38.129469)\n",
            "Accuracy at step 6 - batch 1: 0.9016\n",
            "training loss at step 6 - batch 2: 0.33 (2019-08-04 13:36:38.147435)\n",
            "Accuracy at step 6 - batch 2: 0.9044\n",
            "training loss at step 6 - batch 3: 0.32 (2019-08-04 13:36:38.162881)\n",
            "Accuracy at step 6 - batch 3: 0.9068\n",
            "training loss at step 6 - batch 4: 0.31 (2019-08-04 13:36:38.176312)\n",
            "Accuracy at step 6 - batch 4: 0.9088\n",
            "training loss at step 6 - batch 5: 0.30 (2019-08-04 13:36:38.190781)\n",
            "Accuracy at step 6 - batch 5: 0.9064\n",
            "training loss at step 6 - batch 6: 0.31 (2019-08-04 13:36:38.312940)\n",
            "Accuracy at step 6 - batch 6: 0.9076\n",
            "training loss at step 6 - batch 7: 0.33 (2019-08-04 13:36:38.328742)\n",
            "Accuracy at step 6 - batch 7: 0.9048\n",
            "training loss at step 6 - batch 8: 0.32 (2019-08-04 13:36:38.341491)\n",
            "Accuracy at step 6 - batch 8: 0.9012\n",
            "training loss at step 6 - batch 9: 0.34 (2019-08-04 13:36:38.359744)\n",
            "Accuracy at step 6 - batch 9: 0.9028\n",
            "training loss at step 6 - batch 10: 0.32 (2019-08-04 13:36:38.373558)\n",
            "Accuracy at step 6 - batch 10: 0.902\n",
            "training loss at step 6 - batch 11: 0.33 (2019-08-04 13:36:38.494544)\n",
            "Accuracy at step 6 - batch 11: 0.902\n",
            "training loss at step 6 - batch 12: 0.31 (2019-08-04 13:36:38.508635)\n",
            "Accuracy at step 6 - batch 12: 0.9044\n",
            "training loss at step 6 - batch 13: 0.34 (2019-08-04 13:36:38.521207)\n",
            "Accuracy at step 6 - batch 13: 0.8984\n",
            "training loss at step 6 - batch 14: 0.34 (2019-08-04 13:36:38.533516)\n",
            "Accuracy at step 6 - batch 14: 0.902\n",
            "training loss at step 6 - batch 15: 0.32 (2019-08-04 13:36:38.547513)\n",
            "Accuracy at step 6 - batch 15: 0.904\n",
            "training loss at step 6 - batch 16: 0.32 (2019-08-04 13:36:38.670773)\n",
            "Accuracy at step 6 - batch 16: 0.9076\n",
            "training loss at step 6 - batch 17: 0.32 (2019-08-04 13:36:38.687969)\n",
            "Accuracy at step 6 - batch 17: 0.9064\n",
            "training loss at step 6 - batch 18: 0.32 (2019-08-04 13:36:38.699627)\n",
            "Accuracy at step 6 - batch 18: 0.906\n",
            "training loss at step 6 - batch 19: 0.31 (2019-08-04 13:36:38.712089)\n",
            "Accuracy at step 6 - batch 19: 0.9104\n",
            "training loss at step 6 - batch 20: 0.30 (2019-08-04 13:36:38.725229)\n",
            "Accuracy at step 6 - batch 20: 0.9116\n",
            "training loss at step 6 - batch 21: 0.34 (2019-08-04 13:36:38.845037)\n",
            "Accuracy at step 6 - batch 21: 0.9036\n",
            "training loss at step 6 - batch 22: 0.31 (2019-08-04 13:36:38.859341)\n",
            "Accuracy at step 6 - batch 22: 0.9064\n",
            "training loss at step 6 - batch 23: 0.34 (2019-08-04 13:36:38.872061)\n",
            "Accuracy at step 6 - batch 23: 0.9008\n",
            "training loss at step 6 - batch 24: 0.32 (2019-08-04 13:36:38.887514)\n",
            "Accuracy at step 6 - batch 24: 0.9016\n",
            "training loss at step 6 - batch 25: 0.30 (2019-08-04 13:36:38.899565)\n",
            "Accuracy at step 6 - batch 25: 0.904\n",
            "training loss at step 6 - batch 26: 0.33 (2019-08-04 13:36:39.020866)\n",
            "Accuracy at step 6 - batch 26: 0.9052\n",
            "training loss at step 6 - batch 27: 0.33 (2019-08-04 13:36:39.034222)\n",
            "Accuracy at step 6 - batch 27: 0.9088\n",
            "training loss at step 6 - batch 28: 0.31 (2019-08-04 13:36:39.047238)\n",
            "Accuracy at step 6 - batch 28: 0.906\n",
            "training loss at step 6 - batch 29: 0.31 (2019-08-04 13:36:39.059507)\n",
            "Accuracy at step 6 - batch 29: 0.9056\n",
            "training loss at step 6 - batch 30: 0.31 (2019-08-04 13:36:39.072293)\n",
            "Accuracy at step 6 - batch 30: 0.9116\n",
            "training loss at step 6 - batch 31: 0.34 (2019-08-04 13:36:39.200473)\n",
            "Accuracy at step 6 - batch 31: 0.9032\n",
            "training loss at step 6 - batch 32: 0.33 (2019-08-04 13:36:39.216162)\n",
            "Accuracy at step 6 - batch 32: 0.9\n",
            "training loss at step 6 - batch 33: 0.29 (2019-08-04 13:36:39.228327)\n",
            "Accuracy at step 6 - batch 33: 0.9128\n",
            "training loss at step 6 - batch 34: 0.31 (2019-08-04 13:36:39.240544)\n",
            "Accuracy at step 6 - batch 34: 0.9064\n",
            "training loss at step 6 - batch 35: 0.34 (2019-08-04 13:36:39.253392)\n",
            "Accuracy at step 6 - batch 35: 0.8964\n",
            "training loss at step 6 - batch 36: 0.32 (2019-08-04 13:36:39.366337)\n",
            "Accuracy at step 6 - batch 36: 0.9088\n",
            "training loss at step 6 - batch 37: 0.32 (2019-08-04 13:36:39.379737)\n",
            "Accuracy at step 6 - batch 37: 0.9052\n",
            "training loss at step 6 - batch 38: 0.31 (2019-08-04 13:36:39.393433)\n",
            "Accuracy at step 6 - batch 38: 0.9032\n",
            "training loss at step 6 - batch 39: 0.34 (2019-08-04 13:36:39.407430)\n",
            "Accuracy at step 6 - batch 39: 0.9004\n",
            "training loss at step 6 - batch 40: 0.32 (2019-08-04 13:36:39.420049)\n",
            "Accuracy at step 6 - batch 40: 0.9052\n",
            "training loss at step 6 - batch 41: 0.33 (2019-08-04 13:36:39.539944)\n",
            "Accuracy at step 6 - batch 41: 0.9088\n",
            "training loss at step 6 - batch 42: 0.32 (2019-08-04 13:36:39.553280)\n",
            "Accuracy at step 6 - batch 42: 0.9064\n",
            "training loss at step 6 - batch 43: 0.33 (2019-08-04 13:36:39.565976)\n",
            "Accuracy at step 6 - batch 43: 0.8992\n",
            "training loss at step 6 - batch 44: 0.34 (2019-08-04 13:36:39.578852)\n",
            "Accuracy at step 6 - batch 44: 0.9\n",
            "training loss at step 6 - batch 45: 0.31 (2019-08-04 13:36:39.592238)\n",
            "Accuracy at step 6 - batch 45: 0.9112\n",
            "training loss at step 6 - batch 46: 0.29 (2019-08-04 13:36:39.715736)\n",
            "Accuracy at step 6 - batch 46: 0.9072\n",
            "training loss at step 6 - batch 47: 0.31 (2019-08-04 13:36:39.728297)\n",
            "Accuracy at step 6 - batch 47: 0.912\n",
            "training loss at step 6 - batch 48: 0.30 (2019-08-04 13:36:39.740417)\n",
            "Accuracy at step 6 - batch 48: 0.9136\n",
            "training loss at step 6 - batch 49: 0.32 (2019-08-04 13:36:39.752465)\n",
            "Accuracy at step 6 - batch 49: 0.9008\n",
            "training loss at step 6 - batch 50: 0.34 (2019-08-04 13:36:39.764559)\n",
            "Accuracy at step 6 - batch 50: 0.898\n",
            "training loss at step 6 - batch 51: 0.33 (2019-08-04 13:36:39.883558)\n",
            "Accuracy at step 6 - batch 51: 0.9016\n",
            "training loss at step 6 - batch 52: 0.33 (2019-08-04 13:36:39.900496)\n",
            "Accuracy at step 6 - batch 52: 0.9068\n",
            "training loss at step 6 - batch 53: 0.30 (2019-08-04 13:36:39.912300)\n",
            "Accuracy at step 6 - batch 53: 0.9068\n",
            "training loss at step 6 - batch 54: 0.33 (2019-08-04 13:36:39.928906)\n",
            "Accuracy at step 6 - batch 54: 0.904\n",
            "training loss at step 6 - batch 55: 0.31 (2019-08-04 13:36:39.942296)\n",
            "Accuracy at step 6 - batch 55: 0.904\n",
            "training loss at step 6 - batch 56: 0.33 (2019-08-04 13:36:40.064341)\n",
            "Accuracy at step 6 - batch 56: 0.9012\n",
            "training loss at step 6 - batch 57: 0.31 (2019-08-04 13:36:40.077234)\n",
            "Accuracy at step 6 - batch 57: 0.9044\n",
            "training loss at step 6 - batch 58: 0.31 (2019-08-04 13:36:40.090263)\n",
            "Accuracy at step 6 - batch 58: 0.9076\n",
            "training loss at step 6 - batch 59: 0.33 (2019-08-04 13:36:40.102867)\n",
            "Accuracy at step 6 - batch 59: 0.9024\n",
            "training loss at step 6 - batch 60: 0.31 (2019-08-04 13:36:40.115277)\n",
            "Accuracy at step 6 - batch 60: 0.9088\n",
            "training loss at step 6 - batch 61: 0.34 (2019-08-04 13:36:40.242836)\n",
            "Accuracy at step 6 - batch 61: 0.8996\n",
            "training loss at step 6 - batch 62: 0.33 (2019-08-04 13:36:40.255420)\n",
            "Accuracy at step 6 - batch 62: 0.902\n",
            "training loss at step 6 - batch 63: 0.34 (2019-08-04 13:36:40.268315)\n",
            "Accuracy at step 6 - batch 63: 0.896\n",
            "training loss at step 6 - batch 64: 0.34 (2019-08-04 13:36:40.280249)\n",
            "Accuracy at step 6 - batch 64: 0.8968\n",
            "training loss at step 6 - batch 65: 0.33 (2019-08-04 13:36:40.292935)\n",
            "Accuracy at step 6 - batch 65: 0.9004\n",
            "training loss at step 6 - batch 66: 0.33 (2019-08-04 13:36:40.410998)\n",
            "Accuracy at step 6 - batch 66: 0.8996\n",
            "training loss at step 6 - batch 67: 0.34 (2019-08-04 13:36:40.427192)\n",
            "Accuracy at step 6 - batch 67: 0.8996\n",
            "training loss at step 6 - batch 68: 0.32 (2019-08-04 13:36:40.439405)\n",
            "Accuracy at step 6 - batch 68: 0.9056\n",
            "training loss at step 6 - batch 69: 0.30 (2019-08-04 13:36:40.454198)\n",
            "Accuracy at step 6 - batch 69: 0.9072\n",
            "training loss at step 6 - batch 70: 0.31 (2019-08-04 13:36:40.467281)\n",
            "Accuracy at step 6 - batch 70: 0.906\n",
            "training loss at step 6 - batch 71: 0.28 (2019-08-04 13:36:40.587988)\n",
            "Accuracy at step 6 - batch 71: 0.916\n",
            "training loss at step 6 - batch 72: 0.30 (2019-08-04 13:36:40.605639)\n",
            "Accuracy at step 6 - batch 72: 0.9064\n",
            "training loss at step 6 - batch 73: 0.32 (2019-08-04 13:36:40.617764)\n",
            "Accuracy at step 6 - batch 73: 0.9024\n",
            "training loss at step 6 - batch 74: 0.31 (2019-08-04 13:36:40.630268)\n",
            "Accuracy at step 6 - batch 74: 0.9096\n",
            "training loss at step 6 - batch 75: 0.34 (2019-08-04 13:36:40.643117)\n",
            "Accuracy at step 6 - batch 75: 0.9004\n",
            "training loss at step 6 - batch 76: 0.31 (2019-08-04 13:36:40.778287)\n",
            "Accuracy at step 6 - batch 76: 0.9104\n",
            "training loss at step 6 - batch 77: 0.35 (2019-08-04 13:36:40.792997)\n",
            "Accuracy at step 6 - batch 77: 0.9008\n",
            "training loss at step 6 - batch 78: 0.31 (2019-08-04 13:36:40.809836)\n",
            "Accuracy at step 6 - batch 78: 0.904\n",
            "training loss at step 6 - batch 79: 0.33 (2019-08-04 13:36:40.826583)\n",
            "Accuracy at step 6 - batch 79: 0.9044\n",
            "training loss at step 6 - batch 80: 0.33 (2019-08-04 13:36:40.839132)\n",
            "Accuracy at step 6 - batch 80: 0.9\n",
            "training loss at step 6 - batch 81: 0.32 (2019-08-04 13:36:40.962846)\n",
            "Accuracy at step 6 - batch 81: 0.9044\n",
            "training loss at step 6 - batch 82: 0.31 (2019-08-04 13:36:40.975454)\n",
            "Accuracy at step 6 - batch 82: 0.904\n",
            "training loss at step 6 - batch 83: 0.31 (2019-08-04 13:36:40.991597)\n",
            "Accuracy at step 6 - batch 83: 0.906\n",
            "training loss at step 6 - batch 84: 0.31 (2019-08-04 13:36:41.005643)\n",
            "Accuracy at step 6 - batch 84: 0.9056\n",
            "training loss at step 6 - batch 85: 0.31 (2019-08-04 13:36:41.018236)\n",
            "Accuracy at step 6 - batch 85: 0.9072\n",
            "training loss at step 6 - batch 86: 0.33 (2019-08-04 13:36:41.160931)\n",
            "Accuracy at step 6 - batch 86: 0.9016\n",
            "training loss at step 6 - batch 87: 0.35 (2019-08-04 13:36:41.174831)\n",
            "Accuracy at step 6 - batch 87: 0.8936\n",
            "training loss at step 6 - batch 88: 0.31 (2019-08-04 13:36:41.187862)\n",
            "Accuracy at step 6 - batch 88: 0.908\n",
            "training loss at step 6 - batch 89: 0.32 (2019-08-04 13:36:41.202110)\n",
            "Accuracy at step 6 - batch 89: 0.9064\n",
            "training loss at step 6 - batch 90: 0.32 (2019-08-04 13:36:41.215114)\n",
            "Accuracy at step 6 - batch 90: 0.9032\n",
            "training loss at step 6 - batch 91: 0.31 (2019-08-04 13:36:41.334150)\n",
            "Accuracy at step 6 - batch 91: 0.908\n",
            "training loss at step 6 - batch 92: 0.30 (2019-08-04 13:36:41.349537)\n",
            "Accuracy at step 6 - batch 92: 0.9112\n",
            "training loss at step 6 - batch 93: 0.31 (2019-08-04 13:36:41.361728)\n",
            "Accuracy at step 6 - batch 93: 0.9072\n",
            "training loss at step 6 - batch 94: 0.34 (2019-08-04 13:36:41.375917)\n",
            "Accuracy at step 6 - batch 94: 0.9012\n",
            "training loss at step 6 - batch 95: 0.32 (2019-08-04 13:36:41.389234)\n",
            "Accuracy at step 6 - batch 95: 0.902\n",
            "training loss at step 6 - batch 96: 0.33 (2019-08-04 13:36:41.512871)\n",
            "Accuracy at step 6 - batch 96: 0.8976\n",
            "training loss at step 6 - batch 97: 0.31 (2019-08-04 13:36:41.529930)\n",
            "Accuracy at step 6 - batch 97: 0.9048\n",
            "training loss at step 6 - batch 98: 0.30 (2019-08-04 13:36:41.542308)\n",
            "Accuracy at step 6 - batch 98: 0.9092\n",
            "training loss at step 6 - batch 99: 0.33 (2019-08-04 13:36:41.555169)\n",
            "Accuracy at step 6 - batch 99: 0.9016\n",
            "training loss at step 6 - batch 100: 0.34 (2019-08-04 13:36:41.567220)\n",
            "Accuracy at step 6 - batch 100: 0.898\n",
            "training loss at step 6 - batch 101: 0.32 (2019-08-04 13:36:41.688887)\n",
            "Accuracy at step 6 - batch 101: 0.9048\n",
            "training loss at step 6 - batch 102: 0.32 (2019-08-04 13:36:41.706490)\n",
            "Accuracy at step 6 - batch 102: 0.9028\n",
            "training loss at step 6 - batch 103: 0.32 (2019-08-04 13:36:41.721003)\n",
            "Accuracy at step 6 - batch 103: 0.9072\n",
            "training loss at step 6 - batch 104: 0.34 (2019-08-04 13:36:41.733115)\n",
            "Accuracy at step 6 - batch 104: 0.9004\n",
            "training loss at step 6 - batch 105: 0.31 (2019-08-04 13:36:41.745921)\n",
            "Accuracy at step 6 - batch 105: 0.9072\n",
            "training loss at step 6 - batch 106: 0.30 (2019-08-04 13:36:41.862628)\n",
            "Accuracy at step 6 - batch 106: 0.9108\n",
            "training loss at step 6 - batch 107: 0.33 (2019-08-04 13:36:41.875432)\n",
            "Accuracy at step 6 - batch 107: 0.902\n",
            "training loss at step 6 - batch 108: 0.31 (2019-08-04 13:36:41.889635)\n",
            "Accuracy at step 6 - batch 108: 0.9036\n",
            "training loss at step 6 - batch 109: 0.31 (2019-08-04 13:36:41.902215)\n",
            "Accuracy at step 6 - batch 109: 0.898\n",
            "training loss at step 6 - batch 110: 0.35 (2019-08-04 13:36:41.915669)\n",
            "Accuracy at step 6 - batch 110: 0.902\n",
            "training loss at step 6 - batch 111: 0.32 (2019-08-04 13:36:42.042327)\n",
            "Accuracy at step 6 - batch 111: 0.9032\n",
            "training loss at step 6 - batch 112: 0.31 (2019-08-04 13:36:42.057379)\n",
            "Accuracy at step 6 - batch 112: 0.9072\n",
            "training loss at step 6 - batch 113: 0.34 (2019-08-04 13:36:42.074885)\n",
            "Accuracy at step 6 - batch 113: 0.9028\n",
            "training loss at step 6 - batch 114: 0.30 (2019-08-04 13:36:42.088872)\n",
            "Accuracy at step 6 - batch 114: 0.9052\n",
            "training loss at step 6 - batch 115: 0.32 (2019-08-04 13:36:42.102546)\n",
            "Accuracy at step 6 - batch 115: 0.9076\n",
            "training loss at step 6 - batch 116: 0.30 (2019-08-04 13:36:42.222902)\n",
            "Accuracy at step 6 - batch 116: 0.9108\n",
            "training loss at step 6 - batch 117: 0.32 (2019-08-04 13:36:42.242127)\n",
            "Accuracy at step 6 - batch 117: 0.9088\n",
            "training loss at step 6 - batch 118: 0.31 (2019-08-04 13:36:42.257609)\n",
            "Accuracy at step 6 - batch 118: 0.9068\n",
            "training loss at step 6 - batch 119: 0.31 (2019-08-04 13:36:42.269654)\n",
            "Accuracy at step 6 - batch 119: 0.908\n",
            "training loss at step 6 - batch 120: 0.32 (2019-08-04 13:36:42.282199)\n",
            "Accuracy at step 6 - batch 120: 0.9068\n",
            "training loss at step 6 - batch 121: 0.32 (2019-08-04 13:36:42.399673)\n",
            "Accuracy at step 6 - batch 121: 0.9032\n",
            "training loss at step 6 - batch 122: 0.31 (2019-08-04 13:36:42.412386)\n",
            "Accuracy at step 6 - batch 122: 0.9044\n",
            "training loss at step 6 - batch 123: 0.32 (2019-08-04 13:36:42.424317)\n",
            "Accuracy at step 6 - batch 123: 0.9012\n",
            "training loss at step 6 - batch 124: 0.33 (2019-08-04 13:36:42.436531)\n",
            "Accuracy at step 6 - batch 124: 0.9036\n",
            "training loss at step 6 - batch 125: 0.31 (2019-08-04 13:36:42.448770)\n",
            "Accuracy at step 6 - batch 125: 0.9056\n",
            "training loss at step 6 - batch 126: 0.31 (2019-08-04 13:36:42.569975)\n",
            "Accuracy at step 6 - batch 126: 0.904\n",
            "training loss at step 6 - batch 127: 0.31 (2019-08-04 13:36:42.583179)\n",
            "Accuracy at step 6 - batch 127: 0.9084\n",
            "training loss at step 6 - batch 128: 0.33 (2019-08-04 13:36:42.596168)\n",
            "Accuracy at step 6 - batch 128: 0.9012\n",
            "training loss at step 6 - batch 129: 0.32 (2019-08-04 13:36:42.609251)\n",
            "Accuracy at step 6 - batch 129: 0.9008\n",
            "training loss at step 6 - batch 130: 0.32 (2019-08-04 13:36:42.628388)\n",
            "Accuracy at step 6 - batch 130: 0.8996\n",
            "training loss at step 6 - batch 131: 0.32 (2019-08-04 13:36:42.746309)\n",
            "Accuracy at step 6 - batch 131: 0.912\n",
            "training loss at step 6 - batch 132: 0.30 (2019-08-04 13:36:42.758688)\n",
            "Accuracy at step 6 - batch 132: 0.914\n",
            "training loss at step 6 - batch 133: 0.33 (2019-08-04 13:36:42.771690)\n",
            "Accuracy at step 6 - batch 133: 0.8992\n",
            "training loss at step 6 - batch 134: 0.35 (2019-08-04 13:36:42.786760)\n",
            "Accuracy at step 6 - batch 134: 0.8972\n",
            "training loss at step 6 - batch 135: 0.32 (2019-08-04 13:36:42.800469)\n",
            "Accuracy at step 6 - batch 135: 0.9072\n",
            "training loss at step 6 - batch 136: 0.33 (2019-08-04 13:36:42.920365)\n",
            "Accuracy at step 6 - batch 136: 0.9032\n",
            "training loss at step 6 - batch 137: 0.35 (2019-08-04 13:36:42.934538)\n",
            "Accuracy at step 6 - batch 137: 0.8892\n",
            "training loss at step 6 - batch 138: 0.33 (2019-08-04 13:36:42.948240)\n",
            "Accuracy at step 6 - batch 138: 0.9032\n",
            "training loss at step 6 - batch 139: 0.31 (2019-08-04 13:36:42.961667)\n",
            "Accuracy at step 6 - batch 139: 0.9088\n",
            "training loss at step 6 - batch 140: 0.32 (2019-08-04 13:36:42.976469)\n",
            "Accuracy at step 6 - batch 140: 0.9048\n",
            "training loss at step 6 - batch 141: 0.33 (2019-08-04 13:36:43.104897)\n",
            "Accuracy at step 6 - batch 141: 0.9008\n",
            "training loss at step 6 - batch 142: 0.31 (2019-08-04 13:36:43.119136)\n",
            "Accuracy at step 6 - batch 142: 0.9052\n",
            "training loss at step 6 - batch 143: 0.33 (2019-08-04 13:36:43.133045)\n",
            "Accuracy at step 6 - batch 143: 0.9008\n",
            "training loss at step 6 - batch 144: 0.31 (2019-08-04 13:36:43.146238)\n",
            "Accuracy at step 6 - batch 144: 0.9084\n",
            "training loss at step 6 - batch 145: 0.32 (2019-08-04 13:36:43.159347)\n",
            "Accuracy at step 6 - batch 145: 0.9016\n",
            "training loss at step 6 - batch 146: 0.32 (2019-08-04 13:36:43.287900)\n",
            "Accuracy at step 6 - batch 146: 0.9024\n",
            "training loss at step 6 - batch 147: 0.31 (2019-08-04 13:36:43.304394)\n",
            "Accuracy at step 6 - batch 147: 0.906\n",
            "training loss at step 6 - batch 148: 0.32 (2019-08-04 13:36:43.317340)\n",
            "Accuracy at step 6 - batch 148: 0.9076\n",
            "training loss at step 6 - batch 149: 0.30 (2019-08-04 13:36:43.330211)\n",
            "Accuracy at step 6 - batch 149: 0.9124\n",
            "training loss at step 6 - batch 150: 0.30 (2019-08-04 13:36:43.343274)\n",
            "Accuracy at step 6 - batch 150: 0.9076\n",
            "training loss at step 6 - batch 151: 0.33 (2019-08-04 13:36:43.463966)\n",
            "Accuracy at step 6 - batch 151: 0.9032\n",
            "training loss at step 6 - batch 152: 0.32 (2019-08-04 13:36:43.478042)\n",
            "Accuracy at step 6 - batch 152: 0.9064\n",
            "training loss at step 6 - batch 153: 0.34 (2019-08-04 13:36:43.493449)\n",
            "Accuracy at step 6 - batch 153: 0.9048\n",
            "training loss at step 6 - batch 154: 0.31 (2019-08-04 13:36:43.506440)\n",
            "Accuracy at step 6 - batch 154: 0.906\n",
            "training loss at step 6 - batch 155: 0.31 (2019-08-04 13:36:43.519521)\n",
            "Accuracy at step 6 - batch 155: 0.908\n",
            "training loss at step 6 - batch 156: 0.32 (2019-08-04 13:36:43.640379)\n",
            "Accuracy at step 6 - batch 156: 0.9052\n",
            "training loss at step 6 - batch 157: 0.31 (2019-08-04 13:36:43.652970)\n",
            "Accuracy at step 6 - batch 157: 0.9088\n",
            "training loss at step 6 - batch 158: 0.31 (2019-08-04 13:36:43.666284)\n",
            "Accuracy at step 6 - batch 158: 0.9108\n",
            "training loss at step 6 - batch 159: 0.32 (2019-08-04 13:36:43.678476)\n",
            "Accuracy at step 6 - batch 159: 0.8988\n",
            "training loss at step 6 - batch 160: 0.32 (2019-08-04 13:36:43.692004)\n",
            "Accuracy at step 6 - batch 160: 0.9044\n",
            "training loss at step 6 - batch 161: 0.31 (2019-08-04 13:36:43.815192)\n",
            "Accuracy at step 6 - batch 161: 0.91\n",
            "training loss at step 6 - batch 162: 0.32 (2019-08-04 13:36:43.829578)\n",
            "Accuracy at step 6 - batch 162: 0.9068\n",
            "training loss at step 6 - batch 163: 0.30 (2019-08-04 13:36:43.842204)\n",
            "Accuracy at step 6 - batch 163: 0.9072\n",
            "training loss at step 6 - batch 164: 0.33 (2019-08-04 13:36:43.854616)\n",
            "Accuracy at step 6 - batch 164: 0.9024\n",
            "training loss at step 6 - batch 165: 0.32 (2019-08-04 13:36:43.868377)\n",
            "Accuracy at step 6 - batch 165: 0.9056\n",
            "training loss at step 6 - batch 166: 0.33 (2019-08-04 13:36:43.983078)\n",
            "Accuracy at step 6 - batch 166: 0.9012\n",
            "training loss at step 6 - batch 167: 0.34 (2019-08-04 13:36:43.995655)\n",
            "Accuracy at step 6 - batch 167: 0.8988\n",
            "training loss at step 6 - batch 168: 0.34 (2019-08-04 13:36:44.008686)\n",
            "Accuracy at step 6 - batch 168: 0.8968\n",
            "training loss at step 6 - batch 169: 0.32 (2019-08-04 13:36:44.023220)\n",
            "Accuracy at step 6 - batch 169: 0.9028\n",
            "training loss at step 6 - batch 170: 0.32 (2019-08-04 13:36:44.037361)\n",
            "Accuracy at step 6 - batch 170: 0.9036\n",
            "training loss at step 6 - batch 171: 0.31 (2019-08-04 13:36:44.172987)\n",
            "Accuracy at step 6 - batch 171: 0.9088\n",
            "training loss at step 6 - batch 172: 0.33 (2019-08-04 13:36:44.185417)\n",
            "Accuracy at step 6 - batch 172: 0.8976\n",
            "training loss at step 6 - batch 173: 0.30 (2019-08-04 13:36:44.198095)\n",
            "Accuracy at step 6 - batch 173: 0.9116\n",
            "training loss at step 6 - batch 174: 0.33 (2019-08-04 13:36:44.210618)\n",
            "Accuracy at step 6 - batch 174: 0.9056\n",
            "training loss at step 6 - batch 175: 0.29 (2019-08-04 13:36:44.223146)\n",
            "Accuracy at step 6 - batch 175: 0.9104\n",
            "training loss at step 6 - batch 176: 0.33 (2019-08-04 13:36:44.348655)\n",
            "Accuracy at step 6 - batch 176: 0.9032\n",
            "training loss at step 6 - batch 177: 0.32 (2019-08-04 13:36:44.364251)\n",
            "Accuracy at step 6 - batch 177: 0.9072\n",
            "training loss at step 6 - batch 178: 0.31 (2019-08-04 13:36:44.376273)\n",
            "Accuracy at step 6 - batch 178: 0.9064\n",
            "training loss at step 6 - batch 179: 0.31 (2019-08-04 13:36:44.388756)\n",
            "Accuracy at step 6 - batch 179: 0.904\n",
            "training loss at step 6 - batch 180: 0.31 (2019-08-04 13:36:44.401232)\n",
            "Accuracy at step 6 - batch 180: 0.9048\n",
            "training loss at step 6 - batch 181: 0.33 (2019-08-04 13:36:44.520991)\n",
            "Accuracy at step 6 - batch 181: 0.8992\n",
            "training loss at step 6 - batch 182: 0.31 (2019-08-04 13:36:44.534051)\n",
            "Accuracy at step 6 - batch 182: 0.9072\n",
            "training loss at step 6 - batch 183: 0.32 (2019-08-04 13:36:44.547302)\n",
            "Accuracy at step 6 - batch 183: 0.9056\n",
            "training loss at step 6 - batch 184: 0.33 (2019-08-04 13:36:44.562333)\n",
            "Accuracy at step 6 - batch 184: 0.9028\n",
            "training loss at step 6 - batch 185: 0.31 (2019-08-04 13:36:44.575931)\n",
            "Accuracy at step 6 - batch 185: 0.906\n",
            "training loss at step 6 - batch 186: 0.30 (2019-08-04 13:36:44.698576)\n",
            "Accuracy at step 6 - batch 186: 0.9096\n",
            "training loss at step 6 - batch 187: 0.31 (2019-08-04 13:36:44.714425)\n",
            "Accuracy at step 6 - batch 187: 0.9048\n",
            "training loss at step 6 - batch 188: 0.34 (2019-08-04 13:36:44.728255)\n",
            "Accuracy at step 6 - batch 188: 0.8976\n",
            "training loss at step 6 - batch 189: 0.32 (2019-08-04 13:36:44.740895)\n",
            "Accuracy at step 6 - batch 189: 0.908\n",
            "training loss at step 6 - batch 190: 0.32 (2019-08-04 13:36:44.754459)\n",
            "Accuracy at step 6 - batch 190: 0.9036\n",
            "training loss at step 6 - batch 191: 0.36 (2019-08-04 13:36:44.879045)\n",
            "Accuracy at step 6 - batch 191: 0.892\n",
            "training loss at step 6 - batch 192: 0.30 (2019-08-04 13:36:44.895014)\n",
            "Accuracy at step 6 - batch 192: 0.9064\n",
            "training loss at step 6 - batch 193: 0.32 (2019-08-04 13:36:44.907704)\n",
            "Accuracy at step 6 - batch 193: 0.9088\n",
            "training loss at step 6 - batch 194: 0.33 (2019-08-04 13:36:44.921002)\n",
            "Accuracy at step 6 - batch 194: 0.9044\n",
            "training loss at step 6 - batch 195: 0.33 (2019-08-04 13:36:44.932992)\n",
            "Accuracy at step 6 - batch 195: 0.8964\n",
            "training loss at step 6 - batch 196: 0.32 (2019-08-04 13:36:45.052473)\n",
            "Accuracy at step 6 - batch 196: 0.9044\n",
            "training loss at step 6 - batch 197: 0.31 (2019-08-04 13:36:45.064563)\n",
            "Accuracy at step 6 - batch 197: 0.9088\n",
            "training loss at step 6 - batch 198: 0.31 (2019-08-04 13:36:45.076985)\n",
            "Accuracy at step 6 - batch 198: 0.908\n",
            "training loss at step 6 - batch 199: 0.31 (2019-08-04 13:36:45.093007)\n",
            "Accuracy at step 6 - batch 199: 0.9064\n",
            "training loss at step 6 - batch 200: 0.33 (2019-08-04 13:36:45.106331)\n",
            "Accuracy at step 6 - batch 200: 0.902\n",
            "training loss at step 6 - batch 201: 0.33 (2019-08-04 13:36:45.229673)\n",
            "Accuracy at step 6 - batch 201: 0.9044\n",
            "training loss at step 6 - batch 202: 0.33 (2019-08-04 13:36:45.243235)\n",
            "Accuracy at step 6 - batch 202: 0.8944\n",
            "training loss at step 6 - batch 203: 0.34 (2019-08-04 13:36:45.256272)\n",
            "Accuracy at step 6 - batch 203: 0.9012\n",
            "training loss at step 6 - batch 204: 0.32 (2019-08-04 13:36:45.268271)\n",
            "Accuracy at step 6 - batch 204: 0.908\n",
            "training loss at step 6 - batch 205: 0.33 (2019-08-04 13:36:45.281443)\n",
            "Accuracy at step 6 - batch 205: 0.8992\n",
            "training loss at step 6 - batch 206: 0.32 (2019-08-04 13:36:45.405886)\n",
            "Accuracy at step 6 - batch 206: 0.9016\n",
            "training loss at step 6 - batch 207: 0.31 (2019-08-04 13:36:45.418690)\n",
            "Accuracy at step 6 - batch 207: 0.9068\n",
            "training loss at step 6 - batch 208: 0.29 (2019-08-04 13:36:45.430878)\n",
            "Accuracy at step 6 - batch 208: 0.9096\n",
            "training loss at step 6 - batch 209: 0.31 (2019-08-04 13:36:45.443595)\n",
            "Accuracy at step 6 - batch 209: 0.904\n",
            "training loss at step 6 - batch 210: 0.32 (2019-08-04 13:36:45.456432)\n",
            "Accuracy at step 6 - batch 210: 0.9044\n",
            "training loss at step 6 - batch 211: 0.33 (2019-08-04 13:36:45.573265)\n",
            "Accuracy at step 6 - batch 211: 0.902\n",
            "training loss at step 6 - batch 212: 0.29 (2019-08-04 13:36:45.588180)\n",
            "Accuracy at step 6 - batch 212: 0.9148\n",
            "training loss at step 6 - batch 213: 0.31 (2019-08-04 13:36:45.600724)\n",
            "Accuracy at step 6 - batch 213: 0.902\n",
            "training loss at step 6 - batch 214: 0.31 (2019-08-04 13:36:45.616213)\n",
            "Accuracy at step 6 - batch 214: 0.9104\n",
            "training loss at step 6 - batch 215: 0.31 (2019-08-04 13:36:45.629401)\n",
            "Accuracy at step 6 - batch 215: 0.9068\n",
            "training loss at step 6 - batch 216: 0.33 (2019-08-04 13:36:45.751892)\n",
            "Accuracy at step 6 - batch 216: 0.9008\n",
            "training loss at step 6 - batch 217: 0.29 (2019-08-04 13:36:45.767865)\n",
            "Accuracy at step 6 - batch 217: 0.9144\n",
            "training loss at step 6 - batch 218: 0.31 (2019-08-04 13:36:45.781407)\n",
            "Accuracy at step 6 - batch 218: 0.9092\n",
            "training loss at step 6 - batch 219: 0.31 (2019-08-04 13:36:45.794399)\n",
            "Accuracy at step 6 - batch 219: 0.904\n",
            "training loss at step 6 - batch 220: 0.31 (2019-08-04 13:36:45.809690)\n",
            "Accuracy at step 6 - batch 220: 0.908\n",
            "training loss at step 6 - batch 221: 0.32 (2019-08-04 13:36:45.933988)\n",
            "Accuracy at step 6 - batch 221: 0.9036\n",
            "training loss at step 6 - batch 222: 0.30 (2019-08-04 13:36:45.948634)\n",
            "Accuracy at step 6 - batch 222: 0.9072\n",
            "training loss at step 6 - batch 223: 0.31 (2019-08-04 13:36:45.961068)\n",
            "Accuracy at step 6 - batch 223: 0.9124\n",
            "training loss at step 6 - batch 224: 0.33 (2019-08-04 13:36:45.973778)\n",
            "Accuracy at step 6 - batch 224: 0.8992\n",
            "training loss at step 6 - batch 225: 0.31 (2019-08-04 13:36:45.985735)\n",
            "Accuracy at step 6 - batch 225: 0.9052\n",
            "training loss at step 6 - batch 226: 0.34 (2019-08-04 13:36:46.110714)\n",
            "Accuracy at step 6 - batch 226: 0.8976\n",
            "training loss at step 6 - batch 227: 0.32 (2019-08-04 13:36:46.127004)\n",
            "Accuracy at step 6 - batch 227: 0.906\n",
            "training loss at step 6 - batch 228: 0.33 (2019-08-04 13:36:46.143716)\n",
            "Accuracy at step 6 - batch 228: 0.9072\n",
            "training loss at step 6 - batch 229: 0.32 (2019-08-04 13:36:46.163046)\n",
            "Accuracy at step 6 - batch 229: 0.9056\n",
            "training loss at step 6 - batch 230: 0.34 (2019-08-04 13:36:46.177630)\n",
            "Accuracy at step 6 - batch 230: 0.9012\n",
            "training loss at step 6 - batch 231: 0.33 (2019-08-04 13:36:46.305387)\n",
            "Accuracy at step 6 - batch 231: 0.8992\n",
            "training loss at step 6 - batch 232: 0.32 (2019-08-04 13:36:46.322315)\n",
            "Accuracy at step 6 - batch 232: 0.9052\n",
            "training loss at step 6 - batch 233: 0.32 (2019-08-04 13:36:46.335441)\n",
            "Accuracy at step 6 - batch 233: 0.9088\n",
            "training loss at step 6 - batch 234: 0.32 (2019-08-04 13:36:46.350436)\n",
            "Accuracy at step 6 - batch 234: 0.9064\n",
            "training loss at step 6 - batch 235: 0.32 (2019-08-04 13:36:46.362633)\n",
            "Accuracy at step 6 - batch 235: 0.9048\n",
            "training loss at step 6 - batch 236: 0.32 (2019-08-04 13:36:46.481750)\n",
            "Accuracy at step 6 - batch 236: 0.9104\n",
            "training loss at step 6 - batch 237: 0.32 (2019-08-04 13:36:46.494038)\n",
            "Accuracy at step 6 - batch 237: 0.9052\n",
            "training loss at step 6 - batch 238: 0.31 (2019-08-04 13:36:46.507119)\n",
            "Accuracy at step 6 - batch 238: 0.9048\n",
            "training loss at step 6 - batch 239: 0.32 (2019-08-04 13:36:46.520788)\n",
            "Accuracy at step 6 - batch 239: 0.902\n",
            "training loss at step 6 - batch 240: 0.31 (2019-08-04 13:36:46.534225)\n",
            "Accuracy at step 6 - batch 240: 0.9032\n",
            "training loss at step 6 - batch 241: 0.29 (2019-08-04 13:36:46.656734)\n",
            "Accuracy at step 6 - batch 241: 0.9108\n",
            "training loss at step 6 - batch 242: 0.30 (2019-08-04 13:36:46.671580)\n",
            "Accuracy at step 6 - batch 242: 0.9076\n",
            "training loss at step 6 - batch 243: 0.31 (2019-08-04 13:36:46.684049)\n",
            "Accuracy at step 6 - batch 243: 0.9072\n",
            "training loss at step 6 - batch 244: 0.32 (2019-08-04 13:36:46.696323)\n",
            "Accuracy at step 6 - batch 244: 0.9084\n",
            "training loss at step 6 - batch 245: 0.32 (2019-08-04 13:36:46.710214)\n",
            "Accuracy at step 6 - batch 245: 0.9076\n",
            "training loss at step 6 - batch 246: 0.33 (2019-08-04 13:36:46.832818)\n",
            "Accuracy at step 6 - batch 246: 0.9024\n",
            "training loss at step 6 - batch 247: 0.30 (2019-08-04 13:36:46.845033)\n",
            "Accuracy at step 6 - batch 247: 0.91\n",
            "training loss at step 6 - batch 248: 0.33 (2019-08-04 13:36:46.857462)\n",
            "Accuracy at step 6 - batch 248: 0.9056\n",
            "training loss at step 6 - batch 249: 0.31 (2019-08-04 13:36:46.872798)\n",
            "Accuracy at step 6 - batch 249: 0.9088\n",
            "training loss at step 6 - batch 250: 0.35 (2019-08-04 13:36:46.885385)\n",
            "Accuracy at step 6 - batch 250: 0.8956\n",
            "training loss at step 6 - batch 251: 0.32 (2019-08-04 13:36:47.006565)\n",
            "Accuracy at step 6 - batch 251: 0.9116\n",
            "training loss at step 6 - batch 252: 0.30 (2019-08-04 13:36:47.023237)\n",
            "Accuracy at step 6 - batch 252: 0.9096\n",
            "training loss at step 6 - batch 253: 0.31 (2019-08-04 13:36:47.035266)\n",
            "Accuracy at step 6 - batch 253: 0.9104\n",
            "training loss at step 6 - batch 254: 0.33 (2019-08-04 13:36:47.047860)\n",
            "Accuracy at step 6 - batch 254: 0.9036\n",
            "training loss at step 6 - batch 255: 0.32 (2019-08-04 13:36:47.063047)\n",
            "Accuracy at step 6 - batch 255: 0.9048\n",
            "training loss at step 6 - batch 256: 0.31 (2019-08-04 13:36:47.187356)\n",
            "Accuracy at step 6 - batch 256: 0.9056\n",
            "training loss at step 6 - batch 257: 0.30 (2019-08-04 13:36:47.202066)\n",
            "Accuracy at step 6 - batch 257: 0.9156\n",
            "training loss at step 6 - batch 258: 0.31 (2019-08-04 13:36:47.215224)\n",
            "Accuracy at step 6 - batch 258: 0.9076\n",
            "training loss at step 6 - batch 259: 0.32 (2019-08-04 13:36:47.227169)\n",
            "Accuracy at step 6 - batch 259: 0.9104\n",
            "training loss at step 6 - batch 260: 0.31 (2019-08-04 13:36:47.240214)\n",
            "Accuracy at step 6 - batch 260: 0.906\n",
            "training loss at step 6 - batch 261: 0.31 (2019-08-04 13:36:47.363866)\n",
            "Accuracy at step 6 - batch 261: 0.906\n",
            "training loss at step 6 - batch 262: 0.33 (2019-08-04 13:36:47.377364)\n",
            "Accuracy at step 6 - batch 262: 0.9048\n",
            "training loss at step 6 - batch 263: 0.31 (2019-08-04 13:36:47.390355)\n",
            "Accuracy at step 6 - batch 263: 0.9016\n",
            "training loss at step 6 - batch 264: 0.32 (2019-08-04 13:36:47.403063)\n",
            "Accuracy at step 6 - batch 264: 0.9064\n",
            "training loss at step 6 - batch 265: 0.32 (2019-08-04 13:36:47.416140)\n",
            "Accuracy at step 6 - batch 265: 0.9052\n",
            "training loss at step 6 - batch 266: 0.34 (2019-08-04 13:36:47.536448)\n",
            "Accuracy at step 6 - batch 266: 0.9032\n",
            "training loss at step 6 - batch 267: 0.32 (2019-08-04 13:36:47.554894)\n",
            "Accuracy at step 6 - batch 267: 0.9032\n",
            "training loss at step 6 - batch 268: 0.30 (2019-08-04 13:36:47.569499)\n",
            "Accuracy at step 6 - batch 268: 0.9084\n",
            "training loss at step 6 - batch 269: 0.32 (2019-08-04 13:36:47.587412)\n",
            "Accuracy at step 6 - batch 269: 0.904\n",
            "training loss at step 6 - batch 270: 0.33 (2019-08-04 13:36:47.600172)\n",
            "Accuracy at step 6 - batch 270: 0.9036\n",
            "training loss at step 6 - batch 271: 0.33 (2019-08-04 13:36:47.720646)\n",
            "Accuracy at step 6 - batch 271: 0.8996\n",
            "training loss at step 6 - batch 272: 0.32 (2019-08-04 13:36:47.732560)\n",
            "Accuracy at step 6 - batch 272: 0.9076\n",
            "training loss at step 6 - batch 273: 0.32 (2019-08-04 13:36:47.746399)\n",
            "Accuracy at step 6 - batch 273: 0.9048\n",
            "training loss at step 6 - batch 274: 0.32 (2019-08-04 13:36:47.759253)\n",
            "Accuracy at step 6 - batch 274: 0.9024\n",
            "training loss at step 6 - batch 275: 0.33 (2019-08-04 13:36:47.774691)\n",
            "Accuracy at step 6 - batch 275: 0.902\n",
            "training loss at step 6 - batch 276: 0.31 (2019-08-04 13:36:47.894970)\n",
            "Accuracy at step 6 - batch 276: 0.906\n",
            "training loss at step 6 - batch 277: 0.30 (2019-08-04 13:36:47.908231)\n",
            "Accuracy at step 6 - batch 277: 0.914\n",
            "training loss at step 6 - batch 278: 0.31 (2019-08-04 13:36:47.921950)\n",
            "Accuracy at step 6 - batch 278: 0.9032\n",
            "training loss at step 6 - batch 279: 0.31 (2019-08-04 13:36:47.933945)\n",
            "Accuracy at step 6 - batch 279: 0.9092\n",
            "training loss at step 6 - batch 280: 0.34 (2019-08-04 13:36:47.945832)\n",
            "Accuracy at step 6 - batch 280: 0.8992\n",
            "training loss at step 6 - batch 281: 0.33 (2019-08-04 13:36:48.070398)\n",
            "Accuracy at step 6 - batch 281: 0.9044\n",
            "training loss at step 6 - batch 282: 0.33 (2019-08-04 13:36:48.084144)\n",
            "Accuracy at step 6 - batch 282: 0.9004\n",
            "training loss at step 6 - batch 283: 0.33 (2019-08-04 13:36:48.097353)\n",
            "Accuracy at step 6 - batch 283: 0.9068\n",
            "training loss at step 6 - batch 284: 0.31 (2019-08-04 13:36:48.109117)\n",
            "Accuracy at step 6 - batch 284: 0.914\n",
            "training loss at step 6 - batch 285: 0.31 (2019-08-04 13:36:48.122302)\n",
            "Accuracy at step 6 - batch 285: 0.9096\n",
            "training loss at step 6 - batch 286: 0.31 (2019-08-04 13:36:48.249030)\n",
            "Accuracy at step 6 - batch 286: 0.9068\n",
            "training loss at step 6 - batch 287: 0.27 (2019-08-04 13:36:48.263731)\n",
            "Accuracy at step 6 - batch 287: 0.9164\n",
            "training loss at step 6 - batch 288: 0.35 (2019-08-04 13:36:48.279256)\n",
            "Accuracy at step 6 - batch 288: 0.8964\n",
            "training loss at step 6 - batch 289: 0.33 (2019-08-04 13:36:48.292042)\n",
            "Accuracy at step 6 - batch 289: 0.8988\n",
            "training loss at step 6 - batch 290: 0.33 (2019-08-04 13:36:48.304338)\n",
            "Accuracy at step 6 - batch 290: 0.898\n",
            "training loss at step 6 - batch 291: 0.32 (2019-08-04 13:36:48.426656)\n",
            "Accuracy at step 6 - batch 291: 0.9088\n",
            "training loss at step 6 - batch 292: 0.30 (2019-08-04 13:36:48.443402)\n",
            "Accuracy at step 6 - batch 292: 0.9084\n",
            "training loss at step 6 - batch 293: 0.30 (2019-08-04 13:36:48.456037)\n",
            "Accuracy at step 6 - batch 293: 0.906\n",
            "training loss at step 6 - batch 294: 0.33 (2019-08-04 13:36:48.468392)\n",
            "Accuracy at step 6 - batch 294: 0.9056\n",
            "training loss at step 6 - batch 295: 0.33 (2019-08-04 13:36:48.483860)\n",
            "Accuracy at step 6 - batch 295: 0.9024\n",
            "training loss at step 6 - batch 296: 0.31 (2019-08-04 13:36:48.601665)\n",
            "Accuracy at step 6 - batch 296: 0.9064\n",
            "training loss at step 6 - batch 297: 0.33 (2019-08-04 13:36:48.613994)\n",
            "Accuracy at step 6 - batch 297: 0.9004\n",
            "training loss at step 6 - batch 298: 0.32 (2019-08-04 13:36:48.628354)\n",
            "Accuracy at step 6 - batch 298: 0.9044\n",
            "training loss at step 6 - batch 299: 0.32 (2019-08-04 13:36:48.640972)\n",
            "Accuracy at step 6 - batch 299: 0.9096\n",
            "training loss at step 6 - batch 300: 0.31 (2019-08-04 13:36:48.653847)\n",
            "Accuracy at step 6 - batch 300: 0.9136\n",
            "training loss at step 6 - batch 301: 0.32 (2019-08-04 13:36:48.783058)\n",
            "Accuracy at step 6 - batch 301: 0.902\n",
            "training loss at step 6 - batch 302: 0.33 (2019-08-04 13:36:48.801156)\n",
            "Accuracy at step 6 - batch 302: 0.9028\n",
            "training loss at step 6 - batch 303: 0.32 (2019-08-04 13:36:48.813923)\n",
            "Accuracy at step 6 - batch 303: 0.9068\n",
            "training loss at step 6 - batch 304: 0.32 (2019-08-04 13:36:48.827347)\n",
            "Accuracy at step 6 - batch 304: 0.9048\n",
            "training loss at step 6 - batch 305: 0.33 (2019-08-04 13:36:48.841224)\n",
            "Accuracy at step 6 - batch 305: 0.8968\n",
            "training loss at step 6 - batch 306: 0.32 (2019-08-04 13:36:48.966258)\n",
            "Accuracy at step 6 - batch 306: 0.9036\n",
            "training loss at step 6 - batch 307: 0.31 (2019-08-04 13:36:48.979195)\n",
            "Accuracy at step 6 - batch 307: 0.9064\n",
            "training loss at step 6 - batch 308: 0.30 (2019-08-04 13:36:48.996840)\n",
            "Accuracy at step 6 - batch 308: 0.9144\n",
            "training loss at step 6 - batch 309: 0.30 (2019-08-04 13:36:49.009851)\n",
            "Accuracy at step 6 - batch 309: 0.91\n",
            "training loss at step 6 - batch 310: 0.31 (2019-08-04 13:36:49.021737)\n",
            "Accuracy at step 6 - batch 310: 0.9044\n",
            "training loss at step 6 - batch 311: 0.31 (2019-08-04 13:36:49.145238)\n",
            "Accuracy at step 6 - batch 311: 0.9068\n",
            "training loss at step 6 - batch 312: 0.31 (2019-08-04 13:36:49.158413)\n",
            "Accuracy at step 6 - batch 312: 0.9088\n",
            "training loss at step 6 - batch 313: 0.29 (2019-08-04 13:36:49.172263)\n",
            "Accuracy at step 6 - batch 313: 0.9172\n",
            "training loss at step 6 - batch 314: 0.32 (2019-08-04 13:36:49.184929)\n",
            "Accuracy at step 6 - batch 314: 0.9032\n",
            "training loss at step 6 - batch 315: 0.33 (2019-08-04 13:36:49.196533)\n",
            "Accuracy at step 6 - batch 315: 0.9004\n",
            "training loss at step 6 - batch 316: 0.31 (2019-08-04 13:36:49.324293)\n",
            "Accuracy at step 6 - batch 316: 0.9056\n",
            "training loss at step 6 - batch 317: 0.33 (2019-08-04 13:36:49.338282)\n",
            "Accuracy at step 6 - batch 317: 0.8964\n",
            "training loss at step 6 - batch 318: 0.31 (2019-08-04 13:36:49.350429)\n",
            "Accuracy at step 6 - batch 318: 0.9\n",
            "training loss at step 6 - batch 319: 0.33 (2019-08-04 13:36:49.363175)\n",
            "Accuracy at step 6 - batch 319: 0.9016\n",
            "training loss at step 6 - batch 320: 0.34 (2019-08-04 13:36:49.376489)\n",
            "Accuracy at step 6 - batch 320: 0.8968\n",
            "training loss at step 6 - batch 321: 0.31 (2019-08-04 13:36:49.494644)\n",
            "Accuracy at step 6 - batch 321: 0.9072\n",
            "training loss at step 6 - batch 322: 0.32 (2019-08-04 13:36:49.510676)\n",
            "Accuracy at step 6 - batch 322: 0.9072\n",
            "training loss at step 6 - batch 323: 0.34 (2019-08-04 13:36:49.523364)\n",
            "Accuracy at step 6 - batch 323: 0.9064\n",
            "training loss at step 6 - batch 324: 0.34 (2019-08-04 13:36:49.538052)\n",
            "Accuracy at step 6 - batch 324: 0.894\n",
            "training loss at step 6 - batch 325: 0.31 (2019-08-04 13:36:49.551302)\n",
            "Accuracy at step 6 - batch 325: 0.906\n",
            "training loss at step 6 - batch 326: 0.30 (2019-08-04 13:36:49.670717)\n",
            "Accuracy at step 6 - batch 326: 0.9088\n",
            "training loss at step 6 - batch 327: 0.32 (2019-08-04 13:36:49.686205)\n",
            "Accuracy at step 6 - batch 327: 0.8976\n",
            "training loss at step 6 - batch 328: 0.31 (2019-08-04 13:36:49.699118)\n",
            "Accuracy at step 6 - batch 328: 0.9084\n",
            "training loss at step 6 - batch 329: 0.32 (2019-08-04 13:36:49.712915)\n",
            "Accuracy at step 6 - batch 329: 0.906\n",
            "training loss at step 6 - batch 330: 0.31 (2019-08-04 13:36:49.728134)\n",
            "Accuracy at step 6 - batch 330: 0.9068\n",
            "training loss at step 6 - batch 331: 0.32 (2019-08-04 13:36:49.848533)\n",
            "Accuracy at step 6 - batch 331: 0.9048\n",
            "training loss at step 6 - batch 332: 0.33 (2019-08-04 13:36:49.862670)\n",
            "Accuracy at step 6 - batch 332: 0.9008\n",
            "training loss at step 6 - batch 333: 0.32 (2019-08-04 13:36:49.876848)\n",
            "Accuracy at step 6 - batch 333: 0.904\n",
            "training loss at step 6 - batch 334: 0.31 (2019-08-04 13:36:49.889449)\n",
            "Accuracy at step 6 - batch 334: 0.906\n",
            "training loss at step 6 - batch 335: 0.32 (2019-08-04 13:36:49.901422)\n",
            "Accuracy at step 6 - batch 335: 0.9052\n",
            "training loss at step 6 - batch 336: 0.33 (2019-08-04 13:36:50.014780)\n",
            "Accuracy at step 6 - batch 336: 0.9052\n",
            "training loss at step 6 - batch 337: 0.31 (2019-08-04 13:36:50.029258)\n",
            "Accuracy at step 6 - batch 337: 0.906\n",
            "training loss at step 6 - batch 338: 0.32 (2019-08-04 13:36:50.041189)\n",
            "Accuracy at step 6 - batch 338: 0.9072\n",
            "training loss at step 6 - batch 339: 0.33 (2019-08-04 13:36:50.059914)\n",
            "Accuracy at step 6 - batch 339: 0.9028\n",
            "training loss at step 6 - batch 340: 0.32 (2019-08-04 13:36:50.073851)\n",
            "Accuracy at step 6 - batch 340: 0.9024\n",
            "training loss at step 6 - batch 341: 0.33 (2019-08-04 13:36:50.196972)\n",
            "Accuracy at step 6 - batch 341: 0.902\n",
            "training loss at step 6 - batch 342: 0.34 (2019-08-04 13:36:50.213335)\n",
            "Accuracy at step 6 - batch 342: 0.9016\n",
            "training loss at step 6 - batch 343: 0.32 (2019-08-04 13:36:50.226732)\n",
            "Accuracy at step 6 - batch 343: 0.908\n",
            "training loss at step 6 - batch 344: 0.31 (2019-08-04 13:36:50.239695)\n",
            "Accuracy at step 6 - batch 344: 0.9048\n",
            "training loss at step 6 - batch 345: 0.34 (2019-08-04 13:36:50.255101)\n",
            "Accuracy at step 6 - batch 345: 0.8992\n",
            "training loss at step 6 - batch 346: 0.33 (2019-08-04 13:36:50.383271)\n",
            "Accuracy at step 6 - batch 346: 0.9036\n",
            "training loss at step 6 - batch 347: 0.32 (2019-08-04 13:36:50.395824)\n",
            "Accuracy at step 6 - batch 347: 0.9032\n",
            "training loss at step 6 - batch 348: 0.31 (2019-08-04 13:36:50.408575)\n",
            "Accuracy at step 6 - batch 348: 0.9092\n",
            "training loss at step 6 - batch 349: 0.31 (2019-08-04 13:36:50.420627)\n",
            "Accuracy at step 6 - batch 349: 0.9056\n",
            "training loss at step 6 - batch 350: 0.31 (2019-08-04 13:36:50.433101)\n",
            "Accuracy at step 6 - batch 350: 0.9084\n",
            "training loss at step 6 - batch 351: 0.31 (2019-08-04 13:36:50.553975)\n",
            "Accuracy at step 6 - batch 351: 0.9092\n",
            "training loss at step 6 - batch 352: 0.32 (2019-08-04 13:36:50.567643)\n",
            "Accuracy at step 6 - batch 352: 0.9032\n",
            "training loss at step 6 - batch 353: 0.31 (2019-08-04 13:36:50.581008)\n",
            "Accuracy at step 6 - batch 353: 0.9044\n",
            "training loss at step 6 - batch 354: 0.33 (2019-08-04 13:36:50.594620)\n",
            "Accuracy at step 6 - batch 354: 0.9\n",
            "training loss at step 6 - batch 355: 0.33 (2019-08-04 13:36:50.607239)\n",
            "Accuracy at step 6 - batch 355: 0.9016\n",
            "training loss at step 6 - batch 356: 0.28 (2019-08-04 13:36:50.733007)\n",
            "Accuracy at step 6 - batch 356: 0.92\n",
            "training loss at step 6 - batch 357: 0.32 (2019-08-04 13:36:50.750190)\n",
            "Accuracy at step 6 - batch 357: 0.9032\n",
            "training loss at step 6 - batch 358: 0.32 (2019-08-04 13:36:50.763745)\n",
            "Accuracy at step 6 - batch 358: 0.9052\n",
            "training loss at step 6 - batch 359: 0.29 (2019-08-04 13:36:50.776820)\n",
            "Accuracy at step 6 - batch 359: 0.9132\n",
            "training loss at step 6 - batch 360: 0.32 (2019-08-04 13:36:50.789310)\n",
            "Accuracy at step 6 - batch 360: 0.9092\n",
            "training loss at step 6 - batch 361: 0.33 (2019-08-04 13:36:50.913729)\n",
            "Accuracy at step 6 - batch 361: 0.9068\n",
            "training loss at step 6 - batch 362: 0.30 (2019-08-04 13:36:50.928015)\n",
            "Accuracy at step 6 - batch 362: 0.9144\n",
            "training loss at step 6 - batch 363: 0.31 (2019-08-04 13:36:50.941131)\n",
            "Accuracy at step 6 - batch 363: 0.906\n",
            "training loss at step 6 - batch 364: 0.35 (2019-08-04 13:36:50.953665)\n",
            "Accuracy at step 6 - batch 364: 0.9032\n",
            "training loss at step 6 - batch 365: 0.33 (2019-08-04 13:36:50.965533)\n",
            "Accuracy at step 6 - batch 365: 0.9\n",
            "training loss at step 6 - batch 366: 0.32 (2019-08-04 13:36:51.092572)\n",
            "Accuracy at step 6 - batch 366: 0.908\n",
            "training loss at step 6 - batch 367: 0.32 (2019-08-04 13:36:51.105299)\n",
            "Accuracy at step 6 - batch 367: 0.91\n",
            "training loss at step 6 - batch 368: 0.33 (2019-08-04 13:36:51.120014)\n",
            "Accuracy at step 6 - batch 368: 0.9024\n",
            "training loss at step 6 - batch 369: 0.29 (2019-08-04 13:36:51.132866)\n",
            "Accuracy at step 6 - batch 369: 0.9116\n",
            "training loss at step 6 - batch 370: 0.33 (2019-08-04 13:36:51.144944)\n",
            "Accuracy at step 6 - batch 370: 0.8996\n",
            "training loss at step 6 - batch 371: 0.32 (2019-08-04 13:36:51.307948)\n",
            "Accuracy at step 6 - batch 371: 0.9052\n",
            "training loss at step 6 - batch 372: 0.31 (2019-08-04 13:36:51.327009)\n",
            "Accuracy at step 6 - batch 372: 0.9088\n",
            "training loss at step 6 - batch 373: 0.32 (2019-08-04 13:36:51.340874)\n",
            "Accuracy at step 6 - batch 373: 0.908\n",
            "training loss at step 6 - batch 374: 0.33 (2019-08-04 13:36:51.353446)\n",
            "Accuracy at step 6 - batch 374: 0.9016\n",
            "training loss at step 6 - batch 375: 0.34 (2019-08-04 13:36:51.366227)\n",
            "Accuracy at step 6 - batch 375: 0.9016\n",
            "training loss at step 6 - batch 376: 0.31 (2019-08-04 13:36:51.487740)\n",
            "Accuracy at step 6 - batch 376: 0.9064\n",
            "training loss at step 6 - batch 377: 0.32 (2019-08-04 13:36:51.501891)\n",
            "Accuracy at step 6 - batch 377: 0.902\n",
            "training loss at step 6 - batch 378: 0.32 (2019-08-04 13:36:51.515292)\n",
            "Accuracy at step 6 - batch 378: 0.904\n",
            "training loss at step 6 - batch 379: 0.30 (2019-08-04 13:36:51.528937)\n",
            "Accuracy at step 6 - batch 379: 0.9092\n",
            "training loss at step 6 - batch 380: 0.32 (2019-08-04 13:36:51.542139)\n",
            "Accuracy at step 6 - batch 380: 0.9064\n",
            "training loss at step 6 - batch 381: 0.30 (2019-08-04 13:36:51.657190)\n",
            "Accuracy at step 6 - batch 381: 0.9116\n",
            "training loss at step 6 - batch 382: 0.31 (2019-08-04 13:36:51.669793)\n",
            "Accuracy at step 6 - batch 382: 0.9052\n",
            "training loss at step 6 - batch 383: 0.29 (2019-08-04 13:36:51.683135)\n",
            "Accuracy at step 6 - batch 383: 0.9172\n",
            "training loss at step 6 - batch 384: 0.33 (2019-08-04 13:36:51.695097)\n",
            "Accuracy at step 6 - batch 384: 0.902\n",
            "training loss at step 6 - batch 385: 0.32 (2019-08-04 13:36:51.708152)\n",
            "Accuracy at step 6 - batch 385: 0.9044\n",
            "training loss at step 6 - batch 386: 0.33 (2019-08-04 13:36:51.837329)\n",
            "Accuracy at step 6 - batch 386: 0.8992\n",
            "training loss at step 6 - batch 387: 0.31 (2019-08-04 13:36:51.849415)\n",
            "Accuracy at step 6 - batch 387: 0.9076\n",
            "training loss at step 6 - batch 388: 0.34 (2019-08-04 13:36:51.861784)\n",
            "Accuracy at step 6 - batch 388: 0.9008\n",
            "training loss at step 6 - batch 389: 0.33 (2019-08-04 13:36:51.873856)\n",
            "Accuracy at step 6 - batch 389: 0.906\n",
            "training loss at step 6 - batch 390: 0.32 (2019-08-04 13:36:51.886337)\n",
            "Accuracy at step 6 - batch 390: 0.9036\n",
            "training loss at step 6 - batch 391: 0.33 (2019-08-04 13:36:52.007721)\n",
            "Accuracy at step 6 - batch 391: 0.8992\n",
            "training loss at step 6 - batch 392: 0.34 (2019-08-04 13:36:52.020983)\n",
            "Accuracy at step 6 - batch 392: 0.8964\n",
            "training loss at step 6 - batch 393: 0.28 (2019-08-04 13:36:52.034239)\n",
            "Accuracy at step 6 - batch 393: 0.9132\n",
            "training loss at step 6 - batch 394: 0.33 (2019-08-04 13:36:52.048872)\n",
            "Accuracy at step 6 - batch 394: 0.8988\n",
            "training loss at step 6 - batch 395: 0.31 (2019-08-04 13:36:52.063612)\n",
            "Accuracy at step 6 - batch 395: 0.912\n",
            "training loss at step 6 - batch 396: 0.32 (2019-08-04 13:36:52.179559)\n",
            "Accuracy at step 6 - batch 396: 0.9092\n",
            "training loss at step 6 - batch 397: 0.35 (2019-08-04 13:36:52.191874)\n",
            "Accuracy at step 6 - batch 397: 0.8968\n",
            "training loss at step 6 - batch 398: 0.33 (2019-08-04 13:36:52.207238)\n",
            "Accuracy at step 6 - batch 398: 0.898\n",
            "training loss at step 6 - batch 399: 0.32 (2019-08-04 13:36:52.220972)\n",
            "Accuracy at step 6 - batch 399: 0.902\n",
            "training loss at step 6 - batch 400: 0.33 (2019-08-04 13:36:52.234126)\n",
            "Accuracy at step 6 - batch 400: 0.8988\n",
            "training loss at step 6 - batch 401: 0.33 (2019-08-04 13:36:52.363065)\n",
            "Accuracy at step 6 - batch 401: 0.9028\n",
            "training loss at step 6 - batch 402: 0.33 (2019-08-04 13:36:52.376744)\n",
            "Accuracy at step 6 - batch 402: 0.9012\n",
            "training loss at step 6 - batch 403: 0.33 (2019-08-04 13:36:52.391075)\n",
            "Accuracy at step 6 - batch 403: 0.902\n",
            "training loss at step 6 - batch 404: 0.32 (2019-08-04 13:36:52.403658)\n",
            "Accuracy at step 6 - batch 404: 0.906\n",
            "training loss at step 6 - batch 405: 0.31 (2019-08-04 13:36:52.416539)\n",
            "Accuracy at step 6 - batch 405: 0.9028\n",
            "training loss at step 6 - batch 406: 0.29 (2019-08-04 13:36:52.533958)\n",
            "Accuracy at step 6 - batch 406: 0.91\n",
            "training loss at step 6 - batch 407: 0.30 (2019-08-04 13:36:52.547687)\n",
            "Accuracy at step 6 - batch 407: 0.9084\n",
            "training loss at step 6 - batch 408: 0.32 (2019-08-04 13:36:52.560344)\n",
            "Accuracy at step 6 - batch 408: 0.9068\n",
            "training loss at step 6 - batch 409: 0.33 (2019-08-04 13:36:52.574945)\n",
            "Accuracy at step 6 - batch 409: 0.8984\n",
            "training loss at step 6 - batch 410: 0.29 (2019-08-04 13:36:52.589947)\n",
            "Accuracy at step 6 - batch 410: 0.91\n",
            "training loss at step 6 - batch 411: 0.33 (2019-08-04 13:36:52.706480)\n",
            "Accuracy at step 6 - batch 411: 0.9\n",
            "training loss at step 6 - batch 412: 0.32 (2019-08-04 13:36:52.719036)\n",
            "Accuracy at step 6 - batch 412: 0.902\n",
            "training loss at step 6 - batch 413: 0.34 (2019-08-04 13:36:52.732107)\n",
            "Accuracy at step 6 - batch 413: 0.8988\n",
            "training loss at step 6 - batch 414: 0.33 (2019-08-04 13:36:52.745033)\n",
            "Accuracy at step 6 - batch 414: 0.9032\n",
            "training loss at step 6 - batch 415: 0.32 (2019-08-04 13:36:52.757333)\n",
            "Accuracy at step 6 - batch 415: 0.9032\n",
            "training loss at step 6 - batch 416: 0.32 (2019-08-04 13:36:52.882350)\n",
            "Accuracy at step 6 - batch 416: 0.9072\n",
            "training loss at step 6 - batch 417: 0.33 (2019-08-04 13:36:52.899170)\n",
            "Accuracy at step 6 - batch 417: 0.902\n",
            "training loss at step 6 - batch 418: 0.31 (2019-08-04 13:36:52.913065)\n",
            "Accuracy at step 6 - batch 418: 0.9068\n",
            "training loss at step 6 - batch 419: 0.33 (2019-08-04 13:36:52.926156)\n",
            "Accuracy at step 6 - batch 419: 0.9036\n",
            "training loss at step 6 - batch 420: 0.30 (2019-08-04 13:36:52.940266)\n",
            "Accuracy at step 6 - batch 420: 0.9096\n",
            "training loss at step 6 - batch 421: 0.33 (2019-08-04 13:36:53.061712)\n",
            "Accuracy at step 6 - batch 421: 0.8976\n",
            "training loss at step 6 - batch 422: 0.31 (2019-08-04 13:36:53.079665)\n",
            "Accuracy at step 6 - batch 422: 0.904\n",
            "training loss at step 6 - batch 423: 0.30 (2019-08-04 13:36:53.096375)\n",
            "Accuracy at step 6 - batch 423: 0.9044\n",
            "training loss at step 6 - batch 424: 0.30 (2019-08-04 13:36:53.108768)\n",
            "Accuracy at step 6 - batch 424: 0.9112\n",
            "training loss at step 6 - batch 425: 0.33 (2019-08-04 13:36:53.122169)\n",
            "Accuracy at step 6 - batch 425: 0.8952\n",
            "training loss at step 6 - batch 426: 0.30 (2019-08-04 13:36:53.242577)\n",
            "Accuracy at step 6 - batch 426: 0.9104\n",
            "training loss at step 6 - batch 427: 0.33 (2019-08-04 13:36:53.258103)\n",
            "Accuracy at step 6 - batch 427: 0.896\n",
            "training loss at step 6 - batch 428: 0.34 (2019-08-04 13:36:53.270076)\n",
            "Accuracy at step 6 - batch 428: 0.8956\n",
            "training loss at step 6 - batch 429: 0.31 (2019-08-04 13:36:53.283294)\n",
            "Accuracy at step 6 - batch 429: 0.9084\n",
            "training loss at step 6 - batch 430: 0.30 (2019-08-04 13:36:53.296362)\n",
            "Accuracy at step 6 - batch 430: 0.9096\n",
            "training loss at step 6 - batch 431: 0.32 (2019-08-04 13:36:53.426640)\n",
            "Accuracy at step 6 - batch 431: 0.9028\n",
            "training loss at step 6 - batch 432: 0.32 (2019-08-04 13:36:53.438892)\n",
            "Accuracy at step 6 - batch 432: 0.9048\n",
            "training loss at step 6 - batch 433: 0.28 (2019-08-04 13:36:53.452285)\n",
            "Accuracy at step 6 - batch 433: 0.9148\n",
            "training loss at step 6 - batch 434: 0.32 (2019-08-04 13:36:53.464327)\n",
            "Accuracy at step 6 - batch 434: 0.9084\n",
            "training loss at step 6 - batch 435: 0.33 (2019-08-04 13:36:53.477160)\n",
            "Accuracy at step 6 - batch 435: 0.8988\n",
            "training loss at step 6 - batch 436: 0.33 (2019-08-04 13:36:53.600191)\n",
            "Accuracy at step 6 - batch 436: 0.9004\n",
            "training loss at step 6 - batch 437: 0.32 (2019-08-04 13:36:53.616618)\n",
            "Accuracy at step 6 - batch 437: 0.9072\n",
            "training loss at step 6 - batch 438: 0.31 (2019-08-04 13:36:53.630156)\n",
            "Accuracy at step 6 - batch 438: 0.9048\n",
            "training loss at step 6 - batch 439: 0.30 (2019-08-04 13:36:53.644012)\n",
            "Accuracy at step 6 - batch 439: 0.9092\n",
            "training loss at step 6 - batch 440: 0.32 (2019-08-04 13:36:53.656268)\n",
            "Accuracy at step 6 - batch 440: 0.9032\n",
            "training loss at step 6 - batch 441: 0.32 (2019-08-04 13:36:53.777056)\n",
            "Accuracy at step 6 - batch 441: 0.9032\n",
            "training loss at step 6 - batch 442: 0.33 (2019-08-04 13:36:53.790633)\n",
            "Accuracy at step 6 - batch 442: 0.8984\n",
            "training loss at step 6 - batch 443: 0.33 (2019-08-04 13:36:53.804607)\n",
            "Accuracy at step 6 - batch 443: 0.902\n",
            "training loss at step 6 - batch 444: 0.32 (2019-08-04 13:36:53.818653)\n",
            "Accuracy at step 6 - batch 444: 0.9036\n",
            "training loss at step 6 - batch 445: 0.33 (2019-08-04 13:36:53.830890)\n",
            "Accuracy at step 6 - batch 445: 0.8992\n",
            "training loss at step 6 - batch 446: 0.30 (2019-08-04 13:36:53.953288)\n",
            "Accuracy at step 6 - batch 446: 0.9092\n",
            "training loss at step 6 - batch 447: 0.29 (2019-08-04 13:36:53.969199)\n",
            "Accuracy at step 6 - batch 447: 0.9164\n",
            "training loss at step 6 - batch 448: 0.31 (2019-08-04 13:36:53.981040)\n",
            "Accuracy at step 6 - batch 448: 0.908\n",
            "training loss at step 6 - batch 449: 0.34 (2019-08-04 13:36:53.993915)\n",
            "Accuracy at step 6 - batch 449: 0.8988\n",
            "training loss at step 6 - batch 450: 0.32 (2019-08-04 13:36:54.010293)\n",
            "Accuracy at step 6 - batch 450: 0.9036\n",
            "training loss at step 6 - batch 451: 0.32 (2019-08-04 13:36:54.129047)\n",
            "Accuracy at step 6 - batch 451: 0.9088\n",
            "training loss at step 6 - batch 452: 0.31 (2019-08-04 13:36:54.142173)\n",
            "Accuracy at step 6 - batch 452: 0.9072\n",
            "training loss at step 6 - batch 453: 0.33 (2019-08-04 13:36:54.154648)\n",
            "Accuracy at step 6 - batch 453: 0.9028\n",
            "training loss at step 6 - batch 454: 0.31 (2019-08-04 13:36:54.167260)\n",
            "Accuracy at step 6 - batch 454: 0.9088\n",
            "training loss at step 6 - batch 455: 0.32 (2019-08-04 13:36:54.180214)\n",
            "Accuracy at step 6 - batch 455: 0.9024\n",
            "training loss at step 6 - batch 456: 0.31 (2019-08-04 13:36:54.299854)\n",
            "Accuracy at step 6 - batch 456: 0.9104\n",
            "training loss at step 6 - batch 457: 0.30 (2019-08-04 13:36:54.316209)\n",
            "Accuracy at step 6 - batch 457: 0.9112\n",
            "training loss at step 6 - batch 458: 0.32 (2019-08-04 13:36:54.332067)\n",
            "Accuracy at step 6 - batch 458: 0.906\n",
            "training loss at step 6 - batch 459: 0.31 (2019-08-04 13:36:54.352862)\n",
            "Accuracy at step 6 - batch 459: 0.9084\n",
            "training loss at step 6 - batch 460: 0.33 (2019-08-04 13:36:54.371776)\n",
            "Accuracy at step 6 - batch 460: 0.8992\n",
            "training loss at step 6 - batch 461: 0.31 (2019-08-04 13:36:54.495715)\n",
            "Accuracy at step 6 - batch 461: 0.9072\n",
            "training loss at step 6 - batch 462: 0.33 (2019-08-04 13:36:54.514391)\n",
            "Accuracy at step 6 - batch 462: 0.8976\n",
            "training loss at step 6 - batch 463: 0.30 (2019-08-04 13:36:54.527271)\n",
            "Accuracy at step 6 - batch 463: 0.9084\n",
            "training loss at step 6 - batch 464: 0.32 (2019-08-04 13:36:54.540367)\n",
            "Accuracy at step 6 - batch 464: 0.9048\n",
            "training loss at step 6 - batch 465: 0.34 (2019-08-04 13:36:54.552121)\n",
            "Accuracy at step 6 - batch 465: 0.9032\n",
            "training loss at step 6 - batch 466: 0.33 (2019-08-04 13:36:54.672263)\n",
            "Accuracy at step 6 - batch 466: 0.9056\n",
            "training loss at step 6 - batch 467: 0.33 (2019-08-04 13:36:54.684715)\n",
            "Accuracy at step 6 - batch 467: 0.9048\n",
            "training loss at step 6 - batch 468: 0.33 (2019-08-04 13:36:54.697244)\n",
            "Accuracy at step 6 - batch 468: 0.8976\n",
            "training loss at step 6 - batch 469: 0.33 (2019-08-04 13:36:54.710360)\n",
            "Accuracy at step 6 - batch 469: 0.8976\n",
            "training loss at step 6 - batch 470: 0.31 (2019-08-04 13:36:54.725443)\n",
            "Accuracy at step 6 - batch 470: 0.908\n",
            "training loss at step 6 - batch 471: 0.30 (2019-08-04 13:36:54.849133)\n",
            "Accuracy at step 6 - batch 471: 0.908\n",
            "training loss at step 6 - batch 472: 0.35 (2019-08-04 13:36:54.866756)\n",
            "Accuracy at step 6 - batch 472: 0.8932\n",
            "training loss at step 6 - batch 473: 0.31 (2019-08-04 13:36:54.879591)\n",
            "Accuracy at step 6 - batch 473: 0.9096\n",
            "training loss at step 6 - batch 474: 0.32 (2019-08-04 13:36:54.891344)\n",
            "Accuracy at step 6 - batch 474: 0.906\n",
            "training loss at step 6 - batch 475: 0.32 (2019-08-04 13:36:54.903915)\n",
            "Accuracy at step 6 - batch 475: 0.9064\n",
            "training loss at step 6 - batch 476: 0.30 (2019-08-04 13:36:55.028743)\n",
            "Accuracy at step 6 - batch 476: 0.914\n",
            "training loss at step 6 - batch 477: 0.31 (2019-08-04 13:36:55.040944)\n",
            "Accuracy at step 6 - batch 477: 0.9032\n",
            "training loss at step 6 - batch 478: 0.30 (2019-08-04 13:36:55.054464)\n",
            "Accuracy at step 6 - batch 478: 0.9092\n",
            "training loss at step 6 - batch 479: 0.32 (2019-08-04 13:36:55.068587)\n",
            "Accuracy at step 6 - batch 479: 0.9052\n",
            "training loss at step 6 - batch 480: 0.32 (2019-08-04 13:36:55.080956)\n",
            "Accuracy at step 6 - batch 480: 0.9044\n",
            "training loss at step 6 - batch 481: 0.31 (2019-08-04 13:36:55.197664)\n",
            "Accuracy at step 6 - batch 481: 0.9092\n",
            "training loss at step 6 - batch 482: 0.32 (2019-08-04 13:36:55.211499)\n",
            "Accuracy at step 6 - batch 482: 0.9072\n",
            "training loss at step 6 - batch 483: 0.30 (2019-08-04 13:36:55.224605)\n",
            "Accuracy at step 6 - batch 483: 0.9096\n",
            "training loss at step 6 - batch 484: 0.31 (2019-08-04 13:36:55.241669)\n",
            "Accuracy at step 6 - batch 484: 0.91\n",
            "training loss at step 6 - batch 485: 0.31 (2019-08-04 13:36:55.254332)\n",
            "Accuracy at step 6 - batch 485: 0.9076\n",
            "training loss at step 6 - batch 486: 0.33 (2019-08-04 13:36:55.376999)\n",
            "Accuracy at step 6 - batch 486: 0.902\n",
            "training loss at step 6 - batch 487: 0.32 (2019-08-04 13:36:55.392567)\n",
            "Accuracy at step 6 - batch 487: 0.904\n",
            "training loss at step 6 - batch 488: 0.33 (2019-08-04 13:36:55.405234)\n",
            "Accuracy at step 6 - batch 488: 0.9044\n",
            "training loss at step 6 - batch 489: 0.33 (2019-08-04 13:36:55.418164)\n",
            "Accuracy at step 6 - batch 489: 0.9\n",
            "training loss at step 6 - batch 490: 0.30 (2019-08-04 13:36:55.430571)\n",
            "Accuracy at step 6 - batch 490: 0.9096\n",
            "training loss at step 6 - batch 491: 0.31 (2019-08-04 13:36:55.553515)\n",
            "Accuracy at step 6 - batch 491: 0.9096\n",
            "training loss at step 6 - batch 492: 0.33 (2019-08-04 13:36:55.567146)\n",
            "Accuracy at step 6 - batch 492: 0.9008\n",
            "training loss at step 6 - batch 493: 0.32 (2019-08-04 13:36:55.580636)\n",
            "Accuracy at step 6 - batch 493: 0.9032\n",
            "training loss at step 6 - batch 494: 0.33 (2019-08-04 13:36:55.593272)\n",
            "Accuracy at step 6 - batch 494: 0.9012\n",
            "training loss at step 6 - batch 495: 0.33 (2019-08-04 13:36:55.605147)\n",
            "Accuracy at step 6 - batch 495: 0.904\n",
            "training loss at step 6 - batch 496: 0.33 (2019-08-04 13:36:55.722419)\n",
            "Accuracy at step 6 - batch 496: 0.8984\n",
            "training loss at step 6 - batch 497: 0.32 (2019-08-04 13:36:55.738612)\n",
            "Accuracy at step 6 - batch 497: 0.9008\n",
            "training loss at step 6 - batch 498: 0.31 (2019-08-04 13:36:55.750793)\n",
            "Accuracy at step 6 - batch 498: 0.9072\n",
            "training loss at step 6 - batch 499: 0.33 (2019-08-04 13:36:55.767679)\n",
            "Accuracy at step 6 - batch 499: 0.9032\n",
            "training loss at step 6 - batch 500: 0.31 (2019-08-04 13:36:55.781266)\n",
            "Accuracy at step 6 - batch 500: 0.9052\n",
            "training loss at step 6 - batch 501: 0.31 (2019-08-04 13:36:55.902367)\n",
            "Accuracy at step 6 - batch 501: 0.91\n",
            "training loss at step 6 - batch 502: 0.33 (2019-08-04 13:36:55.918911)\n",
            "Accuracy at step 6 - batch 502: 0.9032\n",
            "training loss at step 6 - batch 503: 0.33 (2019-08-04 13:36:55.931432)\n",
            "Accuracy at step 6 - batch 503: 0.8988\n",
            "training loss at step 6 - batch 504: 0.31 (2019-08-04 13:36:55.944312)\n",
            "Accuracy at step 6 - batch 504: 0.9088\n",
            "training loss at step 6 - batch 505: 0.32 (2019-08-04 13:36:55.956411)\n",
            "Accuracy at step 6 - batch 505: 0.9012\n",
            "training loss at step 6 - batch 506: 0.33 (2019-08-04 13:36:56.077078)\n",
            "Accuracy at step 6 - batch 506: 0.9024\n",
            "training loss at step 6 - batch 507: 0.31 (2019-08-04 13:36:56.094447)\n",
            "Accuracy at step 6 - batch 507: 0.91\n",
            "training loss at step 6 - batch 508: 0.30 (2019-08-04 13:36:56.108181)\n",
            "Accuracy at step 6 - batch 508: 0.9108\n",
            "training loss at step 6 - batch 509: 0.31 (2019-08-04 13:36:56.123374)\n",
            "Accuracy at step 6 - batch 509: 0.908\n",
            "training loss at step 6 - batch 510: 0.33 (2019-08-04 13:36:56.135953)\n",
            "Accuracy at step 6 - batch 510: 0.9072\n",
            "training loss at step 6 - batch 511: 0.33 (2019-08-04 13:36:56.265306)\n",
            "Accuracy at step 6 - batch 511: 0.9048\n",
            "training loss at step 6 - batch 512: 0.32 (2019-08-04 13:36:56.278974)\n",
            "Accuracy at step 6 - batch 512: 0.902\n",
            "training loss at step 6 - batch 513: 0.33 (2019-08-04 13:36:56.294011)\n",
            "Accuracy at step 6 - batch 513: 0.9016\n",
            "training loss at step 6 - batch 514: 0.33 (2019-08-04 13:36:56.308060)\n",
            "Accuracy at step 6 - batch 514: 0.896\n",
            "training loss at step 6 - batch 515: 0.33 (2019-08-04 13:36:56.320387)\n",
            "Accuracy at step 6 - batch 515: 0.9048\n",
            "training loss at step 6 - batch 516: 0.33 (2019-08-04 13:36:56.456141)\n",
            "Accuracy at step 6 - batch 516: 0.9016\n",
            "training loss at step 6 - batch 517: 0.32 (2019-08-04 13:36:56.471438)\n",
            "Accuracy at step 6 - batch 517: 0.8964\n",
            "training loss at step 6 - batch 518: 0.31 (2019-08-04 13:36:56.485741)\n",
            "Accuracy at step 6 - batch 518: 0.9068\n",
            "training loss at step 6 - batch 519: 0.30 (2019-08-04 13:36:56.498633)\n",
            "Accuracy at step 6 - batch 519: 0.9076\n",
            "training loss at step 6 - batch 520: 0.31 (2019-08-04 13:36:56.511313)\n",
            "Accuracy at step 6 - batch 520: 0.9056\n",
            "training loss at step 6 - batch 521: 0.31 (2019-08-04 13:36:56.629137)\n",
            "Accuracy at step 6 - batch 521: 0.9072\n",
            "training loss at step 6 - batch 522: 0.30 (2019-08-04 13:36:56.641606)\n",
            "Accuracy at step 6 - batch 522: 0.9128\n",
            "training loss at step 6 - batch 523: 0.32 (2019-08-04 13:36:56.654404)\n",
            "Accuracy at step 6 - batch 523: 0.9048\n",
            "training loss at step 6 - batch 524: 0.32 (2019-08-04 13:36:56.666776)\n",
            "Accuracy at step 6 - batch 524: 0.9016\n",
            "training loss at step 6 - batch 525: 0.30 (2019-08-04 13:36:56.679443)\n",
            "Accuracy at step 6 - batch 525: 0.9084\n",
            "training loss at step 6 - batch 526: 0.33 (2019-08-04 13:36:56.805900)\n",
            "Accuracy at step 6 - batch 526: 0.9016\n",
            "training loss at step 6 - batch 527: 0.32 (2019-08-04 13:36:56.819641)\n",
            "Accuracy at step 6 - batch 527: 0.9044\n",
            "training loss at step 6 - batch 528: 0.32 (2019-08-04 13:36:56.832151)\n",
            "Accuracy at step 6 - batch 528: 0.902\n",
            "training loss at step 6 - batch 529: 0.34 (2019-08-04 13:36:56.845313)\n",
            "Accuracy at step 6 - batch 529: 0.9016\n",
            "training loss at step 6 - batch 530: 0.31 (2019-08-04 13:36:56.858788)\n",
            "Accuracy at step 6 - batch 530: 0.9104\n",
            "training loss at step 6 - batch 531: 0.32 (2019-08-04 13:36:56.983935)\n",
            "Accuracy at step 6 - batch 531: 0.9008\n",
            "training loss at step 6 - batch 532: 0.28 (2019-08-04 13:36:56.996941)\n",
            "Accuracy at step 6 - batch 532: 0.9204\n",
            "training loss at step 6 - batch 533: 0.31 (2019-08-04 13:36:57.011707)\n",
            "Accuracy at step 6 - batch 533: 0.904\n",
            "training loss at step 6 - batch 534: 0.33 (2019-08-04 13:36:57.024351)\n",
            "Accuracy at step 6 - batch 534: 0.8944\n",
            "training loss at step 6 - batch 535: 0.31 (2019-08-04 13:36:57.038485)\n",
            "Accuracy at step 6 - batch 535: 0.9084\n",
            "training loss at step 6 - batch 536: 0.32 (2019-08-04 13:36:57.160030)\n",
            "Accuracy at step 6 - batch 536: 0.9064\n",
            "training loss at step 6 - batch 537: 0.30 (2019-08-04 13:36:57.174870)\n",
            "Accuracy at step 6 - batch 537: 0.9052\n",
            "training loss at step 6 - batch 538: 0.31 (2019-08-04 13:36:57.187043)\n",
            "Accuracy at step 6 - batch 538: 0.904\n",
            "training loss at step 6 - batch 539: 0.30 (2019-08-04 13:36:57.199271)\n",
            "Accuracy at step 6 - batch 539: 0.9116\n",
            "training loss at step 6 - batch 540: 0.32 (2019-08-04 13:36:57.212272)\n",
            "Accuracy at step 6 - batch 540: 0.9068\n",
            "training loss at step 6 - batch 541: 0.30 (2019-08-04 13:36:57.334946)\n",
            "Accuracy at step 6 - batch 541: 0.9072\n",
            "training loss at step 6 - batch 542: 0.32 (2019-08-04 13:36:57.348651)\n",
            "Accuracy at step 6 - batch 542: 0.9056\n",
            "training loss at step 6 - batch 543: 0.31 (2019-08-04 13:36:57.361665)\n",
            "Accuracy at step 6 - batch 543: 0.9096\n",
            "training loss at step 6 - batch 544: 0.34 (2019-08-04 13:36:57.374089)\n",
            "Accuracy at step 6 - batch 544: 0.9008\n",
            "training loss at step 6 - batch 545: 0.33 (2019-08-04 13:36:57.388990)\n",
            "Accuracy at step 6 - batch 545: 0.8984\n",
            "training loss at step 6 - batch 546: 0.30 (2019-08-04 13:36:57.522726)\n",
            "Accuracy at step 6 - batch 546: 0.9124\n",
            "training loss at step 6 - batch 547: 0.31 (2019-08-04 13:36:57.537106)\n",
            "Accuracy at step 6 - batch 547: 0.9036\n",
            "training loss at step 6 - batch 548: 0.30 (2019-08-04 13:36:57.550221)\n",
            "Accuracy at step 6 - batch 548: 0.9116\n",
            "training loss at step 6 - batch 549: 0.31 (2019-08-04 13:36:57.563143)\n",
            "Accuracy at step 6 - batch 549: 0.9092\n",
            "training loss at step 6 - batch 550: 0.31 (2019-08-04 13:36:57.576217)\n",
            "Accuracy at step 6 - batch 550: 0.9048\n",
            "training loss at step 6 - batch 551: 0.30 (2019-08-04 13:36:57.697587)\n",
            "Accuracy at step 6 - batch 551: 0.91\n",
            "training loss at step 6 - batch 552: 0.31 (2019-08-04 13:36:57.710287)\n",
            "Accuracy at step 6 - batch 552: 0.9056\n",
            "training loss at step 6 - batch 553: 0.32 (2019-08-04 13:36:57.723201)\n",
            "Accuracy at step 6 - batch 553: 0.9052\n",
            "training loss at step 6 - batch 554: 0.34 (2019-08-04 13:36:57.738907)\n",
            "Accuracy at step 6 - batch 554: 0.9\n",
            "training loss at step 6 - batch 555: 0.32 (2019-08-04 13:36:57.751421)\n",
            "Accuracy at step 6 - batch 555: 0.9016\n",
            "training loss at step 6 - batch 556: 0.31 (2019-08-04 13:36:57.872381)\n",
            "Accuracy at step 6 - batch 556: 0.9048\n",
            "training loss at step 6 - batch 557: 0.31 (2019-08-04 13:36:57.885324)\n",
            "Accuracy at step 6 - batch 557: 0.9016\n",
            "training loss at step 6 - batch 558: 0.33 (2019-08-04 13:36:57.898249)\n",
            "Accuracy at step 6 - batch 558: 0.8988\n",
            "training loss at step 6 - batch 559: 0.29 (2019-08-04 13:36:57.911715)\n",
            "Accuracy at step 6 - batch 559: 0.9112\n",
            "training loss at step 6 - batch 560: 0.31 (2019-08-04 13:36:57.924139)\n",
            "Accuracy at step 6 - batch 560: 0.9056\n",
            "training loss at step 6 - batch 561: 0.30 (2019-08-04 13:36:58.046512)\n",
            "Accuracy at step 6 - batch 561: 0.9108\n",
            "training loss at step 6 - batch 562: 0.30 (2019-08-04 13:36:58.059701)\n",
            "Accuracy at step 6 - batch 562: 0.9088\n",
            "training loss at step 6 - batch 563: 0.33 (2019-08-04 13:36:58.072172)\n",
            "Accuracy at step 6 - batch 563: 0.9008\n",
            "training loss at step 6 - batch 564: 0.30 (2019-08-04 13:36:58.085468)\n",
            "Accuracy at step 6 - batch 564: 0.9148\n",
            "training loss at step 6 - batch 565: 0.33 (2019-08-04 13:36:58.098315)\n",
            "Accuracy at step 6 - batch 565: 0.9004\n",
            "training loss at step 6 - batch 566: 0.31 (2019-08-04 13:36:58.213091)\n",
            "Accuracy at step 6 - batch 566: 0.9072\n",
            "training loss at step 6 - batch 567: 0.34 (2019-08-04 13:36:58.226485)\n",
            "Accuracy at step 6 - batch 567: 0.9004\n",
            "training loss at step 6 - batch 568: 0.30 (2019-08-04 13:36:58.239444)\n",
            "Accuracy at step 6 - batch 568: 0.9048\n",
            "training loss at step 6 - batch 569: 0.28 (2019-08-04 13:36:58.254342)\n",
            "Accuracy at step 6 - batch 569: 0.9168\n",
            "training loss at step 6 - batch 570: 0.30 (2019-08-04 13:36:58.267176)\n",
            "Accuracy at step 6 - batch 570: 0.9076\n",
            "training loss at step 6 - batch 571: 0.32 (2019-08-04 13:36:58.387000)\n",
            "Accuracy at step 6 - batch 571: 0.9056\n",
            "training loss at step 6 - batch 572: 0.29 (2019-08-04 13:36:58.403674)\n",
            "Accuracy at step 6 - batch 572: 0.9116\n",
            "training loss at step 6 - batch 573: 0.33 (2019-08-04 13:36:58.416233)\n",
            "Accuracy at step 6 - batch 573: 0.9008\n",
            "training loss at step 6 - batch 574: 0.30 (2019-08-04 13:36:58.428829)\n",
            "Accuracy at step 6 - batch 574: 0.9084\n",
            "training loss at step 6 - batch 575: 0.30 (2019-08-04 13:36:58.441941)\n",
            "Accuracy at step 6 - batch 575: 0.9068\n",
            "training loss at step 6 - batch 576: 0.32 (2019-08-04 13:36:58.567395)\n",
            "Accuracy at step 6 - batch 576: 0.8976\n",
            "training loss at step 6 - batch 577: 0.29 (2019-08-04 13:36:58.579724)\n",
            "Accuracy at step 6 - batch 577: 0.9124\n",
            "training loss at step 6 - batch 578: 0.33 (2019-08-04 13:36:58.592002)\n",
            "Accuracy at step 6 - batch 578: 0.9044\n",
            "training loss at step 6 - batch 579: 0.33 (2019-08-04 13:36:58.603915)\n",
            "Accuracy at step 6 - batch 579: 0.9028\n",
            "training loss at step 6 - batch 580: 0.30 (2019-08-04 13:36:58.617828)\n",
            "Accuracy at step 6 - batch 580: 0.9124\n",
            "training loss at step 6 - batch 581: 0.31 (2019-08-04 13:36:58.738709)\n",
            "Accuracy at step 6 - batch 581: 0.912\n",
            "training loss at step 6 - batch 582: 0.33 (2019-08-04 13:36:58.754974)\n",
            "Accuracy at step 6 - batch 582: 0.906\n",
            "training loss at step 6 - batch 583: 0.32 (2019-08-04 13:36:58.768270)\n",
            "Accuracy at step 6 - batch 583: 0.9092\n",
            "training loss at step 6 - batch 584: 0.31 (2019-08-04 13:36:58.783770)\n",
            "Accuracy at step 6 - batch 584: 0.9072\n",
            "training loss at step 6 - batch 585: 0.32 (2019-08-04 13:36:58.797238)\n",
            "Accuracy at step 6 - batch 585: 0.906\n",
            "training loss at step 6 - batch 586: 0.30 (2019-08-04 13:36:58.921192)\n",
            "Accuracy at step 6 - batch 586: 0.9052\n",
            "training loss at step 6 - batch 587: 0.34 (2019-08-04 13:36:58.935451)\n",
            "Accuracy at step 6 - batch 587: 0.9064\n",
            "training loss at step 6 - batch 588: 0.33 (2019-08-04 13:36:58.948484)\n",
            "Accuracy at step 6 - batch 588: 0.8992\n",
            "training loss at step 6 - batch 589: 0.33 (2019-08-04 13:36:58.961024)\n",
            "Accuracy at step 6 - batch 589: 0.898\n",
            "training loss at step 6 - batch 590: 0.33 (2019-08-04 13:36:58.975649)\n",
            "Accuracy at step 6 - batch 590: 0.9064\n",
            "training loss at step 6 - batch 591: 0.33 (2019-08-04 13:36:59.223696)\n",
            "Accuracy at step 6 - batch 591: 0.8968\n",
            "training loss at step 6 - batch 592: 0.33 (2019-08-04 13:36:59.236317)\n",
            "Accuracy at step 6 - batch 592: 0.9024\n",
            "training loss at step 6 - batch 593: 0.30 (2019-08-04 13:36:59.249855)\n",
            "Accuracy at step 6 - batch 593: 0.908\n",
            "training loss at step 6 - batch 594: 0.32 (2019-08-04 13:36:59.262341)\n",
            "Accuracy at step 6 - batch 594: 0.9048\n",
            "training loss at step 6 - batch 595: 0.31 (2019-08-04 13:36:59.276121)\n",
            "Accuracy at step 6 - batch 595: 0.9064\n",
            "training loss at step 6 - batch 596: 0.31 (2019-08-04 13:36:59.400780)\n",
            "Accuracy at step 6 - batch 596: 0.9044\n",
            "training loss at step 6 - batch 597: 0.31 (2019-08-04 13:36:59.417039)\n",
            "Accuracy at step 6 - batch 597: 0.9036\n",
            "training loss at step 6 - batch 598: 0.32 (2019-08-04 13:36:59.431352)\n",
            "Accuracy at step 6 - batch 598: 0.904\n",
            "training loss at step 6 - batch 599: 0.31 (2019-08-04 13:36:59.446015)\n",
            "Accuracy at step 6 - batch 599: 0.9052\n",
            "training loss at step 6 - batch 600: 0.31 (2019-08-04 13:36:59.459199)\n",
            "Accuracy at step 6 - batch 600: 0.9088\n",
            "training loss at step 6 - batch 601: 0.33 (2019-08-04 13:36:59.584252)\n",
            "Accuracy at step 6 - batch 601: 0.9008\n",
            "training loss at step 6 - batch 602: 0.34 (2019-08-04 13:36:59.596402)\n",
            "Accuracy at step 6 - batch 602: 0.898\n",
            "training loss at step 6 - batch 603: 0.32 (2019-08-04 13:36:59.609331)\n",
            "Accuracy at step 6 - batch 603: 0.9052\n",
            "training loss at step 6 - batch 604: 0.30 (2019-08-04 13:36:59.622693)\n",
            "Accuracy at step 6 - batch 604: 0.9048\n",
            "training loss at step 6 - batch 605: 0.31 (2019-08-04 13:36:59.638183)\n",
            "Accuracy at step 6 - batch 605: 0.9076\n",
            "training loss at step 6 - batch 606: 0.30 (2019-08-04 13:36:59.757701)\n",
            "Accuracy at step 6 - batch 606: 0.9088\n",
            "training loss at step 6 - batch 607: 0.32 (2019-08-04 13:36:59.771070)\n",
            "Accuracy at step 6 - batch 607: 0.9076\n",
            "training loss at step 6 - batch 608: 0.32 (2019-08-04 13:36:59.783949)\n",
            "Accuracy at step 6 - batch 608: 0.9012\n",
            "training loss at step 6 - batch 609: 0.31 (2019-08-04 13:36:59.797908)\n",
            "Accuracy at step 6 - batch 609: 0.9048\n",
            "training loss at step 6 - batch 610: 0.34 (2019-08-04 13:36:59.810367)\n",
            "Accuracy at step 6 - batch 610: 0.902\n",
            "training loss at step 6 - batch 611: 0.31 (2019-08-04 13:36:59.934513)\n",
            "Accuracy at step 6 - batch 611: 0.9016\n",
            "training loss at step 6 - batch 612: 0.31 (2019-08-04 13:36:59.948604)\n",
            "Accuracy at step 6 - batch 612: 0.9124\n",
            "training loss at step 6 - batch 613: 0.32 (2019-08-04 13:36:59.961966)\n",
            "Accuracy at step 6 - batch 613: 0.9032\n",
            "training loss at step 6 - batch 614: 0.29 (2019-08-04 13:36:59.975261)\n",
            "Accuracy at step 6 - batch 614: 0.9116\n",
            "training loss at step 6 - batch 615: 0.32 (2019-08-04 13:36:59.989155)\n",
            "Accuracy at step 6 - batch 615: 0.9044\n",
            "training loss at step 6 - batch 616: 0.31 (2019-08-04 13:37:00.110443)\n",
            "Accuracy at step 6 - batch 616: 0.908\n",
            "training loss at step 6 - batch 617: 0.32 (2019-08-04 13:37:00.128497)\n",
            "Accuracy at step 6 - batch 617: 0.9044\n",
            "training loss at step 6 - batch 618: 0.31 (2019-08-04 13:37:00.143216)\n",
            "Accuracy at step 6 - batch 618: 0.9088\n",
            "training loss at step 6 - batch 619: 0.33 (2019-08-04 13:37:00.156330)\n",
            "Accuracy at step 6 - batch 619: 0.9036\n",
            "training loss at step 6 - batch 620: 0.34 (2019-08-04 13:37:00.169002)\n",
            "Accuracy at step 6 - batch 620: 0.8972\n",
            "training loss at step 6 - batch 621: 0.32 (2019-08-04 13:37:00.291428)\n",
            "Accuracy at step 6 - batch 621: 0.9016\n",
            "training loss at step 6 - batch 622: 0.31 (2019-08-04 13:37:00.307982)\n",
            "Accuracy at step 6 - batch 622: 0.9064\n",
            "training loss at step 6 - batch 623: 0.32 (2019-08-04 13:37:00.320574)\n",
            "Accuracy at step 6 - batch 623: 0.9024\n",
            "training loss at step 6 - batch 624: 0.32 (2019-08-04 13:37:00.332395)\n",
            "Accuracy at step 6 - batch 624: 0.9052\n",
            "training loss at step 6 - batch 625: 0.30 (2019-08-04 13:37:00.348240)\n",
            "Accuracy at step 6 - batch 625: 0.9076\n",
            "training loss at step 6 - batch 626: 0.33 (2019-08-04 13:37:00.472574)\n",
            "Accuracy at step 6 - batch 626: 0.9008\n",
            "training loss at step 6 - batch 627: 0.31 (2019-08-04 13:37:00.490103)\n",
            "Accuracy at step 6 - batch 627: 0.904\n",
            "training loss at step 6 - batch 628: 0.30 (2019-08-04 13:37:00.505698)\n",
            "Accuracy at step 6 - batch 628: 0.9064\n",
            "training loss at step 6 - batch 629: 0.33 (2019-08-04 13:37:00.520389)\n",
            "Accuracy at step 6 - batch 629: 0.9028\n",
            "training loss at step 6 - batch 630: 0.32 (2019-08-04 13:37:00.533453)\n",
            "Accuracy at step 6 - batch 630: 0.902\n",
            "training loss at step 6 - batch 631: 0.33 (2019-08-04 13:37:00.656064)\n",
            "Accuracy at step 6 - batch 631: 0.9016\n",
            "training loss at step 6 - batch 632: 0.33 (2019-08-04 13:37:00.668363)\n",
            "Accuracy at step 6 - batch 632: 0.9044\n",
            "training loss at step 6 - batch 633: 0.32 (2019-08-04 13:37:00.680701)\n",
            "Accuracy at step 6 - batch 633: 0.906\n",
            "training loss at step 6 - batch 634: 0.32 (2019-08-04 13:37:00.692702)\n",
            "Accuracy at step 6 - batch 634: 0.9068\n",
            "training loss at step 6 - batch 635: 0.34 (2019-08-04 13:37:00.705159)\n",
            "Accuracy at step 6 - batch 635: 0.9008\n",
            "training loss at step 6 - batch 636: 0.32 (2019-08-04 13:37:00.821253)\n",
            "Accuracy at step 6 - batch 636: 0.9\n",
            "training loss at step 6 - batch 637: 0.31 (2019-08-04 13:37:00.835544)\n",
            "Accuracy at step 6 - batch 637: 0.9104\n",
            "training loss at step 6 - batch 638: 0.31 (2019-08-04 13:37:00.847215)\n",
            "Accuracy at step 6 - batch 638: 0.9088\n",
            "training loss at step 6 - batch 639: 0.31 (2019-08-04 13:37:00.862948)\n",
            "Accuracy at step 6 - batch 639: 0.9068\n",
            "training loss at step 6 - batch 640: 0.33 (2019-08-04 13:37:00.878440)\n",
            "Accuracy at step 6 - batch 640: 0.8972\n",
            "training loss at step 6 - batch 641: 0.31 (2019-08-04 13:37:00.998576)\n",
            "Accuracy at step 6 - batch 641: 0.9064\n",
            "training loss at step 6 - batch 642: 0.33 (2019-08-04 13:37:01.015915)\n",
            "Accuracy at step 6 - batch 642: 0.9052\n",
            "training loss at step 6 - batch 643: 0.30 (2019-08-04 13:37:01.028301)\n",
            "Accuracy at step 6 - batch 643: 0.9108\n",
            "training loss at step 6 - batch 644: 0.30 (2019-08-04 13:37:01.041278)\n",
            "Accuracy at step 6 - batch 644: 0.9136\n",
            "training loss at step 6 - batch 645: 0.30 (2019-08-04 13:37:01.053669)\n",
            "Accuracy at step 6 - batch 645: 0.9068\n",
            "training loss at step 6 - batch 646: 0.33 (2019-08-04 13:37:01.177578)\n",
            "Accuracy at step 6 - batch 646: 0.9048\n",
            "training loss at step 6 - batch 647: 0.32 (2019-08-04 13:37:01.191896)\n",
            "Accuracy at step 6 - batch 647: 0.9068\n",
            "training loss at step 6 - batch 648: 0.32 (2019-08-04 13:37:01.205897)\n",
            "Accuracy at step 6 - batch 648: 0.902\n",
            "training loss at step 6 - batch 649: 0.32 (2019-08-04 13:37:01.218153)\n",
            "Accuracy at step 6 - batch 649: 0.904\n",
            "training loss at step 6 - batch 650: 0.32 (2019-08-04 13:37:01.231925)\n",
            "Accuracy at step 6 - batch 650: 0.9044\n",
            "training loss at step 6 - batch 651: 0.33 (2019-08-04 13:37:01.372624)\n",
            "Accuracy at step 6 - batch 651: 0.8996\n",
            "training loss at step 6 - batch 652: 0.30 (2019-08-04 13:37:01.387791)\n",
            "Accuracy at step 6 - batch 652: 0.9108\n",
            "training loss at step 6 - batch 653: 0.33 (2019-08-04 13:37:01.400319)\n",
            "Accuracy at step 6 - batch 653: 0.9056\n",
            "training loss at step 6 - batch 654: 0.31 (2019-08-04 13:37:01.413115)\n",
            "Accuracy at step 6 - batch 654: 0.9052\n",
            "training loss at step 6 - batch 655: 0.31 (2019-08-04 13:37:01.426846)\n",
            "Accuracy at step 6 - batch 655: 0.904\n",
            "training loss at step 6 - batch 656: 0.34 (2019-08-04 13:37:01.551069)\n",
            "Accuracy at step 6 - batch 656: 0.9016\n",
            "training loss at step 6 - batch 657: 0.32 (2019-08-04 13:37:01.564339)\n",
            "Accuracy at step 6 - batch 657: 0.9012\n",
            "training loss at step 6 - batch 658: 0.33 (2019-08-04 13:37:01.577060)\n",
            "Accuracy at step 6 - batch 658: 0.9024\n",
            "training loss at step 6 - batch 659: 0.31 (2019-08-04 13:37:01.590635)\n",
            "Accuracy at step 6 - batch 659: 0.9076\n",
            "training loss at step 6 - batch 660: 0.32 (2019-08-04 13:37:01.606454)\n",
            "Accuracy at step 6 - batch 660: 0.9016\n",
            "training loss at step 6 - batch 661: 0.31 (2019-08-04 13:37:01.743694)\n",
            "Accuracy at step 6 - batch 661: 0.912\n",
            "training loss at step 6 - batch 662: 0.31 (2019-08-04 13:37:01.756235)\n",
            "Accuracy at step 6 - batch 662: 0.9104\n",
            "training loss at step 6 - batch 663: 0.31 (2019-08-04 13:37:01.768343)\n",
            "Accuracy at step 6 - batch 663: 0.9084\n",
            "training loss at step 6 - batch 664: 0.30 (2019-08-04 13:37:01.780864)\n",
            "Accuracy at step 6 - batch 664: 0.9164\n",
            "training loss at step 6 - batch 665: 0.32 (2019-08-04 13:37:01.795261)\n",
            "Accuracy at step 6 - batch 665: 0.9044\n",
            "training loss at step 6 - batch 666: 0.32 (2019-08-04 13:37:01.920021)\n",
            "Accuracy at step 6 - batch 666: 0.9076\n",
            "training loss at step 6 - batch 667: 0.29 (2019-08-04 13:37:01.932357)\n",
            "Accuracy at step 6 - batch 667: 0.9084\n",
            "training loss at step 6 - batch 668: 0.31 (2019-08-04 13:37:01.945993)\n",
            "Accuracy at step 6 - batch 668: 0.904\n",
            "training loss at step 6 - batch 669: 0.33 (2019-08-04 13:37:01.959105)\n",
            "Accuracy at step 6 - batch 669: 0.8976\n",
            "training loss at step 6 - batch 670: 0.31 (2019-08-04 13:37:01.971088)\n",
            "Accuracy at step 6 - batch 670: 0.912\n",
            "training loss at step 6 - batch 671: 0.30 (2019-08-04 13:37:02.099668)\n",
            "Accuracy at step 6 - batch 671: 0.9048\n",
            "training loss at step 6 - batch 672: 0.32 (2019-08-04 13:37:02.116687)\n",
            "Accuracy at step 6 - batch 672: 0.906\n",
            "training loss at step 6 - batch 673: 0.34 (2019-08-04 13:37:02.128743)\n",
            "Accuracy at step 6 - batch 673: 0.8948\n",
            "training loss at step 6 - batch 674: 0.32 (2019-08-04 13:37:02.142561)\n",
            "Accuracy at step 6 - batch 674: 0.9048\n",
            "training loss at step 6 - batch 675: 0.31 (2019-08-04 13:37:02.155152)\n",
            "Accuracy at step 6 - batch 675: 0.9048\n",
            "training loss at step 6 - batch 676: 0.32 (2019-08-04 13:37:02.275923)\n",
            "Accuracy at step 6 - batch 676: 0.9048\n",
            "training loss at step 6 - batch 677: 0.31 (2019-08-04 13:37:02.292740)\n",
            "Accuracy at step 6 - batch 677: 0.904\n",
            "training loss at step 6 - batch 678: 0.31 (2019-08-04 13:37:02.306935)\n",
            "Accuracy at step 6 - batch 678: 0.9104\n",
            "training loss at step 6 - batch 679: 0.31 (2019-08-04 13:37:02.320543)\n",
            "Accuracy at step 6 - batch 679: 0.9092\n",
            "training loss at step 6 - batch 680: 0.32 (2019-08-04 13:37:02.333170)\n",
            "Accuracy at step 6 - batch 680: 0.9084\n",
            "training loss at step 6 - batch 681: 0.32 (2019-08-04 13:37:02.454366)\n",
            "Accuracy at step 6 - batch 681: 0.9096\n",
            "training loss at step 6 - batch 682: 0.32 (2019-08-04 13:37:02.466700)\n",
            "Accuracy at step 6 - batch 682: 0.9064\n",
            "training loss at step 6 - batch 683: 0.33 (2019-08-04 13:37:02.479969)\n",
            "Accuracy at step 6 - batch 683: 0.9064\n",
            "training loss at step 6 - batch 684: 0.31 (2019-08-04 13:37:02.492556)\n",
            "Accuracy at step 6 - batch 684: 0.9068\n",
            "training loss at step 6 - batch 685: 0.31 (2019-08-04 13:37:02.505244)\n",
            "Accuracy at step 6 - batch 685: 0.9056\n",
            "training loss at step 6 - batch 686: 0.31 (2019-08-04 13:37:02.635271)\n",
            "Accuracy at step 6 - batch 686: 0.91\n",
            "training loss at step 6 - batch 687: 0.33 (2019-08-04 13:37:02.647928)\n",
            "Accuracy at step 6 - batch 687: 0.906\n",
            "training loss at step 6 - batch 688: 0.33 (2019-08-04 13:37:02.661142)\n",
            "Accuracy at step 6 - batch 688: 0.9016\n",
            "training loss at step 6 - batch 689: 0.33 (2019-08-04 13:37:02.673358)\n",
            "Accuracy at step 6 - batch 689: 0.9064\n",
            "training loss at step 6 - batch 690: 0.33 (2019-08-04 13:37:02.686269)\n",
            "Accuracy at step 6 - batch 690: 0.9024\n",
            "training loss at step 6 - batch 691: 0.32 (2019-08-04 13:37:02.800869)\n",
            "Accuracy at step 6 - batch 691: 0.9044\n",
            "training loss at step 6 - batch 692: 0.34 (2019-08-04 13:37:02.813009)\n",
            "Accuracy at step 6 - batch 692: 0.8964\n",
            "training loss at step 6 - batch 693: 0.32 (2019-08-04 13:37:02.824959)\n",
            "Accuracy at step 6 - batch 693: 0.904\n",
            "training loss at step 6 - batch 694: 0.31 (2019-08-04 13:37:02.840604)\n",
            "Accuracy at step 6 - batch 694: 0.9064\n",
            "training loss at step 6 - batch 695: 0.31 (2019-08-04 13:37:02.853366)\n",
            "Accuracy at step 6 - batch 695: 0.9048\n",
            "training loss at step 6 - batch 696: 0.30 (2019-08-04 13:37:02.971961)\n",
            "Accuracy at step 6 - batch 696: 0.91\n",
            "training loss at step 6 - batch 697: 0.32 (2019-08-04 13:37:02.988914)\n",
            "Accuracy at step 6 - batch 697: 0.9012\n",
            "training loss at step 6 - batch 698: 0.33 (2019-08-04 13:37:03.002352)\n",
            "Accuracy at step 6 - batch 698: 0.904\n",
            "training loss at step 6 - batch 699: 0.29 (2019-08-04 13:37:03.014420)\n",
            "Accuracy at step 6 - batch 699: 0.9172\n",
            "training loss at step 6 - batch 700: 0.32 (2019-08-04 13:37:03.026665)\n",
            "Accuracy at step 6 - batch 700: 0.9012\n",
            "training loss at step 6 - batch 701: 0.30 (2019-08-04 13:37:03.149859)\n",
            "Accuracy at step 6 - batch 701: 0.904\n",
            "training loss at step 6 - batch 702: 0.31 (2019-08-04 13:37:03.163339)\n",
            "Accuracy at step 6 - batch 702: 0.9052\n",
            "training loss at step 6 - batch 703: 0.32 (2019-08-04 13:37:03.176300)\n",
            "Accuracy at step 6 - batch 703: 0.9048\n",
            "training loss at step 6 - batch 704: 0.31 (2019-08-04 13:37:03.189709)\n",
            "Accuracy at step 6 - batch 704: 0.9096\n",
            "training loss at step 6 - batch 705: 0.32 (2019-08-04 13:37:03.201827)\n",
            "Accuracy at step 6 - batch 705: 0.9024\n",
            "training loss at step 6 - batch 706: 0.34 (2019-08-04 13:37:03.320206)\n",
            "Accuracy at step 6 - batch 706: 0.8992\n",
            "training loss at step 6 - batch 707: 0.31 (2019-08-04 13:37:03.336217)\n",
            "Accuracy at step 6 - batch 707: 0.9052\n",
            "training loss at step 6 - batch 708: 0.32 (2019-08-04 13:37:03.348435)\n",
            "Accuracy at step 6 - batch 708: 0.9024\n",
            "training loss at step 6 - batch 709: 0.30 (2019-08-04 13:37:03.365280)\n",
            "Accuracy at step 6 - batch 709: 0.9152\n",
            "training loss at step 6 - batch 710: 0.33 (2019-08-04 13:37:03.377922)\n",
            "Accuracy at step 6 - batch 710: 0.8992\n",
            "training loss at step 6 - batch 711: 0.31 (2019-08-04 13:37:03.499315)\n",
            "Accuracy at step 6 - batch 711: 0.9096\n",
            "training loss at step 6 - batch 712: 0.32 (2019-08-04 13:37:03.511500)\n",
            "Accuracy at step 6 - batch 712: 0.9032\n",
            "training loss at step 6 - batch 713: 0.34 (2019-08-04 13:37:03.523938)\n",
            "Accuracy at step 6 - batch 713: 0.8976\n",
            "training loss at step 6 - batch 714: 0.32 (2019-08-04 13:37:03.536339)\n",
            "Accuracy at step 6 - batch 714: 0.9028\n",
            "training loss at step 6 - batch 715: 0.33 (2019-08-04 13:37:03.549225)\n",
            "Accuracy at step 6 - batch 715: 0.9028\n",
            "training loss at step 6 - batch 716: 0.33 (2019-08-04 13:37:03.681936)\n",
            "Accuracy at step 6 - batch 716: 0.902\n",
            "training loss at step 6 - batch 717: 0.30 (2019-08-04 13:37:03.695593)\n",
            "Accuracy at step 6 - batch 717: 0.9128\n",
            "training loss at step 6 - batch 718: 0.31 (2019-08-04 13:37:03.708199)\n",
            "Accuracy at step 6 - batch 718: 0.9064\n",
            "training loss at step 6 - batch 719: 0.33 (2019-08-04 13:37:03.720563)\n",
            "Accuracy at step 6 - batch 719: 0.9024\n",
            "training loss at step 6 - batch 720: 0.32 (2019-08-04 13:37:03.732904)\n",
            "Accuracy at step 6 - batch 720: 0.9052\n",
            "training loss at step 6 - batch 721: 0.30 (2019-08-04 13:37:03.852246)\n",
            "Accuracy at step 6 - batch 721: 0.9136\n",
            "training loss at step 6 - batch 722: 0.33 (2019-08-04 13:37:03.866141)\n",
            "Accuracy at step 6 - batch 722: 0.8988\n",
            "training loss at step 6 - batch 723: 0.29 (2019-08-04 13:37:03.879057)\n",
            "Accuracy at step 6 - batch 723: 0.9156\n",
            "training loss at step 6 - batch 724: 0.31 (2019-08-04 13:37:03.893600)\n",
            "Accuracy at step 6 - batch 724: 0.904\n",
            "training loss at step 6 - batch 725: 0.30 (2019-08-04 13:37:03.905939)\n",
            "Accuracy at step 6 - batch 725: 0.9024\n",
            "training loss at step 6 - batch 726: 0.31 (2019-08-04 13:37:04.024422)\n",
            "Accuracy at step 6 - batch 726: 0.9044\n",
            "training loss at step 6 - batch 727: 0.30 (2019-08-04 13:37:04.037626)\n",
            "Accuracy at step 6 - batch 727: 0.9092\n",
            "training loss at step 6 - batch 728: 0.33 (2019-08-04 13:37:04.050405)\n",
            "Accuracy at step 6 - batch 728: 0.8992\n",
            "training loss at step 6 - batch 729: 0.30 (2019-08-04 13:37:04.063266)\n",
            "Accuracy at step 6 - batch 729: 0.91\n",
            "training loss at step 6 - batch 730: 0.32 (2019-08-04 13:37:04.076320)\n",
            "Accuracy at step 6 - batch 730: 0.9052\n",
            "training loss at step 6 - batch 731: 0.34 (2019-08-04 13:37:04.203866)\n",
            "Accuracy at step 6 - batch 731: 0.9\n",
            "training loss at step 6 - batch 732: 0.31 (2019-08-04 13:37:04.221234)\n",
            "Accuracy at step 6 - batch 732: 0.906\n",
            "training loss at step 6 - batch 733: 0.33 (2019-08-04 13:37:04.235208)\n",
            "Accuracy at step 6 - batch 733: 0.9008\n",
            "training loss at step 6 - batch 734: 0.32 (2019-08-04 13:37:04.248124)\n",
            "Accuracy at step 6 - batch 734: 0.9048\n",
            "training loss at step 6 - batch 735: 0.35 (2019-08-04 13:37:04.261547)\n",
            "Accuracy at step 6 - batch 735: 0.8944\n",
            "training loss at step 6 - batch 736: 0.33 (2019-08-04 13:37:04.381595)\n",
            "Accuracy at step 6 - batch 736: 0.9072\n",
            "training loss at step 6 - batch 737: 0.31 (2019-08-04 13:37:04.399249)\n",
            "Accuracy at step 6 - batch 737: 0.9076\n",
            "training loss at step 6 - batch 738: 0.32 (2019-08-04 13:37:04.415166)\n",
            "Accuracy at step 6 - batch 738: 0.904\n",
            "training loss at step 6 - batch 739: 0.32 (2019-08-04 13:37:04.429616)\n",
            "Accuracy at step 6 - batch 739: 0.9036\n",
            "training loss at step 6 - batch 740: 0.33 (2019-08-04 13:37:04.441836)\n",
            "Accuracy at step 6 - batch 740: 0.8972\n",
            "training loss at step 6 - batch 741: 0.30 (2019-08-04 13:37:04.564727)\n",
            "Accuracy at step 6 - batch 741: 0.9024\n",
            "training loss at step 6 - batch 742: 0.32 (2019-08-04 13:37:04.579449)\n",
            "Accuracy at step 6 - batch 742: 0.9024\n",
            "training loss at step 6 - batch 743: 0.33 (2019-08-04 13:37:04.594522)\n",
            "Accuracy at step 6 - batch 743: 0.9048\n",
            "training loss at step 6 - batch 744: 0.30 (2019-08-04 13:37:04.610161)\n",
            "Accuracy at step 6 - batch 744: 0.912\n",
            "training loss at step 6 - batch 745: 0.31 (2019-08-04 13:37:04.625517)\n",
            "Accuracy at step 6 - batch 745: 0.9084\n",
            "training loss at step 6 - batch 746: 0.32 (2019-08-04 13:37:04.746566)\n",
            "Accuracy at step 6 - batch 746: 0.902\n",
            "training loss at step 6 - batch 747: 0.33 (2019-08-04 13:37:04.759019)\n",
            "Accuracy at step 6 - batch 747: 0.9004\n",
            "training loss at step 6 - batch 748: 0.31 (2019-08-04 13:37:04.772295)\n",
            "Accuracy at step 6 - batch 748: 0.9068\n",
            "training loss at step 6 - batch 749: 0.31 (2019-08-04 13:37:04.785372)\n",
            "Accuracy at step 6 - batch 749: 0.904\n",
            "training loss at step 6 - batch 750: 0.31 (2019-08-04 13:37:04.798281)\n",
            "Accuracy at step 6 - batch 750: 0.9072\n",
            "training loss at step 6 - batch 751: 0.32 (2019-08-04 13:37:04.919830)\n",
            "Accuracy at step 6 - batch 751: 0.9092\n",
            "training loss at step 6 - batch 752: 0.33 (2019-08-04 13:37:04.937177)\n",
            "Accuracy at step 6 - batch 752: 0.9032\n",
            "training loss at step 6 - batch 753: 0.31 (2019-08-04 13:37:04.951382)\n",
            "Accuracy at step 6 - batch 753: 0.9112\n",
            "training loss at step 6 - batch 754: 0.31 (2019-08-04 13:37:04.966509)\n",
            "Accuracy at step 6 - batch 754: 0.9092\n",
            "training loss at step 6 - batch 755: 0.31 (2019-08-04 13:37:04.979457)\n",
            "Accuracy at step 6 - batch 755: 0.9048\n",
            "training loss at step 6 - batch 756: 0.32 (2019-08-04 13:37:05.104627)\n",
            "Accuracy at step 6 - batch 756: 0.9068\n",
            "training loss at step 6 - batch 757: 0.32 (2019-08-04 13:37:05.121767)\n",
            "Accuracy at step 6 - batch 757: 0.906\n",
            "training loss at step 6 - batch 758: 0.31 (2019-08-04 13:37:05.134734)\n",
            "Accuracy at step 6 - batch 758: 0.9064\n",
            "training loss at step 6 - batch 759: 0.32 (2019-08-04 13:37:05.147177)\n",
            "Accuracy at step 6 - batch 759: 0.9072\n",
            "training loss at step 6 - batch 760: 0.31 (2019-08-04 13:37:05.160269)\n",
            "Accuracy at step 6 - batch 760: 0.9076\n",
            "training loss at step 6 - batch 761: 0.31 (2019-08-04 13:37:05.281702)\n",
            "Accuracy at step 6 - batch 761: 0.9008\n",
            "training loss at step 6 - batch 762: 0.30 (2019-08-04 13:37:05.299026)\n",
            "Accuracy at step 6 - batch 762: 0.9112\n",
            "training loss at step 6 - batch 763: 0.33 (2019-08-04 13:37:05.311104)\n",
            "Accuracy at step 6 - batch 763: 0.9044\n",
            "training loss at step 6 - batch 764: 0.32 (2019-08-04 13:37:05.325205)\n",
            "Accuracy at step 6 - batch 764: 0.9044\n",
            "training loss at step 6 - batch 765: 0.29 (2019-08-04 13:37:05.340445)\n",
            "Accuracy at step 6 - batch 765: 0.918\n",
            "training loss at step 6 - batch 766: 0.32 (2019-08-04 13:37:05.473602)\n",
            "Accuracy at step 6 - batch 766: 0.9024\n",
            "training loss at step 6 - batch 767: 0.31 (2019-08-04 13:37:05.489785)\n",
            "Accuracy at step 6 - batch 767: 0.9056\n",
            "training loss at step 6 - batch 768: 0.30 (2019-08-04 13:37:05.502357)\n",
            "Accuracy at step 6 - batch 768: 0.9084\n",
            "training loss at step 6 - batch 769: 0.30 (2019-08-04 13:37:05.515258)\n",
            "Accuracy at step 6 - batch 769: 0.9084\n",
            "training loss at step 6 - batch 770: 0.29 (2019-08-04 13:37:05.530652)\n",
            "Accuracy at step 6 - batch 770: 0.9084\n",
            "training loss at step 6 - batch 771: 0.30 (2019-08-04 13:37:05.658238)\n",
            "Accuracy at step 6 - batch 771: 0.9108\n",
            "training loss at step 6 - batch 772: 0.31 (2019-08-04 13:37:05.670422)\n",
            "Accuracy at step 6 - batch 772: 0.9024\n",
            "training loss at step 6 - batch 773: 0.32 (2019-08-04 13:37:05.682786)\n",
            "Accuracy at step 6 - batch 773: 0.904\n",
            "training loss at step 6 - batch 774: 0.32 (2019-08-04 13:37:05.695514)\n",
            "Accuracy at step 6 - batch 774: 0.9068\n",
            "training loss at step 6 - batch 775: 0.33 (2019-08-04 13:37:05.707854)\n",
            "Accuracy at step 6 - batch 775: 0.9072\n",
            "training loss at step 6 - batch 776: 0.29 (2019-08-04 13:37:05.836729)\n",
            "Accuracy at step 6 - batch 776: 0.9116\n",
            "training loss at step 6 - batch 777: 0.34 (2019-08-04 13:37:05.849975)\n",
            "Accuracy at step 6 - batch 777: 0.9052\n",
            "training loss at step 6 - batch 778: 0.30 (2019-08-04 13:37:05.863620)\n",
            "Accuracy at step 6 - batch 778: 0.9108\n",
            "training loss at step 6 - batch 779: 0.32 (2019-08-04 13:37:05.875974)\n",
            "Accuracy at step 6 - batch 779: 0.898\n",
            "training loss at step 7 - batch 0: 0.31 (2019-08-04 13:37:05.888852)\n",
            "Accuracy at step 7 - batch 0: 0.9084\n",
            "training loss at step 7 - batch 1: 0.30 (2019-08-04 13:37:06.006402)\n",
            "Accuracy at step 7 - batch 1: 0.9136\n",
            "training loss at step 7 - batch 2: 0.32 (2019-08-04 13:37:06.019585)\n",
            "Accuracy at step 7 - batch 2: 0.9056\n",
            "training loss at step 7 - batch 3: 0.33 (2019-08-04 13:37:06.031852)\n",
            "Accuracy at step 7 - batch 3: 0.9028\n",
            "training loss at step 7 - batch 4: 0.31 (2019-08-04 13:37:06.048618)\n",
            "Accuracy at step 7 - batch 4: 0.9084\n",
            "training loss at step 7 - batch 5: 0.30 (2019-08-04 13:37:06.061417)\n",
            "Accuracy at step 7 - batch 5: 0.908\n",
            "training loss at step 7 - batch 6: 0.28 (2019-08-04 13:37:06.184881)\n",
            "Accuracy at step 7 - batch 6: 0.9132\n",
            "training loss at step 7 - batch 7: 0.34 (2019-08-04 13:37:06.203305)\n",
            "Accuracy at step 7 - batch 7: 0.9\n",
            "training loss at step 7 - batch 8: 0.32 (2019-08-04 13:37:06.216129)\n",
            "Accuracy at step 7 - batch 8: 0.9112\n",
            "training loss at step 7 - batch 9: 0.33 (2019-08-04 13:37:06.230364)\n",
            "Accuracy at step 7 - batch 9: 0.9036\n",
            "training loss at step 7 - batch 10: 0.34 (2019-08-04 13:37:06.243666)\n",
            "Accuracy at step 7 - batch 10: 0.9004\n",
            "training loss at step 7 - batch 11: 0.32 (2019-08-04 13:37:06.387669)\n",
            "Accuracy at step 7 - batch 11: 0.9032\n",
            "training loss at step 7 - batch 12: 0.32 (2019-08-04 13:37:06.401393)\n",
            "Accuracy at step 7 - batch 12: 0.9036\n",
            "training loss at step 7 - batch 13: 0.31 (2019-08-04 13:37:06.413407)\n",
            "Accuracy at step 7 - batch 13: 0.9052\n",
            "training loss at step 7 - batch 14: 0.34 (2019-08-04 13:37:06.425424)\n",
            "Accuracy at step 7 - batch 14: 0.8992\n",
            "training loss at step 7 - batch 15: 0.33 (2019-08-04 13:37:06.437883)\n",
            "Accuracy at step 7 - batch 15: 0.9096\n",
            "training loss at step 7 - batch 16: 0.30 (2019-08-04 13:37:06.557756)\n",
            "Accuracy at step 7 - batch 16: 0.9104\n",
            "training loss at step 7 - batch 17: 0.31 (2019-08-04 13:37:06.575727)\n",
            "Accuracy at step 7 - batch 17: 0.9064\n",
            "training loss at step 7 - batch 18: 0.31 (2019-08-04 13:37:06.588255)\n",
            "Accuracy at step 7 - batch 18: 0.9092\n",
            "training loss at step 7 - batch 19: 0.31 (2019-08-04 13:37:06.602982)\n",
            "Accuracy at step 7 - batch 19: 0.9056\n",
            "training loss at step 7 - batch 20: 0.29 (2019-08-04 13:37:06.615270)\n",
            "Accuracy at step 7 - batch 20: 0.9148\n",
            "training loss at step 7 - batch 21: 0.31 (2019-08-04 13:37:06.741139)\n",
            "Accuracy at step 7 - batch 21: 0.9084\n",
            "training loss at step 7 - batch 22: 0.33 (2019-08-04 13:37:06.754264)\n",
            "Accuracy at step 7 - batch 22: 0.9108\n",
            "training loss at step 7 - batch 23: 0.31 (2019-08-04 13:37:06.766686)\n",
            "Accuracy at step 7 - batch 23: 0.9064\n",
            "training loss at step 7 - batch 24: 0.34 (2019-08-04 13:37:06.778914)\n",
            "Accuracy at step 7 - batch 24: 0.9012\n",
            "training loss at step 7 - batch 25: 0.30 (2019-08-04 13:37:06.791278)\n",
            "Accuracy at step 7 - batch 25: 0.9068\n",
            "training loss at step 7 - batch 26: 0.30 (2019-08-04 13:37:06.910888)\n",
            "Accuracy at step 7 - batch 26: 0.9048\n",
            "training loss at step 7 - batch 27: 0.33 (2019-08-04 13:37:06.927211)\n",
            "Accuracy at step 7 - batch 27: 0.904\n",
            "training loss at step 7 - batch 28: 0.32 (2019-08-04 13:37:06.940031)\n",
            "Accuracy at step 7 - batch 28: 0.9068\n",
            "training loss at step 7 - batch 29: 0.30 (2019-08-04 13:37:06.951996)\n",
            "Accuracy at step 7 - batch 29: 0.9072\n",
            "training loss at step 7 - batch 30: 0.30 (2019-08-04 13:37:06.964499)\n",
            "Accuracy at step 7 - batch 30: 0.9144\n",
            "training loss at step 7 - batch 31: 0.31 (2019-08-04 13:37:07.090272)\n",
            "Accuracy at step 7 - batch 31: 0.9068\n",
            "training loss at step 7 - batch 32: 0.35 (2019-08-04 13:37:07.102857)\n",
            "Accuracy at step 7 - batch 32: 0.8976\n",
            "training loss at step 7 - batch 33: 0.31 (2019-08-04 13:37:07.115275)\n",
            "Accuracy at step 7 - batch 33: 0.9064\n",
            "training loss at step 7 - batch 34: 0.29 (2019-08-04 13:37:07.128301)\n",
            "Accuracy at step 7 - batch 34: 0.906\n",
            "training loss at step 7 - batch 35: 0.30 (2019-08-04 13:37:07.144000)\n",
            "Accuracy at step 7 - batch 35: 0.9108\n",
            "training loss at step 7 - batch 36: 0.33 (2019-08-04 13:37:07.263906)\n",
            "Accuracy at step 7 - batch 36: 0.9012\n",
            "training loss at step 7 - batch 37: 0.32 (2019-08-04 13:37:07.277556)\n",
            "Accuracy at step 7 - batch 37: 0.9096\n",
            "training loss at step 7 - batch 38: 0.32 (2019-08-04 13:37:07.289346)\n",
            "Accuracy at step 7 - batch 38: 0.9076\n",
            "training loss at step 7 - batch 39: 0.31 (2019-08-04 13:37:07.303140)\n",
            "Accuracy at step 7 - batch 39: 0.9052\n",
            "training loss at step 7 - batch 40: 0.35 (2019-08-04 13:37:07.316408)\n",
            "Accuracy at step 7 - batch 40: 0.8972\n",
            "training loss at step 7 - batch 41: 0.32 (2019-08-04 13:37:07.434541)\n",
            "Accuracy at step 7 - batch 41: 0.9032\n",
            "training loss at step 7 - batch 42: 0.30 (2019-08-04 13:37:07.449916)\n",
            "Accuracy at step 7 - batch 42: 0.9124\n",
            "training loss at step 7 - batch 43: 0.32 (2019-08-04 13:37:07.462550)\n",
            "Accuracy at step 7 - batch 43: 0.908\n",
            "training loss at step 7 - batch 44: 0.34 (2019-08-04 13:37:07.476014)\n",
            "Accuracy at step 7 - batch 44: 0.9008\n",
            "training loss at step 7 - batch 45: 0.32 (2019-08-04 13:37:07.488392)\n",
            "Accuracy at step 7 - batch 45: 0.9056\n",
            "training loss at step 7 - batch 46: 0.31 (2019-08-04 13:37:07.610725)\n",
            "Accuracy at step 7 - batch 46: 0.9092\n",
            "training loss at step 7 - batch 47: 0.28 (2019-08-04 13:37:07.624526)\n",
            "Accuracy at step 7 - batch 47: 0.912\n",
            "training loss at step 7 - batch 48: 0.31 (2019-08-04 13:37:07.638151)\n",
            "Accuracy at step 7 - batch 48: 0.908\n",
            "training loss at step 7 - batch 49: 0.30 (2019-08-04 13:37:07.651912)\n",
            "Accuracy at step 7 - batch 49: 0.9136\n",
            "training loss at step 7 - batch 50: 0.32 (2019-08-04 13:37:07.667439)\n",
            "Accuracy at step 7 - batch 50: 0.896\n",
            "training loss at step 7 - batch 51: 0.33 (2019-08-04 13:37:07.790137)\n",
            "Accuracy at step 7 - batch 51: 0.9048\n",
            "training loss at step 7 - batch 52: 0.33 (2019-08-04 13:37:07.803546)\n",
            "Accuracy at step 7 - batch 52: 0.9048\n",
            "training loss at step 7 - batch 53: 0.31 (2019-08-04 13:37:07.817182)\n",
            "Accuracy at step 7 - batch 53: 0.9096\n",
            "training loss at step 7 - batch 54: 0.31 (2019-08-04 13:37:07.832297)\n",
            "Accuracy at step 7 - batch 54: 0.9056\n",
            "training loss at step 7 - batch 55: 0.31 (2019-08-04 13:37:07.845037)\n",
            "Accuracy at step 7 - batch 55: 0.9088\n",
            "training loss at step 7 - batch 56: 0.30 (2019-08-04 13:37:07.963052)\n",
            "Accuracy at step 7 - batch 56: 0.908\n",
            "training loss at step 7 - batch 57: 0.32 (2019-08-04 13:37:07.977576)\n",
            "Accuracy at step 7 - batch 57: 0.8988\n",
            "training loss at step 7 - batch 58: 0.30 (2019-08-04 13:37:07.989390)\n",
            "Accuracy at step 7 - batch 58: 0.9088\n",
            "training loss at step 7 - batch 59: 0.31 (2019-08-04 13:37:08.001892)\n",
            "Accuracy at step 7 - batch 59: 0.9124\n",
            "training loss at step 7 - batch 60: 0.32 (2019-08-04 13:37:08.013907)\n",
            "Accuracy at step 7 - batch 60: 0.9048\n",
            "training loss at step 7 - batch 61: 0.32 (2019-08-04 13:37:08.140857)\n",
            "Accuracy at step 7 - batch 61: 0.902\n",
            "training loss at step 7 - batch 62: 0.32 (2019-08-04 13:37:08.153531)\n",
            "Accuracy at step 7 - batch 62: 0.9064\n",
            "training loss at step 7 - batch 63: 0.34 (2019-08-04 13:37:08.165749)\n",
            "Accuracy at step 7 - batch 63: 0.9012\n",
            "training loss at step 7 - batch 64: 0.33 (2019-08-04 13:37:08.177861)\n",
            "Accuracy at step 7 - batch 64: 0.9004\n",
            "training loss at step 7 - batch 65: 0.34 (2019-08-04 13:37:08.190128)\n",
            "Accuracy at step 7 - batch 65: 0.8968\n",
            "training loss at step 7 - batch 66: 0.33 (2019-08-04 13:37:08.311532)\n",
            "Accuracy at step 7 - batch 66: 0.9052\n",
            "training loss at step 7 - batch 67: 0.33 (2019-08-04 13:37:08.325526)\n",
            "Accuracy at step 7 - batch 67: 0.9\n",
            "training loss at step 7 - batch 68: 0.33 (2019-08-04 13:37:08.337884)\n",
            "Accuracy at step 7 - batch 68: 0.9036\n",
            "training loss at step 7 - batch 69: 0.30 (2019-08-04 13:37:08.354605)\n",
            "Accuracy at step 7 - batch 69: 0.906\n",
            "training loss at step 7 - batch 70: 0.31 (2019-08-04 13:37:08.370409)\n",
            "Accuracy at step 7 - batch 70: 0.9092\n",
            "training loss at step 7 - batch 71: 0.29 (2019-08-04 13:37:08.491444)\n",
            "Accuracy at step 7 - batch 71: 0.9084\n",
            "training loss at step 7 - batch 72: 0.29 (2019-08-04 13:37:08.505559)\n",
            "Accuracy at step 7 - batch 72: 0.9132\n",
            "training loss at step 7 - batch 73: 0.31 (2019-08-04 13:37:08.517976)\n",
            "Accuracy at step 7 - batch 73: 0.9048\n",
            "training loss at step 7 - batch 74: 0.31 (2019-08-04 13:37:08.531153)\n",
            "Accuracy at step 7 - batch 74: 0.908\n",
            "training loss at step 7 - batch 75: 0.32 (2019-08-04 13:37:08.544165)\n",
            "Accuracy at step 7 - batch 75: 0.9068\n",
            "training loss at step 7 - batch 76: 0.33 (2019-08-04 13:37:08.660937)\n",
            "Accuracy at step 7 - batch 76: 0.9032\n",
            "training loss at step 7 - batch 77: 0.32 (2019-08-04 13:37:08.675185)\n",
            "Accuracy at step 7 - batch 77: 0.908\n",
            "training loss at step 7 - batch 78: 0.32 (2019-08-04 13:37:08.692133)\n",
            "Accuracy at step 7 - batch 78: 0.9092\n",
            "training loss at step 7 - batch 79: 0.32 (2019-08-04 13:37:08.706061)\n",
            "Accuracy at step 7 - batch 79: 0.9004\n",
            "training loss at step 7 - batch 80: 0.33 (2019-08-04 13:37:08.721063)\n",
            "Accuracy at step 7 - batch 80: 0.902\n",
            "training loss at step 7 - batch 81: 0.32 (2019-08-04 13:37:08.841641)\n",
            "Accuracy at step 7 - batch 81: 0.9036\n",
            "training loss at step 7 - batch 82: 0.30 (2019-08-04 13:37:08.855865)\n",
            "Accuracy at step 7 - batch 82: 0.9108\n",
            "training loss at step 7 - batch 83: 0.31 (2019-08-04 13:37:08.870228)\n",
            "Accuracy at step 7 - batch 83: 0.906\n",
            "training loss at step 7 - batch 84: 0.31 (2019-08-04 13:37:08.883012)\n",
            "Accuracy at step 7 - batch 84: 0.9032\n",
            "training loss at step 7 - batch 85: 0.32 (2019-08-04 13:37:08.895064)\n",
            "Accuracy at step 7 - batch 85: 0.9064\n",
            "training loss at step 7 - batch 86: 0.30 (2019-08-04 13:37:09.018513)\n",
            "Accuracy at step 7 - batch 86: 0.9084\n",
            "training loss at step 7 - batch 87: 0.32 (2019-08-04 13:37:09.035522)\n",
            "Accuracy at step 7 - batch 87: 0.904\n",
            "training loss at step 7 - batch 88: 0.34 (2019-08-04 13:37:09.047845)\n",
            "Accuracy at step 7 - batch 88: 0.8996\n",
            "training loss at step 7 - batch 89: 0.32 (2019-08-04 13:37:09.061217)\n",
            "Accuracy at step 7 - batch 89: 0.9008\n",
            "training loss at step 7 - batch 90: 0.31 (2019-08-04 13:37:09.075337)\n",
            "Accuracy at step 7 - batch 90: 0.9088\n",
            "training loss at step 7 - batch 91: 0.31 (2019-08-04 13:37:09.195585)\n",
            "Accuracy at step 7 - batch 91: 0.9032\n",
            "training loss at step 7 - batch 92: 0.30 (2019-08-04 13:37:09.209698)\n",
            "Accuracy at step 7 - batch 92: 0.912\n",
            "training loss at step 7 - batch 93: 0.30 (2019-08-04 13:37:09.222556)\n",
            "Accuracy at step 7 - batch 93: 0.9096\n",
            "training loss at step 7 - batch 94: 0.30 (2019-08-04 13:37:09.235673)\n",
            "Accuracy at step 7 - batch 94: 0.91\n",
            "training loss at step 7 - batch 95: 0.34 (2019-08-04 13:37:09.248445)\n",
            "Accuracy at step 7 - batch 95: 0.8968\n",
            "training loss at step 7 - batch 96: 0.31 (2019-08-04 13:37:09.373642)\n",
            "Accuracy at step 7 - batch 96: 0.906\n",
            "training loss at step 7 - batch 97: 0.33 (2019-08-04 13:37:09.389109)\n",
            "Accuracy at step 7 - batch 97: 0.9\n",
            "training loss at step 7 - batch 98: 0.32 (2019-08-04 13:37:09.401825)\n",
            "Accuracy at step 7 - batch 98: 0.904\n",
            "training loss at step 7 - batch 99: 0.29 (2019-08-04 13:37:09.415087)\n",
            "Accuracy at step 7 - batch 99: 0.912\n",
            "training loss at step 7 - batch 100: 0.33 (2019-08-04 13:37:09.427255)\n",
            "Accuracy at step 7 - batch 100: 0.9028\n",
            "training loss at step 7 - batch 101: 0.34 (2019-08-04 13:37:09.542448)\n",
            "Accuracy at step 7 - batch 101: 0.8996\n",
            "training loss at step 7 - batch 102: 0.30 (2019-08-04 13:37:09.556199)\n",
            "Accuracy at step 7 - batch 102: 0.9104\n",
            "training loss at step 7 - batch 103: 0.32 (2019-08-04 13:37:09.569834)\n",
            "Accuracy at step 7 - batch 103: 0.9076\n",
            "training loss at step 7 - batch 104: 0.31 (2019-08-04 13:37:09.586132)\n",
            "Accuracy at step 7 - batch 104: 0.9092\n",
            "training loss at step 7 - batch 105: 0.34 (2019-08-04 13:37:09.600226)\n",
            "Accuracy at step 7 - batch 105: 0.9012\n",
            "training loss at step 7 - batch 106: 0.29 (2019-08-04 13:37:09.725502)\n",
            "Accuracy at step 7 - batch 106: 0.9072\n",
            "training loss at step 7 - batch 107: 0.30 (2019-08-04 13:37:09.739055)\n",
            "Accuracy at step 7 - batch 107: 0.9108\n",
            "training loss at step 7 - batch 108: 0.33 (2019-08-04 13:37:09.751957)\n",
            "Accuracy at step 7 - batch 108: 0.9016\n",
            "training loss at step 7 - batch 109: 0.29 (2019-08-04 13:37:09.764343)\n",
            "Accuracy at step 7 - batch 109: 0.906\n",
            "training loss at step 7 - batch 110: 0.32 (2019-08-04 13:37:09.776840)\n",
            "Accuracy at step 7 - batch 110: 0.9024\n",
            "training loss at step 7 - batch 111: 0.34 (2019-08-04 13:37:09.900198)\n",
            "Accuracy at step 7 - batch 111: 0.9056\n",
            "training loss at step 7 - batch 112: 0.31 (2019-08-04 13:37:09.914249)\n",
            "Accuracy at step 7 - batch 112: 0.908\n",
            "training loss at step 7 - batch 113: 0.31 (2019-08-04 13:37:09.928436)\n",
            "Accuracy at step 7 - batch 113: 0.906\n",
            "training loss at step 7 - batch 114: 0.31 (2019-08-04 13:37:09.941921)\n",
            "Accuracy at step 7 - batch 114: 0.906\n",
            "training loss at step 7 - batch 115: 0.31 (2019-08-04 13:37:09.954262)\n",
            "Accuracy at step 7 - batch 115: 0.9084\n",
            "training loss at step 7 - batch 116: 0.31 (2019-08-04 13:37:10.076271)\n",
            "Accuracy at step 7 - batch 116: 0.9084\n",
            "training loss at step 7 - batch 117: 0.31 (2019-08-04 13:37:10.088337)\n",
            "Accuracy at step 7 - batch 117: 0.9084\n",
            "training loss at step 7 - batch 118: 0.33 (2019-08-04 13:37:10.104518)\n",
            "Accuracy at step 7 - batch 118: 0.9048\n",
            "training loss at step 7 - batch 119: 0.30 (2019-08-04 13:37:10.118174)\n",
            "Accuracy at step 7 - batch 119: 0.9108\n",
            "training loss at step 7 - batch 120: 0.31 (2019-08-04 13:37:10.132942)\n",
            "Accuracy at step 7 - batch 120: 0.9076\n",
            "training loss at step 7 - batch 121: 0.31 (2019-08-04 13:37:10.257922)\n",
            "Accuracy at step 7 - batch 121: 0.9068\n",
            "training loss at step 7 - batch 122: 0.31 (2019-08-04 13:37:10.275502)\n",
            "Accuracy at step 7 - batch 122: 0.9048\n",
            "training loss at step 7 - batch 123: 0.30 (2019-08-04 13:37:10.290508)\n",
            "Accuracy at step 7 - batch 123: 0.9072\n",
            "training loss at step 7 - batch 124: 0.31 (2019-08-04 13:37:10.303549)\n",
            "Accuracy at step 7 - batch 124: 0.906\n",
            "training loss at step 7 - batch 125: 0.34 (2019-08-04 13:37:10.319716)\n",
            "Accuracy at step 7 - batch 125: 0.8996\n",
            "training loss at step 7 - batch 126: 0.28 (2019-08-04 13:37:10.437905)\n",
            "Accuracy at step 7 - batch 126: 0.9156\n",
            "training loss at step 7 - batch 127: 0.32 (2019-08-04 13:37:10.450691)\n",
            "Accuracy at step 7 - batch 127: 0.9028\n",
            "training loss at step 7 - batch 128: 0.31 (2019-08-04 13:37:10.463385)\n",
            "Accuracy at step 7 - batch 128: 0.91\n",
            "training loss at step 7 - batch 129: 0.32 (2019-08-04 13:37:10.476066)\n",
            "Accuracy at step 7 - batch 129: 0.9052\n",
            "training loss at step 7 - batch 130: 0.31 (2019-08-04 13:37:10.489371)\n",
            "Accuracy at step 7 - batch 130: 0.9068\n",
            "training loss at step 7 - batch 131: 0.31 (2019-08-04 13:37:10.608765)\n",
            "Accuracy at step 7 - batch 131: 0.9028\n",
            "training loss at step 7 - batch 132: 0.32 (2019-08-04 13:37:10.625821)\n",
            "Accuracy at step 7 - batch 132: 0.9104\n",
            "training loss at step 7 - batch 133: 0.31 (2019-08-04 13:37:10.638359)\n",
            "Accuracy at step 7 - batch 133: 0.9068\n",
            "training loss at step 7 - batch 134: 0.31 (2019-08-04 13:37:10.650982)\n",
            "Accuracy at step 7 - batch 134: 0.9036\n",
            "training loss at step 7 - batch 135: 0.35 (2019-08-04 13:37:10.662787)\n",
            "Accuracy at step 7 - batch 135: 0.9004\n",
            "training loss at step 7 - batch 136: 0.32 (2019-08-04 13:37:10.789654)\n",
            "Accuracy at step 7 - batch 136: 0.9056\n",
            "training loss at step 7 - batch 137: 0.31 (2019-08-04 13:37:10.803873)\n",
            "Accuracy at step 7 - batch 137: 0.9096\n",
            "training loss at step 7 - batch 138: 0.36 (2019-08-04 13:37:10.820453)\n",
            "Accuracy at step 7 - batch 138: 0.896\n",
            "training loss at step 7 - batch 139: 0.31 (2019-08-04 13:37:10.833205)\n",
            "Accuracy at step 7 - batch 139: 0.9048\n",
            "training loss at step 7 - batch 140: 0.30 (2019-08-04 13:37:10.847167)\n",
            "Accuracy at step 7 - batch 140: 0.9112\n",
            "training loss at step 7 - batch 141: 0.31 (2019-08-04 13:37:10.968432)\n",
            "Accuracy at step 7 - batch 141: 0.9068\n",
            "training loss at step 7 - batch 142: 0.34 (2019-08-04 13:37:10.981316)\n",
            "Accuracy at step 7 - batch 142: 0.8976\n",
            "training loss at step 7 - batch 143: 0.30 (2019-08-04 13:37:10.994448)\n",
            "Accuracy at step 7 - batch 143: 0.9056\n",
            "training loss at step 7 - batch 144: 0.31 (2019-08-04 13:37:11.006330)\n",
            "Accuracy at step 7 - batch 144: 0.906\n",
            "training loss at step 7 - batch 145: 0.32 (2019-08-04 13:37:11.019098)\n",
            "Accuracy at step 7 - batch 145: 0.904\n",
            "training loss at step 7 - batch 146: 0.32 (2019-08-04 13:37:11.138631)\n",
            "Accuracy at step 7 - batch 146: 0.8976\n",
            "training loss at step 7 - batch 147: 0.31 (2019-08-04 13:37:11.152152)\n",
            "Accuracy at step 7 - batch 147: 0.906\n",
            "training loss at step 7 - batch 148: 0.31 (2019-08-04 13:37:11.164001)\n",
            "Accuracy at step 7 - batch 148: 0.9048\n",
            "training loss at step 7 - batch 149: 0.30 (2019-08-04 13:37:11.177279)\n",
            "Accuracy at step 7 - batch 149: 0.9124\n",
            "training loss at step 7 - batch 150: 0.30 (2019-08-04 13:37:11.189989)\n",
            "Accuracy at step 7 - batch 150: 0.908\n",
            "training loss at step 7 - batch 151: 0.31 (2019-08-04 13:37:11.317617)\n",
            "Accuracy at step 7 - batch 151: 0.9072\n",
            "training loss at step 7 - batch 152: 0.32 (2019-08-04 13:37:11.334512)\n",
            "Accuracy at step 7 - batch 152: 0.9012\n",
            "training loss at step 7 - batch 153: 0.33 (2019-08-04 13:37:11.351322)\n",
            "Accuracy at step 7 - batch 153: 0.9016\n",
            "training loss at step 7 - batch 154: 0.33 (2019-08-04 13:37:11.363888)\n",
            "Accuracy at step 7 - batch 154: 0.9036\n",
            "training loss at step 7 - batch 155: 0.29 (2019-08-04 13:37:11.377914)\n",
            "Accuracy at step 7 - batch 155: 0.9128\n",
            "training loss at step 7 - batch 156: 0.30 (2019-08-04 13:37:11.503221)\n",
            "Accuracy at step 7 - batch 156: 0.9072\n",
            "training loss at step 7 - batch 157: 0.33 (2019-08-04 13:37:11.519547)\n",
            "Accuracy at step 7 - batch 157: 0.9044\n",
            "training loss at step 7 - batch 158: 0.29 (2019-08-04 13:37:11.531793)\n",
            "Accuracy at step 7 - batch 158: 0.9176\n",
            "training loss at step 7 - batch 159: 0.33 (2019-08-04 13:37:11.544394)\n",
            "Accuracy at step 7 - batch 159: 0.9016\n",
            "training loss at step 7 - batch 160: 0.31 (2019-08-04 13:37:11.562859)\n",
            "Accuracy at step 7 - batch 160: 0.906\n",
            "training loss at step 7 - batch 161: 0.32 (2019-08-04 13:37:11.684105)\n",
            "Accuracy at step 7 - batch 161: 0.9044\n",
            "training loss at step 7 - batch 162: 0.31 (2019-08-04 13:37:11.698403)\n",
            "Accuracy at step 7 - batch 162: 0.9088\n",
            "training loss at step 7 - batch 163: 0.31 (2019-08-04 13:37:11.711451)\n",
            "Accuracy at step 7 - batch 163: 0.9072\n",
            "training loss at step 7 - batch 164: 0.29 (2019-08-04 13:37:11.724215)\n",
            "Accuracy at step 7 - batch 164: 0.9124\n",
            "training loss at step 7 - batch 165: 0.33 (2019-08-04 13:37:11.737488)\n",
            "Accuracy at step 7 - batch 165: 0.8996\n",
            "training loss at step 7 - batch 166: 0.32 (2019-08-04 13:37:11.868483)\n",
            "Accuracy at step 7 - batch 166: 0.9032\n",
            "training loss at step 7 - batch 167: 0.32 (2019-08-04 13:37:11.882087)\n",
            "Accuracy at step 7 - batch 167: 0.9032\n",
            "training loss at step 7 - batch 168: 0.33 (2019-08-04 13:37:11.896379)\n",
            "Accuracy at step 7 - batch 168: 0.9016\n",
            "training loss at step 7 - batch 169: 0.33 (2019-08-04 13:37:11.909650)\n",
            "Accuracy at step 7 - batch 169: 0.9032\n",
            "training loss at step 7 - batch 170: 0.32 (2019-08-04 13:37:11.925843)\n",
            "Accuracy at step 7 - batch 170: 0.904\n",
            "training loss at step 7 - batch 171: 0.32 (2019-08-04 13:37:12.046241)\n",
            "Accuracy at step 7 - batch 171: 0.9024\n",
            "training loss at step 7 - batch 172: 0.29 (2019-08-04 13:37:12.060640)\n",
            "Accuracy at step 7 - batch 172: 0.916\n",
            "training loss at step 7 - batch 173: 0.33 (2019-08-04 13:37:12.075222)\n",
            "Accuracy at step 7 - batch 173: 0.8996\n",
            "training loss at step 7 - batch 174: 0.30 (2019-08-04 13:37:12.089754)\n",
            "Accuracy at step 7 - batch 174: 0.9092\n",
            "training loss at step 7 - batch 175: 0.32 (2019-08-04 13:37:12.103396)\n",
            "Accuracy at step 7 - batch 175: 0.9076\n",
            "training loss at step 7 - batch 176: 0.29 (2019-08-04 13:37:12.221046)\n",
            "Accuracy at step 7 - batch 176: 0.9092\n",
            "training loss at step 7 - batch 177: 0.32 (2019-08-04 13:37:12.233735)\n",
            "Accuracy at step 7 - batch 177: 0.906\n",
            "training loss at step 7 - batch 178: 0.31 (2019-08-04 13:37:12.245861)\n",
            "Accuracy at step 7 - batch 178: 0.9048\n",
            "training loss at step 7 - batch 179: 0.30 (2019-08-04 13:37:12.258525)\n",
            "Accuracy at step 7 - batch 179: 0.908\n",
            "training loss at step 7 - batch 180: 0.31 (2019-08-04 13:37:12.271437)\n",
            "Accuracy at step 7 - batch 180: 0.9048\n",
            "training loss at step 7 - batch 181: 0.30 (2019-08-04 13:37:12.399176)\n",
            "Accuracy at step 7 - batch 181: 0.908\n",
            "training loss at step 7 - batch 182: 0.33 (2019-08-04 13:37:12.412550)\n",
            "Accuracy at step 7 - batch 182: 0.9\n",
            "training loss at step 7 - batch 183: 0.30 (2019-08-04 13:37:12.429039)\n",
            "Accuracy at step 7 - batch 183: 0.908\n",
            "training loss at step 7 - batch 184: 0.32 (2019-08-04 13:37:12.441947)\n",
            "Accuracy at step 7 - batch 184: 0.9068\n",
            "training loss at step 7 - batch 185: 0.33 (2019-08-04 13:37:12.458608)\n",
            "Accuracy at step 7 - batch 185: 0.9024\n",
            "training loss at step 7 - batch 186: 0.31 (2019-08-04 13:37:12.594504)\n",
            "Accuracy at step 7 - batch 186: 0.9068\n",
            "training loss at step 7 - batch 187: 0.30 (2019-08-04 13:37:12.610992)\n",
            "Accuracy at step 7 - batch 187: 0.9116\n",
            "training loss at step 7 - batch 188: 0.31 (2019-08-04 13:37:12.624388)\n",
            "Accuracy at step 7 - batch 188: 0.9032\n",
            "training loss at step 7 - batch 189: 0.32 (2019-08-04 13:37:12.636496)\n",
            "Accuracy at step 7 - batch 189: 0.9036\n",
            "training loss at step 7 - batch 190: 0.34 (2019-08-04 13:37:12.649063)\n",
            "Accuracy at step 7 - batch 190: 0.9024\n",
            "training loss at step 7 - batch 191: 0.32 (2019-08-04 13:37:12.779695)\n",
            "Accuracy at step 7 - batch 191: 0.902\n",
            "training loss at step 7 - batch 192: 0.33 (2019-08-04 13:37:12.796558)\n",
            "Accuracy at step 7 - batch 192: 0.9012\n",
            "training loss at step 7 - batch 193: 0.30 (2019-08-04 13:37:12.809447)\n",
            "Accuracy at step 7 - batch 193: 0.9076\n",
            "training loss at step 7 - batch 194: 0.33 (2019-08-04 13:37:12.824855)\n",
            "Accuracy at step 7 - batch 194: 0.9068\n",
            "training loss at step 7 - batch 195: 0.32 (2019-08-04 13:37:12.837883)\n",
            "Accuracy at step 7 - batch 195: 0.9072\n",
            "training loss at step 7 - batch 196: 0.33 (2019-08-04 13:37:12.957819)\n",
            "Accuracy at step 7 - batch 196: 0.9\n",
            "training loss at step 7 - batch 197: 0.32 (2019-08-04 13:37:12.975410)\n",
            "Accuracy at step 7 - batch 197: 0.908\n",
            "training loss at step 7 - batch 198: 0.30 (2019-08-04 13:37:12.988520)\n",
            "Accuracy at step 7 - batch 198: 0.9084\n",
            "training loss at step 7 - batch 199: 0.30 (2019-08-04 13:37:13.000700)\n",
            "Accuracy at step 7 - batch 199: 0.9104\n",
            "training loss at step 7 - batch 200: 0.32 (2019-08-04 13:37:13.013151)\n",
            "Accuracy at step 7 - batch 200: 0.906\n",
            "training loss at step 7 - batch 201: 0.32 (2019-08-04 13:37:13.132241)\n",
            "Accuracy at step 7 - batch 201: 0.9016\n",
            "training loss at step 7 - batch 202: 0.33 (2019-08-04 13:37:13.147597)\n",
            "Accuracy at step 7 - batch 202: 0.9064\n",
            "training loss at step 7 - batch 203: 0.34 (2019-08-04 13:37:13.160230)\n",
            "Accuracy at step 7 - batch 203: 0.8956\n",
            "training loss at step 7 - batch 204: 0.31 (2019-08-04 13:37:13.173066)\n",
            "Accuracy at step 7 - batch 204: 0.9048\n",
            "training loss at step 7 - batch 205: 0.33 (2019-08-04 13:37:13.186560)\n",
            "Accuracy at step 7 - batch 205: 0.902\n",
            "training loss at step 7 - batch 206: 0.31 (2019-08-04 13:37:13.308820)\n",
            "Accuracy at step 7 - batch 206: 0.9048\n",
            "training loss at step 7 - batch 207: 0.31 (2019-08-04 13:37:13.323374)\n",
            "Accuracy at step 7 - batch 207: 0.9044\n",
            "training loss at step 7 - batch 208: 0.32 (2019-08-04 13:37:13.340123)\n",
            "Accuracy at step 7 - batch 208: 0.908\n",
            "training loss at step 7 - batch 209: 0.29 (2019-08-04 13:37:13.354164)\n",
            "Accuracy at step 7 - batch 209: 0.908\n",
            "training loss at step 7 - batch 210: 0.32 (2019-08-04 13:37:13.367383)\n",
            "Accuracy at step 7 - batch 210: 0.9052\n",
            "training loss at step 7 - batch 211: 0.32 (2019-08-04 13:37:13.490988)\n",
            "Accuracy at step 7 - batch 211: 0.9044\n",
            "training loss at step 7 - batch 212: 0.31 (2019-08-04 13:37:13.506740)\n",
            "Accuracy at step 7 - batch 212: 0.908\n",
            "training loss at step 7 - batch 213: 0.29 (2019-08-04 13:37:13.519425)\n",
            "Accuracy at step 7 - batch 213: 0.9132\n",
            "training loss at step 7 - batch 214: 0.30 (2019-08-04 13:37:13.531877)\n",
            "Accuracy at step 7 - batch 214: 0.9076\n",
            "training loss at step 7 - batch 215: 0.30 (2019-08-04 13:37:13.543448)\n",
            "Accuracy at step 7 - batch 215: 0.9136\n",
            "training loss at step 7 - batch 216: 0.30 (2019-08-04 13:37:13.663655)\n",
            "Accuracy at step 7 - batch 216: 0.9084\n",
            "training loss at step 7 - batch 217: 0.32 (2019-08-04 13:37:13.679667)\n",
            "Accuracy at step 7 - batch 217: 0.906\n",
            "training loss at step 7 - batch 218: 0.30 (2019-08-04 13:37:13.692368)\n",
            "Accuracy at step 7 - batch 218: 0.9128\n",
            "training loss at step 7 - batch 219: 0.30 (2019-08-04 13:37:13.705139)\n",
            "Accuracy at step 7 - batch 219: 0.9124\n",
            "training loss at step 7 - batch 220: 0.31 (2019-08-04 13:37:13.718344)\n",
            "Accuracy at step 7 - batch 220: 0.9052\n",
            "training loss at step 7 - batch 221: 0.31 (2019-08-04 13:37:13.846674)\n",
            "Accuracy at step 7 - batch 221: 0.9044\n",
            "training loss at step 7 - batch 222: 0.30 (2019-08-04 13:37:13.860327)\n",
            "Accuracy at step 7 - batch 222: 0.9068\n",
            "training loss at step 7 - batch 223: 0.31 (2019-08-04 13:37:13.873495)\n",
            "Accuracy at step 7 - batch 223: 0.904\n",
            "training loss at step 7 - batch 224: 0.30 (2019-08-04 13:37:13.886091)\n",
            "Accuracy at step 7 - batch 224: 0.9148\n",
            "training loss at step 7 - batch 225: 0.33 (2019-08-04 13:37:13.897986)\n",
            "Accuracy at step 7 - batch 225: 0.8952\n",
            "training loss at step 7 - batch 226: 0.31 (2019-08-04 13:37:14.017848)\n",
            "Accuracy at step 7 - batch 226: 0.9032\n",
            "training loss at step 7 - batch 227: 0.33 (2019-08-04 13:37:14.034901)\n",
            "Accuracy at step 7 - batch 227: 0.902\n",
            "training loss at step 7 - batch 228: 0.32 (2019-08-04 13:37:14.048995)\n",
            "Accuracy at step 7 - batch 228: 0.9052\n",
            "training loss at step 7 - batch 229: 0.32 (2019-08-04 13:37:14.065374)\n",
            "Accuracy at step 7 - batch 229: 0.902\n",
            "training loss at step 7 - batch 230: 0.32 (2019-08-04 13:37:14.077670)\n",
            "Accuracy at step 7 - batch 230: 0.906\n",
            "training loss at step 7 - batch 231: 0.33 (2019-08-04 13:37:14.198752)\n",
            "Accuracy at step 7 - batch 231: 0.9032\n",
            "training loss at step 7 - batch 232: 0.33 (2019-08-04 13:37:14.217172)\n",
            "Accuracy at step 7 - batch 232: 0.9048\n",
            "training loss at step 7 - batch 233: 0.30 (2019-08-04 13:37:14.229507)\n",
            "Accuracy at step 7 - batch 233: 0.9088\n",
            "training loss at step 7 - batch 234: 0.32 (2019-08-04 13:37:14.241611)\n",
            "Accuracy at step 7 - batch 234: 0.9056\n",
            "training loss at step 7 - batch 235: 0.32 (2019-08-04 13:37:14.255461)\n",
            "Accuracy at step 7 - batch 235: 0.904\n",
            "training loss at step 7 - batch 236: 0.32 (2019-08-04 13:37:14.385474)\n",
            "Accuracy at step 7 - batch 236: 0.9064\n",
            "training loss at step 7 - batch 237: 0.32 (2019-08-04 13:37:14.398041)\n",
            "Accuracy at step 7 - batch 237: 0.9076\n",
            "training loss at step 7 - batch 238: 0.31 (2019-08-04 13:37:14.411685)\n",
            "Accuracy at step 7 - batch 238: 0.9088\n",
            "training loss at step 7 - batch 239: 0.30 (2019-08-04 13:37:14.423863)\n",
            "Accuracy at step 7 - batch 239: 0.9056\n",
            "training loss at step 7 - batch 240: 0.34 (2019-08-04 13:37:14.437559)\n",
            "Accuracy at step 7 - batch 240: 0.9012\n",
            "training loss at step 7 - batch 241: 0.29 (2019-08-04 13:37:14.566362)\n",
            "Accuracy at step 7 - batch 241: 0.9076\n",
            "training loss at step 7 - batch 242: 0.28 (2019-08-04 13:37:14.578357)\n",
            "Accuracy at step 7 - batch 242: 0.9144\n",
            "training loss at step 7 - batch 243: 0.31 (2019-08-04 13:37:14.591348)\n",
            "Accuracy at step 7 - batch 243: 0.9112\n",
            "training loss at step 7 - batch 244: 0.31 (2019-08-04 13:37:14.605947)\n",
            "Accuracy at step 7 - batch 244: 0.9064\n",
            "training loss at step 7 - batch 245: 0.31 (2019-08-04 13:37:14.618445)\n",
            "Accuracy at step 7 - batch 245: 0.9104\n",
            "training loss at step 7 - batch 246: 0.32 (2019-08-04 13:37:14.734900)\n",
            "Accuracy at step 7 - batch 246: 0.9048\n",
            "training loss at step 7 - batch 247: 0.31 (2019-08-04 13:37:14.748065)\n",
            "Accuracy at step 7 - batch 247: 0.91\n",
            "training loss at step 7 - batch 248: 0.30 (2019-08-04 13:37:14.761522)\n",
            "Accuracy at step 7 - batch 248: 0.908\n",
            "training loss at step 7 - batch 249: 0.33 (2019-08-04 13:37:14.775504)\n",
            "Accuracy at step 7 - batch 249: 0.906\n",
            "training loss at step 7 - batch 250: 0.32 (2019-08-04 13:37:14.790035)\n",
            "Accuracy at step 7 - batch 250: 0.9036\n",
            "training loss at step 7 - batch 251: 0.34 (2019-08-04 13:37:14.921164)\n",
            "Accuracy at step 7 - batch 251: 0.9016\n",
            "training loss at step 7 - batch 252: 0.30 (2019-08-04 13:37:14.938022)\n",
            "Accuracy at step 7 - batch 252: 0.9164\n",
            "training loss at step 7 - batch 253: 0.30 (2019-08-04 13:37:14.950584)\n",
            "Accuracy at step 7 - batch 253: 0.9108\n",
            "training loss at step 7 - batch 254: 0.32 (2019-08-04 13:37:14.962650)\n",
            "Accuracy at step 7 - batch 254: 0.9052\n",
            "training loss at step 7 - batch 255: 0.32 (2019-08-04 13:37:14.974997)\n",
            "Accuracy at step 7 - batch 255: 0.902\n",
            "training loss at step 7 - batch 256: 0.31 (2019-08-04 13:37:15.098480)\n",
            "Accuracy at step 7 - batch 256: 0.908\n",
            "training loss at step 7 - batch 257: 0.31 (2019-08-04 13:37:15.115663)\n",
            "Accuracy at step 7 - batch 257: 0.9028\n",
            "training loss at step 7 - batch 258: 0.30 (2019-08-04 13:37:15.128471)\n",
            "Accuracy at step 7 - batch 258: 0.914\n",
            "training loss at step 7 - batch 259: 0.31 (2019-08-04 13:37:15.140536)\n",
            "Accuracy at step 7 - batch 259: 0.9092\n",
            "training loss at step 7 - batch 260: 0.30 (2019-08-04 13:37:15.152856)\n",
            "Accuracy at step 7 - batch 260: 0.9152\n",
            "training loss at step 7 - batch 261: 0.32 (2019-08-04 13:37:15.272777)\n",
            "Accuracy at step 7 - batch 261: 0.9024\n",
            "training loss at step 7 - batch 262: 0.31 (2019-08-04 13:37:15.286495)\n",
            "Accuracy at step 7 - batch 262: 0.9076\n",
            "training loss at step 7 - batch 263: 0.33 (2019-08-04 13:37:15.298920)\n",
            "Accuracy at step 7 - batch 263: 0.9024\n",
            "training loss at step 7 - batch 264: 0.32 (2019-08-04 13:37:15.313243)\n",
            "Accuracy at step 7 - batch 264: 0.9028\n",
            "training loss at step 7 - batch 265: 0.31 (2019-08-04 13:37:15.327761)\n",
            "Accuracy at step 7 - batch 265: 0.9104\n",
            "training loss at step 7 - batch 266: 0.32 (2019-08-04 13:37:15.454766)\n",
            "Accuracy at step 7 - batch 266: 0.9004\n",
            "training loss at step 7 - batch 267: 0.35 (2019-08-04 13:37:15.469587)\n",
            "Accuracy at step 7 - batch 267: 0.8988\n",
            "training loss at step 7 - batch 268: 0.29 (2019-08-04 13:37:15.482009)\n",
            "Accuracy at step 7 - batch 268: 0.9104\n",
            "training loss at step 7 - batch 269: 0.30 (2019-08-04 13:37:15.493881)\n",
            "Accuracy at step 7 - batch 269: 0.9088\n",
            "training loss at step 7 - batch 270: 0.31 (2019-08-04 13:37:15.505683)\n",
            "Accuracy at step 7 - batch 270: 0.906\n",
            "training loss at step 7 - batch 271: 0.32 (2019-08-04 13:37:15.628606)\n",
            "Accuracy at step 7 - batch 271: 0.9028\n",
            "training loss at step 7 - batch 272: 0.32 (2019-08-04 13:37:15.644739)\n",
            "Accuracy at step 7 - batch 272: 0.9044\n",
            "training loss at step 7 - batch 273: 0.31 (2019-08-04 13:37:15.657020)\n",
            "Accuracy at step 7 - batch 273: 0.9132\n",
            "training loss at step 7 - batch 274: 0.31 (2019-08-04 13:37:15.670352)\n",
            "Accuracy at step 7 - batch 274: 0.9084\n",
            "training loss at step 7 - batch 275: 0.32 (2019-08-04 13:37:15.684221)\n",
            "Accuracy at step 7 - batch 275: 0.9092\n",
            "training loss at step 7 - batch 276: 0.32 (2019-08-04 13:37:15.802545)\n",
            "Accuracy at step 7 - batch 276: 0.9052\n",
            "training loss at step 7 - batch 277: 0.30 (2019-08-04 13:37:15.816683)\n",
            "Accuracy at step 7 - batch 277: 0.9124\n",
            "training loss at step 7 - batch 278: 0.29 (2019-08-04 13:37:15.834281)\n",
            "Accuracy at step 7 - batch 278: 0.9136\n",
            "training loss at step 7 - batch 279: 0.31 (2019-08-04 13:37:15.850248)\n",
            "Accuracy at step 7 - batch 279: 0.9056\n",
            "training loss at step 7 - batch 280: 0.30 (2019-08-04 13:37:15.864186)\n",
            "Accuracy at step 7 - batch 280: 0.9112\n",
            "training loss at step 7 - batch 281: 0.33 (2019-08-04 13:37:15.985475)\n",
            "Accuracy at step 7 - batch 281: 0.9024\n",
            "training loss at step 7 - batch 282: 0.32 (2019-08-04 13:37:16.000784)\n",
            "Accuracy at step 7 - batch 282: 0.9032\n",
            "training loss at step 7 - batch 283: 0.32 (2019-08-04 13:37:16.014319)\n",
            "Accuracy at step 7 - batch 283: 0.902\n",
            "training loss at step 7 - batch 284: 0.32 (2019-08-04 13:37:16.027123)\n",
            "Accuracy at step 7 - batch 284: 0.9104\n",
            "training loss at step 7 - batch 285: 0.32 (2019-08-04 13:37:16.041548)\n",
            "Accuracy at step 7 - batch 285: 0.9104\n",
            "training loss at step 7 - batch 286: 0.29 (2019-08-04 13:37:16.160881)\n",
            "Accuracy at step 7 - batch 286: 0.9104\n",
            "training loss at step 7 - batch 287: 0.30 (2019-08-04 13:37:16.175744)\n",
            "Accuracy at step 7 - batch 287: 0.908\n",
            "training loss at step 7 - batch 288: 0.28 (2019-08-04 13:37:16.190183)\n",
            "Accuracy at step 7 - batch 288: 0.9188\n",
            "training loss at step 7 - batch 289: 0.35 (2019-08-04 13:37:16.202905)\n",
            "Accuracy at step 7 - batch 289: 0.8992\n",
            "training loss at step 7 - batch 290: 0.33 (2019-08-04 13:37:16.216432)\n",
            "Accuracy at step 7 - batch 290: 0.9004\n",
            "training loss at step 7 - batch 291: 0.32 (2019-08-04 13:37:16.352581)\n",
            "Accuracy at step 7 - batch 291: 0.9048\n",
            "training loss at step 7 - batch 292: 0.31 (2019-08-04 13:37:16.366735)\n",
            "Accuracy at step 7 - batch 292: 0.908\n",
            "training loss at step 7 - batch 293: 0.30 (2019-08-04 13:37:16.380736)\n",
            "Accuracy at step 7 - batch 293: 0.9092\n",
            "training loss at step 7 - batch 294: 0.30 (2019-08-04 13:37:16.393651)\n",
            "Accuracy at step 7 - batch 294: 0.9084\n",
            "training loss at step 7 - batch 295: 0.34 (2019-08-04 13:37:16.408216)\n",
            "Accuracy at step 7 - batch 295: 0.9048\n",
            "training loss at step 7 - batch 296: 0.32 (2019-08-04 13:37:16.530933)\n",
            "Accuracy at step 7 - batch 296: 0.9004\n",
            "training loss at step 7 - batch 297: 0.31 (2019-08-04 13:37:16.546683)\n",
            "Accuracy at step 7 - batch 297: 0.9096\n",
            "training loss at step 7 - batch 298: 0.32 (2019-08-04 13:37:16.561562)\n",
            "Accuracy at step 7 - batch 298: 0.9016\n",
            "training loss at step 7 - batch 299: 0.31 (2019-08-04 13:37:16.574677)\n",
            "Accuracy at step 7 - batch 299: 0.9068\n",
            "training loss at step 7 - batch 300: 0.32 (2019-08-04 13:37:16.587022)\n",
            "Accuracy at step 7 - batch 300: 0.906\n",
            "training loss at step 7 - batch 301: 0.29 (2019-08-04 13:37:16.701105)\n",
            "Accuracy at step 7 - batch 301: 0.9172\n",
            "training loss at step 7 - batch 302: 0.32 (2019-08-04 13:37:16.714850)\n",
            "Accuracy at step 7 - batch 302: 0.9\n",
            "training loss at step 7 - batch 303: 0.33 (2019-08-04 13:37:16.726876)\n",
            "Accuracy at step 7 - batch 303: 0.9008\n",
            "training loss at step 7 - batch 304: 0.30 (2019-08-04 13:37:16.740729)\n",
            "Accuracy at step 7 - batch 304: 0.914\n",
            "training loss at step 7 - batch 305: 0.32 (2019-08-04 13:37:16.753553)\n",
            "Accuracy at step 7 - batch 305: 0.9044\n",
            "training loss at step 7 - batch 306: 0.34 (2019-08-04 13:37:16.882428)\n",
            "Accuracy at step 7 - batch 306: 0.9\n",
            "training loss at step 7 - batch 307: 0.30 (2019-08-04 13:37:16.895139)\n",
            "Accuracy at step 7 - batch 307: 0.908\n",
            "training loss at step 7 - batch 308: 0.29 (2019-08-04 13:37:16.907053)\n",
            "Accuracy at step 7 - batch 308: 0.9148\n",
            "training loss at step 7 - batch 309: 0.31 (2019-08-04 13:37:16.921784)\n",
            "Accuracy at step 7 - batch 309: 0.9096\n",
            "training loss at step 7 - batch 310: 0.30 (2019-08-04 13:37:16.934446)\n",
            "Accuracy at step 7 - batch 310: 0.904\n",
            "training loss at step 7 - batch 311: 0.31 (2019-08-04 13:37:17.055035)\n",
            "Accuracy at step 7 - batch 311: 0.9072\n",
            "training loss at step 7 - batch 312: 0.31 (2019-08-04 13:37:17.070503)\n",
            "Accuracy at step 7 - batch 312: 0.9056\n",
            "training loss at step 7 - batch 313: 0.29 (2019-08-04 13:37:17.083729)\n",
            "Accuracy at step 7 - batch 313: 0.9168\n",
            "training loss at step 7 - batch 314: 0.30 (2019-08-04 13:37:17.099119)\n",
            "Accuracy at step 7 - batch 314: 0.9144\n",
            "training loss at step 7 - batch 315: 0.31 (2019-08-04 13:37:17.113027)\n",
            "Accuracy at step 7 - batch 315: 0.9008\n",
            "training loss at step 7 - batch 316: 0.32 (2019-08-04 13:37:17.233044)\n",
            "Accuracy at step 7 - batch 316: 0.9016\n",
            "training loss at step 7 - batch 317: 0.31 (2019-08-04 13:37:17.246123)\n",
            "Accuracy at step 7 - batch 317: 0.9048\n",
            "training loss at step 7 - batch 318: 0.32 (2019-08-04 13:37:17.260313)\n",
            "Accuracy at step 7 - batch 318: 0.8992\n",
            "training loss at step 7 - batch 319: 0.32 (2019-08-04 13:37:17.273340)\n",
            "Accuracy at step 7 - batch 319: 0.9016\n",
            "training loss at step 7 - batch 320: 0.33 (2019-08-04 13:37:17.289886)\n",
            "Accuracy at step 7 - batch 320: 0.9012\n",
            "training loss at step 7 - batch 321: 0.32 (2019-08-04 13:37:17.410790)\n",
            "Accuracy at step 7 - batch 321: 0.902\n",
            "training loss at step 7 - batch 322: 0.31 (2019-08-04 13:37:17.427698)\n",
            "Accuracy at step 7 - batch 322: 0.9084\n",
            "training loss at step 7 - batch 323: 0.32 (2019-08-04 13:37:17.440380)\n",
            "Accuracy at step 7 - batch 323: 0.9072\n",
            "training loss at step 7 - batch 324: 0.33 (2019-08-04 13:37:17.454664)\n",
            "Accuracy at step 7 - batch 324: 0.9052\n",
            "training loss at step 7 - batch 325: 0.33 (2019-08-04 13:37:17.468319)\n",
            "Accuracy at step 7 - batch 325: 0.8976\n",
            "training loss at step 7 - batch 326: 0.31 (2019-08-04 13:37:17.597020)\n",
            "Accuracy at step 7 - batch 326: 0.9064\n",
            "training loss at step 7 - batch 327: 0.29 (2019-08-04 13:37:17.615686)\n",
            "Accuracy at step 7 - batch 327: 0.912\n",
            "training loss at step 7 - batch 328: 0.32 (2019-08-04 13:37:17.628087)\n",
            "Accuracy at step 7 - batch 328: 0.8964\n",
            "training loss at step 7 - batch 329: 0.30 (2019-08-04 13:37:17.640497)\n",
            "Accuracy at step 7 - batch 329: 0.9084\n",
            "training loss at step 7 - batch 330: 0.32 (2019-08-04 13:37:17.653434)\n",
            "Accuracy at step 7 - batch 330: 0.9072\n",
            "training loss at step 7 - batch 331: 0.32 (2019-08-04 13:37:17.774129)\n",
            "Accuracy at step 7 - batch 331: 0.9056\n",
            "training loss at step 7 - batch 332: 0.30 (2019-08-04 13:37:17.789384)\n",
            "Accuracy at step 7 - batch 332: 0.9096\n",
            "training loss at step 7 - batch 333: 0.32 (2019-08-04 13:37:17.803635)\n",
            "Accuracy at step 7 - batch 333: 0.908\n",
            "training loss at step 7 - batch 334: 0.32 (2019-08-04 13:37:17.816693)\n",
            "Accuracy at step 7 - batch 334: 0.9028\n",
            "training loss at step 7 - batch 335: 0.32 (2019-08-04 13:37:17.829441)\n",
            "Accuracy at step 7 - batch 335: 0.9044\n",
            "training loss at step 7 - batch 336: 0.30 (2019-08-04 13:37:17.954342)\n",
            "Accuracy at step 7 - batch 336: 0.9112\n",
            "training loss at step 7 - batch 337: 0.33 (2019-08-04 13:37:17.967024)\n",
            "Accuracy at step 7 - batch 337: 0.9044\n",
            "training loss at step 7 - batch 338: 0.31 (2019-08-04 13:37:17.980964)\n",
            "Accuracy at step 7 - batch 338: 0.9028\n",
            "training loss at step 7 - batch 339: 0.31 (2019-08-04 13:37:17.994369)\n",
            "Accuracy at step 7 - batch 339: 0.91\n",
            "training loss at step 7 - batch 340: 0.32 (2019-08-04 13:37:18.008870)\n",
            "Accuracy at step 7 - batch 340: 0.906\n",
            "training loss at step 7 - batch 341: 0.32 (2019-08-04 13:37:18.130912)\n",
            "Accuracy at step 7 - batch 341: 0.9064\n",
            "training loss at step 7 - batch 342: 0.33 (2019-08-04 13:37:18.148058)\n",
            "Accuracy at step 7 - batch 342: 0.904\n",
            "training loss at step 7 - batch 343: 0.31 (2019-08-04 13:37:18.161074)\n",
            "Accuracy at step 7 - batch 343: 0.9056\n",
            "training loss at step 7 - batch 344: 0.32 (2019-08-04 13:37:18.173695)\n",
            "Accuracy at step 7 - batch 344: 0.9068\n",
            "training loss at step 7 - batch 345: 0.31 (2019-08-04 13:37:18.185834)\n",
            "Accuracy at step 7 - batch 345: 0.906\n",
            "training loss at step 7 - batch 346: 0.33 (2019-08-04 13:37:18.315765)\n",
            "Accuracy at step 7 - batch 346: 0.9048\n",
            "training loss at step 7 - batch 347: 0.33 (2019-08-04 13:37:18.332502)\n",
            "Accuracy at step 7 - batch 347: 0.902\n",
            "training loss at step 7 - batch 348: 0.32 (2019-08-04 13:37:18.345150)\n",
            "Accuracy at step 7 - batch 348: 0.9004\n",
            "training loss at step 7 - batch 349: 0.30 (2019-08-04 13:37:18.358128)\n",
            "Accuracy at step 7 - batch 349: 0.9104\n",
            "training loss at step 7 - batch 350: 0.29 (2019-08-04 13:37:18.371367)\n",
            "Accuracy at step 7 - batch 350: 0.9068\n",
            "training loss at step 7 - batch 351: 0.32 (2019-08-04 13:37:18.493025)\n",
            "Accuracy at step 7 - batch 351: 0.9028\n",
            "training loss at step 7 - batch 352: 0.30 (2019-08-04 13:37:18.510742)\n",
            "Accuracy at step 7 - batch 352: 0.9092\n",
            "training loss at step 7 - batch 353: 0.32 (2019-08-04 13:37:18.526980)\n",
            "Accuracy at step 7 - batch 353: 0.9044\n",
            "training loss at step 7 - batch 354: 0.32 (2019-08-04 13:37:18.540349)\n",
            "Accuracy at step 7 - batch 354: 0.9004\n",
            "training loss at step 7 - batch 355: 0.32 (2019-08-04 13:37:18.553739)\n",
            "Accuracy at step 7 - batch 355: 0.9028\n",
            "training loss at step 7 - batch 356: 0.30 (2019-08-04 13:37:18.673622)\n",
            "Accuracy at step 7 - batch 356: 0.912\n",
            "training loss at step 7 - batch 357: 0.29 (2019-08-04 13:37:18.689958)\n",
            "Accuracy at step 7 - batch 357: 0.9132\n",
            "training loss at step 7 - batch 358: 0.34 (2019-08-04 13:37:18.703397)\n",
            "Accuracy at step 7 - batch 358: 0.9068\n",
            "training loss at step 7 - batch 359: 0.30 (2019-08-04 13:37:18.716386)\n",
            "Accuracy at step 7 - batch 359: 0.9112\n",
            "training loss at step 7 - batch 360: 0.30 (2019-08-04 13:37:18.738138)\n",
            "Accuracy at step 7 - batch 360: 0.9132\n",
            "training loss at step 7 - batch 361: 0.30 (2019-08-04 13:37:18.865043)\n",
            "Accuracy at step 7 - batch 361: 0.9104\n",
            "training loss at step 7 - batch 362: 0.33 (2019-08-04 13:37:18.879212)\n",
            "Accuracy at step 7 - batch 362: 0.9068\n",
            "training loss at step 7 - batch 363: 0.29 (2019-08-04 13:37:18.893487)\n",
            "Accuracy at step 7 - batch 363: 0.9148\n",
            "training loss at step 7 - batch 364: 0.31 (2019-08-04 13:37:18.908557)\n",
            "Accuracy at step 7 - batch 364: 0.9072\n",
            "training loss at step 7 - batch 365: 0.34 (2019-08-04 13:37:18.924246)\n",
            "Accuracy at step 7 - batch 365: 0.9048\n",
            "training loss at step 7 - batch 366: 0.32 (2019-08-04 13:37:19.049236)\n",
            "Accuracy at step 7 - batch 366: 0.9048\n",
            "training loss at step 7 - batch 367: 0.31 (2019-08-04 13:37:19.063326)\n",
            "Accuracy at step 7 - batch 367: 0.9116\n",
            "training loss at step 7 - batch 368: 0.32 (2019-08-04 13:37:19.075681)\n",
            "Accuracy at step 7 - batch 368: 0.904\n",
            "training loss at step 7 - batch 369: 0.32 (2019-08-04 13:37:19.088788)\n",
            "Accuracy at step 7 - batch 369: 0.9024\n",
            "training loss at step 7 - batch 370: 0.30 (2019-08-04 13:37:19.101262)\n",
            "Accuracy at step 7 - batch 370: 0.9072\n",
            "training loss at step 7 - batch 371: 0.32 (2019-08-04 13:37:19.218177)\n",
            "Accuracy at step 7 - batch 371: 0.904\n",
            "training loss at step 7 - batch 372: 0.31 (2019-08-04 13:37:19.233129)\n",
            "Accuracy at step 7 - batch 372: 0.9132\n",
            "training loss at step 7 - batch 373: 0.32 (2019-08-04 13:37:19.245954)\n",
            "Accuracy at step 7 - batch 373: 0.9068\n",
            "training loss at step 7 - batch 374: 0.32 (2019-08-04 13:37:19.259962)\n",
            "Accuracy at step 7 - batch 374: 0.9084\n",
            "training loss at step 7 - batch 375: 0.32 (2019-08-04 13:37:19.272622)\n",
            "Accuracy at step 7 - batch 375: 0.9036\n",
            "training loss at step 7 - batch 376: 0.32 (2019-08-04 13:37:19.392790)\n",
            "Accuracy at step 7 - batch 376: 0.906\n",
            "training loss at step 7 - batch 377: 0.32 (2019-08-04 13:37:19.405402)\n",
            "Accuracy at step 7 - batch 377: 0.9024\n",
            "training loss at step 7 - batch 378: 0.32 (2019-08-04 13:37:19.418072)\n",
            "Accuracy at step 7 - batch 378: 0.9\n",
            "training loss at step 7 - batch 379: 0.31 (2019-08-04 13:37:19.430699)\n",
            "Accuracy at step 7 - batch 379: 0.9052\n",
            "training loss at step 7 - batch 380: 0.30 (2019-08-04 13:37:19.443113)\n",
            "Accuracy at step 7 - batch 380: 0.9092\n",
            "training loss at step 7 - batch 381: 0.31 (2019-08-04 13:37:19.569663)\n",
            "Accuracy at step 7 - batch 381: 0.906\n",
            "training loss at step 7 - batch 382: 0.30 (2019-08-04 13:37:19.584602)\n",
            "Accuracy at step 7 - batch 382: 0.9112\n",
            "training loss at step 7 - batch 383: 0.30 (2019-08-04 13:37:19.597080)\n",
            "Accuracy at step 7 - batch 383: 0.9124\n",
            "training loss at step 7 - batch 384: 0.29 (2019-08-04 13:37:19.608917)\n",
            "Accuracy at step 7 - batch 384: 0.918\n",
            "training loss at step 7 - batch 385: 0.33 (2019-08-04 13:37:19.623175)\n",
            "Accuracy at step 7 - batch 385: 0.9024\n",
            "training loss at step 7 - batch 386: 0.31 (2019-08-04 13:37:19.742826)\n",
            "Accuracy at step 7 - batch 386: 0.9052\n",
            "training loss at step 7 - batch 387: 0.33 (2019-08-04 13:37:19.760629)\n",
            "Accuracy at step 7 - batch 387: 0.9024\n",
            "training loss at step 7 - batch 388: 0.29 (2019-08-04 13:37:19.774996)\n",
            "Accuracy at step 7 - batch 388: 0.9156\n",
            "training loss at step 7 - batch 389: 0.34 (2019-08-04 13:37:19.788234)\n",
            "Accuracy at step 7 - batch 389: 0.9052\n",
            "training loss at step 7 - batch 390: 0.32 (2019-08-04 13:37:19.800830)\n",
            "Accuracy at step 7 - batch 390: 0.9076\n",
            "training loss at step 7 - batch 391: 0.32 (2019-08-04 13:37:19.925002)\n",
            "Accuracy at step 7 - batch 391: 0.9052\n",
            "training loss at step 7 - batch 392: 0.35 (2019-08-04 13:37:19.943666)\n",
            "Accuracy at step 7 - batch 392: 0.8964\n",
            "training loss at step 7 - batch 393: 0.31 (2019-08-04 13:37:19.958033)\n",
            "Accuracy at step 7 - batch 393: 0.9044\n",
            "training loss at step 7 - batch 394: 0.30 (2019-08-04 13:37:19.971495)\n",
            "Accuracy at step 7 - batch 394: 0.912\n",
            "training loss at step 7 - batch 395: 0.31 (2019-08-04 13:37:19.987915)\n",
            "Accuracy at step 7 - batch 395: 0.9032\n",
            "training loss at step 7 - batch 396: 0.32 (2019-08-04 13:37:20.112460)\n",
            "Accuracy at step 7 - batch 396: 0.912\n",
            "training loss at step 7 - batch 397: 0.31 (2019-08-04 13:37:20.129743)\n",
            "Accuracy at step 7 - batch 397: 0.908\n",
            "training loss at step 7 - batch 398: 0.34 (2019-08-04 13:37:20.141668)\n",
            "Accuracy at step 7 - batch 398: 0.8988\n",
            "training loss at step 7 - batch 399: 0.32 (2019-08-04 13:37:20.155982)\n",
            "Accuracy at step 7 - batch 399: 0.904\n",
            "training loss at step 7 - batch 400: 0.31 (2019-08-04 13:37:20.169160)\n",
            "Accuracy at step 7 - batch 400: 0.9056\n",
            "training loss at step 7 - batch 401: 0.32 (2019-08-04 13:37:20.295182)\n",
            "Accuracy at step 7 - batch 401: 0.9032\n",
            "training loss at step 7 - batch 402: 0.32 (2019-08-04 13:37:20.308787)\n",
            "Accuracy at step 7 - batch 402: 0.9052\n",
            "training loss at step 7 - batch 403: 0.33 (2019-08-04 13:37:20.321429)\n",
            "Accuracy at step 7 - batch 403: 0.9004\n",
            "training loss at step 7 - batch 404: 0.32 (2019-08-04 13:37:20.333944)\n",
            "Accuracy at step 7 - batch 404: 0.9084\n",
            "training loss at step 7 - batch 405: 0.31 (2019-08-04 13:37:20.346967)\n",
            "Accuracy at step 7 - batch 405: 0.9044\n",
            "training loss at step 7 - batch 406: 0.30 (2019-08-04 13:37:20.465962)\n",
            "Accuracy at step 7 - batch 406: 0.9056\n",
            "training loss at step 7 - batch 407: 0.30 (2019-08-04 13:37:20.479633)\n",
            "Accuracy at step 7 - batch 407: 0.9116\n",
            "training loss at step 7 - batch 408: 0.30 (2019-08-04 13:37:20.492220)\n",
            "Accuracy at step 7 - batch 408: 0.9112\n",
            "training loss at step 7 - batch 409: 0.32 (2019-08-04 13:37:20.507125)\n",
            "Accuracy at step 7 - batch 409: 0.9036\n",
            "training loss at step 7 - batch 410: 0.30 (2019-08-04 13:37:20.520154)\n",
            "Accuracy at step 7 - batch 410: 0.9076\n",
            "training loss at step 7 - batch 411: 0.29 (2019-08-04 13:37:20.643586)\n",
            "Accuracy at step 7 - batch 411: 0.9128\n",
            "training loss at step 7 - batch 412: 0.33 (2019-08-04 13:37:20.662084)\n",
            "Accuracy at step 7 - batch 412: 0.9016\n",
            "training loss at step 7 - batch 413: 0.32 (2019-08-04 13:37:20.675520)\n",
            "Accuracy at step 7 - batch 413: 0.9068\n",
            "training loss at step 7 - batch 414: 0.32 (2019-08-04 13:37:20.688616)\n",
            "Accuracy at step 7 - batch 414: 0.9064\n",
            "training loss at step 7 - batch 415: 0.31 (2019-08-04 13:37:20.700435)\n",
            "Accuracy at step 7 - batch 415: 0.9088\n",
            "training loss at step 7 - batch 416: 0.33 (2019-08-04 13:37:20.821636)\n",
            "Accuracy at step 7 - batch 416: 0.9008\n",
            "training loss at step 7 - batch 417: 0.32 (2019-08-04 13:37:20.834086)\n",
            "Accuracy at step 7 - batch 417: 0.9092\n",
            "training loss at step 7 - batch 418: 0.31 (2019-08-04 13:37:20.846670)\n",
            "Accuracy at step 7 - batch 418: 0.904\n",
            "training loss at step 7 - batch 419: 0.31 (2019-08-04 13:37:20.859316)\n",
            "Accuracy at step 7 - batch 419: 0.904\n",
            "training loss at step 7 - batch 420: 0.31 (2019-08-04 13:37:20.873507)\n",
            "Accuracy at step 7 - batch 420: 0.9092\n",
            "training loss at step 7 - batch 421: 0.29 (2019-08-04 13:37:21.002788)\n",
            "Accuracy at step 7 - batch 421: 0.9076\n",
            "training loss at step 7 - batch 422: 0.32 (2019-08-04 13:37:21.015957)\n",
            "Accuracy at step 7 - batch 422: 0.8984\n",
            "training loss at step 7 - batch 423: 0.32 (2019-08-04 13:37:21.032455)\n",
            "Accuracy at step 7 - batch 423: 0.9028\n",
            "training loss at step 7 - batch 424: 0.29 (2019-08-04 13:37:21.046329)\n",
            "Accuracy at step 7 - batch 424: 0.9116\n",
            "training loss at step 7 - batch 425: 0.30 (2019-08-04 13:37:21.058584)\n",
            "Accuracy at step 7 - batch 425: 0.908\n",
            "training loss at step 7 - batch 426: 0.32 (2019-08-04 13:37:21.181967)\n",
            "Accuracy at step 7 - batch 426: 0.9\n",
            "training loss at step 7 - batch 427: 0.31 (2019-08-04 13:37:21.198439)\n",
            "Accuracy at step 7 - batch 427: 0.9052\n",
            "training loss at step 7 - batch 428: 0.34 (2019-08-04 13:37:21.212404)\n",
            "Accuracy at step 7 - batch 428: 0.8956\n",
            "training loss at step 7 - batch 429: 0.31 (2019-08-04 13:37:21.227909)\n",
            "Accuracy at step 7 - batch 429: 0.9\n",
            "training loss at step 7 - batch 430: 0.30 (2019-08-04 13:37:21.245538)\n",
            "Accuracy at step 7 - batch 430: 0.9084\n",
            "training loss at step 7 - batch 431: 0.31 (2019-08-04 13:37:21.373643)\n",
            "Accuracy at step 7 - batch 431: 0.9068\n",
            "training loss at step 7 - batch 432: 0.31 (2019-08-04 13:37:21.389521)\n",
            "Accuracy at step 7 - batch 432: 0.9112\n",
            "training loss at step 7 - batch 433: 0.31 (2019-08-04 13:37:21.402634)\n",
            "Accuracy at step 7 - batch 433: 0.9056\n",
            "training loss at step 7 - batch 434: 0.28 (2019-08-04 13:37:21.415888)\n",
            "Accuracy at step 7 - batch 434: 0.914\n",
            "training loss at step 7 - batch 435: 0.33 (2019-08-04 13:37:21.428322)\n",
            "Accuracy at step 7 - batch 435: 0.9056\n",
            "training loss at step 7 - batch 436: 0.33 (2019-08-04 13:37:21.564352)\n",
            "Accuracy at step 7 - batch 436: 0.8972\n",
            "training loss at step 7 - batch 437: 0.32 (2019-08-04 13:37:21.579237)\n",
            "Accuracy at step 7 - batch 437: 0.9072\n",
            "training loss at step 7 - batch 438: 0.30 (2019-08-04 13:37:21.591849)\n",
            "Accuracy at step 7 - batch 438: 0.9076\n",
            "training loss at step 7 - batch 439: 0.30 (2019-08-04 13:37:21.603947)\n",
            "Accuracy at step 7 - batch 439: 0.904\n",
            "training loss at step 7 - batch 440: 0.29 (2019-08-04 13:37:21.618560)\n",
            "Accuracy at step 7 - batch 440: 0.9096\n",
            "training loss at step 7 - batch 441: 0.31 (2019-08-04 13:37:21.741873)\n",
            "Accuracy at step 7 - batch 441: 0.9072\n",
            "training loss at step 7 - batch 442: 0.33 (2019-08-04 13:37:21.755389)\n",
            "Accuracy at step 7 - batch 442: 0.8956\n",
            "training loss at step 7 - batch 443: 0.33 (2019-08-04 13:37:21.770868)\n",
            "Accuracy at step 7 - batch 443: 0.9008\n",
            "training loss at step 7 - batch 444: 0.32 (2019-08-04 13:37:21.783424)\n",
            "Accuracy at step 7 - batch 444: 0.8992\n",
            "training loss at step 7 - batch 445: 0.31 (2019-08-04 13:37:21.796208)\n",
            "Accuracy at step 7 - batch 445: 0.9056\n",
            "training loss at step 7 - batch 446: 0.31 (2019-08-04 13:37:21.922514)\n",
            "Accuracy at step 7 - batch 446: 0.9064\n",
            "training loss at step 7 - batch 447: 0.30 (2019-08-04 13:37:21.939681)\n",
            "Accuracy at step 7 - batch 447: 0.9132\n",
            "training loss at step 7 - batch 448: 0.29 (2019-08-04 13:37:21.952667)\n",
            "Accuracy at step 7 - batch 448: 0.91\n",
            "training loss at step 7 - batch 449: 0.33 (2019-08-04 13:37:21.964552)\n",
            "Accuracy at step 7 - batch 449: 0.9036\n",
            "training loss at step 7 - batch 450: 0.32 (2019-08-04 13:37:21.985978)\n",
            "Accuracy at step 7 - batch 450: 0.9032\n",
            "training loss at step 7 - batch 451: 0.32 (2019-08-04 13:37:22.110929)\n",
            "Accuracy at step 7 - batch 451: 0.9052\n",
            "training loss at step 7 - batch 452: 0.30 (2019-08-04 13:37:22.124328)\n",
            "Accuracy at step 7 - batch 452: 0.9104\n",
            "training loss at step 7 - batch 453: 0.31 (2019-08-04 13:37:22.137873)\n",
            "Accuracy at step 7 - batch 453: 0.9064\n",
            "training loss at step 7 - batch 454: 0.33 (2019-08-04 13:37:22.152113)\n",
            "Accuracy at step 7 - batch 454: 0.9036\n",
            "training loss at step 7 - batch 455: 0.30 (2019-08-04 13:37:22.165871)\n",
            "Accuracy at step 7 - batch 455: 0.9144\n",
            "training loss at step 7 - batch 456: 0.31 (2019-08-04 13:37:22.293776)\n",
            "Accuracy at step 7 - batch 456: 0.9068\n",
            "training loss at step 7 - batch 457: 0.31 (2019-08-04 13:37:22.306971)\n",
            "Accuracy at step 7 - batch 457: 0.9124\n",
            "training loss at step 7 - batch 458: 0.30 (2019-08-04 13:37:22.319661)\n",
            "Accuracy at step 7 - batch 458: 0.9124\n",
            "training loss at step 7 - batch 459: 0.29 (2019-08-04 13:37:22.334112)\n",
            "Accuracy at step 7 - batch 459: 0.912\n",
            "training loss at step 7 - batch 460: 0.31 (2019-08-04 13:37:22.346783)\n",
            "Accuracy at step 7 - batch 460: 0.906\n",
            "training loss at step 7 - batch 461: 0.32 (2019-08-04 13:37:22.471476)\n",
            "Accuracy at step 7 - batch 461: 0.9072\n",
            "training loss at step 7 - batch 462: 0.32 (2019-08-04 13:37:22.487380)\n",
            "Accuracy at step 7 - batch 462: 0.9092\n",
            "training loss at step 7 - batch 463: 0.32 (2019-08-04 13:37:22.501849)\n",
            "Accuracy at step 7 - batch 463: 0.8988\n",
            "training loss at step 7 - batch 464: 0.29 (2019-08-04 13:37:22.515865)\n",
            "Accuracy at step 7 - batch 464: 0.9108\n",
            "training loss at step 7 - batch 465: 0.31 (2019-08-04 13:37:22.528418)\n",
            "Accuracy at step 7 - batch 465: 0.9048\n",
            "training loss at step 7 - batch 466: 0.34 (2019-08-04 13:37:22.650988)\n",
            "Accuracy at step 7 - batch 466: 0.904\n",
            "training loss at step 7 - batch 467: 0.32 (2019-08-04 13:37:22.664654)\n",
            "Accuracy at step 7 - batch 467: 0.904\n",
            "training loss at step 7 - batch 468: 0.31 (2019-08-04 13:37:22.677470)\n",
            "Accuracy at step 7 - batch 468: 0.91\n",
            "training loss at step 7 - batch 469: 0.33 (2019-08-04 13:37:22.690577)\n",
            "Accuracy at step 7 - batch 469: 0.898\n",
            "training loss at step 7 - batch 470: 0.32 (2019-08-04 13:37:22.703843)\n",
            "Accuracy at step 7 - batch 470: 0.9036\n",
            "training loss at step 7 - batch 471: 0.30 (2019-08-04 13:37:22.825035)\n",
            "Accuracy at step 7 - batch 471: 0.9108\n",
            "training loss at step 7 - batch 472: 0.32 (2019-08-04 13:37:22.838778)\n",
            "Accuracy at step 7 - batch 472: 0.9036\n",
            "training loss at step 7 - batch 473: 0.34 (2019-08-04 13:37:22.851479)\n",
            "Accuracy at step 7 - batch 473: 0.9008\n",
            "training loss at step 7 - batch 474: 0.32 (2019-08-04 13:37:22.865064)\n",
            "Accuracy at step 7 - batch 474: 0.9068\n",
            "training loss at step 7 - batch 475: 0.31 (2019-08-04 13:37:22.880647)\n",
            "Accuracy at step 7 - batch 475: 0.9116\n",
            "training loss at step 7 - batch 476: 0.31 (2019-08-04 13:37:23.009353)\n",
            "Accuracy at step 7 - batch 476: 0.91\n",
            "training loss at step 7 - batch 477: 0.29 (2019-08-04 13:37:23.023701)\n",
            "Accuracy at step 7 - batch 477: 0.9148\n",
            "training loss at step 7 - batch 478: 0.31 (2019-08-04 13:37:23.037581)\n",
            "Accuracy at step 7 - batch 478: 0.9024\n",
            "training loss at step 7 - batch 479: 0.29 (2019-08-04 13:37:23.050013)\n",
            "Accuracy at step 7 - batch 479: 0.916\n",
            "training loss at step 7 - batch 480: 0.32 (2019-08-04 13:37:23.062093)\n",
            "Accuracy at step 7 - batch 480: 0.9064\n",
            "training loss at step 7 - batch 481: 0.31 (2019-08-04 13:37:23.184033)\n",
            "Accuracy at step 7 - batch 481: 0.9036\n",
            "training loss at step 7 - batch 482: 0.30 (2019-08-04 13:37:23.201402)\n",
            "Accuracy at step 7 - batch 482: 0.9128\n",
            "training loss at step 7 - batch 483: 0.32 (2019-08-04 13:37:23.216583)\n",
            "Accuracy at step 7 - batch 483: 0.9052\n",
            "training loss at step 7 - batch 484: 0.29 (2019-08-04 13:37:23.230063)\n",
            "Accuracy at step 7 - batch 484: 0.9088\n",
            "training loss at step 7 - batch 485: 0.30 (2019-08-04 13:37:23.244472)\n",
            "Accuracy at step 7 - batch 485: 0.9096\n",
            "training loss at step 7 - batch 486: 0.34 (2019-08-04 13:37:23.382916)\n",
            "Accuracy at step 7 - batch 486: 0.8988\n",
            "training loss at step 7 - batch 487: 0.32 (2019-08-04 13:37:23.398430)\n",
            "Accuracy at step 7 - batch 487: 0.9064\n",
            "training loss at step 7 - batch 488: 0.31 (2019-08-04 13:37:23.421724)\n",
            "Accuracy at step 7 - batch 488: 0.9084\n",
            "training loss at step 7 - batch 489: 0.33 (2019-08-04 13:37:23.434521)\n",
            "Accuracy at step 7 - batch 489: 0.9036\n",
            "training loss at step 7 - batch 490: 0.31 (2019-08-04 13:37:23.448316)\n",
            "Accuracy at step 7 - batch 490: 0.9076\n",
            "training loss at step 7 - batch 491: 0.30 (2019-08-04 13:37:23.570504)\n",
            "Accuracy at step 7 - batch 491: 0.912\n",
            "training loss at step 7 - batch 492: 0.30 (2019-08-04 13:37:23.583180)\n",
            "Accuracy at step 7 - batch 492: 0.9096\n",
            "training loss at step 7 - batch 493: 0.33 (2019-08-04 13:37:23.595914)\n",
            "Accuracy at step 7 - batch 493: 0.902\n",
            "training loss at step 7 - batch 494: 0.31 (2019-08-04 13:37:23.608689)\n",
            "Accuracy at step 7 - batch 494: 0.9084\n",
            "training loss at step 7 - batch 495: 0.32 (2019-08-04 13:37:23.620795)\n",
            "Accuracy at step 7 - batch 495: 0.9072\n",
            "training loss at step 7 - batch 496: 0.31 (2019-08-04 13:37:23.742887)\n",
            "Accuracy at step 7 - batch 496: 0.9052\n",
            "training loss at step 7 - batch 497: 0.32 (2019-08-04 13:37:23.760750)\n",
            "Accuracy at step 7 - batch 497: 0.9036\n",
            "training loss at step 7 - batch 498: 0.32 (2019-08-04 13:37:23.773602)\n",
            "Accuracy at step 7 - batch 498: 0.8992\n",
            "training loss at step 7 - batch 499: 0.28 (2019-08-04 13:37:23.786143)\n",
            "Accuracy at step 7 - batch 499: 0.9156\n",
            "training loss at step 7 - batch 500: 0.34 (2019-08-04 13:37:23.799501)\n",
            "Accuracy at step 7 - batch 500: 0.8972\n",
            "training loss at step 7 - batch 501: 0.29 (2019-08-04 13:37:23.922968)\n",
            "Accuracy at step 7 - batch 501: 0.9092\n",
            "training loss at step 7 - batch 502: 0.32 (2019-08-04 13:37:23.939029)\n",
            "Accuracy at step 7 - batch 502: 0.9016\n",
            "training loss at step 7 - batch 503: 0.31 (2019-08-04 13:37:23.953743)\n",
            "Accuracy at step 7 - batch 503: 0.9112\n",
            "training loss at step 7 - batch 504: 0.32 (2019-08-04 13:37:23.967301)\n",
            "Accuracy at step 7 - batch 504: 0.9064\n",
            "training loss at step 7 - batch 505: 0.32 (2019-08-04 13:37:23.980122)\n",
            "Accuracy at step 7 - batch 505: 0.906\n",
            "training loss at step 7 - batch 506: 0.32 (2019-08-04 13:37:24.108836)\n",
            "Accuracy at step 7 - batch 506: 0.9012\n",
            "training loss at step 7 - batch 507: 0.32 (2019-08-04 13:37:24.122101)\n",
            "Accuracy at step 7 - batch 507: 0.9032\n",
            "training loss at step 7 - batch 508: 0.29 (2019-08-04 13:37:24.135408)\n",
            "Accuracy at step 7 - batch 508: 0.916\n",
            "training loss at step 7 - batch 509: 0.29 (2019-08-04 13:37:24.148329)\n",
            "Accuracy at step 7 - batch 509: 0.9144\n",
            "training loss at step 7 - batch 510: 0.32 (2019-08-04 13:37:24.162567)\n",
            "Accuracy at step 7 - batch 510: 0.9088\n",
            "training loss at step 7 - batch 511: 0.30 (2019-08-04 13:37:24.284938)\n",
            "Accuracy at step 7 - batch 511: 0.908\n",
            "training loss at step 7 - batch 512: 0.33 (2019-08-04 13:37:24.300919)\n",
            "Accuracy at step 7 - batch 512: 0.9052\n",
            "training loss at step 7 - batch 513: 0.32 (2019-08-04 13:37:24.315686)\n",
            "Accuracy at step 7 - batch 513: 0.904\n",
            "training loss at step 7 - batch 514: 0.34 (2019-08-04 13:37:24.328281)\n",
            "Accuracy at step 7 - batch 514: 0.8984\n",
            "training loss at step 7 - batch 515: 0.31 (2019-08-04 13:37:24.341274)\n",
            "Accuracy at step 7 - batch 515: 0.9048\n",
            "training loss at step 7 - batch 516: 0.32 (2019-08-04 13:37:24.464503)\n",
            "Accuracy at step 7 - batch 516: 0.9076\n",
            "training loss at step 7 - batch 517: 0.32 (2019-08-04 13:37:24.476977)\n",
            "Accuracy at step 7 - batch 517: 0.9016\n",
            "training loss at step 7 - batch 518: 0.33 (2019-08-04 13:37:24.490162)\n",
            "Accuracy at step 7 - batch 518: 0.8936\n",
            "training loss at step 7 - batch 519: 0.30 (2019-08-04 13:37:24.502087)\n",
            "Accuracy at step 7 - batch 519: 0.906\n",
            "training loss at step 7 - batch 520: 0.30 (2019-08-04 13:37:24.514516)\n",
            "Accuracy at step 7 - batch 520: 0.908\n",
            "training loss at step 7 - batch 521: 0.30 (2019-08-04 13:37:24.634746)\n",
            "Accuracy at step 7 - batch 521: 0.9092\n",
            "training loss at step 7 - batch 522: 0.31 (2019-08-04 13:37:24.654965)\n",
            "Accuracy at step 7 - batch 522: 0.9084\n",
            "training loss at step 7 - batch 523: 0.30 (2019-08-04 13:37:24.668325)\n",
            "Accuracy at step 7 - batch 523: 0.9088\n",
            "training loss at step 7 - batch 524: 0.32 (2019-08-04 13:37:24.683021)\n",
            "Accuracy at step 7 - batch 524: 0.904\n",
            "training loss at step 7 - batch 525: 0.31 (2019-08-04 13:37:24.695968)\n",
            "Accuracy at step 7 - batch 525: 0.904\n",
            "training loss at step 7 - batch 526: 0.31 (2019-08-04 13:37:24.821672)\n",
            "Accuracy at step 7 - batch 526: 0.908\n",
            "training loss at step 7 - batch 527: 0.32 (2019-08-04 13:37:24.834468)\n",
            "Accuracy at step 7 - batch 527: 0.9056\n",
            "training loss at step 7 - batch 528: 0.31 (2019-08-04 13:37:24.847134)\n",
            "Accuracy at step 7 - batch 528: 0.9084\n",
            "training loss at step 7 - batch 529: 0.31 (2019-08-04 13:37:24.860375)\n",
            "Accuracy at step 7 - batch 529: 0.9028\n",
            "training loss at step 7 - batch 530: 0.33 (2019-08-04 13:37:24.874987)\n",
            "Accuracy at step 7 - batch 530: 0.9012\n",
            "training loss at step 7 - batch 531: 0.30 (2019-08-04 13:37:25.001523)\n",
            "Accuracy at step 7 - batch 531: 0.9104\n",
            "training loss at step 7 - batch 532: 0.31 (2019-08-04 13:37:25.017094)\n",
            "Accuracy at step 7 - batch 532: 0.9032\n",
            "training loss at step 7 - batch 533: 0.28 (2019-08-04 13:37:25.033773)\n",
            "Accuracy at step 7 - batch 533: 0.9132\n",
            "training loss at step 7 - batch 534: 0.31 (2019-08-04 13:37:25.056062)\n",
            "Accuracy at step 7 - batch 534: 0.906\n",
            "training loss at step 7 - batch 535: 0.32 (2019-08-04 13:37:25.069342)\n",
            "Accuracy at step 7 - batch 535: 0.8984\n",
            "training loss at step 7 - batch 536: 0.30 (2019-08-04 13:37:25.203744)\n",
            "Accuracy at step 7 - batch 536: 0.912\n",
            "training loss at step 7 - batch 537: 0.30 (2019-08-04 13:37:25.218449)\n",
            "Accuracy at step 7 - batch 537: 0.9044\n",
            "training loss at step 7 - batch 538: 0.31 (2019-08-04 13:37:25.231632)\n",
            "Accuracy at step 7 - batch 538: 0.9056\n",
            "training loss at step 7 - batch 539: 0.31 (2019-08-04 13:37:25.243758)\n",
            "Accuracy at step 7 - batch 539: 0.902\n",
            "training loss at step 7 - batch 540: 0.31 (2019-08-04 13:37:25.256348)\n",
            "Accuracy at step 7 - batch 540: 0.912\n",
            "training loss at step 7 - batch 541: 0.30 (2019-08-04 13:37:25.375465)\n",
            "Accuracy at step 7 - batch 541: 0.9108\n",
            "training loss at step 7 - batch 542: 0.30 (2019-08-04 13:37:25.389377)\n",
            "Accuracy at step 7 - batch 542: 0.9124\n",
            "training loss at step 7 - batch 543: 0.30 (2019-08-04 13:37:25.402322)\n",
            "Accuracy at step 7 - batch 543: 0.9088\n",
            "training loss at step 7 - batch 544: 0.31 (2019-08-04 13:37:25.419900)\n",
            "Accuracy at step 7 - batch 544: 0.9064\n",
            "training loss at step 7 - batch 545: 0.34 (2019-08-04 13:37:25.431771)\n",
            "Accuracy at step 7 - batch 545: 0.8984\n",
            "training loss at step 7 - batch 546: 0.33 (2019-08-04 13:37:25.554756)\n",
            "Accuracy at step 7 - batch 546: 0.9044\n",
            "training loss at step 7 - batch 547: 0.30 (2019-08-04 13:37:25.569766)\n",
            "Accuracy at step 7 - batch 547: 0.9112\n",
            "training loss at step 7 - batch 548: 0.31 (2019-08-04 13:37:25.583222)\n",
            "Accuracy at step 7 - batch 548: 0.904\n",
            "training loss at step 7 - batch 549: 0.29 (2019-08-04 13:37:25.596906)\n",
            "Accuracy at step 7 - batch 549: 0.914\n",
            "training loss at step 7 - batch 550: 0.32 (2019-08-04 13:37:25.609905)\n",
            "Accuracy at step 7 - batch 550: 0.9064\n",
            "training loss at step 7 - batch 551: 0.30 (2019-08-04 13:37:25.737483)\n",
            "Accuracy at step 7 - batch 551: 0.9108\n",
            "training loss at step 7 - batch 552: 0.30 (2019-08-04 13:37:25.752946)\n",
            "Accuracy at step 7 - batch 552: 0.9104\n",
            "training loss at step 7 - batch 553: 0.31 (2019-08-04 13:37:25.766458)\n",
            "Accuracy at step 7 - batch 553: 0.9128\n",
            "training loss at step 7 - batch 554: 0.31 (2019-08-04 13:37:25.778817)\n",
            "Accuracy at step 7 - batch 554: 0.9048\n",
            "training loss at step 7 - batch 555: 0.33 (2019-08-04 13:37:25.791557)\n",
            "Accuracy at step 7 - batch 555: 0.9032\n",
            "training loss at step 7 - batch 556: 0.32 (2019-08-04 13:37:25.915935)\n",
            "Accuracy at step 7 - batch 556: 0.9064\n",
            "training loss at step 7 - batch 557: 0.30 (2019-08-04 13:37:25.930740)\n",
            "Accuracy at step 7 - batch 557: 0.9044\n",
            "training loss at step 7 - batch 558: 0.32 (2019-08-04 13:37:25.946085)\n",
            "Accuracy at step 7 - batch 558: 0.9016\n",
            "training loss at step 7 - batch 559: 0.31 (2019-08-04 13:37:25.958990)\n",
            "Accuracy at step 7 - batch 559: 0.908\n",
            "training loss at step 7 - batch 560: 0.29 (2019-08-04 13:37:25.971428)\n",
            "Accuracy at step 7 - batch 560: 0.9124\n",
            "training loss at step 7 - batch 561: 0.31 (2019-08-04 13:37:26.099093)\n",
            "Accuracy at step 7 - batch 561: 0.9072\n",
            "training loss at step 7 - batch 562: 0.30 (2019-08-04 13:37:26.112552)\n",
            "Accuracy at step 7 - batch 562: 0.9076\n",
            "training loss at step 7 - batch 563: 0.30 (2019-08-04 13:37:26.124695)\n",
            "Accuracy at step 7 - batch 563: 0.9052\n",
            "training loss at step 7 - batch 564: 0.32 (2019-08-04 13:37:26.136918)\n",
            "Accuracy at step 7 - batch 564: 0.9032\n",
            "training loss at step 7 - batch 565: 0.30 (2019-08-04 13:37:26.151593)\n",
            "Accuracy at step 7 - batch 565: 0.9124\n",
            "training loss at step 7 - batch 566: 0.32 (2019-08-04 13:37:26.276563)\n",
            "Accuracy at step 7 - batch 566: 0.9064\n",
            "training loss at step 7 - batch 567: 0.32 (2019-08-04 13:37:26.290790)\n",
            "Accuracy at step 7 - batch 567: 0.9064\n",
            "training loss at step 7 - batch 568: 0.32 (2019-08-04 13:37:26.303392)\n",
            "Accuracy at step 7 - batch 568: 0.906\n",
            "training loss at step 7 - batch 569: 0.30 (2019-08-04 13:37:26.316198)\n",
            "Accuracy at step 7 - batch 569: 0.9112\n",
            "training loss at step 7 - batch 570: 0.28 (2019-08-04 13:37:26.329434)\n",
            "Accuracy at step 7 - batch 570: 0.914\n",
            "training loss at step 7 - batch 571: 0.29 (2019-08-04 13:37:26.461318)\n",
            "Accuracy at step 7 - batch 571: 0.91\n",
            "training loss at step 7 - batch 572: 0.32 (2019-08-04 13:37:26.474119)\n",
            "Accuracy at step 7 - batch 572: 0.9092\n",
            "training loss at step 7 - batch 573: 0.30 (2019-08-04 13:37:26.486770)\n",
            "Accuracy at step 7 - batch 573: 0.9152\n",
            "training loss at step 7 - batch 574: 0.31 (2019-08-04 13:37:26.499641)\n",
            "Accuracy at step 7 - batch 574: 0.9056\n",
            "training loss at step 7 - batch 575: 0.30 (2019-08-04 13:37:26.512952)\n",
            "Accuracy at step 7 - batch 575: 0.9056\n",
            "training loss at step 7 - batch 576: 0.30 (2019-08-04 13:37:26.644249)\n",
            "Accuracy at step 7 - batch 576: 0.9112\n",
            "training loss at step 7 - batch 577: 0.32 (2019-08-04 13:37:26.657288)\n",
            "Accuracy at step 7 - batch 577: 0.906\n",
            "training loss at step 7 - batch 578: 0.29 (2019-08-04 13:37:26.673443)\n",
            "Accuracy at step 7 - batch 578: 0.9136\n",
            "training loss at step 7 - batch 579: 0.31 (2019-08-04 13:37:26.685607)\n",
            "Accuracy at step 7 - batch 579: 0.9084\n",
            "training loss at step 7 - batch 580: 0.33 (2019-08-04 13:37:26.698395)\n",
            "Accuracy at step 7 - batch 580: 0.9008\n",
            "training loss at step 7 - batch 581: 0.30 (2019-08-04 13:37:26.821219)\n",
            "Accuracy at step 7 - batch 581: 0.9124\n",
            "training loss at step 7 - batch 582: 0.31 (2019-08-04 13:37:26.835853)\n",
            "Accuracy at step 7 - batch 582: 0.9088\n",
            "training loss at step 7 - batch 583: 0.33 (2019-08-04 13:37:26.848729)\n",
            "Accuracy at step 7 - batch 583: 0.9032\n",
            "training loss at step 7 - batch 584: 0.31 (2019-08-04 13:37:26.861580)\n",
            "Accuracy at step 7 - batch 584: 0.9068\n",
            "training loss at step 7 - batch 585: 0.30 (2019-08-04 13:37:26.877696)\n",
            "Accuracy at step 7 - batch 585: 0.9148\n",
            "training loss at step 7 - batch 586: 0.32 (2019-08-04 13:37:27.002943)\n",
            "Accuracy at step 7 - batch 586: 0.9028\n",
            "training loss at step 7 - batch 587: 0.30 (2019-08-04 13:37:27.020599)\n",
            "Accuracy at step 7 - batch 587: 0.9072\n",
            "training loss at step 7 - batch 588: 0.35 (2019-08-04 13:37:27.033223)\n",
            "Accuracy at step 7 - batch 588: 0.9008\n",
            "training loss at step 7 - batch 589: 0.32 (2019-08-04 13:37:27.046222)\n",
            "Accuracy at step 7 - batch 589: 0.902\n",
            "training loss at step 7 - batch 590: 0.34 (2019-08-04 13:37:27.060906)\n",
            "Accuracy at step 7 - batch 590: 0.8944\n",
            "training loss at step 7 - batch 591: 0.33 (2019-08-04 13:37:27.199147)\n",
            "Accuracy at step 7 - batch 591: 0.9112\n",
            "training loss at step 7 - batch 592: 0.33 (2019-08-04 13:37:27.213258)\n",
            "Accuracy at step 7 - batch 592: 0.8972\n",
            "training loss at step 7 - batch 593: 0.32 (2019-08-04 13:37:27.225256)\n",
            "Accuracy at step 7 - batch 593: 0.9028\n",
            "training loss at step 7 - batch 594: 0.30 (2019-08-04 13:37:27.237375)\n",
            "Accuracy at step 7 - batch 594: 0.9104\n",
            "training loss at step 7 - batch 595: 0.33 (2019-08-04 13:37:27.250762)\n",
            "Accuracy at step 7 - batch 595: 0.9016\n",
            "training loss at step 7 - batch 596: 0.30 (2019-08-04 13:37:27.372145)\n",
            "Accuracy at step 7 - batch 596: 0.9104\n",
            "training loss at step 7 - batch 597: 0.29 (2019-08-04 13:37:27.389966)\n",
            "Accuracy at step 7 - batch 597: 0.9036\n",
            "training loss at step 7 - batch 598: 0.31 (2019-08-04 13:37:27.404652)\n",
            "Accuracy at step 7 - batch 598: 0.9068\n",
            "training loss at step 7 - batch 599: 0.32 (2019-08-04 13:37:27.417231)\n",
            "Accuracy at step 7 - batch 599: 0.9008\n",
            "training loss at step 7 - batch 600: 0.31 (2019-08-04 13:37:27.429110)\n",
            "Accuracy at step 7 - batch 600: 0.9088\n",
            "training loss at step 7 - batch 601: 0.31 (2019-08-04 13:37:27.550350)\n",
            "Accuracy at step 7 - batch 601: 0.9064\n",
            "training loss at step 7 - batch 602: 0.32 (2019-08-04 13:37:27.565815)\n",
            "Accuracy at step 7 - batch 602: 0.9016\n",
            "training loss at step 7 - batch 603: 0.35 (2019-08-04 13:37:27.578686)\n",
            "Accuracy at step 7 - batch 603: 0.894\n",
            "training loss at step 7 - batch 604: 0.30 (2019-08-04 13:37:27.591066)\n",
            "Accuracy at step 7 - batch 604: 0.9072\n",
            "training loss at step 7 - batch 605: 0.30 (2019-08-04 13:37:27.604882)\n",
            "Accuracy at step 7 - batch 605: 0.9056\n",
            "training loss at step 7 - batch 606: 0.30 (2019-08-04 13:37:27.735380)\n",
            "Accuracy at step 7 - batch 606: 0.906\n",
            "training loss at step 7 - batch 607: 0.30 (2019-08-04 13:37:27.752692)\n",
            "Accuracy at step 7 - batch 607: 0.9064\n",
            "training loss at step 7 - batch 608: 0.31 (2019-08-04 13:37:27.764972)\n",
            "Accuracy at step 7 - batch 608: 0.9088\n",
            "training loss at step 7 - batch 609: 0.32 (2019-08-04 13:37:27.778176)\n",
            "Accuracy at step 7 - batch 609: 0.9016\n",
            "training loss at step 7 - batch 610: 0.32 (2019-08-04 13:37:27.790416)\n",
            "Accuracy at step 7 - batch 610: 0.9064\n",
            "training loss at step 7 - batch 611: 0.32 (2019-08-04 13:37:27.915474)\n",
            "Accuracy at step 7 - batch 611: 0.9008\n",
            "training loss at step 7 - batch 612: 0.31 (2019-08-04 13:37:27.933627)\n",
            "Accuracy at step 7 - batch 612: 0.9028\n",
            "training loss at step 7 - batch 613: 0.30 (2019-08-04 13:37:27.946868)\n",
            "Accuracy at step 7 - batch 613: 0.9084\n",
            "training loss at step 7 - batch 614: 0.31 (2019-08-04 13:37:27.959653)\n",
            "Accuracy at step 7 - batch 614: 0.9068\n",
            "training loss at step 7 - batch 615: 0.29 (2019-08-04 13:37:27.972388)\n",
            "Accuracy at step 7 - batch 615: 0.91\n",
            "training loss at step 7 - batch 616: 0.31 (2019-08-04 13:37:28.103175)\n",
            "Accuracy at step 7 - batch 616: 0.9084\n",
            "training loss at step 7 - batch 617: 0.32 (2019-08-04 13:37:28.126069)\n",
            "Accuracy at step 7 - batch 617: 0.9052\n",
            "training loss at step 7 - batch 618: 0.31 (2019-08-04 13:37:28.139270)\n",
            "Accuracy at step 7 - batch 618: 0.9048\n",
            "training loss at step 7 - batch 619: 0.31 (2019-08-04 13:37:28.151612)\n",
            "Accuracy at step 7 - batch 619: 0.9116\n",
            "training loss at step 7 - batch 620: 0.31 (2019-08-04 13:37:28.163624)\n",
            "Accuracy at step 7 - batch 620: 0.9132\n",
            "training loss at step 7 - batch 621: 0.35 (2019-08-04 13:37:28.289367)\n",
            "Accuracy at step 7 - batch 621: 0.8924\n",
            "training loss at step 7 - batch 622: 0.30 (2019-08-04 13:37:28.303083)\n",
            "Accuracy at step 7 - batch 622: 0.9084\n",
            "training loss at step 7 - batch 623: 0.30 (2019-08-04 13:37:28.315819)\n",
            "Accuracy at step 7 - batch 623: 0.9068\n",
            "training loss at step 7 - batch 624: 0.33 (2019-08-04 13:37:28.329445)\n",
            "Accuracy at step 7 - batch 624: 0.902\n",
            "training loss at step 7 - batch 625: 0.30 (2019-08-04 13:37:28.343060)\n",
            "Accuracy at step 7 - batch 625: 0.908\n",
            "training loss at step 7 - batch 626: 0.31 (2019-08-04 13:37:28.461826)\n",
            "Accuracy at step 7 - batch 626: 0.9064\n",
            "training loss at step 7 - batch 627: 0.31 (2019-08-04 13:37:28.475731)\n",
            "Accuracy at step 7 - batch 627: 0.904\n",
            "training loss at step 7 - batch 628: 0.30 (2019-08-04 13:37:28.488397)\n",
            "Accuracy at step 7 - batch 628: 0.9048\n",
            "training loss at step 7 - batch 629: 0.31 (2019-08-04 13:37:28.502216)\n",
            "Accuracy at step 7 - batch 629: 0.9104\n",
            "training loss at step 7 - batch 630: 0.33 (2019-08-04 13:37:28.515278)\n",
            "Accuracy at step 7 - batch 630: 0.9004\n",
            "training loss at step 7 - batch 631: 0.33 (2019-08-04 13:37:28.638499)\n",
            "Accuracy at step 7 - batch 631: 0.8984\n",
            "training loss at step 7 - batch 632: 0.31 (2019-08-04 13:37:28.651300)\n",
            "Accuracy at step 7 - batch 632: 0.9068\n",
            "training loss at step 7 - batch 633: 0.33 (2019-08-04 13:37:28.664076)\n",
            "Accuracy at step 7 - batch 633: 0.9028\n",
            "training loss at step 7 - batch 634: 0.30 (2019-08-04 13:37:28.677579)\n",
            "Accuracy at step 7 - batch 634: 0.9104\n",
            "training loss at step 7 - batch 635: 0.33 (2019-08-04 13:37:28.690621)\n",
            "Accuracy at step 7 - batch 635: 0.9044\n",
            "training loss at step 7 - batch 636: 0.33 (2019-08-04 13:37:28.811823)\n",
            "Accuracy at step 7 - batch 636: 0.9012\n",
            "training loss at step 7 - batch 637: 0.32 (2019-08-04 13:37:28.828407)\n",
            "Accuracy at step 7 - batch 637: 0.9036\n",
            "training loss at step 7 - batch 638: 0.31 (2019-08-04 13:37:28.841623)\n",
            "Accuracy at step 7 - batch 638: 0.91\n",
            "training loss at step 7 - batch 639: 0.30 (2019-08-04 13:37:28.857894)\n",
            "Accuracy at step 7 - batch 639: 0.9076\n",
            "training loss at step 7 - batch 640: 0.31 (2019-08-04 13:37:28.871366)\n",
            "Accuracy at step 7 - batch 640: 0.9088\n",
            "training loss at step 7 - batch 641: 0.33 (2019-08-04 13:37:28.992397)\n",
            "Accuracy at step 7 - batch 641: 0.898\n",
            "training loss at step 7 - batch 642: 0.31 (2019-08-04 13:37:29.009965)\n",
            "Accuracy at step 7 - batch 642: 0.9104\n",
            "training loss at step 7 - batch 643: 0.32 (2019-08-04 13:37:29.022188)\n",
            "Accuracy at step 7 - batch 643: 0.906\n",
            "training loss at step 7 - batch 644: 0.30 (2019-08-04 13:37:29.035476)\n",
            "Accuracy at step 7 - batch 644: 0.9104\n",
            "training loss at step 7 - batch 645: 0.30 (2019-08-04 13:37:29.049657)\n",
            "Accuracy at step 7 - batch 645: 0.912\n",
            "training loss at step 7 - batch 646: 0.31 (2019-08-04 13:37:29.183090)\n",
            "Accuracy at step 7 - batch 646: 0.9108\n",
            "training loss at step 7 - batch 647: 0.34 (2019-08-04 13:37:29.197251)\n",
            "Accuracy at step 7 - batch 647: 0.902\n",
            "training loss at step 7 - batch 648: 0.30 (2019-08-04 13:37:29.210446)\n",
            "Accuracy at step 7 - batch 648: 0.91\n",
            "training loss at step 7 - batch 649: 0.32 (2019-08-04 13:37:29.223468)\n",
            "Accuracy at step 7 - batch 649: 0.9036\n",
            "training loss at step 7 - batch 650: 0.32 (2019-08-04 13:37:29.236206)\n",
            "Accuracy at step 7 - batch 650: 0.906\n",
            "training loss at step 7 - batch 651: 0.31 (2019-08-04 13:37:29.361691)\n",
            "Accuracy at step 7 - batch 651: 0.908\n",
            "training loss at step 7 - batch 652: 0.33 (2019-08-04 13:37:29.374573)\n",
            "Accuracy at step 7 - batch 652: 0.8988\n",
            "training loss at step 7 - batch 653: 0.30 (2019-08-04 13:37:29.387402)\n",
            "Accuracy at step 7 - batch 653: 0.9116\n",
            "training loss at step 7 - batch 654: 0.29 (2019-08-04 13:37:29.399704)\n",
            "Accuracy at step 7 - batch 654: 0.9116\n",
            "training loss at step 7 - batch 655: 0.32 (2019-08-04 13:37:29.412221)\n",
            "Accuracy at step 7 - batch 655: 0.9016\n",
            "training loss at step 7 - batch 656: 0.31 (2019-08-04 13:37:29.530788)\n",
            "Accuracy at step 7 - batch 656: 0.9068\n",
            "training loss at step 7 - batch 657: 0.33 (2019-08-04 13:37:29.543225)\n",
            "Accuracy at step 7 - batch 657: 0.9016\n",
            "training loss at step 7 - batch 658: 0.31 (2019-08-04 13:37:29.556383)\n",
            "Accuracy at step 7 - batch 658: 0.9044\n",
            "training loss at step 7 - batch 659: 0.33 (2019-08-04 13:37:29.571593)\n",
            "Accuracy at step 7 - batch 659: 0.9028\n",
            "training loss at step 7 - batch 660: 0.30 (2019-08-04 13:37:29.584340)\n",
            "Accuracy at step 7 - batch 660: 0.9068\n",
            "training loss at step 7 - batch 661: 0.30 (2019-08-04 13:37:29.702879)\n",
            "Accuracy at step 7 - batch 661: 0.9084\n",
            "training loss at step 7 - batch 662: 0.31 (2019-08-04 13:37:29.720493)\n",
            "Accuracy at step 7 - batch 662: 0.9104\n",
            "training loss at step 7 - batch 663: 0.30 (2019-08-04 13:37:29.733510)\n",
            "Accuracy at step 7 - batch 663: 0.912\n",
            "training loss at step 7 - batch 664: 0.31 (2019-08-04 13:37:29.746345)\n",
            "Accuracy at step 7 - batch 664: 0.9092\n",
            "training loss at step 7 - batch 665: 0.29 (2019-08-04 13:37:29.758318)\n",
            "Accuracy at step 7 - batch 665: 0.918\n",
            "training loss at step 7 - batch 666: 0.31 (2019-08-04 13:37:29.884712)\n",
            "Accuracy at step 7 - batch 666: 0.9064\n",
            "training loss at step 7 - batch 667: 0.31 (2019-08-04 13:37:29.903089)\n",
            "Accuracy at step 7 - batch 667: 0.9076\n",
            "training loss at step 7 - batch 668: 0.30 (2019-08-04 13:37:29.915636)\n",
            "Accuracy at step 7 - batch 668: 0.9068\n",
            "training loss at step 7 - batch 669: 0.31 (2019-08-04 13:37:29.928401)\n",
            "Accuracy at step 7 - batch 669: 0.9052\n",
            "training loss at step 7 - batch 670: 0.32 (2019-08-04 13:37:29.941021)\n",
            "Accuracy at step 7 - batch 670: 0.9012\n",
            "training loss at step 7 - batch 671: 0.31 (2019-08-04 13:37:30.059908)\n",
            "Accuracy at step 7 - batch 671: 0.9096\n",
            "training loss at step 7 - batch 672: 0.30 (2019-08-04 13:37:30.074218)\n",
            "Accuracy at step 7 - batch 672: 0.9076\n",
            "training loss at step 7 - batch 673: 0.32 (2019-08-04 13:37:30.090722)\n",
            "Accuracy at step 7 - batch 673: 0.9044\n",
            "training loss at step 7 - batch 674: 0.33 (2019-08-04 13:37:30.103606)\n",
            "Accuracy at step 7 - batch 674: 0.9\n",
            "training loss at step 7 - batch 675: 0.32 (2019-08-04 13:37:30.116690)\n",
            "Accuracy at step 7 - batch 675: 0.9032\n",
            "training loss at step 7 - batch 676: 0.30 (2019-08-04 13:37:30.246669)\n",
            "Accuracy at step 7 - batch 676: 0.9096\n",
            "training loss at step 7 - batch 677: 0.32 (2019-08-04 13:37:30.261342)\n",
            "Accuracy at step 7 - batch 677: 0.9064\n",
            "training loss at step 7 - batch 678: 0.32 (2019-08-04 13:37:30.274602)\n",
            "Accuracy at step 7 - batch 678: 0.9056\n",
            "training loss at step 7 - batch 679: 0.29 (2019-08-04 13:37:30.287309)\n",
            "Accuracy at step 7 - batch 679: 0.9112\n",
            "training loss at step 7 - batch 680: 0.32 (2019-08-04 13:37:30.302011)\n",
            "Accuracy at step 7 - batch 680: 0.906\n",
            "training loss at step 7 - batch 681: 0.30 (2019-08-04 13:37:30.423122)\n",
            "Accuracy at step 7 - batch 681: 0.9084\n",
            "training loss at step 7 - batch 682: 0.33 (2019-08-04 13:37:30.437023)\n",
            "Accuracy at step 7 - batch 682: 0.9036\n",
            "training loss at step 7 - batch 683: 0.31 (2019-08-04 13:37:30.449735)\n",
            "Accuracy at step 7 - batch 683: 0.9056\n",
            "training loss at step 7 - batch 684: 0.31 (2019-08-04 13:37:30.461938)\n",
            "Accuracy at step 7 - batch 684: 0.9084\n",
            "training loss at step 7 - batch 685: 0.31 (2019-08-04 13:37:30.475757)\n",
            "Accuracy at step 7 - batch 685: 0.912\n",
            "training loss at step 7 - batch 686: 0.30 (2019-08-04 13:37:30.601368)\n",
            "Accuracy at step 7 - batch 686: 0.9076\n",
            "training loss at step 7 - batch 687: 0.32 (2019-08-04 13:37:30.615697)\n",
            "Accuracy at step 7 - batch 687: 0.9072\n",
            "training loss at step 7 - batch 688: 0.33 (2019-08-04 13:37:30.628061)\n",
            "Accuracy at step 7 - batch 688: 0.9056\n",
            "training loss at step 7 - batch 689: 0.31 (2019-08-04 13:37:30.641673)\n",
            "Accuracy at step 7 - batch 689: 0.9068\n",
            "training loss at step 7 - batch 690: 0.33 (2019-08-04 13:37:30.655251)\n",
            "Accuracy at step 7 - batch 690: 0.9068\n",
            "training loss at step 7 - batch 691: 0.33 (2019-08-04 13:37:30.779991)\n",
            "Accuracy at step 7 - batch 691: 0.9032\n",
            "training loss at step 7 - batch 692: 0.33 (2019-08-04 13:37:30.792405)\n",
            "Accuracy at step 7 - batch 692: 0.9032\n",
            "training loss at step 7 - batch 693: 0.33 (2019-08-04 13:37:30.809437)\n",
            "Accuracy at step 7 - batch 693: 0.8964\n",
            "training loss at step 7 - batch 694: 0.31 (2019-08-04 13:37:30.826132)\n",
            "Accuracy at step 7 - batch 694: 0.906\n",
            "training loss at step 7 - batch 695: 0.29 (2019-08-04 13:37:30.839287)\n",
            "Accuracy at step 7 - batch 695: 0.9116\n",
            "training loss at step 7 - batch 696: 0.33 (2019-08-04 13:37:30.962076)\n",
            "Accuracy at step 7 - batch 696: 0.9016\n",
            "training loss at step 7 - batch 697: 0.30 (2019-08-04 13:37:30.977648)\n",
            "Accuracy at step 7 - batch 697: 0.9084\n",
            "training loss at step 7 - batch 698: 0.32 (2019-08-04 13:37:30.990796)\n",
            "Accuracy at step 7 - batch 698: 0.9064\n",
            "training loss at step 7 - batch 699: 0.32 (2019-08-04 13:37:31.003937)\n",
            "Accuracy at step 7 - batch 699: 0.9072\n",
            "training loss at step 7 - batch 700: 0.29 (2019-08-04 13:37:31.019168)\n",
            "Accuracy at step 7 - batch 700: 0.9164\n",
            "training loss at step 7 - batch 701: 0.32 (2019-08-04 13:37:31.143835)\n",
            "Accuracy at step 7 - batch 701: 0.8988\n",
            "training loss at step 7 - batch 702: 0.30 (2019-08-04 13:37:31.161823)\n",
            "Accuracy at step 7 - batch 702: 0.9116\n",
            "training loss at step 7 - batch 703: 0.31 (2019-08-04 13:37:31.178049)\n",
            "Accuracy at step 7 - batch 703: 0.9032\n",
            "training loss at step 7 - batch 704: 0.31 (2019-08-04 13:37:31.192833)\n",
            "Accuracy at step 7 - batch 704: 0.9084\n",
            "training loss at step 7 - batch 705: 0.31 (2019-08-04 13:37:31.206438)\n",
            "Accuracy at step 7 - batch 705: 0.9076\n",
            "training loss at step 7 - batch 706: 0.32 (2019-08-04 13:37:31.327121)\n",
            "Accuracy at step 7 - batch 706: 0.904\n",
            "training loss at step 7 - batch 707: 0.32 (2019-08-04 13:37:31.339576)\n",
            "Accuracy at step 7 - batch 707: 0.904\n",
            "training loss at step 7 - batch 708: 0.31 (2019-08-04 13:37:31.353233)\n",
            "Accuracy at step 7 - batch 708: 0.9084\n",
            "training loss at step 7 - batch 709: 0.31 (2019-08-04 13:37:31.367241)\n",
            "Accuracy at step 7 - batch 709: 0.9088\n",
            "training loss at step 7 - batch 710: 0.29 (2019-08-04 13:37:31.380122)\n",
            "Accuracy at step 7 - batch 710: 0.914\n",
            "training loss at step 7 - batch 711: 0.32 (2019-08-04 13:37:31.510117)\n",
            "Accuracy at step 7 - batch 711: 0.9024\n",
            "training loss at step 7 - batch 712: 0.31 (2019-08-04 13:37:31.526190)\n",
            "Accuracy at step 7 - batch 712: 0.9068\n",
            "training loss at step 7 - batch 713: 0.33 (2019-08-04 13:37:31.542756)\n",
            "Accuracy at step 7 - batch 713: 0.9044\n",
            "training loss at step 7 - batch 714: 0.32 (2019-08-04 13:37:31.556194)\n",
            "Accuracy at step 7 - batch 714: 0.9024\n",
            "training loss at step 7 - batch 715: 0.33 (2019-08-04 13:37:31.570001)\n",
            "Accuracy at step 7 - batch 715: 0.9036\n",
            "training loss at step 7 - batch 716: 0.33 (2019-08-04 13:37:31.696264)\n",
            "Accuracy at step 7 - batch 716: 0.9016\n",
            "training loss at step 7 - batch 717: 0.32 (2019-08-04 13:37:31.710656)\n",
            "Accuracy at step 7 - batch 717: 0.9088\n",
            "training loss at step 7 - batch 718: 0.30 (2019-08-04 13:37:31.724209)\n",
            "Accuracy at step 7 - batch 718: 0.9108\n",
            "training loss at step 7 - batch 719: 0.30 (2019-08-04 13:37:31.736062)\n",
            "Accuracy at step 7 - batch 719: 0.9064\n",
            "training loss at step 7 - batch 720: 0.33 (2019-08-04 13:37:31.750463)\n",
            "Accuracy at step 7 - batch 720: 0.9052\n",
            "training loss at step 7 - batch 721: 0.31 (2019-08-04 13:37:31.874494)\n",
            "Accuracy at step 7 - batch 721: 0.9092\n",
            "training loss at step 7 - batch 722: 0.31 (2019-08-04 13:37:31.890624)\n",
            "Accuracy at step 7 - batch 722: 0.9056\n",
            "training loss at step 7 - batch 723: 0.31 (2019-08-04 13:37:31.902379)\n",
            "Accuracy at step 7 - batch 723: 0.9056\n",
            "training loss at step 7 - batch 724: 0.29 (2019-08-04 13:37:31.914669)\n",
            "Accuracy at step 7 - batch 724: 0.9104\n",
            "training loss at step 7 - batch 725: 0.30 (2019-08-04 13:37:31.928979)\n",
            "Accuracy at step 7 - batch 725: 0.9056\n",
            "training loss at step 7 - batch 726: 0.31 (2019-08-04 13:37:32.052042)\n",
            "Accuracy at step 7 - batch 726: 0.9028\n",
            "training loss at step 7 - batch 727: 0.29 (2019-08-04 13:37:32.064668)\n",
            "Accuracy at step 7 - batch 727: 0.906\n",
            "training loss at step 7 - batch 728: 0.31 (2019-08-04 13:37:32.077661)\n",
            "Accuracy at step 7 - batch 728: 0.9076\n",
            "training loss at step 7 - batch 729: 0.33 (2019-08-04 13:37:32.091194)\n",
            "Accuracy at step 7 - batch 729: 0.8984\n",
            "training loss at step 7 - batch 730: 0.29 (2019-08-04 13:37:32.104275)\n",
            "Accuracy at step 7 - batch 730: 0.9136\n",
            "training loss at step 7 - batch 731: 0.34 (2019-08-04 13:37:32.236497)\n",
            "Accuracy at step 7 - batch 731: 0.9008\n",
            "training loss at step 7 - batch 732: 0.32 (2019-08-04 13:37:32.251422)\n",
            "Accuracy at step 7 - batch 732: 0.9044\n",
            "training loss at step 7 - batch 733: 0.31 (2019-08-04 13:37:32.268164)\n",
            "Accuracy at step 7 - batch 733: 0.9048\n",
            "training loss at step 7 - batch 734: 0.33 (2019-08-04 13:37:32.280136)\n",
            "Accuracy at step 7 - batch 734: 0.8976\n",
            "training loss at step 7 - batch 735: 0.32 (2019-08-04 13:37:32.293282)\n",
            "Accuracy at step 7 - batch 735: 0.9064\n",
            "training loss at step 7 - batch 736: 0.34 (2019-08-04 13:37:32.418768)\n",
            "Accuracy at step 7 - batch 736: 0.8996\n",
            "training loss at step 7 - batch 737: 0.33 (2019-08-04 13:37:32.434718)\n",
            "Accuracy at step 7 - batch 737: 0.9016\n",
            "training loss at step 7 - batch 738: 0.30 (2019-08-04 13:37:32.447436)\n",
            "Accuracy at step 7 - batch 738: 0.906\n",
            "training loss at step 7 - batch 739: 0.30 (2019-08-04 13:37:32.459157)\n",
            "Accuracy at step 7 - batch 739: 0.9104\n",
            "training loss at step 7 - batch 740: 0.31 (2019-08-04 13:37:32.473602)\n",
            "Accuracy at step 7 - batch 740: 0.9008\n",
            "training loss at step 7 - batch 741: 0.32 (2019-08-04 13:37:32.593771)\n",
            "Accuracy at step 7 - batch 741: 0.9012\n",
            "training loss at step 7 - batch 742: 0.30 (2019-08-04 13:37:32.611463)\n",
            "Accuracy at step 7 - batch 742: 0.904\n",
            "training loss at step 7 - batch 743: 0.33 (2019-08-04 13:37:32.624497)\n",
            "Accuracy at step 7 - batch 743: 0.9012\n",
            "training loss at step 7 - batch 744: 0.30 (2019-08-04 13:37:32.637358)\n",
            "Accuracy at step 7 - batch 744: 0.9064\n",
            "training loss at step 7 - batch 745: 0.29 (2019-08-04 13:37:32.650854)\n",
            "Accuracy at step 7 - batch 745: 0.9084\n",
            "training loss at step 7 - batch 746: 0.30 (2019-08-04 13:37:32.778135)\n",
            "Accuracy at step 7 - batch 746: 0.9088\n",
            "training loss at step 7 - batch 747: 0.32 (2019-08-04 13:37:32.791903)\n",
            "Accuracy at step 7 - batch 747: 0.9012\n",
            "training loss at step 7 - batch 748: 0.33 (2019-08-04 13:37:32.805186)\n",
            "Accuracy at step 7 - batch 748: 0.8996\n",
            "training loss at step 7 - batch 749: 0.30 (2019-08-04 13:37:32.816971)\n",
            "Accuracy at step 7 - batch 749: 0.9156\n",
            "training loss at step 7 - batch 750: 0.32 (2019-08-04 13:37:32.828975)\n",
            "Accuracy at step 7 - batch 750: 0.9048\n",
            "training loss at step 7 - batch 751: 0.31 (2019-08-04 13:37:32.948082)\n",
            "Accuracy at step 7 - batch 751: 0.9064\n",
            "training loss at step 7 - batch 752: 0.32 (2019-08-04 13:37:32.962531)\n",
            "Accuracy at step 7 - batch 752: 0.9112\n",
            "training loss at step 7 - batch 753: 0.32 (2019-08-04 13:37:32.975226)\n",
            "Accuracy at step 7 - batch 753: 0.904\n",
            "training loss at step 7 - batch 754: 0.30 (2019-08-04 13:37:32.989917)\n",
            "Accuracy at step 7 - batch 754: 0.912\n",
            "training loss at step 7 - batch 755: 0.30 (2019-08-04 13:37:33.003282)\n",
            "Accuracy at step 7 - batch 755: 0.9084\n",
            "training loss at step 7 - batch 756: 0.33 (2019-08-04 13:37:33.257790)\n",
            "Accuracy at step 7 - batch 756: 0.9064\n",
            "training loss at step 7 - batch 757: 0.30 (2019-08-04 13:37:33.270932)\n",
            "Accuracy at step 7 - batch 757: 0.91\n",
            "training loss at step 7 - batch 758: 0.31 (2019-08-04 13:37:33.284377)\n",
            "Accuracy at step 7 - batch 758: 0.912\n",
            "training loss at step 7 - batch 759: 0.31 (2019-08-04 13:37:33.297682)\n",
            "Accuracy at step 7 - batch 759: 0.9056\n",
            "training loss at step 7 - batch 760: 0.31 (2019-08-04 13:37:33.310228)\n",
            "Accuracy at step 7 - batch 760: 0.908\n",
            "training loss at step 7 - batch 761: 0.31 (2019-08-04 13:37:33.425221)\n",
            "Accuracy at step 7 - batch 761: 0.908\n",
            "training loss at step 7 - batch 762: 0.31 (2019-08-04 13:37:33.438463)\n",
            "Accuracy at step 7 - batch 762: 0.9\n",
            "training loss at step 7 - batch 763: 0.31 (2019-08-04 13:37:33.451018)\n",
            "Accuracy at step 7 - batch 763: 0.9096\n",
            "training loss at step 7 - batch 764: 0.33 (2019-08-04 13:37:33.465269)\n",
            "Accuracy at step 7 - batch 764: 0.9012\n",
            "training loss at step 7 - batch 765: 0.31 (2019-08-04 13:37:33.477439)\n",
            "Accuracy at step 7 - batch 765: 0.9068\n",
            "training loss at step 7 - batch 766: 0.29 (2019-08-04 13:37:33.594757)\n",
            "Accuracy at step 7 - batch 766: 0.9168\n",
            "training loss at step 7 - batch 767: 0.32 (2019-08-04 13:37:33.612367)\n",
            "Accuracy at step 7 - batch 767: 0.9032\n",
            "training loss at step 7 - batch 768: 0.31 (2019-08-04 13:37:33.626412)\n",
            "Accuracy at step 7 - batch 768: 0.908\n",
            "training loss at step 7 - batch 769: 0.29 (2019-08-04 13:37:33.638648)\n",
            "Accuracy at step 7 - batch 769: 0.9148\n",
            "training loss at step 7 - batch 770: 0.29 (2019-08-04 13:37:33.651203)\n",
            "Accuracy at step 7 - batch 770: 0.9124\n",
            "training loss at step 7 - batch 771: 0.31 (2019-08-04 13:37:33.770316)\n",
            "Accuracy at step 7 - batch 771: 0.9076\n",
            "training loss at step 7 - batch 772: 0.29 (2019-08-04 13:37:33.782472)\n",
            "Accuracy at step 7 - batch 772: 0.9116\n",
            "training loss at step 7 - batch 773: 0.30 (2019-08-04 13:37:33.796067)\n",
            "Accuracy at step 7 - batch 773: 0.9072\n",
            "training loss at step 7 - batch 774: 0.34 (2019-08-04 13:37:33.809412)\n",
            "Accuracy at step 7 - batch 774: 0.8996\n",
            "training loss at step 7 - batch 775: 0.30 (2019-08-04 13:37:33.822611)\n",
            "Accuracy at step 7 - batch 775: 0.9092\n",
            "training loss at step 7 - batch 776: 0.32 (2019-08-04 13:37:33.964564)\n",
            "Accuracy at step 7 - batch 776: 0.908\n",
            "training loss at step 7 - batch 777: 0.30 (2019-08-04 13:37:33.978977)\n",
            "Accuracy at step 7 - batch 777: 0.9148\n",
            "training loss at step 7 - batch 778: 0.32 (2019-08-04 13:37:33.991265)\n",
            "Accuracy at step 7 - batch 778: 0.9092\n",
            "training loss at step 7 - batch 779: 0.30 (2019-08-04 13:37:34.005363)\n",
            "Accuracy at step 7 - batch 779: 0.9076\n",
            "training loss at step 8 - batch 0: 0.31 (2019-08-04 13:37:34.021071)\n",
            "Accuracy at step 8 - batch 0: 0.9032\n",
            "training loss at step 8 - batch 1: 0.30 (2019-08-04 13:37:34.139299)\n",
            "Accuracy at step 8 - batch 1: 0.9096\n",
            "training loss at step 8 - batch 2: 0.30 (2019-08-04 13:37:34.151706)\n",
            "Accuracy at step 8 - batch 2: 0.9084\n",
            "training loss at step 8 - batch 3: 0.33 (2019-08-04 13:37:34.164306)\n",
            "Accuracy at step 8 - batch 3: 0.9048\n",
            "training loss at step 8 - batch 4: 0.32 (2019-08-04 13:37:34.176774)\n",
            "Accuracy at step 8 - batch 4: 0.9048\n",
            "training loss at step 8 - batch 5: 0.31 (2019-08-04 13:37:34.191849)\n",
            "Accuracy at step 8 - batch 5: 0.9096\n",
            "training loss at step 8 - batch 6: 0.28 (2019-08-04 13:37:34.317962)\n",
            "Accuracy at step 8 - batch 6: 0.9128\n",
            "training loss at step 8 - batch 7: 0.28 (2019-08-04 13:37:34.331674)\n",
            "Accuracy at step 8 - batch 7: 0.9144\n",
            "training loss at step 8 - batch 8: 0.33 (2019-08-04 13:37:34.346288)\n",
            "Accuracy at step 8 - batch 8: 0.9012\n",
            "training loss at step 8 - batch 9: 0.31 (2019-08-04 13:37:34.359755)\n",
            "Accuracy at step 8 - batch 9: 0.9112\n",
            "training loss at step 8 - batch 10: 0.33 (2019-08-04 13:37:34.372208)\n",
            "Accuracy at step 8 - batch 10: 0.9016\n",
            "training loss at step 8 - batch 11: 0.33 (2019-08-04 13:37:34.494738)\n",
            "Accuracy at step 8 - batch 11: 0.9048\n",
            "training loss at step 8 - batch 12: 0.32 (2019-08-04 13:37:34.512095)\n",
            "Accuracy at step 8 - batch 12: 0.9036\n",
            "training loss at step 8 - batch 13: 0.30 (2019-08-04 13:37:34.525598)\n",
            "Accuracy at step 8 - batch 13: 0.9084\n",
            "training loss at step 8 - batch 14: 0.31 (2019-08-04 13:37:34.538009)\n",
            "Accuracy at step 8 - batch 14: 0.9056\n",
            "training loss at step 8 - batch 15: 0.33 (2019-08-04 13:37:34.550723)\n",
            "Accuracy at step 8 - batch 15: 0.8976\n",
            "training loss at step 8 - batch 16: 0.31 (2019-08-04 13:37:34.666788)\n",
            "Accuracy at step 8 - batch 16: 0.9112\n",
            "training loss at step 8 - batch 17: 0.29 (2019-08-04 13:37:34.678874)\n",
            "Accuracy at step 8 - batch 17: 0.9148\n",
            "training loss at step 8 - batch 18: 0.31 (2019-08-04 13:37:34.691231)\n",
            "Accuracy at step 8 - batch 18: 0.9096\n",
            "training loss at step 8 - batch 19: 0.31 (2019-08-04 13:37:34.705784)\n",
            "Accuracy at step 8 - batch 19: 0.91\n",
            "training loss at step 8 - batch 20: 0.30 (2019-08-04 13:37:34.718278)\n",
            "Accuracy at step 8 - batch 20: 0.9088\n",
            "training loss at step 8 - batch 21: 0.28 (2019-08-04 13:37:34.838008)\n",
            "Accuracy at step 8 - batch 21: 0.9168\n",
            "training loss at step 8 - batch 22: 0.32 (2019-08-04 13:37:34.855121)\n",
            "Accuracy at step 8 - batch 22: 0.9036\n",
            "training loss at step 8 - batch 23: 0.32 (2019-08-04 13:37:34.867936)\n",
            "Accuracy at step 8 - batch 23: 0.9096\n",
            "training loss at step 8 - batch 24: 0.32 (2019-08-04 13:37:34.880874)\n",
            "Accuracy at step 8 - batch 24: 0.9048\n",
            "training loss at step 8 - batch 25: 0.33 (2019-08-04 13:37:34.892767)\n",
            "Accuracy at step 8 - batch 25: 0.902\n",
            "training loss at step 8 - batch 26: 0.29 (2019-08-04 13:37:35.017908)\n",
            "Accuracy at step 8 - batch 26: 0.9104\n",
            "training loss at step 8 - batch 27: 0.31 (2019-08-04 13:37:35.031120)\n",
            "Accuracy at step 8 - batch 27: 0.9012\n",
            "training loss at step 8 - batch 28: 0.32 (2019-08-04 13:37:35.046749)\n",
            "Accuracy at step 8 - batch 28: 0.9072\n",
            "training loss at step 8 - batch 29: 0.30 (2019-08-04 13:37:35.059290)\n",
            "Accuracy at step 8 - batch 29: 0.91\n",
            "training loss at step 8 - batch 30: 0.31 (2019-08-04 13:37:35.072200)\n",
            "Accuracy at step 8 - batch 30: 0.906\n",
            "training loss at step 8 - batch 31: 0.28 (2019-08-04 13:37:35.195095)\n",
            "Accuracy at step 8 - batch 31: 0.9136\n",
            "training loss at step 8 - batch 32: 0.31 (2019-08-04 13:37:35.211419)\n",
            "Accuracy at step 8 - batch 32: 0.908\n",
            "training loss at step 8 - batch 33: 0.35 (2019-08-04 13:37:35.225404)\n",
            "Accuracy at step 8 - batch 33: 0.8952\n",
            "training loss at step 8 - batch 34: 0.29 (2019-08-04 13:37:35.238661)\n",
            "Accuracy at step 8 - batch 34: 0.9116\n",
            "training loss at step 8 - batch 35: 0.30 (2019-08-04 13:37:35.256956)\n",
            "Accuracy at step 8 - batch 35: 0.9056\n",
            "training loss at step 8 - batch 36: 0.31 (2019-08-04 13:37:35.383195)\n",
            "Accuracy at step 8 - batch 36: 0.9064\n",
            "training loss at step 8 - batch 37: 0.32 (2019-08-04 13:37:35.395822)\n",
            "Accuracy at step 8 - batch 37: 0.9016\n",
            "training loss at step 8 - batch 38: 0.31 (2019-08-04 13:37:35.409427)\n",
            "Accuracy at step 8 - batch 38: 0.9096\n",
            "training loss at step 8 - batch 39: 0.31 (2019-08-04 13:37:35.421958)\n",
            "Accuracy at step 8 - batch 39: 0.908\n",
            "training loss at step 8 - batch 40: 0.33 (2019-08-04 13:37:35.438163)\n",
            "Accuracy at step 8 - batch 40: 0.8984\n",
            "training loss at step 8 - batch 41: 0.31 (2019-08-04 13:37:35.558783)\n",
            "Accuracy at step 8 - batch 41: 0.9072\n",
            "training loss at step 8 - batch 42: 0.31 (2019-08-04 13:37:35.573698)\n",
            "Accuracy at step 8 - batch 42: 0.9084\n",
            "training loss at step 8 - batch 43: 0.31 (2019-08-04 13:37:35.586211)\n",
            "Accuracy at step 8 - batch 43: 0.9124\n",
            "training loss at step 8 - batch 44: 0.30 (2019-08-04 13:37:35.599195)\n",
            "Accuracy at step 8 - batch 44: 0.9104\n",
            "training loss at step 8 - batch 45: 0.33 (2019-08-04 13:37:35.611573)\n",
            "Accuracy at step 8 - batch 45: 0.9028\n",
            "training loss at step 8 - batch 46: 0.33 (2019-08-04 13:37:35.731832)\n",
            "Accuracy at step 8 - batch 46: 0.9048\n",
            "training loss at step 8 - batch 47: 0.29 (2019-08-04 13:37:35.744058)\n",
            "Accuracy at step 8 - batch 47: 0.9104\n",
            "training loss at step 8 - batch 48: 0.28 (2019-08-04 13:37:35.757195)\n",
            "Accuracy at step 8 - batch 48: 0.9124\n",
            "training loss at step 8 - batch 49: 0.31 (2019-08-04 13:37:35.770685)\n",
            "Accuracy at step 8 - batch 49: 0.9108\n",
            "training loss at step 8 - batch 50: 0.30 (2019-08-04 13:37:35.783168)\n",
            "Accuracy at step 8 - batch 50: 0.9116\n",
            "training loss at step 8 - batch 51: 0.31 (2019-08-04 13:37:35.899153)\n",
            "Accuracy at step 8 - batch 51: 0.8964\n",
            "training loss at step 8 - batch 52: 0.32 (2019-08-04 13:37:35.911178)\n",
            "Accuracy at step 8 - batch 52: 0.9036\n",
            "training loss at step 8 - batch 53: 0.33 (2019-08-04 13:37:35.924189)\n",
            "Accuracy at step 8 - batch 53: 0.9032\n",
            "training loss at step 8 - batch 54: 0.29 (2019-08-04 13:37:35.938576)\n",
            "Accuracy at step 8 - batch 54: 0.9088\n",
            "training loss at step 8 - batch 55: 0.31 (2019-08-04 13:37:35.951234)\n",
            "Accuracy at step 8 - batch 55: 0.91\n",
            "training loss at step 8 - batch 56: 0.31 (2019-08-04 13:37:36.074535)\n",
            "Accuracy at step 8 - batch 56: 0.9048\n",
            "training loss at step 8 - batch 57: 0.30 (2019-08-04 13:37:36.092303)\n",
            "Accuracy at step 8 - batch 57: 0.9084\n",
            "training loss at step 8 - batch 58: 0.31 (2019-08-04 13:37:36.105333)\n",
            "Accuracy at step 8 - batch 58: 0.9056\n",
            "training loss at step 8 - batch 59: 0.31 (2019-08-04 13:37:36.118302)\n",
            "Accuracy at step 8 - batch 59: 0.9072\n",
            "training loss at step 8 - batch 60: 0.30 (2019-08-04 13:37:36.131284)\n",
            "Accuracy at step 8 - batch 60: 0.912\n",
            "training loss at step 8 - batch 61: 0.32 (2019-08-04 13:37:36.256052)\n",
            "Accuracy at step 8 - batch 61: 0.9028\n",
            "training loss at step 8 - batch 62: 0.32 (2019-08-04 13:37:36.276454)\n",
            "Accuracy at step 8 - batch 62: 0.9028\n",
            "training loss at step 8 - batch 63: 0.30 (2019-08-04 13:37:36.290001)\n",
            "Accuracy at step 8 - batch 63: 0.9076\n",
            "training loss at step 8 - batch 64: 0.34 (2019-08-04 13:37:36.302343)\n",
            "Accuracy at step 8 - batch 64: 0.8976\n",
            "training loss at step 8 - batch 65: 0.32 (2019-08-04 13:37:36.315124)\n",
            "Accuracy at step 8 - batch 65: 0.9036\n",
            "training loss at step 8 - batch 66: 0.34 (2019-08-04 13:37:36.438748)\n",
            "Accuracy at step 8 - batch 66: 0.8948\n",
            "training loss at step 8 - batch 67: 0.32 (2019-08-04 13:37:36.456496)\n",
            "Accuracy at step 8 - batch 67: 0.9056\n",
            "training loss at step 8 - batch 68: 0.32 (2019-08-04 13:37:36.471310)\n",
            "Accuracy at step 8 - batch 68: 0.9056\n",
            "training loss at step 8 - batch 69: 0.33 (2019-08-04 13:37:36.484445)\n",
            "Accuracy at step 8 - batch 69: 0.9016\n",
            "training loss at step 8 - batch 70: 0.30 (2019-08-04 13:37:36.497441)\n",
            "Accuracy at step 8 - batch 70: 0.9044\n",
            "training loss at step 8 - batch 71: 0.29 (2019-08-04 13:37:36.631008)\n",
            "Accuracy at step 8 - batch 71: 0.9104\n",
            "training loss at step 8 - batch 72: 0.30 (2019-08-04 13:37:36.644957)\n",
            "Accuracy at step 8 - batch 72: 0.9084\n",
            "training loss at step 8 - batch 73: 0.28 (2019-08-04 13:37:36.658227)\n",
            "Accuracy at step 8 - batch 73: 0.916\n",
            "training loss at step 8 - batch 74: 0.32 (2019-08-04 13:37:36.674155)\n",
            "Accuracy at step 8 - batch 74: 0.902\n",
            "training loss at step 8 - batch 75: 0.30 (2019-08-04 13:37:36.687219)\n",
            "Accuracy at step 8 - batch 75: 0.9132\n",
            "training loss at step 8 - batch 76: 0.31 (2019-08-04 13:37:36.806361)\n",
            "Accuracy at step 8 - batch 76: 0.904\n",
            "training loss at step 8 - batch 77: 0.32 (2019-08-04 13:37:36.819728)\n",
            "Accuracy at step 8 - batch 77: 0.9068\n",
            "training loss at step 8 - batch 78: 0.33 (2019-08-04 13:37:36.833140)\n",
            "Accuracy at step 8 - batch 78: 0.9028\n",
            "training loss at step 8 - batch 79: 0.31 (2019-08-04 13:37:36.847147)\n",
            "Accuracy at step 8 - batch 79: 0.908\n",
            "training loss at step 8 - batch 80: 0.32 (2019-08-04 13:37:36.859041)\n",
            "Accuracy at step 8 - batch 80: 0.9036\n",
            "training loss at step 8 - batch 81: 0.32 (2019-08-04 13:37:36.984313)\n",
            "Accuracy at step 8 - batch 81: 0.9044\n",
            "training loss at step 8 - batch 82: 0.32 (2019-08-04 13:37:36.997235)\n",
            "Accuracy at step 8 - batch 82: 0.902\n",
            "training loss at step 8 - batch 83: 0.31 (2019-08-04 13:37:37.011381)\n",
            "Accuracy at step 8 - batch 83: 0.9056\n",
            "training loss at step 8 - batch 84: 0.30 (2019-08-04 13:37:37.023394)\n",
            "Accuracy at step 8 - batch 84: 0.9096\n",
            "training loss at step 8 - batch 85: 0.32 (2019-08-04 13:37:37.035609)\n",
            "Accuracy at step 8 - batch 85: 0.9064\n",
            "training loss at step 8 - batch 86: 0.29 (2019-08-04 13:37:37.156624)\n",
            "Accuracy at step 8 - batch 86: 0.912\n",
            "training loss at step 8 - batch 87: 0.31 (2019-08-04 13:37:37.174760)\n",
            "Accuracy at step 8 - batch 87: 0.9016\n",
            "training loss at step 8 - batch 88: 0.32 (2019-08-04 13:37:37.190111)\n",
            "Accuracy at step 8 - batch 88: 0.9088\n",
            "training loss at step 8 - batch 89: 0.33 (2019-08-04 13:37:37.204354)\n",
            "Accuracy at step 8 - batch 89: 0.9016\n",
            "training loss at step 8 - batch 90: 0.32 (2019-08-04 13:37:37.216728)\n",
            "Accuracy at step 8 - batch 90: 0.906\n",
            "training loss at step 8 - batch 91: 0.31 (2019-08-04 13:37:37.345405)\n",
            "Accuracy at step 8 - batch 91: 0.9104\n",
            "training loss at step 8 - batch 92: 0.31 (2019-08-04 13:37:37.359395)\n",
            "Accuracy at step 8 - batch 92: 0.9072\n",
            "training loss at step 8 - batch 93: 0.28 (2019-08-04 13:37:37.373906)\n",
            "Accuracy at step 8 - batch 93: 0.9168\n",
            "training loss at step 8 - batch 94: 0.30 (2019-08-04 13:37:37.387259)\n",
            "Accuracy at step 8 - batch 94: 0.9108\n",
            "training loss at step 8 - batch 95: 0.30 (2019-08-04 13:37:37.402360)\n",
            "Accuracy at step 8 - batch 95: 0.906\n",
            "training loss at step 8 - batch 96: 0.34 (2019-08-04 13:37:37.521499)\n",
            "Accuracy at step 8 - batch 96: 0.9008\n",
            "training loss at step 8 - batch 97: 0.31 (2019-08-04 13:37:37.538595)\n",
            "Accuracy at step 8 - batch 97: 0.904\n",
            "training loss at step 8 - batch 98: 0.32 (2019-08-04 13:37:37.551645)\n",
            "Accuracy at step 8 - batch 98: 0.9044\n",
            "training loss at step 8 - batch 99: 0.31 (2019-08-04 13:37:37.564176)\n",
            "Accuracy at step 8 - batch 99: 0.908\n",
            "training loss at step 8 - batch 100: 0.29 (2019-08-04 13:37:37.576260)\n",
            "Accuracy at step 8 - batch 100: 0.916\n",
            "training loss at step 8 - batch 101: 0.34 (2019-08-04 13:37:37.696929)\n",
            "Accuracy at step 8 - batch 101: 0.898\n",
            "training loss at step 8 - batch 102: 0.32 (2019-08-04 13:37:37.710743)\n",
            "Accuracy at step 8 - batch 102: 0.9032\n",
            "training loss at step 8 - batch 103: 0.30 (2019-08-04 13:37:37.725092)\n",
            "Accuracy at step 8 - batch 103: 0.9108\n",
            "training loss at step 8 - batch 104: 0.32 (2019-08-04 13:37:37.737798)\n",
            "Accuracy at step 8 - batch 104: 0.9072\n",
            "training loss at step 8 - batch 105: 0.31 (2019-08-04 13:37:37.752168)\n",
            "Accuracy at step 8 - batch 105: 0.9096\n",
            "training loss at step 8 - batch 106: 0.32 (2019-08-04 13:37:37.867565)\n",
            "Accuracy at step 8 - batch 106: 0.9016\n",
            "training loss at step 8 - batch 107: 0.29 (2019-08-04 13:37:37.880298)\n",
            "Accuracy at step 8 - batch 107: 0.91\n",
            "training loss at step 8 - batch 108: 0.31 (2019-08-04 13:37:37.893590)\n",
            "Accuracy at step 8 - batch 108: 0.91\n",
            "training loss at step 8 - batch 109: 0.31 (2019-08-04 13:37:37.908241)\n",
            "Accuracy at step 8 - batch 109: 0.9064\n",
            "training loss at step 8 - batch 110: 0.29 (2019-08-04 13:37:37.921888)\n",
            "Accuracy at step 8 - batch 110: 0.9068\n",
            "training loss at step 8 - batch 111: 0.33 (2019-08-04 13:37:38.047225)\n",
            "Accuracy at step 8 - batch 111: 0.9012\n",
            "training loss at step 8 - batch 112: 0.32 (2019-08-04 13:37:38.063912)\n",
            "Accuracy at step 8 - batch 112: 0.9084\n",
            "training loss at step 8 - batch 113: 0.31 (2019-08-04 13:37:38.076280)\n",
            "Accuracy at step 8 - batch 113: 0.91\n",
            "training loss at step 8 - batch 114: 0.31 (2019-08-04 13:37:38.089477)\n",
            "Accuracy at step 8 - batch 114: 0.9048\n",
            "training loss at step 8 - batch 115: 0.30 (2019-08-04 13:37:38.102113)\n",
            "Accuracy at step 8 - batch 115: 0.9068\n",
            "training loss at step 8 - batch 116: 0.30 (2019-08-04 13:37:38.226788)\n",
            "Accuracy at step 8 - batch 116: 0.9088\n",
            "training loss at step 8 - batch 117: 0.30 (2019-08-04 13:37:38.240126)\n",
            "Accuracy at step 8 - batch 117: 0.914\n",
            "training loss at step 8 - batch 118: 0.32 (2019-08-04 13:37:38.253271)\n",
            "Accuracy at step 8 - batch 118: 0.904\n",
            "training loss at step 8 - batch 119: 0.31 (2019-08-04 13:37:38.266340)\n",
            "Accuracy at step 8 - batch 119: 0.9132\n",
            "training loss at step 8 - batch 120: 0.30 (2019-08-04 13:37:38.279308)\n",
            "Accuracy at step 8 - batch 120: 0.9108\n",
            "training loss at step 8 - batch 121: 0.31 (2019-08-04 13:37:38.407125)\n",
            "Accuracy at step 8 - batch 121: 0.9068\n",
            "training loss at step 8 - batch 122: 0.31 (2019-08-04 13:37:38.421999)\n",
            "Accuracy at step 8 - batch 122: 0.9064\n",
            "training loss at step 8 - batch 123: 0.29 (2019-08-04 13:37:38.436924)\n",
            "Accuracy at step 8 - batch 123: 0.9108\n",
            "training loss at step 8 - batch 124: 0.32 (2019-08-04 13:37:38.450127)\n",
            "Accuracy at step 8 - batch 124: 0.9084\n",
            "training loss at step 8 - batch 125: 0.31 (2019-08-04 13:37:38.462929)\n",
            "Accuracy at step 8 - batch 125: 0.9084\n",
            "training loss at step 8 - batch 126: 0.32 (2019-08-04 13:37:38.579086)\n",
            "Accuracy at step 8 - batch 126: 0.8976\n",
            "training loss at step 8 - batch 127: 0.28 (2019-08-04 13:37:38.592781)\n",
            "Accuracy at step 8 - batch 127: 0.916\n",
            "training loss at step 8 - batch 128: 0.31 (2019-08-04 13:37:38.608120)\n",
            "Accuracy at step 8 - batch 128: 0.904\n",
            "training loss at step 8 - batch 129: 0.30 (2019-08-04 13:37:38.621704)\n",
            "Accuracy at step 8 - batch 129: 0.9136\n",
            "training loss at step 8 - batch 130: 0.33 (2019-08-04 13:37:38.633880)\n",
            "Accuracy at step 8 - batch 130: 0.8996\n",
            "training loss at step 8 - batch 131: 0.30 (2019-08-04 13:37:38.756202)\n",
            "Accuracy at step 8 - batch 131: 0.9072\n",
            "training loss at step 8 - batch 132: 0.29 (2019-08-04 13:37:38.768772)\n",
            "Accuracy at step 8 - batch 132: 0.9104\n",
            "training loss at step 8 - batch 133: 0.32 (2019-08-04 13:37:38.783092)\n",
            "Accuracy at step 8 - batch 133: 0.9124\n",
            "training loss at step 8 - batch 134: 0.31 (2019-08-04 13:37:38.796766)\n",
            "Accuracy at step 8 - batch 134: 0.9044\n",
            "training loss at step 8 - batch 135: 0.32 (2019-08-04 13:37:38.809086)\n",
            "Accuracy at step 8 - batch 135: 0.9056\n",
            "training loss at step 8 - batch 136: 0.33 (2019-08-04 13:37:38.933503)\n",
            "Accuracy at step 8 - batch 136: 0.9036\n",
            "training loss at step 8 - batch 137: 0.31 (2019-08-04 13:37:38.950771)\n",
            "Accuracy at step 8 - batch 137: 0.9016\n",
            "training loss at step 8 - batch 138: 0.31 (2019-08-04 13:37:38.966194)\n",
            "Accuracy at step 8 - batch 138: 0.9116\n",
            "training loss at step 8 - batch 139: 0.36 (2019-08-04 13:37:38.979176)\n",
            "Accuracy at step 8 - batch 139: 0.8932\n",
            "training loss at step 8 - batch 140: 0.30 (2019-08-04 13:37:38.991311)\n",
            "Accuracy at step 8 - batch 140: 0.9088\n",
            "training loss at step 8 - batch 141: 0.29 (2019-08-04 13:37:39.110475)\n",
            "Accuracy at step 8 - batch 141: 0.9068\n",
            "training loss at step 8 - batch 142: 0.31 (2019-08-04 13:37:39.124366)\n",
            "Accuracy at step 8 - batch 142: 0.9092\n",
            "training loss at step 8 - batch 143: 0.34 (2019-08-04 13:37:39.136726)\n",
            "Accuracy at step 8 - batch 143: 0.8976\n",
            "training loss at step 8 - batch 144: 0.31 (2019-08-04 13:37:39.148694)\n",
            "Accuracy at step 8 - batch 144: 0.9064\n",
            "training loss at step 8 - batch 145: 0.29 (2019-08-04 13:37:39.160955)\n",
            "Accuracy at step 8 - batch 145: 0.9076\n",
            "training loss at step 8 - batch 146: 0.31 (2019-08-04 13:37:39.288118)\n",
            "Accuracy at step 8 - batch 146: 0.9056\n",
            "training loss at step 8 - batch 147: 0.31 (2019-08-04 13:37:39.302554)\n",
            "Accuracy at step 8 - batch 147: 0.904\n",
            "training loss at step 8 - batch 148: 0.31 (2019-08-04 13:37:39.316108)\n",
            "Accuracy at step 8 - batch 148: 0.9044\n",
            "training loss at step 8 - batch 149: 0.31 (2019-08-04 13:37:39.329771)\n",
            "Accuracy at step 8 - batch 149: 0.908\n",
            "training loss at step 8 - batch 150: 0.30 (2019-08-04 13:37:39.347413)\n",
            "Accuracy at step 8 - batch 150: 0.9132\n",
            "training loss at step 8 - batch 151: 0.29 (2019-08-04 13:37:39.470412)\n",
            "Accuracy at step 8 - batch 151: 0.9092\n",
            "training loss at step 8 - batch 152: 0.31 (2019-08-04 13:37:39.487234)\n",
            "Accuracy at step 8 - batch 152: 0.9072\n",
            "training loss at step 8 - batch 153: 0.33 (2019-08-04 13:37:39.503554)\n",
            "Accuracy at step 8 - batch 153: 0.9036\n",
            "training loss at step 8 - batch 154: 0.33 (2019-08-04 13:37:39.516184)\n",
            "Accuracy at step 8 - batch 154: 0.902\n",
            "training loss at step 8 - batch 155: 0.31 (2019-08-04 13:37:39.528676)\n",
            "Accuracy at step 8 - batch 155: 0.9048\n",
            "training loss at step 8 - batch 156: 0.30 (2019-08-04 13:37:39.649283)\n",
            "Accuracy at step 8 - batch 156: 0.9116\n",
            "training loss at step 8 - batch 157: 0.30 (2019-08-04 13:37:39.666612)\n",
            "Accuracy at step 8 - batch 157: 0.9064\n",
            "training loss at step 8 - batch 158: 0.32 (2019-08-04 13:37:39.679320)\n",
            "Accuracy at step 8 - batch 158: 0.9048\n",
            "training loss at step 8 - batch 159: 0.30 (2019-08-04 13:37:39.692393)\n",
            "Accuracy at step 8 - batch 159: 0.9136\n",
            "training loss at step 8 - batch 160: 0.32 (2019-08-04 13:37:39.708993)\n",
            "Accuracy at step 8 - batch 160: 0.9028\n",
            "training loss at step 8 - batch 161: 0.31 (2019-08-04 13:37:39.831953)\n",
            "Accuracy at step 8 - batch 161: 0.9052\n",
            "training loss at step 8 - batch 162: 0.30 (2019-08-04 13:37:39.845997)\n",
            "Accuracy at step 8 - batch 162: 0.9088\n",
            "training loss at step 8 - batch 163: 0.32 (2019-08-04 13:37:39.858723)\n",
            "Accuracy at step 8 - batch 163: 0.9068\n",
            "training loss at step 8 - batch 164: 0.30 (2019-08-04 13:37:39.872569)\n",
            "Accuracy at step 8 - batch 164: 0.9064\n",
            "training loss at step 8 - batch 165: 0.29 (2019-08-04 13:37:39.885944)\n",
            "Accuracy at step 8 - batch 165: 0.9124\n",
            "training loss at step 8 - batch 166: 0.32 (2019-08-04 13:37:40.006837)\n",
            "Accuracy at step 8 - batch 166: 0.9024\n",
            "training loss at step 8 - batch 167: 0.32 (2019-08-04 13:37:40.020294)\n",
            "Accuracy at step 8 - batch 167: 0.9064\n",
            "training loss at step 8 - batch 168: 0.32 (2019-08-04 13:37:40.032918)\n",
            "Accuracy at step 8 - batch 168: 0.9012\n",
            "training loss at step 8 - batch 169: 0.33 (2019-08-04 13:37:40.045920)\n",
            "Accuracy at step 8 - batch 169: 0.902\n",
            "training loss at step 8 - batch 170: 0.31 (2019-08-04 13:37:40.058571)\n",
            "Accuracy at step 8 - batch 170: 0.9092\n",
            "training loss at step 8 - batch 171: 0.33 (2019-08-04 13:37:40.177641)\n",
            "Accuracy at step 8 - batch 171: 0.9064\n",
            "training loss at step 8 - batch 172: 0.30 (2019-08-04 13:37:40.192499)\n",
            "Accuracy at step 8 - batch 172: 0.906\n",
            "training loss at step 8 - batch 173: 0.30 (2019-08-04 13:37:40.204401)\n",
            "Accuracy at step 8 - batch 173: 0.9128\n",
            "training loss at step 8 - batch 174: 0.32 (2019-08-04 13:37:40.220079)\n",
            "Accuracy at step 8 - batch 174: 0.9028\n",
            "training loss at step 8 - batch 175: 0.30 (2019-08-04 13:37:40.232625)\n",
            "Accuracy at step 8 - batch 175: 0.9112\n",
            "training loss at step 8 - batch 176: 0.31 (2019-08-04 13:37:40.356628)\n",
            "Accuracy at step 8 - batch 176: 0.9068\n",
            "training loss at step 8 - batch 177: 0.29 (2019-08-04 13:37:40.371529)\n",
            "Accuracy at step 8 - batch 177: 0.916\n",
            "training loss at step 8 - batch 178: 0.30 (2019-08-04 13:37:40.384905)\n",
            "Accuracy at step 8 - batch 178: 0.9132\n",
            "training loss at step 8 - batch 179: 0.32 (2019-08-04 13:37:40.397785)\n",
            "Accuracy at step 8 - batch 179: 0.902\n",
            "training loss at step 8 - batch 180: 0.30 (2019-08-04 13:37:40.410259)\n",
            "Accuracy at step 8 - batch 180: 0.9076\n",
            "training loss at step 8 - batch 181: 0.30 (2019-08-04 13:37:40.541538)\n",
            "Accuracy at step 8 - batch 181: 0.9104\n",
            "training loss at step 8 - batch 182: 0.30 (2019-08-04 13:37:40.555005)\n",
            "Accuracy at step 8 - batch 182: 0.9056\n",
            "training loss at step 8 - batch 183: 0.32 (2019-08-04 13:37:40.568422)\n",
            "Accuracy at step 8 - batch 183: 0.904\n",
            "training loss at step 8 - batch 184: 0.31 (2019-08-04 13:37:40.580975)\n",
            "Accuracy at step 8 - batch 184: 0.9072\n",
            "training loss at step 8 - batch 185: 0.32 (2019-08-04 13:37:40.593617)\n",
            "Accuracy at step 8 - batch 185: 0.9052\n",
            "training loss at step 8 - batch 186: 0.32 (2019-08-04 13:37:40.710553)\n",
            "Accuracy at step 8 - batch 186: 0.9012\n",
            "training loss at step 8 - batch 187: 0.30 (2019-08-04 13:37:40.722759)\n",
            "Accuracy at step 8 - batch 187: 0.912\n",
            "training loss at step 8 - batch 188: 0.30 (2019-08-04 13:37:40.735464)\n",
            "Accuracy at step 8 - batch 188: 0.912\n",
            "training loss at step 8 - batch 189: 0.32 (2019-08-04 13:37:40.750708)\n",
            "Accuracy at step 8 - batch 189: 0.9\n",
            "training loss at step 8 - batch 190: 0.32 (2019-08-04 13:37:40.763571)\n",
            "Accuracy at step 8 - batch 190: 0.9052\n",
            "training loss at step 8 - batch 191: 0.34 (2019-08-04 13:37:40.887787)\n",
            "Accuracy at step 8 - batch 191: 0.9024\n",
            "training loss at step 8 - batch 192: 0.33 (2019-08-04 13:37:40.901724)\n",
            "Accuracy at step 8 - batch 192: 0.9008\n",
            "training loss at step 8 - batch 193: 0.32 (2019-08-04 13:37:40.914374)\n",
            "Accuracy at step 8 - batch 193: 0.9072\n",
            "training loss at step 8 - batch 194: 0.30 (2019-08-04 13:37:40.926953)\n",
            "Accuracy at step 8 - batch 194: 0.9084\n",
            "training loss at step 8 - batch 195: 0.32 (2019-08-04 13:37:40.939237)\n",
            "Accuracy at step 8 - batch 195: 0.9068\n",
            "training loss at step 8 - batch 196: 0.31 (2019-08-04 13:37:41.065267)\n",
            "Accuracy at step 8 - batch 196: 0.904\n",
            "training loss at step 8 - batch 197: 0.33 (2019-08-04 13:37:41.081182)\n",
            "Accuracy at step 8 - batch 197: 0.8996\n",
            "training loss at step 8 - batch 198: 0.30 (2019-08-04 13:37:41.093641)\n",
            "Accuracy at step 8 - batch 198: 0.9128\n",
            "training loss at step 8 - batch 199: 0.30 (2019-08-04 13:37:41.106294)\n",
            "Accuracy at step 8 - batch 199: 0.9096\n",
            "training loss at step 8 - batch 200: 0.30 (2019-08-04 13:37:41.119214)\n",
            "Accuracy at step 8 - batch 200: 0.9128\n",
            "training loss at step 8 - batch 201: 0.31 (2019-08-04 13:37:41.240244)\n",
            "Accuracy at step 8 - batch 201: 0.9068\n",
            "training loss at step 8 - batch 202: 0.33 (2019-08-04 13:37:41.252720)\n",
            "Accuracy at step 8 - batch 202: 0.8992\n",
            "training loss at step 8 - batch 203: 0.32 (2019-08-04 13:37:41.265304)\n",
            "Accuracy at step 8 - batch 203: 0.9064\n",
            "training loss at step 8 - batch 204: 0.34 (2019-08-04 13:37:41.282815)\n",
            "Accuracy at step 8 - batch 204: 0.8996\n",
            "training loss at step 8 - batch 205: 0.32 (2019-08-04 13:37:41.294702)\n",
            "Accuracy at step 8 - batch 205: 0.9032\n",
            "training loss at step 8 - batch 206: 0.33 (2019-08-04 13:37:41.422662)\n",
            "Accuracy at step 8 - batch 206: 0.8992\n",
            "training loss at step 8 - batch 207: 0.31 (2019-08-04 13:37:41.437114)\n",
            "Accuracy at step 8 - batch 207: 0.9072\n",
            "training loss at step 8 - batch 208: 0.31 (2019-08-04 13:37:41.451387)\n",
            "Accuracy at step 8 - batch 208: 0.9072\n",
            "training loss at step 8 - batch 209: 0.31 (2019-08-04 13:37:41.464235)\n",
            "Accuracy at step 8 - batch 209: 0.9056\n",
            "training loss at step 8 - batch 210: 0.29 (2019-08-04 13:37:41.479579)\n",
            "Accuracy at step 8 - batch 210: 0.9116\n",
            "training loss at step 8 - batch 211: 0.31 (2019-08-04 13:37:41.613792)\n",
            "Accuracy at step 8 - batch 211: 0.9052\n",
            "training loss at step 8 - batch 212: 0.33 (2019-08-04 13:37:41.627322)\n",
            "Accuracy at step 8 - batch 212: 0.9036\n",
            "training loss at step 8 - batch 213: 0.29 (2019-08-04 13:37:41.641027)\n",
            "Accuracy at step 8 - batch 213: 0.9128\n",
            "training loss at step 8 - batch 214: 0.30 (2019-08-04 13:37:41.654625)\n",
            "Accuracy at step 8 - batch 214: 0.9132\n",
            "training loss at step 8 - batch 215: 0.29 (2019-08-04 13:37:41.667790)\n",
            "Accuracy at step 8 - batch 215: 0.9116\n",
            "training loss at step 8 - batch 216: 0.30 (2019-08-04 13:37:41.796567)\n",
            "Accuracy at step 8 - batch 216: 0.9128\n",
            "training loss at step 8 - batch 217: 0.30 (2019-08-04 13:37:41.811175)\n",
            "Accuracy at step 8 - batch 217: 0.9076\n",
            "training loss at step 8 - batch 218: 0.31 (2019-08-04 13:37:41.824008)\n",
            "Accuracy at step 8 - batch 218: 0.906\n",
            "training loss at step 8 - batch 219: 0.30 (2019-08-04 13:37:41.836870)\n",
            "Accuracy at step 8 - batch 219: 0.9112\n",
            "training loss at step 8 - batch 220: 0.30 (2019-08-04 13:37:41.850498)\n",
            "Accuracy at step 8 - batch 220: 0.9084\n",
            "training loss at step 8 - batch 221: 0.30 (2019-08-04 13:37:41.969034)\n",
            "Accuracy at step 8 - batch 221: 0.9104\n",
            "training loss at step 8 - batch 222: 0.31 (2019-08-04 13:37:41.986689)\n",
            "Accuracy at step 8 - batch 222: 0.9064\n",
            "training loss at step 8 - batch 223: 0.30 (2019-08-04 13:37:42.000937)\n",
            "Accuracy at step 8 - batch 223: 0.9036\n",
            "training loss at step 8 - batch 224: 0.32 (2019-08-04 13:37:42.015362)\n",
            "Accuracy at step 8 - batch 224: 0.906\n",
            "training loss at step 8 - batch 225: 0.30 (2019-08-04 13:37:42.029587)\n",
            "Accuracy at step 8 - batch 225: 0.9104\n",
            "training loss at step 8 - batch 226: 0.32 (2019-08-04 13:37:42.155967)\n",
            "Accuracy at step 8 - batch 226: 0.9\n",
            "training loss at step 8 - batch 227: 0.32 (2019-08-04 13:37:42.167960)\n",
            "Accuracy at step 8 - batch 227: 0.9068\n",
            "training loss at step 8 - batch 228: 0.31 (2019-08-04 13:37:42.181253)\n",
            "Accuracy at step 8 - batch 228: 0.9072\n",
            "training loss at step 8 - batch 229: 0.33 (2019-08-04 13:37:42.194422)\n",
            "Accuracy at step 8 - batch 229: 0.9064\n",
            "training loss at step 8 - batch 230: 0.31 (2019-08-04 13:37:42.208963)\n",
            "Accuracy at step 8 - batch 230: 0.9072\n",
            "training loss at step 8 - batch 231: 0.32 (2019-08-04 13:37:42.332596)\n",
            "Accuracy at step 8 - batch 231: 0.9056\n",
            "training loss at step 8 - batch 232: 0.33 (2019-08-04 13:37:42.347493)\n",
            "Accuracy at step 8 - batch 232: 0.9024\n",
            "training loss at step 8 - batch 233: 0.32 (2019-08-04 13:37:42.359658)\n",
            "Accuracy at step 8 - batch 233: 0.9044\n",
            "training loss at step 8 - batch 234: 0.30 (2019-08-04 13:37:42.372261)\n",
            "Accuracy at step 8 - batch 234: 0.9144\n",
            "training loss at step 8 - batch 235: 0.33 (2019-08-04 13:37:42.385247)\n",
            "Accuracy at step 8 - batch 235: 0.9048\n",
            "training loss at step 8 - batch 236: 0.32 (2019-08-04 13:37:42.517908)\n",
            "Accuracy at step 8 - batch 236: 0.904\n",
            "training loss at step 8 - batch 237: 0.31 (2019-08-04 13:37:42.531587)\n",
            "Accuracy at step 8 - batch 237: 0.9116\n",
            "training loss at step 8 - batch 238: 0.30 (2019-08-04 13:37:42.544256)\n",
            "Accuracy at step 8 - batch 238: 0.914\n",
            "training loss at step 8 - batch 239: 0.32 (2019-08-04 13:37:42.556597)\n",
            "Accuracy at step 8 - batch 239: 0.9036\n",
            "training loss at step 8 - batch 240: 0.30 (2019-08-04 13:37:42.569367)\n",
            "Accuracy at step 8 - batch 240: 0.9036\n",
            "training loss at step 8 - batch 241: 0.32 (2019-08-04 13:37:42.688890)\n",
            "Accuracy at step 8 - batch 241: 0.9044\n",
            "training loss at step 8 - batch 242: 0.29 (2019-08-04 13:37:42.700704)\n",
            "Accuracy at step 8 - batch 242: 0.9068\n",
            "training loss at step 8 - batch 243: 0.28 (2019-08-04 13:37:42.713305)\n",
            "Accuracy at step 8 - batch 243: 0.9132\n",
            "training loss at step 8 - batch 244: 0.29 (2019-08-04 13:37:42.728851)\n",
            "Accuracy at step 8 - batch 244: 0.914\n",
            "training loss at step 8 - batch 245: 0.32 (2019-08-04 13:37:42.741442)\n",
            "Accuracy at step 8 - batch 245: 0.9056\n",
            "training loss at step 8 - batch 246: 0.31 (2019-08-04 13:37:42.860396)\n",
            "Accuracy at step 8 - batch 246: 0.9092\n",
            "training loss at step 8 - batch 247: 0.31 (2019-08-04 13:37:42.874696)\n",
            "Accuracy at step 8 - batch 247: 0.908\n",
            "training loss at step 8 - batch 248: 0.30 (2019-08-04 13:37:42.889373)\n",
            "Accuracy at step 8 - batch 248: 0.9084\n",
            "training loss at step 8 - batch 249: 0.31 (2019-08-04 13:37:42.901623)\n",
            "Accuracy at step 8 - batch 249: 0.9044\n",
            "training loss at step 8 - batch 250: 0.32 (2019-08-04 13:37:42.913390)\n",
            "Accuracy at step 8 - batch 250: 0.9032\n",
            "training loss at step 8 - batch 251: 0.33 (2019-08-04 13:37:43.036256)\n",
            "Accuracy at step 8 - batch 251: 0.9072\n",
            "training loss at step 8 - batch 252: 0.33 (2019-08-04 13:37:43.050228)\n",
            "Accuracy at step 8 - batch 252: 0.904\n",
            "training loss at step 8 - batch 253: 0.29 (2019-08-04 13:37:43.064544)\n",
            "Accuracy at step 8 - batch 253: 0.9156\n",
            "training loss at step 8 - batch 254: 0.29 (2019-08-04 13:37:43.076719)\n",
            "Accuracy at step 8 - batch 254: 0.9124\n",
            "training loss at step 8 - batch 255: 0.32 (2019-08-04 13:37:43.089320)\n",
            "Accuracy at step 8 - batch 255: 0.9064\n",
            "training loss at step 8 - batch 256: 0.31 (2019-08-04 13:37:43.210178)\n",
            "Accuracy at step 8 - batch 256: 0.9052\n",
            "training loss at step 8 - batch 257: 0.31 (2019-08-04 13:37:43.223754)\n",
            "Accuracy at step 8 - batch 257: 0.9104\n",
            "training loss at step 8 - batch 258: 0.31 (2019-08-04 13:37:43.236474)\n",
            "Accuracy at step 8 - batch 258: 0.9076\n",
            "training loss at step 8 - batch 259: 0.29 (2019-08-04 13:37:43.255349)\n",
            "Accuracy at step 8 - batch 259: 0.916\n",
            "training loss at step 8 - batch 260: 0.32 (2019-08-04 13:37:43.268379)\n",
            "Accuracy at step 8 - batch 260: 0.9084\n",
            "training loss at step 8 - batch 261: 0.30 (2019-08-04 13:37:43.392543)\n",
            "Accuracy at step 8 - batch 261: 0.9128\n",
            "training loss at step 8 - batch 262: 0.31 (2019-08-04 13:37:43.410009)\n",
            "Accuracy at step 8 - batch 262: 0.906\n",
            "training loss at step 8 - batch 263: 0.31 (2019-08-04 13:37:43.430963)\n",
            "Accuracy at step 8 - batch 263: 0.9088\n",
            "training loss at step 8 - batch 264: 0.32 (2019-08-04 13:37:43.444185)\n",
            "Accuracy at step 8 - batch 264: 0.9032\n",
            "training loss at step 8 - batch 265: 0.31 (2019-08-04 13:37:43.456380)\n",
            "Accuracy at step 8 - batch 265: 0.9068\n",
            "training loss at step 8 - batch 266: 0.31 (2019-08-04 13:37:43.587439)\n",
            "Accuracy at step 8 - batch 266: 0.9092\n",
            "training loss at step 8 - batch 267: 0.33 (2019-08-04 13:37:43.604414)\n",
            "Accuracy at step 8 - batch 267: 0.9016\n",
            "training loss at step 8 - batch 268: 0.31 (2019-08-04 13:37:43.618230)\n",
            "Accuracy at step 8 - batch 268: 0.9044\n",
            "training loss at step 8 - batch 269: 0.29 (2019-08-04 13:37:43.631747)\n",
            "Accuracy at step 8 - batch 269: 0.9088\n",
            "training loss at step 8 - batch 270: 0.30 (2019-08-04 13:37:43.643689)\n",
            "Accuracy at step 8 - batch 270: 0.9112\n",
            "training loss at step 8 - batch 271: 0.32 (2019-08-04 13:37:43.765302)\n",
            "Accuracy at step 8 - batch 271: 0.9056\n",
            "training loss at step 8 - batch 272: 0.31 (2019-08-04 13:37:43.779033)\n",
            "Accuracy at step 8 - batch 272: 0.9068\n",
            "training loss at step 8 - batch 273: 0.32 (2019-08-04 13:37:43.792105)\n",
            "Accuracy at step 8 - batch 273: 0.9088\n",
            "training loss at step 8 - batch 274: 0.32 (2019-08-04 13:37:43.804788)\n",
            "Accuracy at step 8 - batch 274: 0.9072\n",
            "training loss at step 8 - batch 275: 0.31 (2019-08-04 13:37:43.817408)\n",
            "Accuracy at step 8 - batch 275: 0.9088\n",
            "training loss at step 8 - batch 276: 0.30 (2019-08-04 13:37:43.935796)\n",
            "Accuracy at step 8 - batch 276: 0.914\n",
            "training loss at step 8 - batch 277: 0.32 (2019-08-04 13:37:43.949641)\n",
            "Accuracy at step 8 - batch 277: 0.9056\n",
            "training loss at step 8 - batch 278: 0.30 (2019-08-04 13:37:43.962136)\n",
            "Accuracy at step 8 - batch 278: 0.916\n",
            "training loss at step 8 - batch 279: 0.28 (2019-08-04 13:37:43.976135)\n",
            "Accuracy at step 8 - batch 279: 0.9116\n",
            "training loss at step 8 - batch 280: 0.32 (2019-08-04 13:37:43.988631)\n",
            "Accuracy at step 8 - batch 280: 0.9084\n",
            "training loss at step 8 - batch 281: 0.30 (2019-08-04 13:37:44.113916)\n",
            "Accuracy at step 8 - batch 281: 0.9096\n",
            "training loss at step 8 - batch 282: 0.33 (2019-08-04 13:37:44.126864)\n",
            "Accuracy at step 8 - batch 282: 0.898\n",
            "training loss at step 8 - batch 283: 0.31 (2019-08-04 13:37:44.139762)\n",
            "Accuracy at step 8 - batch 283: 0.9072\n",
            "training loss at step 8 - batch 284: 0.32 (2019-08-04 13:37:44.152233)\n",
            "Accuracy at step 8 - batch 284: 0.904\n",
            "training loss at step 8 - batch 285: 0.32 (2019-08-04 13:37:44.165915)\n",
            "Accuracy at step 8 - batch 285: 0.91\n",
            "training loss at step 8 - batch 286: 0.29 (2019-08-04 13:37:44.287002)\n",
            "Accuracy at step 8 - batch 286: 0.9172\n",
            "training loss at step 8 - batch 287: 0.30 (2019-08-04 13:37:44.301837)\n",
            "Accuracy at step 8 - batch 287: 0.9108\n",
            "training loss at step 8 - batch 288: 0.29 (2019-08-04 13:37:44.316582)\n",
            "Accuracy at step 8 - batch 288: 0.9072\n",
            "training loss at step 8 - batch 289: 0.30 (2019-08-04 13:37:44.329406)\n",
            "Accuracy at step 8 - batch 289: 0.9132\n",
            "training loss at step 8 - batch 290: 0.33 (2019-08-04 13:37:44.344727)\n",
            "Accuracy at step 8 - batch 290: 0.9056\n",
            "training loss at step 8 - batch 291: 0.33 (2019-08-04 13:37:44.475268)\n",
            "Accuracy at step 8 - batch 291: 0.8996\n",
            "training loss at step 8 - batch 292: 0.31 (2019-08-04 13:37:44.488955)\n",
            "Accuracy at step 8 - batch 292: 0.9088\n",
            "training loss at step 8 - batch 293: 0.31 (2019-08-04 13:37:44.504344)\n",
            "Accuracy at step 8 - batch 293: 0.9116\n",
            "training loss at step 8 - batch 294: 0.30 (2019-08-04 13:37:44.517250)\n",
            "Accuracy at step 8 - batch 294: 0.9068\n",
            "training loss at step 8 - batch 295: 0.31 (2019-08-04 13:37:44.530342)\n",
            "Accuracy at step 8 - batch 295: 0.9064\n",
            "training loss at step 8 - batch 296: 0.33 (2019-08-04 13:37:44.649440)\n",
            "Accuracy at step 8 - batch 296: 0.904\n",
            "training loss at step 8 - batch 297: 0.30 (2019-08-04 13:37:44.661347)\n",
            "Accuracy at step 8 - batch 297: 0.9072\n",
            "training loss at step 8 - batch 298: 0.31 (2019-08-04 13:37:44.675003)\n",
            "Accuracy at step 8 - batch 298: 0.904\n",
            "training loss at step 8 - batch 299: 0.33 (2019-08-04 13:37:44.687898)\n",
            "Accuracy at step 8 - batch 299: 0.8988\n",
            "training loss at step 8 - batch 300: 0.31 (2019-08-04 13:37:44.704122)\n",
            "Accuracy at step 8 - batch 300: 0.912\n",
            "training loss at step 8 - batch 301: 0.30 (2019-08-04 13:37:44.827297)\n",
            "Accuracy at step 8 - batch 301: 0.9152\n",
            "training loss at step 8 - batch 302: 0.30 (2019-08-04 13:37:44.841636)\n",
            "Accuracy at step 8 - batch 302: 0.9096\n",
            "training loss at step 8 - batch 303: 0.32 (2019-08-04 13:37:44.855658)\n",
            "Accuracy at step 8 - batch 303: 0.9036\n",
            "training loss at step 8 - batch 304: 0.32 (2019-08-04 13:37:44.868011)\n",
            "Accuracy at step 8 - batch 304: 0.9044\n",
            "training loss at step 8 - batch 305: 0.30 (2019-08-04 13:37:44.881190)\n",
            "Accuracy at step 8 - batch 305: 0.9116\n",
            "training loss at step 8 - batch 306: 0.32 (2019-08-04 13:37:45.007909)\n",
            "Accuracy at step 8 - batch 306: 0.9024\n",
            "training loss at step 8 - batch 307: 0.32 (2019-08-04 13:37:45.022147)\n",
            "Accuracy at step 8 - batch 307: 0.904\n",
            "training loss at step 8 - batch 308: 0.29 (2019-08-04 13:37:45.035268)\n",
            "Accuracy at step 8 - batch 308: 0.9136\n",
            "training loss at step 8 - batch 309: 0.30 (2019-08-04 13:37:45.048234)\n",
            "Accuracy at step 8 - batch 309: 0.9072\n",
            "training loss at step 8 - batch 310: 0.30 (2019-08-04 13:37:45.061142)\n",
            "Accuracy at step 8 - batch 310: 0.9072\n",
            "training loss at step 8 - batch 311: 0.29 (2019-08-04 13:37:45.190404)\n",
            "Accuracy at step 8 - batch 311: 0.9096\n",
            "training loss at step 8 - batch 312: 0.30 (2019-08-04 13:37:45.203791)\n",
            "Accuracy at step 8 - batch 312: 0.9068\n",
            "training loss at step 8 - batch 313: 0.31 (2019-08-04 13:37:45.221724)\n",
            "Accuracy at step 8 - batch 313: 0.9072\n",
            "training loss at step 8 - batch 314: 0.29 (2019-08-04 13:37:45.234703)\n",
            "Accuracy at step 8 - batch 314: 0.91\n",
            "training loss at step 8 - batch 315: 0.29 (2019-08-04 13:37:45.248697)\n",
            "Accuracy at step 8 - batch 315: 0.9148\n",
            "training loss at step 8 - batch 316: 0.30 (2019-08-04 13:37:45.379316)\n",
            "Accuracy at step 8 - batch 316: 0.9056\n",
            "training loss at step 8 - batch 317: 0.33 (2019-08-04 13:37:45.395412)\n",
            "Accuracy at step 8 - batch 317: 0.9\n",
            "training loss at step 8 - batch 318: 0.31 (2019-08-04 13:37:45.407785)\n",
            "Accuracy at step 8 - batch 318: 0.9036\n",
            "training loss at step 8 - batch 319: 0.30 (2019-08-04 13:37:45.420435)\n",
            "Accuracy at step 8 - batch 319: 0.9052\n",
            "training loss at step 8 - batch 320: 0.32 (2019-08-04 13:37:45.436017)\n",
            "Accuracy at step 8 - batch 320: 0.904\n",
            "training loss at step 8 - batch 321: 0.33 (2019-08-04 13:37:45.565841)\n",
            "Accuracy at step 8 - batch 321: 0.9012\n",
            "training loss at step 8 - batch 322: 0.31 (2019-08-04 13:37:45.580040)\n",
            "Accuracy at step 8 - batch 322: 0.9012\n",
            "training loss at step 8 - batch 323: 0.31 (2019-08-04 13:37:45.593487)\n",
            "Accuracy at step 8 - batch 323: 0.9084\n",
            "training loss at step 8 - batch 324: 0.32 (2019-08-04 13:37:45.606334)\n",
            "Accuracy at step 8 - batch 324: 0.9104\n",
            "training loss at step 8 - batch 325: 0.33 (2019-08-04 13:37:45.619480)\n",
            "Accuracy at step 8 - batch 325: 0.9064\n",
            "training loss at step 8 - batch 326: 0.31 (2019-08-04 13:37:45.744666)\n",
            "Accuracy at step 8 - batch 326: 0.9064\n",
            "training loss at step 8 - batch 327: 0.31 (2019-08-04 13:37:45.758202)\n",
            "Accuracy at step 8 - batch 327: 0.906\n",
            "training loss at step 8 - batch 328: 0.30 (2019-08-04 13:37:45.770615)\n",
            "Accuracy at step 8 - batch 328: 0.9108\n",
            "training loss at step 8 - batch 329: 0.31 (2019-08-04 13:37:45.784760)\n",
            "Accuracy at step 8 - batch 329: 0.9012\n",
            "training loss at step 8 - batch 330: 0.30 (2019-08-04 13:37:45.796860)\n",
            "Accuracy at step 8 - batch 330: 0.9096\n",
            "training loss at step 8 - batch 331: 0.31 (2019-08-04 13:37:45.921634)\n",
            "Accuracy at step 8 - batch 331: 0.9036\n",
            "training loss at step 8 - batch 332: 0.30 (2019-08-04 13:37:45.935183)\n",
            "Accuracy at step 8 - batch 332: 0.9116\n",
            "training loss at step 8 - batch 333: 0.33 (2019-08-04 13:37:45.949230)\n",
            "Accuracy at step 8 - batch 333: 0.9012\n",
            "training loss at step 8 - batch 334: 0.31 (2019-08-04 13:37:45.962690)\n",
            "Accuracy at step 8 - batch 334: 0.9076\n",
            "training loss at step 8 - batch 335: 0.30 (2019-08-04 13:37:45.975763)\n",
            "Accuracy at step 8 - batch 335: 0.9104\n",
            "training loss at step 8 - batch 336: 0.30 (2019-08-04 13:37:46.099141)\n",
            "Accuracy at step 8 - batch 336: 0.9104\n",
            "training loss at step 8 - batch 337: 0.30 (2019-08-04 13:37:46.117008)\n",
            "Accuracy at step 8 - batch 337: 0.912\n",
            "training loss at step 8 - batch 338: 0.31 (2019-08-04 13:37:46.129560)\n",
            "Accuracy at step 8 - batch 338: 0.9104\n",
            "training loss at step 8 - batch 339: 0.31 (2019-08-04 13:37:46.142249)\n",
            "Accuracy at step 8 - batch 339: 0.9008\n",
            "training loss at step 8 - batch 340: 0.32 (2019-08-04 13:37:46.156263)\n",
            "Accuracy at step 8 - batch 340: 0.9072\n",
            "training loss at step 8 - batch 341: 0.32 (2019-08-04 13:37:46.289391)\n",
            "Accuracy at step 8 - batch 341: 0.9104\n",
            "training loss at step 8 - batch 342: 0.32 (2019-08-04 13:37:46.301515)\n",
            "Accuracy at step 8 - batch 342: 0.9056\n",
            "training loss at step 8 - batch 343: 0.33 (2019-08-04 13:37:46.315460)\n",
            "Accuracy at step 8 - batch 343: 0.8996\n",
            "training loss at step 8 - batch 344: 0.30 (2019-08-04 13:37:46.328357)\n",
            "Accuracy at step 8 - batch 344: 0.91\n",
            "training loss at step 8 - batch 345: 0.31 (2019-08-04 13:37:46.341092)\n",
            "Accuracy at step 8 - batch 345: 0.908\n",
            "training loss at step 8 - batch 346: 0.32 (2019-08-04 13:37:46.463907)\n",
            "Accuracy at step 8 - batch 346: 0.8972\n",
            "training loss at step 8 - batch 347: 0.32 (2019-08-04 13:37:46.482114)\n",
            "Accuracy at step 8 - batch 347: 0.9068\n",
            "training loss at step 8 - batch 348: 0.32 (2019-08-04 13:37:46.498194)\n",
            "Accuracy at step 8 - batch 348: 0.9036\n",
            "training loss at step 8 - batch 349: 0.32 (2019-08-04 13:37:46.510840)\n",
            "Accuracy at step 8 - batch 349: 0.9004\n",
            "training loss at step 8 - batch 350: 0.29 (2019-08-04 13:37:46.524585)\n",
            "Accuracy at step 8 - batch 350: 0.9116\n",
            "training loss at step 8 - batch 351: 0.30 (2019-08-04 13:37:46.657568)\n",
            "Accuracy at step 8 - batch 351: 0.9052\n",
            "training loss at step 8 - batch 352: 0.31 (2019-08-04 13:37:46.673856)\n",
            "Accuracy at step 8 - batch 352: 0.906\n",
            "training loss at step 8 - batch 353: 0.31 (2019-08-04 13:37:46.690313)\n",
            "Accuracy at step 8 - batch 353: 0.908\n",
            "training loss at step 8 - batch 354: 0.31 (2019-08-04 13:37:46.705750)\n",
            "Accuracy at step 8 - batch 354: 0.9056\n",
            "training loss at step 8 - batch 355: 0.32 (2019-08-04 13:37:46.719338)\n",
            "Accuracy at step 8 - batch 355: 0.8988\n",
            "training loss at step 8 - batch 356: 0.31 (2019-08-04 13:37:46.842633)\n",
            "Accuracy at step 8 - batch 356: 0.9056\n",
            "training loss at step 8 - batch 357: 0.28 (2019-08-04 13:37:46.856884)\n",
            "Accuracy at step 8 - batch 357: 0.9176\n",
            "training loss at step 8 - batch 358: 0.30 (2019-08-04 13:37:46.869366)\n",
            "Accuracy at step 8 - batch 358: 0.912\n",
            "training loss at step 8 - batch 359: 0.33 (2019-08-04 13:37:46.884557)\n",
            "Accuracy at step 8 - batch 359: 0.9024\n",
            "training loss at step 8 - batch 360: 0.29 (2019-08-04 13:37:46.896983)\n",
            "Accuracy at step 8 - batch 360: 0.9112\n",
            "training loss at step 8 - batch 361: 0.31 (2019-08-04 13:37:47.018156)\n",
            "Accuracy at step 8 - batch 361: 0.91\n",
            "training loss at step 8 - batch 362: 0.31 (2019-08-04 13:37:47.035540)\n",
            "Accuracy at step 8 - batch 362: 0.9064\n",
            "training loss at step 8 - batch 363: 0.31 (2019-08-04 13:37:47.048756)\n",
            "Accuracy at step 8 - batch 363: 0.9104\n",
            "training loss at step 8 - batch 364: 0.29 (2019-08-04 13:37:47.061039)\n",
            "Accuracy at step 8 - batch 364: 0.9156\n",
            "training loss at step 8 - batch 365: 0.32 (2019-08-04 13:37:47.073610)\n",
            "Accuracy at step 8 - batch 365: 0.9064\n",
            "training loss at step 8 - batch 366: 0.33 (2019-08-04 13:37:47.200466)\n",
            "Accuracy at step 8 - batch 366: 0.9052\n",
            "training loss at step 8 - batch 367: 0.31 (2019-08-04 13:37:47.217401)\n",
            "Accuracy at step 8 - batch 367: 0.9072\n",
            "training loss at step 8 - batch 368: 0.30 (2019-08-04 13:37:47.231225)\n",
            "Accuracy at step 8 - batch 368: 0.916\n",
            "training loss at step 8 - batch 369: 0.33 (2019-08-04 13:37:47.244041)\n",
            "Accuracy at step 8 - batch 369: 0.9024\n",
            "training loss at step 8 - batch 370: 0.30 (2019-08-04 13:37:47.257188)\n",
            "Accuracy at step 8 - batch 370: 0.906\n",
            "training loss at step 8 - batch 371: 0.31 (2019-08-04 13:37:47.374185)\n",
            "Accuracy at step 8 - batch 371: 0.9084\n",
            "training loss at step 8 - batch 372: 0.30 (2019-08-04 13:37:47.387979)\n",
            "Accuracy at step 8 - batch 372: 0.9028\n",
            "training loss at step 8 - batch 373: 0.30 (2019-08-04 13:37:47.400973)\n",
            "Accuracy at step 8 - batch 373: 0.912\n",
            "training loss at step 8 - batch 374: 0.31 (2019-08-04 13:37:47.416756)\n",
            "Accuracy at step 8 - batch 374: 0.9104\n",
            "training loss at step 8 - batch 375: 0.31 (2019-08-04 13:37:47.428923)\n",
            "Accuracy at step 8 - batch 375: 0.91\n",
            "training loss at step 8 - batch 376: 0.35 (2019-08-04 13:37:47.560589)\n",
            "Accuracy at step 8 - batch 376: 0.8992\n",
            "training loss at step 8 - batch 377: 0.31 (2019-08-04 13:37:47.573031)\n",
            "Accuracy at step 8 - batch 377: 0.9076\n",
            "training loss at step 8 - batch 378: 0.30 (2019-08-04 13:37:47.586424)\n",
            "Accuracy at step 8 - batch 378: 0.9044\n",
            "training loss at step 8 - batch 379: 0.31 (2019-08-04 13:37:47.598904)\n",
            "Accuracy at step 8 - batch 379: 0.9032\n",
            "training loss at step 8 - batch 380: 0.32 (2019-08-04 13:37:47.612361)\n",
            "Accuracy at step 8 - batch 380: 0.9032\n",
            "training loss at step 8 - batch 381: 0.29 (2019-08-04 13:37:47.740391)\n",
            "Accuracy at step 8 - batch 381: 0.9128\n",
            "training loss at step 8 - batch 382: 0.31 (2019-08-04 13:37:47.752349)\n",
            "Accuracy at step 8 - batch 382: 0.906\n",
            "training loss at step 8 - batch 383: 0.30 (2019-08-04 13:37:47.764520)\n",
            "Accuracy at step 8 - batch 383: 0.9104\n",
            "training loss at step 8 - batch 384: 0.30 (2019-08-04 13:37:47.777604)\n",
            "Accuracy at step 8 - batch 384: 0.9128\n",
            "training loss at step 8 - batch 385: 0.28 (2019-08-04 13:37:47.789704)\n",
            "Accuracy at step 8 - batch 385: 0.9172\n",
            "training loss at step 8 - batch 386: 0.33 (2019-08-04 13:37:47.908777)\n",
            "Accuracy at step 8 - batch 386: 0.9016\n",
            "training loss at step 8 - batch 387: 0.30 (2019-08-04 13:37:47.924743)\n",
            "Accuracy at step 8 - batch 387: 0.9088\n",
            "training loss at step 8 - batch 388: 0.30 (2019-08-04 13:37:47.937408)\n",
            "Accuracy at step 8 - batch 388: 0.9088\n",
            "training loss at step 8 - batch 389: 0.31 (2019-08-04 13:37:47.960817)\n",
            "Accuracy at step 8 - batch 389: 0.906\n",
            "training loss at step 8 - batch 390: 0.33 (2019-08-04 13:37:47.974312)\n",
            "Accuracy at step 8 - batch 390: 0.9064\n",
            "training loss at step 8 - batch 391: 0.31 (2019-08-04 13:37:48.101761)\n",
            "Accuracy at step 8 - batch 391: 0.9096\n",
            "training loss at step 8 - batch 392: 0.33 (2019-08-04 13:37:48.113976)\n",
            "Accuracy at step 8 - batch 392: 0.8996\n",
            "training loss at step 8 - batch 393: 0.34 (2019-08-04 13:37:48.126985)\n",
            "Accuracy at step 8 - batch 393: 0.9028\n",
            "training loss at step 8 - batch 394: 0.29 (2019-08-04 13:37:48.140301)\n",
            "Accuracy at step 8 - batch 394: 0.9108\n",
            "training loss at step 8 - batch 395: 0.31 (2019-08-04 13:37:48.152755)\n",
            "Accuracy at step 8 - batch 395: 0.9096\n",
            "training loss at step 8 - batch 396: 0.30 (2019-08-04 13:37:48.279526)\n",
            "Accuracy at step 8 - batch 396: 0.906\n",
            "training loss at step 8 - batch 397: 0.31 (2019-08-04 13:37:48.292074)\n",
            "Accuracy at step 8 - batch 397: 0.914\n",
            "training loss at step 8 - batch 398: 0.32 (2019-08-04 13:37:48.304568)\n",
            "Accuracy at step 8 - batch 398: 0.9052\n",
            "training loss at step 8 - batch 399: 0.33 (2019-08-04 13:37:48.317001)\n",
            "Accuracy at step 8 - batch 399: 0.9044\n",
            "training loss at step 8 - batch 400: 0.31 (2019-08-04 13:37:48.330055)\n",
            "Accuracy at step 8 - batch 400: 0.9012\n",
            "training loss at step 8 - batch 401: 0.31 (2019-08-04 13:37:48.453908)\n",
            "Accuracy at step 8 - batch 401: 0.9056\n",
            "training loss at step 8 - batch 402: 0.32 (2019-08-04 13:37:48.470948)\n",
            "Accuracy at step 8 - batch 402: 0.9\n",
            "training loss at step 8 - batch 403: 0.32 (2019-08-04 13:37:48.485761)\n",
            "Accuracy at step 8 - batch 403: 0.9032\n",
            "training loss at step 8 - batch 404: 0.33 (2019-08-04 13:37:48.500024)\n",
            "Accuracy at step 8 - batch 404: 0.8984\n",
            "training loss at step 8 - batch 405: 0.31 (2019-08-04 13:37:48.513328)\n",
            "Accuracy at step 8 - batch 405: 0.9104\n",
            "training loss at step 8 - batch 406: 0.31 (2019-08-04 13:37:48.644001)\n",
            "Accuracy at step 8 - batch 406: 0.9068\n",
            "training loss at step 8 - batch 407: 0.29 (2019-08-04 13:37:48.656753)\n",
            "Accuracy at step 8 - batch 407: 0.9112\n",
            "training loss at step 8 - batch 408: 0.29 (2019-08-04 13:37:48.668791)\n",
            "Accuracy at step 8 - batch 408: 0.9112\n",
            "training loss at step 8 - batch 409: 0.30 (2019-08-04 13:37:48.682223)\n",
            "Accuracy at step 8 - batch 409: 0.9128\n",
            "training loss at step 8 - batch 410: 0.32 (2019-08-04 13:37:48.699644)\n",
            "Accuracy at step 8 - batch 410: 0.9036\n",
            "training loss at step 8 - batch 411: 0.29 (2019-08-04 13:37:48.821949)\n",
            "Accuracy at step 8 - batch 411: 0.9132\n",
            "training loss at step 8 - batch 412: 0.30 (2019-08-04 13:37:48.837662)\n",
            "Accuracy at step 8 - batch 412: 0.9116\n",
            "training loss at step 8 - batch 413: 0.32 (2019-08-04 13:37:48.850279)\n",
            "Accuracy at step 8 - batch 413: 0.9008\n",
            "training loss at step 8 - batch 414: 0.32 (2019-08-04 13:37:48.863259)\n",
            "Accuracy at step 8 - batch 414: 0.9052\n",
            "training loss at step 8 - batch 415: 0.31 (2019-08-04 13:37:48.877034)\n",
            "Accuracy at step 8 - batch 415: 0.9076\n",
            "training loss at step 8 - batch 416: 0.31 (2019-08-04 13:37:48.999650)\n",
            "Accuracy at step 8 - batch 416: 0.908\n",
            "training loss at step 8 - batch 417: 0.32 (2019-08-04 13:37:49.012446)\n",
            "Accuracy at step 8 - batch 417: 0.906\n",
            "training loss at step 8 - batch 418: 0.33 (2019-08-04 13:37:49.025348)\n",
            "Accuracy at step 8 - batch 418: 0.904\n",
            "training loss at step 8 - batch 419: 0.30 (2019-08-04 13:37:49.038150)\n",
            "Accuracy at step 8 - batch 419: 0.9056\n",
            "training loss at step 8 - batch 420: 0.32 (2019-08-04 13:37:49.051566)\n",
            "Accuracy at step 8 - batch 420: 0.9048\n",
            "training loss at step 8 - batch 421: 0.30 (2019-08-04 13:37:49.166864)\n",
            "Accuracy at step 8 - batch 421: 0.9068\n",
            "training loss at step 8 - batch 422: 0.29 (2019-08-04 13:37:49.180503)\n",
            "Accuracy at step 8 - batch 422: 0.9048\n",
            "training loss at step 8 - batch 423: 0.32 (2019-08-04 13:37:49.192772)\n",
            "Accuracy at step 8 - batch 423: 0.9016\n",
            "training loss at step 8 - batch 424: 0.32 (2019-08-04 13:37:49.206562)\n",
            "Accuracy at step 8 - batch 424: 0.9044\n",
            "training loss at step 8 - batch 425: 0.30 (2019-08-04 13:37:49.219948)\n",
            "Accuracy at step 8 - batch 425: 0.9076\n",
            "training loss at step 8 - batch 426: 0.29 (2019-08-04 13:37:49.344743)\n",
            "Accuracy at step 8 - batch 426: 0.9088\n",
            "training loss at step 8 - batch 427: 0.30 (2019-08-04 13:37:49.358642)\n",
            "Accuracy at step 8 - batch 427: 0.9068\n",
            "training loss at step 8 - batch 428: 0.31 (2019-08-04 13:37:49.371736)\n",
            "Accuracy at step 8 - batch 428: 0.9032\n",
            "training loss at step 8 - batch 429: 0.33 (2019-08-04 13:37:49.386263)\n",
            "Accuracy at step 8 - batch 429: 0.8944\n",
            "training loss at step 8 - batch 430: 0.30 (2019-08-04 13:37:49.399261)\n",
            "Accuracy at step 8 - batch 430: 0.9032\n",
            "training loss at step 8 - batch 431: 0.30 (2019-08-04 13:37:49.524017)\n",
            "Accuracy at step 8 - batch 431: 0.9116\n",
            "training loss at step 8 - batch 432: 0.31 (2019-08-04 13:37:49.542175)\n",
            "Accuracy at step 8 - batch 432: 0.9048\n",
            "training loss at step 8 - batch 433: 0.31 (2019-08-04 13:37:49.556353)\n",
            "Accuracy at step 8 - batch 433: 0.91\n",
            "training loss at step 8 - batch 434: 0.30 (2019-08-04 13:37:49.570612)\n",
            "Accuracy at step 8 - batch 434: 0.91\n",
            "training loss at step 8 - batch 435: 0.27 (2019-08-04 13:37:49.582902)\n",
            "Accuracy at step 8 - batch 435: 0.9212\n",
            "training loss at step 8 - batch 436: 0.33 (2019-08-04 13:37:49.701597)\n",
            "Accuracy at step 8 - batch 436: 0.9004\n",
            "training loss at step 8 - batch 437: 0.33 (2019-08-04 13:37:49.714378)\n",
            "Accuracy at step 8 - batch 437: 0.9024\n",
            "training loss at step 8 - batch 438: 0.31 (2019-08-04 13:37:49.729279)\n",
            "Accuracy at step 8 - batch 438: 0.9072\n",
            "training loss at step 8 - batch 439: 0.30 (2019-08-04 13:37:49.741644)\n",
            "Accuracy at step 8 - batch 439: 0.9076\n",
            "training loss at step 8 - batch 440: 0.29 (2019-08-04 13:37:49.757387)\n",
            "Accuracy at step 8 - batch 440: 0.9072\n",
            "training loss at step 8 - batch 441: 0.30 (2019-08-04 13:37:49.874705)\n",
            "Accuracy at step 8 - batch 441: 0.908\n",
            "training loss at step 8 - batch 442: 0.32 (2019-08-04 13:37:49.888654)\n",
            "Accuracy at step 8 - batch 442: 0.9032\n",
            "training loss at step 8 - batch 443: 0.33 (2019-08-04 13:37:49.901292)\n",
            "Accuracy at step 8 - batch 443: 0.8952\n",
            "training loss at step 8 - batch 444: 0.32 (2019-08-04 13:37:49.914567)\n",
            "Accuracy at step 8 - batch 444: 0.9048\n",
            "training loss at step 8 - batch 445: 0.31 (2019-08-04 13:37:49.927112)\n",
            "Accuracy at step 8 - batch 445: 0.908\n",
            "training loss at step 8 - batch 446: 0.32 (2019-08-04 13:37:50.058769)\n",
            "Accuracy at step 8 - batch 446: 0.9032\n",
            "training loss at step 8 - batch 447: 0.30 (2019-08-04 13:37:50.072829)\n",
            "Accuracy at step 8 - batch 447: 0.9112\n",
            "training loss at step 8 - batch 448: 0.29 (2019-08-04 13:37:50.085582)\n",
            "Accuracy at step 8 - batch 448: 0.9176\n",
            "training loss at step 8 - batch 449: 0.28 (2019-08-04 13:37:50.099093)\n",
            "Accuracy at step 8 - batch 449: 0.9144\n",
            "training loss at step 8 - batch 450: 0.34 (2019-08-04 13:37:50.111116)\n",
            "Accuracy at step 8 - batch 450: 0.8996\n",
            "training loss at step 8 - batch 451: 0.31 (2019-08-04 13:37:50.230071)\n",
            "Accuracy at step 8 - batch 451: 0.9028\n",
            "training loss at step 8 - batch 452: 0.31 (2019-08-04 13:37:50.247180)\n",
            "Accuracy at step 8 - batch 452: 0.908\n",
            "training loss at step 8 - batch 453: 0.29 (2019-08-04 13:37:50.260308)\n",
            "Accuracy at step 8 - batch 453: 0.912\n",
            "training loss at step 8 - batch 454: 0.32 (2019-08-04 13:37:50.275832)\n",
            "Accuracy at step 8 - batch 454: 0.9048\n",
            "training loss at step 8 - batch 455: 0.31 (2019-08-04 13:37:50.288465)\n",
            "Accuracy at step 8 - batch 455: 0.9132\n",
            "training loss at step 8 - batch 456: 0.30 (2019-08-04 13:37:50.410282)\n",
            "Accuracy at step 8 - batch 456: 0.9104\n",
            "training loss at step 8 - batch 457: 0.30 (2019-08-04 13:37:50.426232)\n",
            "Accuracy at step 8 - batch 457: 0.9064\n",
            "training loss at step 8 - batch 458: 0.31 (2019-08-04 13:37:50.440211)\n",
            "Accuracy at step 8 - batch 458: 0.9088\n",
            "training loss at step 8 - batch 459: 0.30 (2019-08-04 13:37:50.453045)\n",
            "Accuracy at step 8 - batch 459: 0.9136\n",
            "training loss at step 8 - batch 460: 0.29 (2019-08-04 13:37:50.465241)\n",
            "Accuracy at step 8 - batch 460: 0.9108\n",
            "training loss at step 8 - batch 461: 0.31 (2019-08-04 13:37:50.597208)\n",
            "Accuracy at step 8 - batch 461: 0.9064\n",
            "training loss at step 8 - batch 462: 0.31 (2019-08-04 13:37:50.609907)\n",
            "Accuracy at step 8 - batch 462: 0.9084\n",
            "training loss at step 8 - batch 463: 0.32 (2019-08-04 13:37:50.621851)\n",
            "Accuracy at step 8 - batch 463: 0.9076\n",
            "training loss at step 8 - batch 464: 0.30 (2019-08-04 13:37:50.635098)\n",
            "Accuracy at step 8 - batch 464: 0.9012\n",
            "training loss at step 8 - batch 465: 0.28 (2019-08-04 13:37:50.647482)\n",
            "Accuracy at step 8 - batch 465: 0.914\n",
            "training loss at step 8 - batch 466: 0.33 (2019-08-04 13:37:50.773767)\n",
            "Accuracy at step 8 - batch 466: 0.8992\n",
            "training loss at step 8 - batch 467: 0.32 (2019-08-04 13:37:50.790395)\n",
            "Accuracy at step 8 - batch 467: 0.9088\n",
            "training loss at step 8 - batch 468: 0.33 (2019-08-04 13:37:50.802816)\n",
            "Accuracy at step 8 - batch 468: 0.9048\n",
            "training loss at step 8 - batch 469: 0.30 (2019-08-04 13:37:50.815116)\n",
            "Accuracy at step 8 - batch 469: 0.908\n",
            "training loss at step 8 - batch 470: 0.33 (2019-08-04 13:37:50.827309)\n",
            "Accuracy at step 8 - batch 470: 0.8996\n",
            "training loss at step 8 - batch 471: 0.33 (2019-08-04 13:37:50.948827)\n",
            "Accuracy at step 8 - batch 471: 0.9056\n",
            "training loss at step 8 - batch 472: 0.30 (2019-08-04 13:37:50.962789)\n",
            "Accuracy at step 8 - batch 472: 0.9072\n",
            "training loss at step 8 - batch 473: 0.33 (2019-08-04 13:37:50.979062)\n",
            "Accuracy at step 8 - batch 473: 0.8968\n",
            "training loss at step 8 - batch 474: 0.32 (2019-08-04 13:37:50.993139)\n",
            "Accuracy at step 8 - batch 474: 0.904\n",
            "training loss at step 8 - batch 475: 0.32 (2019-08-04 13:37:51.006159)\n",
            "Accuracy at step 8 - batch 475: 0.906\n",
            "training loss at step 8 - batch 476: 0.30 (2019-08-04 13:37:51.122501)\n",
            "Accuracy at step 8 - batch 476: 0.9132\n",
            "training loss at step 8 - batch 477: 0.30 (2019-08-04 13:37:51.136948)\n",
            "Accuracy at step 8 - batch 477: 0.9092\n",
            "training loss at step 8 - batch 478: 0.30 (2019-08-04 13:37:51.150022)\n",
            "Accuracy at step 8 - batch 478: 0.9116\n",
            "training loss at step 8 - batch 479: 0.28 (2019-08-04 13:37:51.163085)\n",
            "Accuracy at step 8 - batch 479: 0.9124\n",
            "training loss at step 8 - batch 480: 0.32 (2019-08-04 13:37:51.176188)\n",
            "Accuracy at step 8 - batch 480: 0.9104\n",
            "training loss at step 8 - batch 481: 0.31 (2019-08-04 13:37:51.301187)\n",
            "Accuracy at step 8 - batch 481: 0.9064\n",
            "training loss at step 8 - batch 482: 0.31 (2019-08-04 13:37:51.314825)\n",
            "Accuracy at step 8 - batch 482: 0.904\n",
            "training loss at step 8 - batch 483: 0.29 (2019-08-04 13:37:51.327896)\n",
            "Accuracy at step 8 - batch 483: 0.916\n",
            "training loss at step 8 - batch 484: 0.31 (2019-08-04 13:37:51.340825)\n",
            "Accuracy at step 8 - batch 484: 0.9096\n",
            "training loss at step 8 - batch 485: 0.29 (2019-08-04 13:37:51.352827)\n",
            "Accuracy at step 8 - batch 485: 0.9076\n",
            "training loss at step 8 - batch 486: 0.30 (2019-08-04 13:37:51.475794)\n",
            "Accuracy at step 8 - batch 486: 0.9112\n",
            "training loss at step 8 - batch 487: 0.33 (2019-08-04 13:37:51.489772)\n",
            "Accuracy at step 8 - batch 487: 0.902\n",
            "training loss at step 8 - batch 488: 0.29 (2019-08-04 13:37:51.501881)\n",
            "Accuracy at step 8 - batch 488: 0.9124\n",
            "training loss at step 8 - batch 489: 0.32 (2019-08-04 13:37:51.520672)\n",
            "Accuracy at step 8 - batch 489: 0.9056\n",
            "training loss at step 8 - batch 490: 0.31 (2019-08-04 13:37:51.533046)\n",
            "Accuracy at step 8 - batch 490: 0.9164\n",
            "training loss at step 8 - batch 491: 0.31 (2019-08-04 13:37:51.664843)\n",
            "Accuracy at step 8 - batch 491: 0.902\n",
            "training loss at step 8 - batch 492: 0.29 (2019-08-04 13:37:51.677894)\n",
            "Accuracy at step 8 - batch 492: 0.914\n",
            "training loss at step 8 - batch 493: 0.30 (2019-08-04 13:37:51.690674)\n",
            "Accuracy at step 8 - batch 493: 0.9072\n",
            "training loss at step 8 - batch 494: 0.32 (2019-08-04 13:37:51.704173)\n",
            "Accuracy at step 8 - batch 494: 0.904\n",
            "training loss at step 8 - batch 495: 0.31 (2019-08-04 13:37:51.716379)\n",
            "Accuracy at step 8 - batch 495: 0.9016\n",
            "training loss at step 8 - batch 496: 0.33 (2019-08-04 13:37:51.846965)\n",
            "Accuracy at step 8 - batch 496: 0.9068\n",
            "training loss at step 8 - batch 497: 0.32 (2019-08-04 13:37:51.864082)\n",
            "Accuracy at step 8 - batch 497: 0.9024\n",
            "training loss at step 8 - batch 498: 0.31 (2019-08-04 13:37:51.876553)\n",
            "Accuracy at step 8 - batch 498: 0.9072\n",
            "training loss at step 8 - batch 499: 0.30 (2019-08-04 13:37:51.889859)\n",
            "Accuracy at step 8 - batch 499: 0.9064\n",
            "training loss at step 8 - batch 500: 0.29 (2019-08-04 13:37:51.902289)\n",
            "Accuracy at step 8 - batch 500: 0.9132\n",
            "training loss at step 8 - batch 501: 0.32 (2019-08-04 13:37:52.027054)\n",
            "Accuracy at step 8 - batch 501: 0.9048\n",
            "training loss at step 8 - batch 502: 0.29 (2019-08-04 13:37:52.041827)\n",
            "Accuracy at step 8 - batch 502: 0.91\n",
            "training loss at step 8 - batch 503: 0.33 (2019-08-04 13:37:52.057450)\n",
            "Accuracy at step 8 - batch 503: 0.9008\n",
            "training loss at step 8 - batch 504: 0.31 (2019-08-04 13:37:52.070599)\n",
            "Accuracy at step 8 - batch 504: 0.9088\n",
            "training loss at step 8 - batch 505: 0.31 (2019-08-04 13:37:52.083855)\n",
            "Accuracy at step 8 - batch 505: 0.9064\n",
            "training loss at step 8 - batch 506: 0.31 (2019-08-04 13:37:52.208436)\n",
            "Accuracy at step 8 - batch 506: 0.9064\n",
            "training loss at step 8 - batch 507: 0.32 (2019-08-04 13:37:52.223551)\n",
            "Accuracy at step 8 - batch 507: 0.9028\n",
            "training loss at step 8 - batch 508: 0.31 (2019-08-04 13:37:52.236118)\n",
            "Accuracy at step 8 - batch 508: 0.906\n",
            "training loss at step 8 - batch 509: 0.28 (2019-08-04 13:37:52.248541)\n",
            "Accuracy at step 8 - batch 509: 0.918\n",
            "training loss at step 8 - batch 510: 0.29 (2019-08-04 13:37:52.262730)\n",
            "Accuracy at step 8 - batch 510: 0.9136\n",
            "training loss at step 8 - batch 511: 0.32 (2019-08-04 13:37:52.385598)\n",
            "Accuracy at step 8 - batch 511: 0.9056\n",
            "training loss at step 8 - batch 512: 0.30 (2019-08-04 13:37:52.398176)\n",
            "Accuracy at step 8 - batch 512: 0.9096\n",
            "training loss at step 8 - batch 513: 0.33 (2019-08-04 13:37:52.411219)\n",
            "Accuracy at step 8 - batch 513: 0.9048\n",
            "training loss at step 8 - batch 514: 0.32 (2019-08-04 13:37:52.426516)\n",
            "Accuracy at step 8 - batch 514: 0.9044\n",
            "training loss at step 8 - batch 515: 0.32 (2019-08-04 13:37:52.439060)\n",
            "Accuracy at step 8 - batch 515: 0.9032\n",
            "training loss at step 8 - batch 516: 0.31 (2019-08-04 13:37:52.564977)\n",
            "Accuracy at step 8 - batch 516: 0.906\n",
            "training loss at step 8 - batch 517: 0.31 (2019-08-04 13:37:52.577054)\n",
            "Accuracy at step 8 - batch 517: 0.9068\n",
            "training loss at step 8 - batch 518: 0.32 (2019-08-04 13:37:52.590354)\n",
            "Accuracy at step 8 - batch 518: 0.9012\n",
            "training loss at step 8 - batch 519: 0.32 (2019-08-04 13:37:52.605017)\n",
            "Accuracy at step 8 - batch 519: 0.8972\n",
            "training loss at step 8 - batch 520: 0.29 (2019-08-04 13:37:52.621431)\n",
            "Accuracy at step 8 - batch 520: 0.9036\n",
            "training loss at step 8 - batch 521: 0.29 (2019-08-04 13:37:52.747934)\n",
            "Accuracy at step 8 - batch 521: 0.9108\n",
            "training loss at step 8 - batch 522: 0.29 (2019-08-04 13:37:52.766243)\n",
            "Accuracy at step 8 - batch 522: 0.9108\n",
            "training loss at step 8 - batch 523: 0.30 (2019-08-04 13:37:52.781988)\n",
            "Accuracy at step 8 - batch 523: 0.9116\n",
            "training loss at step 8 - batch 524: 0.29 (2019-08-04 13:37:52.794912)\n",
            "Accuracy at step 8 - batch 524: 0.9068\n",
            "training loss at step 8 - batch 525: 0.31 (2019-08-04 13:37:52.808117)\n",
            "Accuracy at step 8 - batch 525: 0.9064\n",
            "training loss at step 8 - batch 526: 0.31 (2019-08-04 13:37:52.932191)\n",
            "Accuracy at step 8 - batch 526: 0.908\n",
            "training loss at step 8 - batch 527: 0.31 (2019-08-04 13:37:52.944693)\n",
            "Accuracy at step 8 - batch 527: 0.9088\n",
            "training loss at step 8 - batch 528: 0.31 (2019-08-04 13:37:52.957043)\n",
            "Accuracy at step 8 - batch 528: 0.904\n",
            "training loss at step 8 - batch 529: 0.31 (2019-08-04 13:37:52.972765)\n",
            "Accuracy at step 8 - batch 529: 0.906\n",
            "training loss at step 8 - batch 530: 0.30 (2019-08-04 13:37:52.988655)\n",
            "Accuracy at step 8 - batch 530: 0.9044\n",
            "training loss at step 8 - batch 531: 0.32 (2019-08-04 13:37:53.111586)\n",
            "Accuracy at step 8 - batch 531: 0.9036\n",
            "training loss at step 8 - batch 532: 0.31 (2019-08-04 13:37:53.127665)\n",
            "Accuracy at step 8 - batch 532: 0.9052\n",
            "training loss at step 8 - batch 533: 0.28 (2019-08-04 13:37:53.140173)\n",
            "Accuracy at step 8 - batch 533: 0.9128\n",
            "training loss at step 8 - batch 534: 0.29 (2019-08-04 13:37:53.152484)\n",
            "Accuracy at step 8 - batch 534: 0.9104\n",
            "training loss at step 8 - batch 535: 0.31 (2019-08-04 13:37:53.165410)\n",
            "Accuracy at step 8 - batch 535: 0.9024\n",
            "training loss at step 8 - batch 536: 0.31 (2019-08-04 13:37:53.285329)\n",
            "Accuracy at step 8 - batch 536: 0.902\n",
            "training loss at step 8 - batch 537: 0.30 (2019-08-04 13:37:53.298547)\n",
            "Accuracy at step 8 - batch 537: 0.9132\n",
            "training loss at step 8 - batch 538: 0.30 (2019-08-04 13:37:53.312420)\n",
            "Accuracy at step 8 - batch 538: 0.9056\n",
            "training loss at step 8 - batch 539: 0.30 (2019-08-04 13:37:53.325369)\n",
            "Accuracy at step 8 - batch 539: 0.9052\n",
            "training loss at step 8 - batch 540: 0.28 (2019-08-04 13:37:53.338261)\n",
            "Accuracy at step 8 - batch 540: 0.912\n",
            "training loss at step 8 - batch 541: 0.32 (2019-08-04 13:37:53.458481)\n",
            "Accuracy at step 8 - batch 541: 0.9092\n",
            "training loss at step 8 - batch 542: 0.28 (2019-08-04 13:37:53.473399)\n",
            "Accuracy at step 8 - batch 542: 0.9132\n",
            "training loss at step 8 - batch 543: 0.31 (2019-08-04 13:37:53.486539)\n",
            "Accuracy at step 8 - batch 543: 0.91\n",
            "training loss at step 8 - batch 544: 0.31 (2019-08-04 13:37:53.502599)\n",
            "Accuracy at step 8 - batch 544: 0.9104\n",
            "training loss at step 8 - batch 545: 0.30 (2019-08-04 13:37:53.515056)\n",
            "Accuracy at step 8 - batch 545: 0.9088\n",
            "training loss at step 8 - batch 546: 0.34 (2019-08-04 13:37:53.637012)\n",
            "Accuracy at step 8 - batch 546: 0.8952\n",
            "training loss at step 8 - batch 547: 0.31 (2019-08-04 13:37:53.652408)\n",
            "Accuracy at step 8 - batch 547: 0.9032\n",
            "training loss at step 8 - batch 548: 0.30 (2019-08-04 13:37:53.664776)\n",
            "Accuracy at step 8 - batch 548: 0.9096\n",
            "training loss at step 8 - batch 549: 0.29 (2019-08-04 13:37:53.678502)\n",
            "Accuracy at step 8 - batch 549: 0.912\n",
            "training loss at step 8 - batch 550: 0.29 (2019-08-04 13:37:53.692748)\n",
            "Accuracy at step 8 - batch 550: 0.9148\n",
            "training loss at step 8 - batch 551: 0.32 (2019-08-04 13:37:53.818429)\n",
            "Accuracy at step 8 - batch 551: 0.902\n",
            "training loss at step 8 - batch 552: 0.30 (2019-08-04 13:37:53.836715)\n",
            "Accuracy at step 8 - batch 552: 0.9096\n",
            "training loss at step 8 - batch 553: 0.28 (2019-08-04 13:37:53.849679)\n",
            "Accuracy at step 8 - batch 553: 0.9132\n",
            "training loss at step 8 - batch 554: 0.32 (2019-08-04 13:37:53.863451)\n",
            "Accuracy at step 8 - batch 554: 0.9068\n",
            "training loss at step 8 - batch 555: 0.31 (2019-08-04 13:37:53.875538)\n",
            "Accuracy at step 8 - batch 555: 0.9028\n",
            "training loss at step 8 - batch 556: 0.33 (2019-08-04 13:37:53.996883)\n",
            "Accuracy at step 8 - batch 556: 0.9\n",
            "training loss at step 8 - batch 557: 0.32 (2019-08-04 13:37:54.013315)\n",
            "Accuracy at step 8 - batch 557: 0.908\n",
            "training loss at step 8 - batch 558: 0.30 (2019-08-04 13:37:54.026411)\n",
            "Accuracy at step 8 - batch 558: 0.9072\n",
            "training loss at step 8 - batch 559: 0.32 (2019-08-04 13:37:54.039230)\n",
            "Accuracy at step 8 - batch 559: 0.9036\n",
            "training loss at step 8 - batch 560: 0.29 (2019-08-04 13:37:54.051061)\n",
            "Accuracy at step 8 - batch 560: 0.9092\n",
            "training loss at step 8 - batch 561: 0.29 (2019-08-04 13:37:54.170452)\n",
            "Accuracy at step 8 - batch 561: 0.9108\n",
            "training loss at step 8 - batch 562: 0.30 (2019-08-04 13:37:54.185327)\n",
            "Accuracy at step 8 - batch 562: 0.9052\n",
            "training loss at step 8 - batch 563: 0.29 (2019-08-04 13:37:54.198050)\n",
            "Accuracy at step 8 - batch 563: 0.9112\n",
            "training loss at step 8 - batch 564: 0.31 (2019-08-04 13:37:54.214464)\n",
            "Accuracy at step 8 - batch 564: 0.9048\n",
            "training loss at step 8 - batch 565: 0.31 (2019-08-04 13:37:54.227078)\n",
            "Accuracy at step 8 - batch 565: 0.9068\n",
            "training loss at step 8 - batch 566: 0.31 (2019-08-04 13:37:54.344795)\n",
            "Accuracy at step 8 - batch 566: 0.9104\n",
            "training loss at step 8 - batch 567: 0.31 (2019-08-04 13:37:54.358000)\n",
            "Accuracy at step 8 - batch 567: 0.908\n",
            "training loss at step 8 - batch 568: 0.32 (2019-08-04 13:37:54.370059)\n",
            "Accuracy at step 8 - batch 568: 0.8996\n",
            "training loss at step 8 - batch 569: 0.31 (2019-08-04 13:37:54.384059)\n",
            "Accuracy at step 8 - batch 569: 0.9076\n",
            "training loss at step 8 - batch 570: 0.29 (2019-08-04 13:37:54.396891)\n",
            "Accuracy at step 8 - batch 570: 0.9152\n",
            "training loss at step 8 - batch 571: 0.28 (2019-08-04 13:37:54.520975)\n",
            "Accuracy at step 8 - batch 571: 0.9172\n",
            "training loss at step 8 - batch 572: 0.29 (2019-08-04 13:37:54.535738)\n",
            "Accuracy at step 8 - batch 572: 0.9084\n",
            "training loss at step 8 - batch 573: 0.31 (2019-08-04 13:37:54.547907)\n",
            "Accuracy at step 8 - batch 573: 0.9068\n",
            "training loss at step 8 - batch 574: 0.31 (2019-08-04 13:37:54.560852)\n",
            "Accuracy at step 8 - batch 574: 0.9092\n",
            "training loss at step 8 - batch 575: 0.30 (2019-08-04 13:37:54.573499)\n",
            "Accuracy at step 8 - batch 575: 0.9084\n",
            "training loss at step 8 - batch 576: 0.31 (2019-08-04 13:37:54.698052)\n",
            "Accuracy at step 8 - batch 576: 0.9052\n",
            "training loss at step 8 - batch 577: 0.31 (2019-08-04 13:37:54.711482)\n",
            "Accuracy at step 8 - batch 577: 0.906\n",
            "training loss at step 8 - batch 578: 0.30 (2019-08-04 13:37:54.726878)\n",
            "Accuracy at step 8 - batch 578: 0.9128\n",
            "training loss at step 8 - batch 579: 0.29 (2019-08-04 13:37:54.740452)\n",
            "Accuracy at step 8 - batch 579: 0.9104\n",
            "training loss at step 8 - batch 580: 0.32 (2019-08-04 13:37:54.752214)\n",
            "Accuracy at step 8 - batch 580: 0.9072\n",
            "training loss at step 8 - batch 581: 0.32 (2019-08-04 13:37:54.875707)\n",
            "Accuracy at step 8 - batch 581: 0.9016\n",
            "training loss at step 8 - batch 582: 0.30 (2019-08-04 13:37:54.892319)\n",
            "Accuracy at step 8 - batch 582: 0.9136\n",
            "training loss at step 8 - batch 583: 0.31 (2019-08-04 13:37:54.906934)\n",
            "Accuracy at step 8 - batch 583: 0.9056\n",
            "training loss at step 8 - batch 584: 0.32 (2019-08-04 13:37:54.920471)\n",
            "Accuracy at step 8 - batch 584: 0.9088\n",
            "training loss at step 8 - batch 585: 0.30 (2019-08-04 13:37:54.936531)\n",
            "Accuracy at step 8 - batch 585: 0.9064\n",
            "training loss at step 8 - batch 586: 0.30 (2019-08-04 13:37:55.055480)\n",
            "Accuracy at step 8 - batch 586: 0.9148\n",
            "training loss at step 8 - batch 587: 0.31 (2019-08-04 13:37:55.068215)\n",
            "Accuracy at step 8 - batch 587: 0.906\n",
            "training loss at step 8 - batch 588: 0.30 (2019-08-04 13:37:55.081040)\n",
            "Accuracy at step 8 - batch 588: 0.9116\n",
            "training loss at step 8 - batch 589: 0.34 (2019-08-04 13:37:55.093687)\n",
            "Accuracy at step 8 - batch 589: 0.9016\n",
            "training loss at step 8 - batch 590: 0.33 (2019-08-04 13:37:55.106159)\n",
            "Accuracy at step 8 - batch 590: 0.9012\n",
            "training loss at step 8 - batch 591: 0.31 (2019-08-04 13:37:55.233426)\n",
            "Accuracy at step 8 - batch 591: 0.9072\n",
            "training loss at step 8 - batch 592: 0.33 (2019-08-04 13:37:55.249105)\n",
            "Accuracy at step 8 - batch 592: 0.9036\n",
            "training loss at step 8 - batch 593: 0.32 (2019-08-04 13:37:55.261834)\n",
            "Accuracy at step 8 - batch 593: 0.904\n",
            "training loss at step 8 - batch 594: 0.32 (2019-08-04 13:37:55.275170)\n",
            "Accuracy at step 8 - batch 594: 0.9072\n",
            "training loss at step 8 - batch 595: 0.29 (2019-08-04 13:37:55.288908)\n",
            "Accuracy at step 8 - batch 595: 0.9104\n",
            "training loss at step 8 - batch 596: 0.31 (2019-08-04 13:37:55.411157)\n",
            "Accuracy at step 8 - batch 596: 0.9064\n",
            "training loss at step 8 - batch 597: 0.29 (2019-08-04 13:37:55.429012)\n",
            "Accuracy at step 8 - batch 597: 0.9116\n",
            "training loss at step 8 - batch 598: 0.30 (2019-08-04 13:37:55.444110)\n",
            "Accuracy at step 8 - batch 598: 0.906\n",
            "training loss at step 8 - batch 599: 0.31 (2019-08-04 13:37:55.456406)\n",
            "Accuracy at step 8 - batch 599: 0.9024\n",
            "training loss at step 8 - batch 600: 0.31 (2019-08-04 13:37:55.469001)\n",
            "Accuracy at step 8 - batch 600: 0.9012\n",
            "training loss at step 8 - batch 601: 0.29 (2019-08-04 13:37:55.583583)\n",
            "Accuracy at step 8 - batch 601: 0.9156\n",
            "training loss at step 8 - batch 602: 0.33 (2019-08-04 13:37:55.596592)\n",
            "Accuracy at step 8 - batch 602: 0.9028\n",
            "training loss at step 8 - batch 603: 0.32 (2019-08-04 13:37:55.610315)\n",
            "Accuracy at step 8 - batch 603: 0.8964\n",
            "training loss at step 8 - batch 604: 0.33 (2019-08-04 13:37:55.623761)\n",
            "Accuracy at step 8 - batch 604: 0.9024\n",
            "training loss at step 8 - batch 605: 0.30 (2019-08-04 13:37:55.636174)\n",
            "Accuracy at step 8 - batch 605: 0.9068\n",
            "training loss at step 8 - batch 606: 0.29 (2019-08-04 13:37:55.796404)\n",
            "Accuracy at step 8 - batch 606: 0.91\n",
            "training loss at step 8 - batch 607: 0.29 (2019-08-04 13:37:55.811670)\n",
            "Accuracy at step 8 - batch 607: 0.9132\n",
            "training loss at step 8 - batch 608: 0.31 (2019-08-04 13:37:55.824721)\n",
            "Accuracy at step 8 - batch 608: 0.9052\n",
            "training loss at step 8 - batch 609: 0.31 (2019-08-04 13:37:55.837744)\n",
            "Accuracy at step 8 - batch 609: 0.9092\n",
            "training loss at step 8 - batch 610: 0.32 (2019-08-04 13:37:55.851492)\n",
            "Accuracy at step 8 - batch 610: 0.9036\n",
            "training loss at step 8 - batch 611: 0.32 (2019-08-04 13:37:55.973077)\n",
            "Accuracy at step 8 - batch 611: 0.9044\n",
            "training loss at step 8 - batch 612: 0.32 (2019-08-04 13:37:55.986842)\n",
            "Accuracy at step 8 - batch 612: 0.906\n",
            "training loss at step 8 - batch 613: 0.31 (2019-08-04 13:37:56.001920)\n",
            "Accuracy at step 8 - batch 613: 0.904\n",
            "training loss at step 8 - batch 614: 0.31 (2019-08-04 13:37:56.016174)\n",
            "Accuracy at step 8 - batch 614: 0.9088\n",
            "training loss at step 8 - batch 615: 0.29 (2019-08-04 13:37:56.029397)\n",
            "Accuracy at step 8 - batch 615: 0.912\n",
            "training loss at step 8 - batch 616: 0.31 (2019-08-04 13:37:56.149178)\n",
            "Accuracy at step 8 - batch 616: 0.904\n",
            "training loss at step 8 - batch 617: 0.31 (2019-08-04 13:37:56.163212)\n",
            "Accuracy at step 8 - batch 617: 0.9084\n",
            "training loss at step 8 - batch 618: 0.32 (2019-08-04 13:37:56.175168)\n",
            "Accuracy at step 8 - batch 618: 0.9072\n",
            "training loss at step 8 - batch 619: 0.30 (2019-08-04 13:37:56.188400)\n",
            "Accuracy at step 8 - batch 619: 0.9108\n",
            "training loss at step 8 - batch 620: 0.31 (2019-08-04 13:37:56.201907)\n",
            "Accuracy at step 8 - batch 620: 0.9112\n",
            "training loss at step 8 - batch 621: 0.31 (2019-08-04 13:37:56.335365)\n",
            "Accuracy at step 8 - batch 621: 0.9048\n",
            "training loss at step 8 - batch 622: 0.34 (2019-08-04 13:37:56.348892)\n",
            "Accuracy at step 8 - batch 622: 0.8956\n",
            "training loss at step 8 - batch 623: 0.29 (2019-08-04 13:37:56.361746)\n",
            "Accuracy at step 8 - batch 623: 0.9124\n",
            "training loss at step 8 - batch 624: 0.30 (2019-08-04 13:37:56.373649)\n",
            "Accuracy at step 8 - batch 624: 0.9076\n",
            "training loss at step 8 - batch 625: 0.34 (2019-08-04 13:37:56.386326)\n",
            "Accuracy at step 8 - batch 625: 0.898\n",
            "training loss at step 8 - batch 626: 0.29 (2019-08-04 13:37:56.509677)\n",
            "Accuracy at step 8 - batch 626: 0.9076\n",
            "training loss at step 8 - batch 627: 0.31 (2019-08-04 13:37:56.526095)\n",
            "Accuracy at step 8 - batch 627: 0.9072\n",
            "training loss at step 8 - batch 628: 0.31 (2019-08-04 13:37:56.539942)\n",
            "Accuracy at step 8 - batch 628: 0.906\n",
            "training loss at step 8 - batch 629: 0.30 (2019-08-04 13:37:56.553074)\n",
            "Accuracy at step 8 - batch 629: 0.9072\n",
            "training loss at step 8 - batch 630: 0.30 (2019-08-04 13:37:56.565382)\n",
            "Accuracy at step 8 - batch 630: 0.9084\n",
            "training loss at step 8 - batch 631: 0.32 (2019-08-04 13:37:56.697873)\n",
            "Accuracy at step 8 - batch 631: 0.9008\n",
            "training loss at step 8 - batch 632: 0.32 (2019-08-04 13:37:56.713017)\n",
            "Accuracy at step 8 - batch 632: 0.9032\n",
            "training loss at step 8 - batch 633: 0.31 (2019-08-04 13:37:56.730412)\n",
            "Accuracy at step 8 - batch 633: 0.9064\n",
            "training loss at step 8 - batch 634: 0.32 (2019-08-04 13:37:56.745019)\n",
            "Accuracy at step 8 - batch 634: 0.9036\n",
            "training loss at step 8 - batch 635: 0.31 (2019-08-04 13:37:56.760435)\n",
            "Accuracy at step 8 - batch 635: 0.9116\n",
            "training loss at step 8 - batch 636: 0.33 (2019-08-04 13:37:56.882699)\n",
            "Accuracy at step 8 - batch 636: 0.906\n",
            "training loss at step 8 - batch 637: 0.32 (2019-08-04 13:37:56.894971)\n",
            "Accuracy at step 8 - batch 637: 0.9028\n",
            "training loss at step 8 - batch 638: 0.30 (2019-08-04 13:37:56.908428)\n",
            "Accuracy at step 8 - batch 638: 0.9064\n",
            "training loss at step 8 - batch 639: 0.32 (2019-08-04 13:37:56.921208)\n",
            "Accuracy at step 8 - batch 639: 0.9076\n",
            "training loss at step 8 - batch 640: 0.29 (2019-08-04 13:37:56.935633)\n",
            "Accuracy at step 8 - batch 640: 0.91\n",
            "training loss at step 8 - batch 641: 0.32 (2019-08-04 13:37:57.060419)\n",
            "Accuracy at step 8 - batch 641: 0.9036\n",
            "training loss at step 8 - batch 642: 0.31 (2019-08-04 13:37:57.074008)\n",
            "Accuracy at step 8 - batch 642: 0.9052\n",
            "training loss at step 8 - batch 643: 0.31 (2019-08-04 13:37:57.086064)\n",
            "Accuracy at step 8 - batch 643: 0.9096\n",
            "training loss at step 8 - batch 644: 0.31 (2019-08-04 13:37:57.099238)\n",
            "Accuracy at step 8 - batch 644: 0.9084\n",
            "training loss at step 8 - batch 645: 0.29 (2019-08-04 13:37:57.112249)\n",
            "Accuracy at step 8 - batch 645: 0.9064\n",
            "training loss at step 8 - batch 646: 0.29 (2019-08-04 13:37:57.233994)\n",
            "Accuracy at step 8 - batch 646: 0.9104\n",
            "training loss at step 8 - batch 647: 0.31 (2019-08-04 13:37:57.246728)\n",
            "Accuracy at step 8 - batch 647: 0.9084\n",
            "training loss at step 8 - batch 648: 0.32 (2019-08-04 13:37:57.260320)\n",
            "Accuracy at step 8 - batch 648: 0.904\n",
            "training loss at step 8 - batch 649: 0.30 (2019-08-04 13:37:57.277007)\n",
            "Accuracy at step 8 - batch 649: 0.9128\n",
            "training loss at step 8 - batch 650: 0.32 (2019-08-04 13:37:57.290187)\n",
            "Accuracy at step 8 - batch 650: 0.9004\n",
            "training loss at step 8 - batch 651: 0.31 (2019-08-04 13:37:57.409775)\n",
            "Accuracy at step 8 - batch 651: 0.9064\n",
            "training loss at step 8 - batch 652: 0.32 (2019-08-04 13:37:57.427989)\n",
            "Accuracy at step 8 - batch 652: 0.9044\n",
            "training loss at step 8 - batch 653: 0.30 (2019-08-04 13:37:57.441244)\n",
            "Accuracy at step 8 - batch 653: 0.9088\n",
            "training loss at step 8 - batch 654: 0.30 (2019-08-04 13:37:57.454517)\n",
            "Accuracy at step 8 - batch 654: 0.9096\n",
            "training loss at step 8 - batch 655: 0.29 (2019-08-04 13:37:57.466644)\n",
            "Accuracy at step 8 - batch 655: 0.9052\n",
            "training loss at step 8 - batch 656: 0.32 (2019-08-04 13:37:57.591196)\n",
            "Accuracy at step 8 - batch 656: 0.9016\n",
            "training loss at step 8 - batch 657: 0.32 (2019-08-04 13:37:57.605483)\n",
            "Accuracy at step 8 - batch 657: 0.9056\n",
            "training loss at step 8 - batch 658: 0.32 (2019-08-04 13:37:57.617798)\n",
            "Accuracy at step 8 - batch 658: 0.904\n",
            "training loss at step 8 - batch 659: 0.31 (2019-08-04 13:37:57.630343)\n",
            "Accuracy at step 8 - batch 659: 0.9024\n",
            "training loss at step 8 - batch 660: 0.31 (2019-08-04 13:37:57.643016)\n",
            "Accuracy at step 8 - batch 660: 0.904\n",
            "training loss at step 8 - batch 661: 0.32 (2019-08-04 13:37:57.771419)\n",
            "Accuracy at step 8 - batch 661: 0.8992\n",
            "training loss at step 8 - batch 662: 0.29 (2019-08-04 13:37:57.785039)\n",
            "Accuracy at step 8 - batch 662: 0.9116\n",
            "training loss at step 8 - batch 663: 0.29 (2019-08-04 13:37:57.800588)\n",
            "Accuracy at step 8 - batch 663: 0.914\n",
            "training loss at step 8 - batch 664: 0.31 (2019-08-04 13:37:57.814366)\n",
            "Accuracy at step 8 - batch 664: 0.906\n",
            "training loss at step 8 - batch 665: 0.30 (2019-08-04 13:37:57.826502)\n",
            "Accuracy at step 8 - batch 665: 0.9104\n",
            "training loss at step 8 - batch 666: 0.30 (2019-08-04 13:37:57.945022)\n",
            "Accuracy at step 8 - batch 666: 0.914\n",
            "training loss at step 8 - batch 667: 0.31 (2019-08-04 13:37:57.958779)\n",
            "Accuracy at step 8 - batch 667: 0.9076\n",
            "training loss at step 8 - batch 668: 0.30 (2019-08-04 13:37:57.972083)\n",
            "Accuracy at step 8 - batch 668: 0.9088\n",
            "training loss at step 8 - batch 669: 0.30 (2019-08-04 13:37:57.985298)\n",
            "Accuracy at step 8 - batch 669: 0.904\n",
            "training loss at step 8 - batch 670: 0.30 (2019-08-04 13:37:57.998150)\n",
            "Accuracy at step 8 - batch 670: 0.9108\n",
            "training loss at step 8 - batch 671: 0.31 (2019-08-04 13:37:58.121337)\n",
            "Accuracy at step 8 - batch 671: 0.9056\n",
            "training loss at step 8 - batch 672: 0.31 (2019-08-04 13:37:58.136066)\n",
            "Accuracy at step 8 - batch 672: 0.9104\n",
            "training loss at step 8 - batch 673: 0.30 (2019-08-04 13:37:58.148135)\n",
            "Accuracy at step 8 - batch 673: 0.9084\n",
            "training loss at step 8 - batch 674: 0.31 (2019-08-04 13:37:58.161050)\n",
            "Accuracy at step 8 - batch 674: 0.9096\n",
            "training loss at step 8 - batch 675: 0.33 (2019-08-04 13:37:58.174312)\n",
            "Accuracy at step 8 - batch 675: 0.9024\n",
            "training loss at step 8 - batch 676: 0.30 (2019-08-04 13:37:58.293329)\n",
            "Accuracy at step 8 - batch 676: 0.9084\n",
            "training loss at step 8 - batch 677: 0.30 (2019-08-04 13:37:58.306840)\n",
            "Accuracy at step 8 - batch 677: 0.9096\n",
            "training loss at step 8 - batch 678: 0.32 (2019-08-04 13:37:58.319343)\n",
            "Accuracy at step 8 - batch 678: 0.904\n",
            "training loss at step 8 - batch 679: 0.31 (2019-08-04 13:37:58.335010)\n",
            "Accuracy at step 8 - batch 679: 0.9112\n",
            "training loss at step 8 - batch 680: 0.29 (2019-08-04 13:37:58.348913)\n",
            "Accuracy at step 8 - batch 680: 0.9108\n",
            "training loss at step 8 - batch 681: 0.31 (2019-08-04 13:37:58.467848)\n",
            "Accuracy at step 8 - batch 681: 0.908\n",
            "training loss at step 8 - batch 682: 0.30 (2019-08-04 13:37:58.483168)\n",
            "Accuracy at step 8 - batch 682: 0.9108\n",
            "training loss at step 8 - batch 683: 0.32 (2019-08-04 13:37:58.495127)\n",
            "Accuracy at step 8 - batch 683: 0.9036\n",
            "training loss at step 8 - batch 684: 0.31 (2019-08-04 13:37:58.508009)\n",
            "Accuracy at step 8 - batch 684: 0.9084\n",
            "training loss at step 8 - batch 685: 0.30 (2019-08-04 13:37:58.520367)\n",
            "Accuracy at step 8 - batch 685: 0.9072\n",
            "training loss at step 8 - batch 686: 0.31 (2019-08-04 13:37:58.642984)\n",
            "Accuracy at step 8 - batch 686: 0.9084\n",
            "training loss at step 8 - batch 687: 0.30 (2019-08-04 13:37:58.655727)\n",
            "Accuracy at step 8 - batch 687: 0.9104\n",
            "training loss at step 8 - batch 688: 0.30 (2019-08-04 13:37:58.668889)\n",
            "Accuracy at step 8 - batch 688: 0.9132\n",
            "training loss at step 8 - batch 689: 0.34 (2019-08-04 13:37:58.681457)\n",
            "Accuracy at step 8 - batch 689: 0.9012\n",
            "training loss at step 8 - batch 690: 0.31 (2019-08-04 13:37:58.695104)\n",
            "Accuracy at step 8 - batch 690: 0.9052\n",
            "training loss at step 8 - batch 691: 0.31 (2019-08-04 13:37:58.824722)\n",
            "Accuracy at step 8 - batch 691: 0.9064\n",
            "training loss at step 8 - batch 692: 0.31 (2019-08-04 13:37:58.839285)\n",
            "Accuracy at step 8 - batch 692: 0.9052\n",
            "training loss at step 8 - batch 693: 0.33 (2019-08-04 13:37:58.854785)\n",
            "Accuracy at step 8 - batch 693: 0.8992\n",
            "training loss at step 8 - batch 694: 0.33 (2019-08-04 13:37:58.867226)\n",
            "Accuracy at step 8 - batch 694: 0.9008\n",
            "training loss at step 8 - batch 695: 0.31 (2019-08-04 13:37:58.879257)\n",
            "Accuracy at step 8 - batch 695: 0.9108\n",
            "training loss at step 8 - batch 696: 0.29 (2019-08-04 13:37:58.997870)\n",
            "Accuracy at step 8 - batch 696: 0.9112\n",
            "training loss at step 8 - batch 697: 0.31 (2019-08-04 13:37:59.011952)\n",
            "Accuracy at step 8 - batch 697: 0.906\n",
            "training loss at step 8 - batch 698: 0.30 (2019-08-04 13:37:59.023989)\n",
            "Accuracy at step 8 - batch 698: 0.9032\n",
            "training loss at step 8 - batch 699: 0.32 (2019-08-04 13:37:59.037221)\n",
            "Accuracy at step 8 - batch 699: 0.906\n",
            "training loss at step 8 - batch 700: 0.31 (2019-08-04 13:37:59.050363)\n",
            "Accuracy at step 8 - batch 700: 0.9104\n",
            "training loss at step 8 - batch 701: 0.29 (2019-08-04 13:37:59.178857)\n",
            "Accuracy at step 8 - batch 701: 0.9136\n",
            "training loss at step 8 - batch 702: 0.30 (2019-08-04 13:37:59.196999)\n",
            "Accuracy at step 8 - batch 702: 0.906\n",
            "training loss at step 8 - batch 703: 0.30 (2019-08-04 13:37:59.210246)\n",
            "Accuracy at step 8 - batch 703: 0.9076\n",
            "training loss at step 8 - batch 704: 0.31 (2019-08-04 13:37:59.223483)\n",
            "Accuracy at step 8 - batch 704: 0.9044\n",
            "training loss at step 8 - batch 705: 0.29 (2019-08-04 13:37:59.236150)\n",
            "Accuracy at step 8 - batch 705: 0.912\n",
            "training loss at step 8 - batch 706: 0.31 (2019-08-04 13:37:59.353536)\n",
            "Accuracy at step 8 - batch 706: 0.9076\n",
            "training loss at step 8 - batch 707: 0.32 (2019-08-04 13:37:59.367339)\n",
            "Accuracy at step 8 - batch 707: 0.9024\n",
            "training loss at step 8 - batch 708: 0.32 (2019-08-04 13:37:59.379554)\n",
            "Accuracy at step 8 - batch 708: 0.908\n",
            "training loss at step 8 - batch 709: 0.31 (2019-08-04 13:37:59.395173)\n",
            "Accuracy at step 8 - batch 709: 0.906\n",
            "training loss at step 8 - batch 710: 0.31 (2019-08-04 13:37:59.408403)\n",
            "Accuracy at step 8 - batch 710: 0.9076\n",
            "training loss at step 8 - batch 711: 0.29 (2019-08-04 13:37:59.530568)\n",
            "Accuracy at step 8 - batch 711: 0.9108\n",
            "training loss at step 8 - batch 712: 0.32 (2019-08-04 13:37:59.546931)\n",
            "Accuracy at step 8 - batch 712: 0.9068\n",
            "training loss at step 8 - batch 713: 0.31 (2019-08-04 13:37:59.559299)\n",
            "Accuracy at step 8 - batch 713: 0.9088\n",
            "training loss at step 8 - batch 714: 0.32 (2019-08-04 13:37:59.571766)\n",
            "Accuracy at step 8 - batch 714: 0.906\n",
            "training loss at step 8 - batch 715: 0.32 (2019-08-04 13:37:59.585608)\n",
            "Accuracy at step 8 - batch 715: 0.904\n",
            "training loss at step 8 - batch 716: 0.32 (2019-08-04 13:37:59.711043)\n",
            "Accuracy at step 8 - batch 716: 0.9056\n",
            "training loss at step 8 - batch 717: 0.32 (2019-08-04 13:37:59.728389)\n",
            "Accuracy at step 8 - batch 717: 0.898\n",
            "training loss at step 8 - batch 718: 0.31 (2019-08-04 13:37:59.742787)\n",
            "Accuracy at step 8 - batch 718: 0.9092\n",
            "training loss at step 8 - batch 719: 0.29 (2019-08-04 13:37:59.757953)\n",
            "Accuracy at step 8 - batch 719: 0.912\n",
            "training loss at step 8 - batch 720: 0.31 (2019-08-04 13:37:59.772785)\n",
            "Accuracy at step 8 - batch 720: 0.9064\n",
            "training loss at step 8 - batch 721: 0.31 (2019-08-04 13:37:59.898360)\n",
            "Accuracy at step 8 - batch 721: 0.9096\n",
            "training loss at step 8 - batch 722: 0.31 (2019-08-04 13:37:59.921107)\n",
            "Accuracy at step 8 - batch 722: 0.9052\n",
            "training loss at step 8 - batch 723: 0.30 (2019-08-04 13:37:59.934909)\n",
            "Accuracy at step 8 - batch 723: 0.9052\n",
            "training loss at step 8 - batch 724: 0.30 (2019-08-04 13:37:59.949436)\n",
            "Accuracy at step 8 - batch 724: 0.9092\n",
            "training loss at step 8 - batch 725: 0.29 (2019-08-04 13:37:59.962284)\n",
            "Accuracy at step 8 - batch 725: 0.9132\n",
            "training loss at step 8 - batch 726: 0.31 (2019-08-04 13:38:00.083148)\n",
            "Accuracy at step 8 - batch 726: 0.9008\n",
            "training loss at step 8 - batch 727: 0.30 (2019-08-04 13:38:00.095648)\n",
            "Accuracy at step 8 - batch 727: 0.9064\n",
            "training loss at step 8 - batch 728: 0.30 (2019-08-04 13:38:00.107870)\n",
            "Accuracy at step 8 - batch 728: 0.91\n",
            "training loss at step 8 - batch 729: 0.32 (2019-08-04 13:38:00.120410)\n",
            "Accuracy at step 8 - batch 729: 0.9064\n",
            "training loss at step 8 - batch 730: 0.31 (2019-08-04 13:38:00.137234)\n",
            "Accuracy at step 8 - batch 730: 0.9036\n",
            "training loss at step 8 - batch 731: 0.30 (2019-08-04 13:38:00.264064)\n",
            "Accuracy at step 8 - batch 731: 0.9096\n",
            "training loss at step 8 - batch 732: 0.33 (2019-08-04 13:38:00.280909)\n",
            "Accuracy at step 8 - batch 732: 0.9028\n",
            "training loss at step 8 - batch 733: 0.31 (2019-08-04 13:38:00.293362)\n",
            "Accuracy at step 8 - batch 733: 0.904\n",
            "training loss at step 8 - batch 734: 0.31 (2019-08-04 13:38:00.305620)\n",
            "Accuracy at step 8 - batch 734: 0.9032\n",
            "training loss at step 8 - batch 735: 0.33 (2019-08-04 13:38:00.317897)\n",
            "Accuracy at step 8 - batch 735: 0.8996\n",
            "training loss at step 8 - batch 736: 0.33 (2019-08-04 13:38:00.444566)\n",
            "Accuracy at step 8 - batch 736: 0.8992\n",
            "training loss at step 8 - batch 737: 0.33 (2019-08-04 13:38:00.457354)\n",
            "Accuracy at step 8 - batch 737: 0.9028\n",
            "training loss at step 8 - batch 738: 0.31 (2019-08-04 13:38:00.469457)\n",
            "Accuracy at step 8 - batch 738: 0.9104\n",
            "training loss at step 8 - batch 739: 0.30 (2019-08-04 13:38:00.482005)\n",
            "Accuracy at step 8 - batch 739: 0.906\n",
            "training loss at step 8 - batch 740: 0.29 (2019-08-04 13:38:00.493671)\n",
            "Accuracy at step 8 - batch 740: 0.9156\n",
            "training loss at step 8 - batch 741: 0.32 (2019-08-04 13:38:00.610452)\n",
            "Accuracy at step 8 - batch 741: 0.8988\n",
            "training loss at step 8 - batch 742: 0.31 (2019-08-04 13:38:00.624116)\n",
            "Accuracy at step 8 - batch 742: 0.9036\n",
            "training loss at step 8 - batch 743: 0.30 (2019-08-04 13:38:00.636550)\n",
            "Accuracy at step 8 - batch 743: 0.906\n",
            "training loss at step 8 - batch 744: 0.33 (2019-08-04 13:38:00.650274)\n",
            "Accuracy at step 8 - batch 744: 0.9036\n",
            "training loss at step 8 - batch 745: 0.29 (2019-08-04 13:38:00.663351)\n",
            "Accuracy at step 8 - batch 745: 0.9084\n",
            "training loss at step 8 - batch 746: 0.29 (2019-08-04 13:38:00.789025)\n",
            "Accuracy at step 8 - batch 746: 0.91\n",
            "training loss at step 8 - batch 747: 0.31 (2019-08-04 13:38:00.804147)\n",
            "Accuracy at step 8 - batch 747: 0.9056\n",
            "training loss at step 8 - batch 748: 0.31 (2019-08-04 13:38:00.818187)\n",
            "Accuracy at step 8 - batch 748: 0.9072\n",
            "training loss at step 8 - batch 749: 0.32 (2019-08-04 13:38:00.833605)\n",
            "Accuracy at step 8 - batch 749: 0.9072\n",
            "training loss at step 8 - batch 750: 0.31 (2019-08-04 13:38:00.846210)\n",
            "Accuracy at step 8 - batch 750: 0.908\n",
            "training loss at step 8 - batch 751: 0.30 (2019-08-04 13:38:00.976030)\n",
            "Accuracy at step 8 - batch 751: 0.9064\n",
            "training loss at step 8 - batch 752: 0.31 (2019-08-04 13:38:00.988550)\n",
            "Accuracy at step 8 - batch 752: 0.9064\n",
            "training loss at step 8 - batch 753: 0.31 (2019-08-04 13:38:01.001102)\n",
            "Accuracy at step 8 - batch 753: 0.912\n",
            "training loss at step 8 - batch 754: 0.32 (2019-08-04 13:38:01.013534)\n",
            "Accuracy at step 8 - batch 754: 0.9056\n",
            "training loss at step 8 - batch 755: 0.28 (2019-08-04 13:38:01.025945)\n",
            "Accuracy at step 8 - batch 755: 0.914\n",
            "training loss at step 8 - batch 756: 0.32 (2019-08-04 13:38:01.144196)\n",
            "Accuracy at step 8 - batch 756: 0.9028\n",
            "training loss at step 8 - batch 757: 0.31 (2019-08-04 13:38:01.161408)\n",
            "Accuracy at step 8 - batch 757: 0.9128\n",
            "training loss at step 8 - batch 758: 0.32 (2019-08-04 13:38:01.174292)\n",
            "Accuracy at step 8 - batch 758: 0.9028\n",
            "training loss at step 8 - batch 759: 0.29 (2019-08-04 13:38:01.189822)\n",
            "Accuracy at step 8 - batch 759: 0.9152\n",
            "training loss at step 8 - batch 760: 0.32 (2019-08-04 13:38:01.202878)\n",
            "Accuracy at step 8 - batch 760: 0.906\n",
            "training loss at step 8 - batch 761: 0.30 (2019-08-04 13:38:01.325035)\n",
            "Accuracy at step 8 - batch 761: 0.908\n",
            "training loss at step 8 - batch 762: 0.31 (2019-08-04 13:38:01.338843)\n",
            "Accuracy at step 8 - batch 762: 0.908\n",
            "training loss at step 8 - batch 763: 0.32 (2019-08-04 13:38:01.353109)\n",
            "Accuracy at step 8 - batch 763: 0.9012\n",
            "training loss at step 8 - batch 764: 0.30 (2019-08-04 13:38:01.366446)\n",
            "Accuracy at step 8 - batch 764: 0.9152\n",
            "training loss at step 8 - batch 765: 0.32 (2019-08-04 13:38:01.378732)\n",
            "Accuracy at step 8 - batch 765: 0.9004\n",
            "training loss at step 8 - batch 766: 0.30 (2019-08-04 13:38:01.499478)\n",
            "Accuracy at step 8 - batch 766: 0.9096\n",
            "training loss at step 8 - batch 767: 0.30 (2019-08-04 13:38:01.513105)\n",
            "Accuracy at step 8 - batch 767: 0.9112\n",
            "training loss at step 8 - batch 768: 0.32 (2019-08-04 13:38:01.526218)\n",
            "Accuracy at step 8 - batch 768: 0.9\n",
            "training loss at step 8 - batch 769: 0.29 (2019-08-04 13:38:01.539753)\n",
            "Accuracy at step 8 - batch 769: 0.9124\n",
            "training loss at step 8 - batch 770: 0.30 (2019-08-04 13:38:01.552318)\n",
            "Accuracy at step 8 - batch 770: 0.9164\n",
            "training loss at step 8 - batch 771: 0.29 (2019-08-04 13:38:01.673475)\n",
            "Accuracy at step 8 - batch 771: 0.9096\n",
            "training loss at step 8 - batch 772: 0.29 (2019-08-04 13:38:01.687576)\n",
            "Accuracy at step 8 - batch 772: 0.91\n",
            "training loss at step 8 - batch 773: 0.30 (2019-08-04 13:38:01.699934)\n",
            "Accuracy at step 8 - batch 773: 0.9104\n",
            "training loss at step 8 - batch 774: 0.30 (2019-08-04 13:38:01.715771)\n",
            "Accuracy at step 8 - batch 774: 0.9092\n",
            "training loss at step 8 - batch 775: 0.34 (2019-08-04 13:38:01.729578)\n",
            "Accuracy at step 8 - batch 775: 0.8976\n",
            "training loss at step 8 - batch 776: 0.30 (2019-08-04 13:38:01.874486)\n",
            "Accuracy at step 8 - batch 776: 0.9116\n",
            "training loss at step 8 - batch 777: 0.31 (2019-08-04 13:38:01.888705)\n",
            "Accuracy at step 8 - batch 777: 0.9132\n",
            "training loss at step 8 - batch 778: 0.30 (2019-08-04 13:38:01.900611)\n",
            "Accuracy at step 8 - batch 778: 0.91\n",
            "training loss at step 8 - batch 779: 0.31 (2019-08-04 13:38:01.913170)\n",
            "Accuracy at step 8 - batch 779: 0.9072\n",
            "training loss at step 9 - batch 0: 0.30 (2019-08-04 13:38:01.929885)\n",
            "Accuracy at step 9 - batch 0: 0.9056\n",
            "training loss at step 9 - batch 1: 0.31 (2019-08-04 13:38:02.051636)\n",
            "Accuracy at step 9 - batch 1: 0.9052\n",
            "training loss at step 9 - batch 2: 0.29 (2019-08-04 13:38:02.066248)\n",
            "Accuracy at step 9 - batch 2: 0.9084\n",
            "training loss at step 9 - batch 3: 0.31 (2019-08-04 13:38:02.079754)\n",
            "Accuracy at step 9 - batch 3: 0.9056\n",
            "training loss at step 9 - batch 4: 0.33 (2019-08-04 13:38:02.093918)\n",
            "Accuracy at step 9 - batch 4: 0.9056\n",
            "training loss at step 9 - batch 5: 0.32 (2019-08-04 13:38:02.107053)\n",
            "Accuracy at step 9 - batch 5: 0.9056\n",
            "training loss at step 9 - batch 6: 0.30 (2019-08-04 13:38:02.233090)\n",
            "Accuracy at step 9 - batch 6: 0.9092\n",
            "training loss at step 9 - batch 7: 0.28 (2019-08-04 13:38:02.247914)\n",
            "Accuracy at step 9 - batch 7: 0.9136\n",
            "training loss at step 9 - batch 8: 0.29 (2019-08-04 13:38:02.260385)\n",
            "Accuracy at step 9 - batch 8: 0.9148\n",
            "training loss at step 9 - batch 9: 0.32 (2019-08-04 13:38:02.272490)\n",
            "Accuracy at step 9 - batch 9: 0.9036\n",
            "training loss at step 9 - batch 10: 0.31 (2019-08-04 13:38:02.284473)\n",
            "Accuracy at step 9 - batch 10: 0.91\n",
            "training loss at step 9 - batch 11: 0.32 (2019-08-04 13:38:02.401013)\n",
            "Accuracy at step 9 - batch 11: 0.9076\n",
            "training loss at step 9 - batch 12: 0.32 (2019-08-04 13:38:02.412943)\n",
            "Accuracy at step 9 - batch 12: 0.9008\n",
            "training loss at step 9 - batch 13: 0.32 (2019-08-04 13:38:02.425434)\n",
            "Accuracy at step 9 - batch 13: 0.902\n",
            "training loss at step 9 - batch 14: 0.30 (2019-08-04 13:38:02.439246)\n",
            "Accuracy at step 9 - batch 14: 0.9092\n",
            "training loss at step 9 - batch 15: 0.31 (2019-08-04 13:38:02.451988)\n",
            "Accuracy at step 9 - batch 15: 0.9088\n",
            "training loss at step 9 - batch 16: 0.33 (2019-08-04 13:38:02.571791)\n",
            "Accuracy at step 9 - batch 16: 0.9036\n",
            "training loss at step 9 - batch 17: 0.30 (2019-08-04 13:38:02.586145)\n",
            "Accuracy at step 9 - batch 17: 0.9124\n",
            "training loss at step 9 - batch 18: 0.29 (2019-08-04 13:38:02.599212)\n",
            "Accuracy at step 9 - batch 18: 0.9136\n",
            "training loss at step 9 - batch 19: 0.32 (2019-08-04 13:38:02.611753)\n",
            "Accuracy at step 9 - batch 19: 0.9076\n",
            "training loss at step 9 - batch 20: 0.29 (2019-08-04 13:38:02.627902)\n",
            "Accuracy at step 9 - batch 20: 0.9128\n",
            "training loss at step 9 - batch 21: 0.31 (2019-08-04 13:38:02.749134)\n",
            "Accuracy at step 9 - batch 21: 0.912\n",
            "training loss at step 9 - batch 22: 0.29 (2019-08-04 13:38:02.762000)\n",
            "Accuracy at step 9 - batch 22: 0.9116\n",
            "training loss at step 9 - batch 23: 0.32 (2019-08-04 13:38:02.774242)\n",
            "Accuracy at step 9 - batch 23: 0.904\n",
            "training loss at step 9 - batch 24: 0.31 (2019-08-04 13:38:02.786331)\n",
            "Accuracy at step 9 - batch 24: 0.91\n",
            "training loss at step 9 - batch 25: 0.31 (2019-08-04 13:38:02.799266)\n",
            "Accuracy at step 9 - batch 25: 0.9072\n",
            "training loss at step 9 - batch 26: 0.32 (2019-08-04 13:38:02.925307)\n",
            "Accuracy at step 9 - batch 26: 0.9016\n",
            "training loss at step 9 - batch 27: 0.29 (2019-08-04 13:38:02.937786)\n",
            "Accuracy at step 9 - batch 27: 0.9116\n",
            "training loss at step 9 - batch 28: 0.32 (2019-08-04 13:38:02.950209)\n",
            "Accuracy at step 9 - batch 28: 0.9052\n",
            "training loss at step 9 - batch 29: 0.32 (2019-08-04 13:38:02.966893)\n",
            "Accuracy at step 9 - batch 29: 0.9092\n",
            "training loss at step 9 - batch 30: 0.29 (2019-08-04 13:38:02.979410)\n",
            "Accuracy at step 9 - batch 30: 0.9084\n",
            "training loss at step 9 - batch 31: 0.30 (2019-08-04 13:38:03.097928)\n",
            "Accuracy at step 9 - batch 31: 0.9084\n",
            "training loss at step 9 - batch 32: 0.28 (2019-08-04 13:38:03.109705)\n",
            "Accuracy at step 9 - batch 32: 0.9156\n",
            "training loss at step 9 - batch 33: 0.32 (2019-08-04 13:38:03.122234)\n",
            "Accuracy at step 9 - batch 33: 0.904\n",
            "training loss at step 9 - batch 34: 0.34 (2019-08-04 13:38:03.134482)\n",
            "Accuracy at step 9 - batch 34: 0.898\n",
            "training loss at step 9 - batch 35: 0.28 (2019-08-04 13:38:03.148840)\n",
            "Accuracy at step 9 - batch 35: 0.9152\n",
            "training loss at step 9 - batch 36: 0.30 (2019-08-04 13:38:03.269766)\n",
            "Accuracy at step 9 - batch 36: 0.9044\n",
            "training loss at step 9 - batch 37: 0.31 (2019-08-04 13:38:03.285624)\n",
            "Accuracy at step 9 - batch 37: 0.9088\n",
            "training loss at step 9 - batch 38: 0.31 (2019-08-04 13:38:03.297500)\n",
            "Accuracy at step 9 - batch 38: 0.9084\n",
            "training loss at step 9 - batch 39: 0.31 (2019-08-04 13:38:03.311111)\n",
            "Accuracy at step 9 - batch 39: 0.9072\n",
            "training loss at step 9 - batch 40: 0.30 (2019-08-04 13:38:03.323014)\n",
            "Accuracy at step 9 - batch 40: 0.9072\n",
            "training loss at step 9 - batch 41: 0.32 (2019-08-04 13:38:03.438825)\n",
            "Accuracy at step 9 - batch 41: 0.9028\n",
            "training loss at step 9 - batch 42: 0.30 (2019-08-04 13:38:03.455406)\n",
            "Accuracy at step 9 - batch 42: 0.9084\n",
            "training loss at step 9 - batch 43: 0.30 (2019-08-04 13:38:03.468178)\n",
            "Accuracy at step 9 - batch 43: 0.9116\n",
            "training loss at step 9 - batch 44: 0.32 (2019-08-04 13:38:03.483538)\n",
            "Accuracy at step 9 - batch 44: 0.9096\n",
            "training loss at step 9 - batch 45: 0.32 (2019-08-04 13:38:03.495818)\n",
            "Accuracy at step 9 - batch 45: 0.9064\n",
            "training loss at step 9 - batch 46: 0.32 (2019-08-04 13:38:03.616013)\n",
            "Accuracy at step 9 - batch 46: 0.9016\n",
            "training loss at step 9 - batch 47: 0.32 (2019-08-04 13:38:03.629930)\n",
            "Accuracy at step 9 - batch 47: 0.9072\n",
            "training loss at step 9 - batch 48: 0.28 (2019-08-04 13:38:03.642403)\n",
            "Accuracy at step 9 - batch 48: 0.9128\n",
            "training loss at step 9 - batch 49: 0.28 (2019-08-04 13:38:03.655079)\n",
            "Accuracy at step 9 - batch 49: 0.9156\n",
            "training loss at step 9 - batch 50: 0.29 (2019-08-04 13:38:03.668332)\n",
            "Accuracy at step 9 - batch 50: 0.9144\n",
            "training loss at step 9 - batch 51: 0.31 (2019-08-04 13:38:03.791075)\n",
            "Accuracy at step 9 - batch 51: 0.9052\n",
            "training loss at step 9 - batch 52: 0.32 (2019-08-04 13:38:03.803667)\n",
            "Accuracy at step 9 - batch 52: 0.9012\n",
            "training loss at step 9 - batch 53: 0.31 (2019-08-04 13:38:03.816063)\n",
            "Accuracy at step 9 - batch 53: 0.908\n",
            "training loss at step 9 - batch 54: 0.34 (2019-08-04 13:38:03.830236)\n",
            "Accuracy at step 9 - batch 54: 0.9\n",
            "training loss at step 9 - batch 55: 0.28 (2019-08-04 13:38:03.845917)\n",
            "Accuracy at step 9 - batch 55: 0.9132\n",
            "training loss at step 9 - batch 56: 0.31 (2019-08-04 13:38:03.970050)\n",
            "Accuracy at step 9 - batch 56: 0.9096\n",
            "training loss at step 9 - batch 57: 0.31 (2019-08-04 13:38:03.984292)\n",
            "Accuracy at step 9 - batch 57: 0.9036\n",
            "training loss at step 9 - batch 58: 0.30 (2019-08-04 13:38:04.000007)\n",
            "Accuracy at step 9 - batch 58: 0.9108\n",
            "training loss at step 9 - batch 59: 0.30 (2019-08-04 13:38:04.014433)\n",
            "Accuracy at step 9 - batch 59: 0.9072\n",
            "training loss at step 9 - batch 60: 0.30 (2019-08-04 13:38:04.029642)\n",
            "Accuracy at step 9 - batch 60: 0.9084\n",
            "training loss at step 9 - batch 61: 0.31 (2019-08-04 13:38:04.153141)\n",
            "Accuracy at step 9 - batch 61: 0.9124\n",
            "training loss at step 9 - batch 62: 0.30 (2019-08-04 13:38:04.169030)\n",
            "Accuracy at step 9 - batch 62: 0.906\n",
            "training loss at step 9 - batch 63: 0.32 (2019-08-04 13:38:04.182242)\n",
            "Accuracy at step 9 - batch 63: 0.9036\n",
            "training loss at step 9 - batch 64: 0.31 (2019-08-04 13:38:04.195353)\n",
            "Accuracy at step 9 - batch 64: 0.91\n",
            "training loss at step 9 - batch 65: 0.34 (2019-08-04 13:38:04.209517)\n",
            "Accuracy at step 9 - batch 65: 0.8996\n",
            "training loss at step 9 - batch 66: 0.32 (2019-08-04 13:38:04.330980)\n",
            "Accuracy at step 9 - batch 66: 0.9028\n",
            "training loss at step 9 - batch 67: 0.33 (2019-08-04 13:38:04.344916)\n",
            "Accuracy at step 9 - batch 67: 0.9008\n",
            "training loss at step 9 - batch 68: 0.31 (2019-08-04 13:38:04.358385)\n",
            "Accuracy at step 9 - batch 68: 0.9096\n",
            "training loss at step 9 - batch 69: 0.32 (2019-08-04 13:38:04.371351)\n",
            "Accuracy at step 9 - batch 69: 0.9024\n",
            "training loss at step 9 - batch 70: 0.32 (2019-08-04 13:38:04.383957)\n",
            "Accuracy at step 9 - batch 70: 0.902\n",
            "training loss at step 9 - batch 71: 0.30 (2019-08-04 13:38:04.509181)\n",
            "Accuracy at step 9 - batch 71: 0.9016\n",
            "training loss at step 9 - batch 72: 0.29 (2019-08-04 13:38:04.521968)\n",
            "Accuracy at step 9 - batch 72: 0.9084\n",
            "training loss at step 9 - batch 73: 0.28 (2019-08-04 13:38:04.534657)\n",
            "Accuracy at step 9 - batch 73: 0.916\n",
            "training loss at step 9 - batch 74: 0.27 (2019-08-04 13:38:04.547484)\n",
            "Accuracy at step 9 - batch 74: 0.9172\n",
            "training loss at step 9 - batch 75: 0.32 (2019-08-04 13:38:04.560133)\n",
            "Accuracy at step 9 - batch 75: 0.9\n",
            "training loss at step 9 - batch 76: 0.30 (2019-08-04 13:38:04.683767)\n",
            "Accuracy at step 9 - batch 76: 0.9088\n",
            "training loss at step 9 - batch 77: 0.32 (2019-08-04 13:38:04.701661)\n",
            "Accuracy at step 9 - batch 77: 0.9072\n",
            "training loss at step 9 - batch 78: 0.30 (2019-08-04 13:38:04.716784)\n",
            "Accuracy at step 9 - batch 78: 0.9124\n",
            "training loss at step 9 - batch 79: 0.32 (2019-08-04 13:38:04.730018)\n",
            "Accuracy at step 9 - batch 79: 0.9072\n",
            "training loss at step 9 - batch 80: 0.31 (2019-08-04 13:38:04.743298)\n",
            "Accuracy at step 9 - batch 80: 0.9064\n",
            "training loss at step 9 - batch 81: 0.31 (2019-08-04 13:38:04.868883)\n",
            "Accuracy at step 9 - batch 81: 0.9076\n",
            "training loss at step 9 - batch 82: 0.32 (2019-08-04 13:38:04.884852)\n",
            "Accuracy at step 9 - batch 82: 0.9036\n",
            "training loss at step 9 - batch 83: 0.31 (2019-08-04 13:38:04.898637)\n",
            "Accuracy at step 9 - batch 83: 0.9064\n",
            "training loss at step 9 - batch 84: 0.31 (2019-08-04 13:38:04.912168)\n",
            "Accuracy at step 9 - batch 84: 0.9072\n",
            "training loss at step 9 - batch 85: 0.29 (2019-08-04 13:38:04.929702)\n",
            "Accuracy at step 9 - batch 85: 0.91\n",
            "training loss at step 9 - batch 86: 0.32 (2019-08-04 13:38:05.056860)\n",
            "Accuracy at step 9 - batch 86: 0.9068\n",
            "training loss at step 9 - batch 87: 0.28 (2019-08-04 13:38:05.069725)\n",
            "Accuracy at step 9 - batch 87: 0.9172\n",
            "training loss at step 9 - batch 88: 0.32 (2019-08-04 13:38:05.082041)\n",
            "Accuracy at step 9 - batch 88: 0.9032\n",
            "training loss at step 9 - batch 89: 0.32 (2019-08-04 13:38:05.095746)\n",
            "Accuracy at step 9 - batch 89: 0.906\n",
            "training loss at step 9 - batch 90: 0.31 (2019-08-04 13:38:05.109326)\n",
            "Accuracy at step 9 - batch 90: 0.91\n",
            "training loss at step 9 - batch 91: 0.32 (2019-08-04 13:38:05.232470)\n",
            "Accuracy at step 9 - batch 91: 0.902\n",
            "training loss at step 9 - batch 92: 0.30 (2019-08-04 13:38:05.246412)\n",
            "Accuracy at step 9 - batch 92: 0.9096\n",
            "training loss at step 9 - batch 93: 0.30 (2019-08-04 13:38:05.258499)\n",
            "Accuracy at step 9 - batch 93: 0.9096\n",
            "training loss at step 9 - batch 94: 0.28 (2019-08-04 13:38:05.270790)\n",
            "Accuracy at step 9 - batch 94: 0.9176\n",
            "training loss at step 9 - batch 95: 0.30 (2019-08-04 13:38:05.283272)\n",
            "Accuracy at step 9 - batch 95: 0.9088\n",
            "training loss at step 9 - batch 96: 0.33 (2019-08-04 13:38:05.404422)\n",
            "Accuracy at step 9 - batch 96: 0.9012\n",
            "training loss at step 9 - batch 97: 0.32 (2019-08-04 13:38:05.418965)\n",
            "Accuracy at step 9 - batch 97: 0.9056\n",
            "training loss at step 9 - batch 98: 0.31 (2019-08-04 13:38:05.431470)\n",
            "Accuracy at step 9 - batch 98: 0.9052\n",
            "training loss at step 9 - batch 99: 0.32 (2019-08-04 13:38:05.450903)\n",
            "Accuracy at step 9 - batch 99: 0.9052\n",
            "training loss at step 9 - batch 100: 0.29 (2019-08-04 13:38:05.462946)\n",
            "Accuracy at step 9 - batch 100: 0.9128\n",
            "training loss at step 9 - batch 101: 0.30 (2019-08-04 13:38:05.579380)\n",
            "Accuracy at step 9 - batch 101: 0.9128\n",
            "training loss at step 9 - batch 102: 0.34 (2019-08-04 13:38:05.592677)\n",
            "Accuracy at step 9 - batch 102: 0.9004\n",
            "training loss at step 9 - batch 103: 0.32 (2019-08-04 13:38:05.606602)\n",
            "Accuracy at step 9 - batch 103: 0.9056\n",
            "training loss at step 9 - batch 104: 0.30 (2019-08-04 13:38:05.619230)\n",
            "Accuracy at step 9 - batch 104: 0.9112\n",
            "training loss at step 9 - batch 105: 0.30 (2019-08-04 13:38:05.632830)\n",
            "Accuracy at step 9 - batch 105: 0.9072\n",
            "training loss at step 9 - batch 106: 0.33 (2019-08-04 13:38:05.757537)\n",
            "Accuracy at step 9 - batch 106: 0.9052\n",
            "training loss at step 9 - batch 107: 0.30 (2019-08-04 13:38:05.769856)\n",
            "Accuracy at step 9 - batch 107: 0.9084\n",
            "training loss at step 9 - batch 108: 0.29 (2019-08-04 13:38:05.783153)\n",
            "Accuracy at step 9 - batch 108: 0.914\n",
            "training loss at step 9 - batch 109: 0.31 (2019-08-04 13:38:05.796087)\n",
            "Accuracy at step 9 - batch 109: 0.9096\n",
            "training loss at step 9 - batch 110: 0.31 (2019-08-04 13:38:05.811138)\n",
            "Accuracy at step 9 - batch 110: 0.908\n",
            "training loss at step 9 - batch 111: 0.29 (2019-08-04 13:38:05.939776)\n",
            "Accuracy at step 9 - batch 111: 0.9068\n",
            "training loss at step 9 - batch 112: 0.33 (2019-08-04 13:38:05.953203)\n",
            "Accuracy at step 9 - batch 112: 0.9036\n",
            "training loss at step 9 - batch 113: 0.32 (2019-08-04 13:38:05.969387)\n",
            "Accuracy at step 9 - batch 113: 0.9056\n",
            "training loss at step 9 - batch 114: 0.29 (2019-08-04 13:38:05.981922)\n",
            "Accuracy at step 9 - batch 114: 0.9104\n",
            "training loss at step 9 - batch 115: 0.31 (2019-08-04 13:38:05.994563)\n",
            "Accuracy at step 9 - batch 115: 0.906\n",
            "training loss at step 9 - batch 116: 0.30 (2019-08-04 13:38:06.115961)\n",
            "Accuracy at step 9 - batch 116: 0.9044\n",
            "training loss at step 9 - batch 117: 0.30 (2019-08-04 13:38:06.130952)\n",
            "Accuracy at step 9 - batch 117: 0.9084\n",
            "training loss at step 9 - batch 118: 0.30 (2019-08-04 13:38:06.144226)\n",
            "Accuracy at step 9 - batch 118: 0.9124\n",
            "training loss at step 9 - batch 119: 0.30 (2019-08-04 13:38:06.156264)\n",
            "Accuracy at step 9 - batch 119: 0.9096\n",
            "training loss at step 9 - batch 120: 0.31 (2019-08-04 13:38:06.178382)\n",
            "Accuracy at step 9 - batch 120: 0.9096\n",
            "training loss at step 9 - batch 121: 0.29 (2019-08-04 13:38:06.308387)\n",
            "Accuracy at step 9 - batch 121: 0.9112\n",
            "training loss at step 9 - batch 122: 0.31 (2019-08-04 13:38:06.322830)\n",
            "Accuracy at step 9 - batch 122: 0.9092\n",
            "training loss at step 9 - batch 123: 0.30 (2019-08-04 13:38:06.337091)\n",
            "Accuracy at step 9 - batch 123: 0.906\n",
            "training loss at step 9 - batch 124: 0.30 (2019-08-04 13:38:06.350044)\n",
            "Accuracy at step 9 - batch 124: 0.912\n",
            "training loss at step 9 - batch 125: 0.32 (2019-08-04 13:38:06.362422)\n",
            "Accuracy at step 9 - batch 125: 0.902\n",
            "training loss at step 9 - batch 126: 0.31 (2019-08-04 13:38:06.482111)\n",
            "Accuracy at step 9 - batch 126: 0.91\n",
            "training loss at step 9 - batch 127: 0.30 (2019-08-04 13:38:06.494482)\n",
            "Accuracy at step 9 - batch 127: 0.9056\n",
            "training loss at step 9 - batch 128: 0.30 (2019-08-04 13:38:06.507011)\n",
            "Accuracy at step 9 - batch 128: 0.9108\n",
            "training loss at step 9 - batch 129: 0.30 (2019-08-04 13:38:06.520323)\n",
            "Accuracy at step 9 - batch 129: 0.9128\n",
            "training loss at step 9 - batch 130: 0.30 (2019-08-04 13:38:06.533468)\n",
            "Accuracy at step 9 - batch 130: 0.912\n",
            "training loss at step 9 - batch 131: 0.33 (2019-08-04 13:38:06.652776)\n",
            "Accuracy at step 9 - batch 131: 0.9004\n",
            "training loss at step 9 - batch 132: 0.31 (2019-08-04 13:38:06.669068)\n",
            "Accuracy at step 9 - batch 132: 0.9008\n",
            "training loss at step 9 - batch 133: 0.29 (2019-08-04 13:38:06.681660)\n",
            "Accuracy at step 9 - batch 133: 0.9152\n",
            "training loss at step 9 - batch 134: 0.31 (2019-08-04 13:38:06.697761)\n",
            "Accuracy at step 9 - batch 134: 0.9116\n",
            "training loss at step 9 - batch 135: 0.32 (2019-08-04 13:38:06.710499)\n",
            "Accuracy at step 9 - batch 135: 0.9036\n",
            "training loss at step 9 - batch 136: 0.32 (2019-08-04 13:38:06.838559)\n",
            "Accuracy at step 9 - batch 136: 0.906\n",
            "training loss at step 9 - batch 137: 0.32 (2019-08-04 13:38:06.857242)\n",
            "Accuracy at step 9 - batch 137: 0.9052\n",
            "training loss at step 9 - batch 138: 0.31 (2019-08-04 13:38:06.870586)\n",
            "Accuracy at step 9 - batch 138: 0.9036\n",
            "training loss at step 9 - batch 139: 0.31 (2019-08-04 13:38:06.884783)\n",
            "Accuracy at step 9 - batch 139: 0.9064\n",
            "training loss at step 9 - batch 140: 0.34 (2019-08-04 13:38:06.899982)\n",
            "Accuracy at step 9 - batch 140: 0.9\n",
            "training loss at step 9 - batch 141: 0.30 (2019-08-04 13:38:07.039397)\n",
            "Accuracy at step 9 - batch 141: 0.9048\n",
            "training loss at step 9 - batch 142: 0.30 (2019-08-04 13:38:07.054688)\n",
            "Accuracy at step 9 - batch 142: 0.91\n",
            "training loss at step 9 - batch 143: 0.32 (2019-08-04 13:38:07.067283)\n",
            "Accuracy at step 9 - batch 143: 0.9056\n",
            "training loss at step 9 - batch 144: 0.31 (2019-08-04 13:38:07.082036)\n",
            "Accuracy at step 9 - batch 144: 0.9004\n",
            "training loss at step 9 - batch 145: 0.31 (2019-08-04 13:38:07.095267)\n",
            "Accuracy at step 9 - batch 145: 0.9068\n",
            "training loss at step 9 - batch 146: 0.31 (2019-08-04 13:38:07.218120)\n",
            "Accuracy at step 9 - batch 146: 0.91\n",
            "training loss at step 9 - batch 147: 0.31 (2019-08-04 13:38:07.234401)\n",
            "Accuracy at step 9 - batch 147: 0.904\n",
            "training loss at step 9 - batch 148: 0.30 (2019-08-04 13:38:07.246207)\n",
            "Accuracy at step 9 - batch 148: 0.908\n",
            "training loss at step 9 - batch 149: 0.30 (2019-08-04 13:38:07.259407)\n",
            "Accuracy at step 9 - batch 149: 0.906\n",
            "training loss at step 9 - batch 150: 0.31 (2019-08-04 13:38:07.271943)\n",
            "Accuracy at step 9 - batch 150: 0.9104\n",
            "training loss at step 9 - batch 151: 0.29 (2019-08-04 13:38:07.390886)\n",
            "Accuracy at step 9 - batch 151: 0.9144\n",
            "training loss at step 9 - batch 152: 0.30 (2019-08-04 13:38:07.403186)\n",
            "Accuracy at step 9 - batch 152: 0.9056\n",
            "training loss at step 9 - batch 153: 0.31 (2019-08-04 13:38:07.416493)\n",
            "Accuracy at step 9 - batch 153: 0.9108\n",
            "training loss at step 9 - batch 154: 0.30 (2019-08-04 13:38:07.432158)\n",
            "Accuracy at step 9 - batch 154: 0.9072\n",
            "training loss at step 9 - batch 155: 0.33 (2019-08-04 13:38:07.444657)\n",
            "Accuracy at step 9 - batch 155: 0.9044\n",
            "training loss at step 9 - batch 156: 0.32 (2019-08-04 13:38:07.565701)\n",
            "Accuracy at step 9 - batch 156: 0.9056\n",
            "training loss at step 9 - batch 157: 0.29 (2019-08-04 13:38:07.582305)\n",
            "Accuracy at step 9 - batch 157: 0.9136\n",
            "training loss at step 9 - batch 158: 0.29 (2019-08-04 13:38:07.596091)\n",
            "Accuracy at step 9 - batch 158: 0.9088\n",
            "training loss at step 9 - batch 159: 0.31 (2019-08-04 13:38:07.608026)\n",
            "Accuracy at step 9 - batch 159: 0.91\n",
            "training loss at step 9 - batch 160: 0.31 (2019-08-04 13:38:07.620401)\n",
            "Accuracy at step 9 - batch 160: 0.91\n",
            "training loss at step 9 - batch 161: 0.32 (2019-08-04 13:38:07.863142)\n",
            "Accuracy at step 9 - batch 161: 0.904\n",
            "training loss at step 9 - batch 162: 0.31 (2019-08-04 13:38:07.879491)\n",
            "Accuracy at step 9 - batch 162: 0.9072\n",
            "training loss at step 9 - batch 163: 0.31 (2019-08-04 13:38:07.892109)\n",
            "Accuracy at step 9 - batch 163: 0.91\n",
            "training loss at step 9 - batch 164: 0.32 (2019-08-04 13:38:07.905201)\n",
            "Accuracy at step 9 - batch 164: 0.906\n",
            "training loss at step 9 - batch 165: 0.30 (2019-08-04 13:38:07.918351)\n",
            "Accuracy at step 9 - batch 165: 0.908\n",
            "training loss at step 9 - batch 166: 0.30 (2019-08-04 13:38:08.048718)\n",
            "Accuracy at step 9 - batch 166: 0.9104\n",
            "training loss at step 9 - batch 167: 0.32 (2019-08-04 13:38:08.068490)\n",
            "Accuracy at step 9 - batch 167: 0.908\n",
            "training loss at step 9 - batch 168: 0.32 (2019-08-04 13:38:08.081579)\n",
            "Accuracy at step 9 - batch 168: 0.9056\n",
            "training loss at step 9 - batch 169: 0.31 (2019-08-04 13:38:08.093506)\n",
            "Accuracy at step 9 - batch 169: 0.9092\n",
            "training loss at step 9 - batch 170: 0.33 (2019-08-04 13:38:08.105412)\n",
            "Accuracy at step 9 - batch 170: 0.9016\n",
            "training loss at step 9 - batch 171: 0.31 (2019-08-04 13:38:08.227670)\n",
            "Accuracy at step 9 - batch 171: 0.9076\n",
            "training loss at step 9 - batch 172: 0.30 (2019-08-04 13:38:08.243971)\n",
            "Accuracy at step 9 - batch 172: 0.9136\n",
            "training loss at step 9 - batch 173: 0.31 (2019-08-04 13:38:08.255760)\n",
            "Accuracy at step 9 - batch 173: 0.904\n",
            "training loss at step 9 - batch 174: 0.30 (2019-08-04 13:38:08.268410)\n",
            "Accuracy at step 9 - batch 174: 0.9112\n",
            "training loss at step 9 - batch 175: 0.31 (2019-08-04 13:38:08.285651)\n",
            "Accuracy at step 9 - batch 175: 0.9028\n",
            "training loss at step 9 - batch 176: 0.29 (2019-08-04 13:38:08.400772)\n",
            "Accuracy at step 9 - batch 176: 0.91\n",
            "training loss at step 9 - batch 177: 0.31 (2019-08-04 13:38:08.412380)\n",
            "Accuracy at step 9 - batch 177: 0.9064\n",
            "training loss at step 9 - batch 178: 0.30 (2019-08-04 13:38:08.425549)\n",
            "Accuracy at step 9 - batch 178: 0.9084\n",
            "training loss at step 9 - batch 179: 0.31 (2019-08-04 13:38:08.440105)\n",
            "Accuracy at step 9 - batch 179: 0.9092\n",
            "training loss at step 9 - batch 180: 0.31 (2019-08-04 13:38:08.452780)\n",
            "Accuracy at step 9 - batch 180: 0.908\n",
            "training loss at step 9 - batch 181: 0.30 (2019-08-04 13:38:08.577022)\n",
            "Accuracy at step 9 - batch 181: 0.9048\n",
            "training loss at step 9 - batch 182: 0.30 (2019-08-04 13:38:08.594828)\n",
            "Accuracy at step 9 - batch 182: 0.91\n",
            "training loss at step 9 - batch 183: 0.29 (2019-08-04 13:38:08.607215)\n",
            "Accuracy at step 9 - batch 183: 0.9116\n",
            "training loss at step 9 - batch 184: 0.31 (2019-08-04 13:38:08.620065)\n",
            "Accuracy at step 9 - batch 184: 0.902\n",
            "training loss at step 9 - batch 185: 0.32 (2019-08-04 13:38:08.632363)\n",
            "Accuracy at step 9 - batch 185: 0.9088\n",
            "training loss at step 9 - batch 186: 0.31 (2019-08-04 13:38:08.750136)\n",
            "Accuracy at step 9 - batch 186: 0.904\n",
            "training loss at step 9 - batch 187: 0.31 (2019-08-04 13:38:08.764719)\n",
            "Accuracy at step 9 - batch 187: 0.9052\n",
            "training loss at step 9 - batch 188: 0.28 (2019-08-04 13:38:08.777921)\n",
            "Accuracy at step 9 - batch 188: 0.9136\n",
            "training loss at step 9 - batch 189: 0.29 (2019-08-04 13:38:08.795083)\n",
            "Accuracy at step 9 - batch 189: 0.9104\n",
            "training loss at step 9 - batch 190: 0.32 (2019-08-04 13:38:08.806870)\n",
            "Accuracy at step 9 - batch 190: 0.898\n",
            "training loss at step 9 - batch 191: 0.32 (2019-08-04 13:38:08.924949)\n",
            "Accuracy at step 9 - batch 191: 0.9064\n",
            "training loss at step 9 - batch 192: 0.32 (2019-08-04 13:38:08.938613)\n",
            "Accuracy at step 9 - batch 192: 0.9044\n",
            "training loss at step 9 - batch 193: 0.34 (2019-08-04 13:38:08.953088)\n",
            "Accuracy at step 9 - batch 193: 0.8996\n",
            "training loss at step 9 - batch 194: 0.30 (2019-08-04 13:38:08.972216)\n",
            "Accuracy at step 9 - batch 194: 0.9076\n",
            "training loss at step 9 - batch 195: 0.31 (2019-08-04 13:38:08.986909)\n",
            "Accuracy at step 9 - batch 195: 0.9076\n",
            "training loss at step 9 - batch 196: 0.33 (2019-08-04 13:38:09.114917)\n",
            "Accuracy at step 9 - batch 196: 0.902\n",
            "training loss at step 9 - batch 197: 0.30 (2019-08-04 13:38:09.130631)\n",
            "Accuracy at step 9 - batch 197: 0.9092\n",
            "training loss at step 9 - batch 198: 0.33 (2019-08-04 13:38:09.143031)\n",
            "Accuracy at step 9 - batch 198: 0.8964\n",
            "training loss at step 9 - batch 199: 0.29 (2019-08-04 13:38:09.156344)\n",
            "Accuracy at step 9 - batch 199: 0.9152\n",
            "training loss at step 9 - batch 200: 0.31 (2019-08-04 13:38:09.169382)\n",
            "Accuracy at step 9 - batch 200: 0.9032\n",
            "training loss at step 9 - batch 201: 0.29 (2019-08-04 13:38:09.294369)\n",
            "Accuracy at step 9 - batch 201: 0.9116\n",
            "training loss at step 9 - batch 202: 0.31 (2019-08-04 13:38:09.310845)\n",
            "Accuracy at step 9 - batch 202: 0.9084\n",
            "training loss at step 9 - batch 203: 0.31 (2019-08-04 13:38:09.324225)\n",
            "Accuracy at step 9 - batch 203: 0.9016\n",
            "training loss at step 9 - batch 204: 0.32 (2019-08-04 13:38:09.336926)\n",
            "Accuracy at step 9 - batch 204: 0.9036\n",
            "training loss at step 9 - batch 205: 0.32 (2019-08-04 13:38:09.349181)\n",
            "Accuracy at step 9 - batch 205: 0.9032\n",
            "training loss at step 9 - batch 206: 0.31 (2019-08-04 13:38:09.463682)\n",
            "Accuracy at step 9 - batch 206: 0.9072\n",
            "training loss at step 9 - batch 207: 0.33 (2019-08-04 13:38:09.476208)\n",
            "Accuracy at step 9 - batch 207: 0.9008\n",
            "training loss at step 9 - batch 208: 0.31 (2019-08-04 13:38:09.488336)\n",
            "Accuracy at step 9 - batch 208: 0.906\n",
            "training loss at step 9 - batch 209: 0.31 (2019-08-04 13:38:09.502761)\n",
            "Accuracy at step 9 - batch 209: 0.9068\n",
            "training loss at step 9 - batch 210: 0.29 (2019-08-04 13:38:09.515166)\n",
            "Accuracy at step 9 - batch 210: 0.9084\n",
            "training loss at step 9 - batch 211: 0.29 (2019-08-04 13:38:09.633833)\n",
            "Accuracy at step 9 - batch 211: 0.912\n",
            "training loss at step 9 - batch 212: 0.31 (2019-08-04 13:38:09.648990)\n",
            "Accuracy at step 9 - batch 212: 0.9056\n",
            "training loss at step 9 - batch 213: 0.33 (2019-08-04 13:38:09.663002)\n",
            "Accuracy at step 9 - batch 213: 0.9008\n",
            "training loss at step 9 - batch 214: 0.28 (2019-08-04 13:38:09.676506)\n",
            "Accuracy at step 9 - batch 214: 0.9168\n",
            "training loss at step 9 - batch 215: 0.30 (2019-08-04 13:38:09.688437)\n",
            "Accuracy at step 9 - batch 215: 0.908\n",
            "training loss at step 9 - batch 216: 0.28 (2019-08-04 13:38:09.813644)\n",
            "Accuracy at step 9 - batch 216: 0.9156\n",
            "training loss at step 9 - batch 217: 0.29 (2019-08-04 13:38:09.828785)\n",
            "Accuracy at step 9 - batch 217: 0.9148\n",
            "training loss at step 9 - batch 218: 0.31 (2019-08-04 13:38:09.841780)\n",
            "Accuracy at step 9 - batch 218: 0.9048\n",
            "training loss at step 9 - batch 219: 0.30 (2019-08-04 13:38:09.854585)\n",
            "Accuracy at step 9 - batch 219: 0.91\n",
            "training loss at step 9 - batch 220: 0.29 (2019-08-04 13:38:09.868155)\n",
            "Accuracy at step 9 - batch 220: 0.9172\n",
            "training loss at step 9 - batch 221: 0.30 (2019-08-04 13:38:09.995459)\n",
            "Accuracy at step 9 - batch 221: 0.9068\n",
            "training loss at step 9 - batch 222: 0.30 (2019-08-04 13:38:10.008995)\n",
            "Accuracy at step 9 - batch 222: 0.9104\n",
            "training loss at step 9 - batch 223: 0.30 (2019-08-04 13:38:10.024975)\n",
            "Accuracy at step 9 - batch 223: 0.9084\n",
            "training loss at step 9 - batch 224: 0.30 (2019-08-04 13:38:10.038954)\n",
            "Accuracy at step 9 - batch 224: 0.9068\n",
            "training loss at step 9 - batch 225: 0.31 (2019-08-04 13:38:10.051014)\n",
            "Accuracy at step 9 - batch 225: 0.9048\n",
            "training loss at step 9 - batch 226: 0.30 (2019-08-04 13:38:10.173263)\n",
            "Accuracy at step 9 - batch 226: 0.9092\n",
            "training loss at step 9 - batch 227: 0.31 (2019-08-04 13:38:10.187267)\n",
            "Accuracy at step 9 - batch 227: 0.9048\n",
            "training loss at step 9 - batch 228: 0.33 (2019-08-04 13:38:10.200069)\n",
            "Accuracy at step 9 - batch 228: 0.904\n",
            "training loss at step 9 - batch 229: 0.30 (2019-08-04 13:38:10.213255)\n",
            "Accuracy at step 9 - batch 229: 0.906\n",
            "training loss at step 9 - batch 230: 0.32 (2019-08-04 13:38:10.225196)\n",
            "Accuracy at step 9 - batch 230: 0.9096\n",
            "training loss at step 9 - batch 231: 0.31 (2019-08-04 13:38:10.346153)\n",
            "Accuracy at step 9 - batch 231: 0.9052\n",
            "training loss at step 9 - batch 232: 0.32 (2019-08-04 13:38:10.360715)\n",
            "Accuracy at step 9 - batch 232: 0.9032\n",
            "training loss at step 9 - batch 233: 0.33 (2019-08-04 13:38:10.373357)\n",
            "Accuracy at step 9 - batch 233: 0.9024\n",
            "training loss at step 9 - batch 234: 0.30 (2019-08-04 13:38:10.385345)\n",
            "Accuracy at step 9 - batch 234: 0.9144\n",
            "training loss at step 9 - batch 235: 0.30 (2019-08-04 13:38:10.398233)\n",
            "Accuracy at step 9 - batch 235: 0.9092\n",
            "training loss at step 9 - batch 236: 0.32 (2019-08-04 13:38:10.522420)\n",
            "Accuracy at step 9 - batch 236: 0.9064\n",
            "training loss at step 9 - batch 237: 0.31 (2019-08-04 13:38:10.537494)\n",
            "Accuracy at step 9 - batch 237: 0.9084\n",
            "training loss at step 9 - batch 238: 0.30 (2019-08-04 13:38:10.550323)\n",
            "Accuracy at step 9 - batch 238: 0.9132\n",
            "training loss at step 9 - batch 239: 0.31 (2019-08-04 13:38:10.562414)\n",
            "Accuracy at step 9 - batch 239: 0.9116\n",
            "training loss at step 9 - batch 240: 0.31 (2019-08-04 13:38:10.574201)\n",
            "Accuracy at step 9 - batch 240: 0.9068\n",
            "training loss at step 9 - batch 241: 0.31 (2019-08-04 13:38:10.692767)\n",
            "Accuracy at step 9 - batch 241: 0.9032\n",
            "training loss at step 9 - batch 242: 0.30 (2019-08-04 13:38:10.706161)\n",
            "Accuracy at step 9 - batch 242: 0.9084\n",
            "training loss at step 9 - batch 243: 0.28 (2019-08-04 13:38:10.719306)\n",
            "Accuracy at step 9 - batch 243: 0.9136\n",
            "training loss at step 9 - batch 244: 0.29 (2019-08-04 13:38:10.734005)\n",
            "Accuracy at step 9 - batch 244: 0.9148\n",
            "training loss at step 9 - batch 245: 0.29 (2019-08-04 13:38:10.746892)\n",
            "Accuracy at step 9 - batch 245: 0.9168\n",
            "training loss at step 9 - batch 246: 0.31 (2019-08-04 13:38:10.866270)\n",
            "Accuracy at step 9 - batch 246: 0.91\n",
            "training loss at step 9 - batch 247: 0.29 (2019-08-04 13:38:10.881434)\n",
            "Accuracy at step 9 - batch 247: 0.9132\n",
            "training loss at step 9 - batch 248: 0.31 (2019-08-04 13:38:10.894485)\n",
            "Accuracy at step 9 - batch 248: 0.9064\n",
            "training loss at step 9 - batch 249: 0.28 (2019-08-04 13:38:10.906593)\n",
            "Accuracy at step 9 - batch 249: 0.912\n",
            "training loss at step 9 - batch 250: 0.34 (2019-08-04 13:38:10.918397)\n",
            "Accuracy at step 9 - batch 250: 0.8976\n",
            "training loss at step 9 - batch 251: 0.30 (2019-08-04 13:38:11.048884)\n",
            "Accuracy at step 9 - batch 251: 0.9112\n",
            "training loss at step 9 - batch 252: 0.32 (2019-08-04 13:38:11.062303)\n",
            "Accuracy at step 9 - batch 252: 0.9096\n",
            "training loss at step 9 - batch 253: 0.33 (2019-08-04 13:38:11.074643)\n",
            "Accuracy at step 9 - batch 253: 0.9072\n",
            "training loss at step 9 - batch 254: 0.30 (2019-08-04 13:38:11.086525)\n",
            "Accuracy at step 9 - batch 254: 0.9096\n",
            "training loss at step 9 - batch 255: 0.28 (2019-08-04 13:38:11.098818)\n",
            "Accuracy at step 9 - batch 255: 0.914\n",
            "training loss at step 9 - batch 256: 0.31 (2019-08-04 13:38:11.214843)\n",
            "Accuracy at step 9 - batch 256: 0.9048\n",
            "training loss at step 9 - batch 257: 0.31 (2019-08-04 13:38:11.231381)\n",
            "Accuracy at step 9 - batch 257: 0.9072\n",
            "training loss at step 9 - batch 258: 0.31 (2019-08-04 13:38:11.246106)\n",
            "Accuracy at step 9 - batch 258: 0.9072\n",
            "training loss at step 9 - batch 259: 0.28 (2019-08-04 13:38:11.260264)\n",
            "Accuracy at step 9 - batch 259: 0.9136\n",
            "training loss at step 9 - batch 260: 0.31 (2019-08-04 13:38:11.272887)\n",
            "Accuracy at step 9 - batch 260: 0.9104\n",
            "training loss at step 9 - batch 261: 0.31 (2019-08-04 13:38:11.390731)\n",
            "Accuracy at step 9 - batch 261: 0.9092\n",
            "training loss at step 9 - batch 262: 0.30 (2019-08-04 13:38:11.403359)\n",
            "Accuracy at step 9 - batch 262: 0.9112\n",
            "training loss at step 9 - batch 263: 0.31 (2019-08-04 13:38:11.415335)\n",
            "Accuracy at step 9 - batch 263: 0.9012\n",
            "training loss at step 9 - batch 264: 0.32 (2019-08-04 13:38:11.428050)\n",
            "Accuracy at step 9 - batch 264: 0.9068\n",
            "training loss at step 9 - batch 265: 0.31 (2019-08-04 13:38:11.441200)\n",
            "Accuracy at step 9 - batch 265: 0.9032\n",
            "training loss at step 9 - batch 266: 0.31 (2019-08-04 13:38:11.566557)\n",
            "Accuracy at step 9 - batch 266: 0.9116\n",
            "training loss at step 9 - batch 267: 0.31 (2019-08-04 13:38:11.579386)\n",
            "Accuracy at step 9 - batch 267: 0.9056\n",
            "training loss at step 9 - batch 268: 0.33 (2019-08-04 13:38:11.592071)\n",
            "Accuracy at step 9 - batch 268: 0.904\n",
            "training loss at step 9 - batch 269: 0.32 (2019-08-04 13:38:11.605436)\n",
            "Accuracy at step 9 - batch 269: 0.9044\n",
            "training loss at step 9 - batch 270: 0.27 (2019-08-04 13:38:11.618131)\n",
            "Accuracy at step 9 - batch 270: 0.9148\n",
            "training loss at step 9 - batch 271: 0.30 (2019-08-04 13:38:11.736062)\n",
            "Accuracy at step 9 - batch 271: 0.9116\n",
            "training loss at step 9 - batch 272: 0.32 (2019-08-04 13:38:11.750071)\n",
            "Accuracy at step 9 - batch 272: 0.9044\n",
            "training loss at step 9 - batch 273: 0.30 (2019-08-04 13:38:11.764253)\n",
            "Accuracy at step 9 - batch 273: 0.9092\n",
            "training loss at step 9 - batch 274: 0.32 (2019-08-04 13:38:11.780393)\n",
            "Accuracy at step 9 - batch 274: 0.9076\n",
            "training loss at step 9 - batch 275: 0.30 (2019-08-04 13:38:11.793037)\n",
            "Accuracy at step 9 - batch 275: 0.9076\n",
            "training loss at step 9 - batch 276: 0.30 (2019-08-04 13:38:11.917175)\n",
            "Accuracy at step 9 - batch 276: 0.9072\n",
            "training loss at step 9 - batch 277: 0.31 (2019-08-04 13:38:11.931251)\n",
            "Accuracy at step 9 - batch 277: 0.9116\n",
            "training loss at step 9 - batch 278: 0.31 (2019-08-04 13:38:11.946090)\n",
            "Accuracy at step 9 - batch 278: 0.9076\n",
            "training loss at step 9 - batch 279: 0.30 (2019-08-04 13:38:11.960386)\n",
            "Accuracy at step 9 - batch 279: 0.9144\n",
            "training loss at step 9 - batch 280: 0.29 (2019-08-04 13:38:11.974075)\n",
            "Accuracy at step 9 - batch 280: 0.9132\n",
            "training loss at step 9 - batch 281: 0.30 (2019-08-04 13:38:12.113579)\n",
            "Accuracy at step 9 - batch 281: 0.9136\n",
            "training loss at step 9 - batch 282: 0.30 (2019-08-04 13:38:12.125610)\n",
            "Accuracy at step 9 - batch 282: 0.908\n",
            "training loss at step 9 - batch 283: 0.33 (2019-08-04 13:38:12.140511)\n",
            "Accuracy at step 9 - batch 283: 0.9024\n",
            "training loss at step 9 - batch 284: 0.31 (2019-08-04 13:38:12.152991)\n",
            "Accuracy at step 9 - batch 284: 0.9076\n",
            "training loss at step 9 - batch 285: 0.30 (2019-08-04 13:38:12.165871)\n",
            "Accuracy at step 9 - batch 285: 0.9132\n",
            "training loss at step 9 - batch 286: 0.31 (2019-08-04 13:38:12.283939)\n",
            "Accuracy at step 9 - batch 286: 0.91\n",
            "training loss at step 9 - batch 287: 0.28 (2019-08-04 13:38:12.299699)\n",
            "Accuracy at step 9 - batch 287: 0.9172\n",
            "training loss at step 9 - batch 288: 0.30 (2019-08-04 13:38:12.311732)\n",
            "Accuracy at step 9 - batch 288: 0.9088\n",
            "training loss at step 9 - batch 289: 0.28 (2019-08-04 13:38:12.326486)\n",
            "Accuracy at step 9 - batch 289: 0.9168\n",
            "training loss at step 9 - batch 290: 0.33 (2019-08-04 13:38:12.340338)\n",
            "Accuracy at step 9 - batch 290: 0.9068\n",
            "training loss at step 9 - batch 291: 0.33 (2019-08-04 13:38:12.459020)\n",
            "Accuracy at step 9 - batch 291: 0.9028\n",
            "training loss at step 9 - batch 292: 0.32 (2019-08-04 13:38:12.476537)\n",
            "Accuracy at step 9 - batch 292: 0.9004\n",
            "training loss at step 9 - batch 293: 0.32 (2019-08-04 13:38:12.489797)\n",
            "Accuracy at step 9 - batch 293: 0.9084\n",
            "training loss at step 9 - batch 294: 0.29 (2019-08-04 13:38:12.502354)\n",
            "Accuracy at step 9 - batch 294: 0.9148\n",
            "training loss at step 9 - batch 295: 0.30 (2019-08-04 13:38:12.516834)\n",
            "Accuracy at step 9 - batch 295: 0.9044\n",
            "training loss at step 9 - batch 296: 0.31 (2019-08-04 13:38:12.639697)\n",
            "Accuracy at step 9 - batch 296: 0.9076\n",
            "training loss at step 9 - batch 297: 0.32 (2019-08-04 13:38:12.651342)\n",
            "Accuracy at step 9 - batch 297: 0.9048\n",
            "training loss at step 9 - batch 298: 0.30 (2019-08-04 13:38:12.664350)\n",
            "Accuracy at step 9 - batch 298: 0.9088\n",
            "training loss at step 9 - batch 299: 0.32 (2019-08-04 13:38:12.676896)\n",
            "Accuracy at step 9 - batch 299: 0.904\n",
            "training loss at step 9 - batch 300: 0.31 (2019-08-04 13:38:12.689193)\n",
            "Accuracy at step 9 - batch 300: 0.9032\n",
            "training loss at step 9 - batch 301: 0.29 (2019-08-04 13:38:12.807850)\n",
            "Accuracy at step 9 - batch 301: 0.9164\n",
            "training loss at step 9 - batch 302: 0.31 (2019-08-04 13:38:12.824552)\n",
            "Accuracy at step 9 - batch 302: 0.9092\n",
            "training loss at step 9 - batch 303: 0.29 (2019-08-04 13:38:12.836769)\n",
            "Accuracy at step 9 - batch 303: 0.9128\n",
            "training loss at step 9 - batch 304: 0.32 (2019-08-04 13:38:12.853314)\n",
            "Accuracy at step 9 - batch 304: 0.9044\n",
            "training loss at step 9 - batch 305: 0.31 (2019-08-04 13:38:12.866459)\n",
            "Accuracy at step 9 - batch 305: 0.904\n",
            "training loss at step 9 - batch 306: 0.30 (2019-08-04 13:38:12.987123)\n",
            "Accuracy at step 9 - batch 306: 0.9092\n",
            "training loss at step 9 - batch 307: 0.32 (2019-08-04 13:38:12.999438)\n",
            "Accuracy at step 9 - batch 307: 0.9032\n",
            "training loss at step 9 - batch 308: 0.31 (2019-08-04 13:38:13.011657)\n",
            "Accuracy at step 9 - batch 308: 0.9016\n",
            "training loss at step 9 - batch 309: 0.30 (2019-08-04 13:38:13.024061)\n",
            "Accuracy at step 9 - batch 309: 0.9132\n",
            "training loss at step 9 - batch 310: 0.30 (2019-08-04 13:38:13.037257)\n",
            "Accuracy at step 9 - batch 310: 0.9084\n",
            "training loss at step 9 - batch 311: 0.28 (2019-08-04 13:38:13.163256)\n",
            "Accuracy at step 9 - batch 311: 0.9128\n",
            "training loss at step 9 - batch 312: 0.30 (2019-08-04 13:38:13.177747)\n",
            "Accuracy at step 9 - batch 312: 0.9084\n",
            "training loss at step 9 - batch 313: 0.29 (2019-08-04 13:38:13.191527)\n",
            "Accuracy at step 9 - batch 313: 0.906\n",
            "training loss at step 9 - batch 314: 0.30 (2019-08-04 13:38:13.206073)\n",
            "Accuracy at step 9 - batch 314: 0.9084\n",
            "training loss at step 9 - batch 315: 0.29 (2019-08-04 13:38:13.220295)\n",
            "Accuracy at step 9 - batch 315: 0.9148\n",
            "training loss at step 9 - batch 316: 0.29 (2019-08-04 13:38:13.336345)\n",
            "Accuracy at step 9 - batch 316: 0.9128\n",
            "training loss at step 9 - batch 317: 0.32 (2019-08-04 13:38:13.350422)\n",
            "Accuracy at step 9 - batch 317: 0.9008\n",
            "training loss at step 9 - batch 318: 0.30 (2019-08-04 13:38:13.362693)\n",
            "Accuracy at step 9 - batch 318: 0.9052\n",
            "training loss at step 9 - batch 319: 0.31 (2019-08-04 13:38:13.378169)\n",
            "Accuracy at step 9 - batch 319: 0.9048\n",
            "training loss at step 9 - batch 320: 0.32 (2019-08-04 13:38:13.390616)\n",
            "Accuracy at step 9 - batch 320: 0.9024\n",
            "training loss at step 9 - batch 321: 0.30 (2019-08-04 13:38:13.510145)\n",
            "Accuracy at step 9 - batch 321: 0.9064\n",
            "training loss at step 9 - batch 322: 0.33 (2019-08-04 13:38:13.525784)\n",
            "Accuracy at step 9 - batch 322: 0.8984\n",
            "training loss at step 9 - batch 323: 0.29 (2019-08-04 13:38:13.539285)\n",
            "Accuracy at step 9 - batch 323: 0.91\n",
            "training loss at step 9 - batch 324: 0.32 (2019-08-04 13:38:13.552583)\n",
            "Accuracy at step 9 - batch 324: 0.9064\n",
            "training loss at step 9 - batch 325: 0.31 (2019-08-04 13:38:13.564856)\n",
            "Accuracy at step 9 - batch 325: 0.9168\n",
            "training loss at step 9 - batch 326: 0.34 (2019-08-04 13:38:13.687153)\n",
            "Accuracy at step 9 - batch 326: 0.9028\n",
            "training loss at step 9 - batch 327: 0.30 (2019-08-04 13:38:13.703850)\n",
            "Accuracy at step 9 - batch 327: 0.9092\n",
            "training loss at step 9 - batch 328: 0.29 (2019-08-04 13:38:13.715667)\n",
            "Accuracy at step 9 - batch 328: 0.9116\n",
            "training loss at step 9 - batch 329: 0.32 (2019-08-04 13:38:13.728077)\n",
            "Accuracy at step 9 - batch 329: 0.9032\n",
            "training loss at step 9 - batch 330: 0.31 (2019-08-04 13:38:13.741249)\n",
            "Accuracy at step 9 - batch 330: 0.9048\n",
            "training loss at step 9 - batch 331: 0.30 (2019-08-04 13:38:13.864693)\n",
            "Accuracy at step 9 - batch 331: 0.9032\n",
            "training loss at step 9 - batch 332: 0.30 (2019-08-04 13:38:13.881009)\n",
            "Accuracy at step 9 - batch 332: 0.906\n",
            "training loss at step 9 - batch 333: 0.30 (2019-08-04 13:38:13.895681)\n",
            "Accuracy at step 9 - batch 333: 0.9128\n",
            "training loss at step 9 - batch 334: 0.32 (2019-08-04 13:38:13.908221)\n",
            "Accuracy at step 9 - batch 334: 0.9028\n",
            "training loss at step 9 - batch 335: 0.31 (2019-08-04 13:38:13.920591)\n",
            "Accuracy at step 9 - batch 335: 0.9088\n",
            "training loss at step 9 - batch 336: 0.31 (2019-08-04 13:38:14.043286)\n",
            "Accuracy at step 9 - batch 336: 0.906\n",
            "training loss at step 9 - batch 337: 0.30 (2019-08-04 13:38:14.060699)\n",
            "Accuracy at step 9 - batch 337: 0.9084\n",
            "training loss at step 9 - batch 338: 0.30 (2019-08-04 13:38:14.079427)\n",
            "Accuracy at step 9 - batch 338: 0.9112\n",
            "training loss at step 9 - batch 339: 0.32 (2019-08-04 13:38:14.097659)\n",
            "Accuracy at step 9 - batch 339: 0.908\n",
            "training loss at step 9 - batch 340: 0.29 (2019-08-04 13:38:14.112559)\n",
            "Accuracy at step 9 - batch 340: 0.9124\n",
            "training loss at step 9 - batch 341: 0.31 (2019-08-04 13:38:14.239050)\n",
            "Accuracy at step 9 - batch 341: 0.9108\n",
            "training loss at step 9 - batch 342: 0.31 (2019-08-04 13:38:14.251747)\n",
            "Accuracy at step 9 - batch 342: 0.9124\n",
            "training loss at step 9 - batch 343: 0.32 (2019-08-04 13:38:14.266366)\n",
            "Accuracy at step 9 - batch 343: 0.9036\n",
            "training loss at step 9 - batch 344: 0.31 (2019-08-04 13:38:14.278467)\n",
            "Accuracy at step 9 - batch 344: 0.9068\n",
            "training loss at step 9 - batch 345: 0.29 (2019-08-04 13:38:14.290391)\n",
            "Accuracy at step 9 - batch 345: 0.9096\n",
            "training loss at step 9 - batch 346: 0.31 (2019-08-04 13:38:14.414616)\n",
            "Accuracy at step 9 - batch 346: 0.9084\n",
            "training loss at step 9 - batch 347: 0.32 (2019-08-04 13:38:14.430650)\n",
            "Accuracy at step 9 - batch 347: 0.9036\n",
            "training loss at step 9 - batch 348: 0.32 (2019-08-04 13:38:14.443236)\n",
            "Accuracy at step 9 - batch 348: 0.9056\n",
            "training loss at step 9 - batch 349: 0.33 (2019-08-04 13:38:14.455942)\n",
            "Accuracy at step 9 - batch 349: 0.9032\n",
            "training loss at step 9 - batch 350: 0.31 (2019-08-04 13:38:14.468386)\n",
            "Accuracy at step 9 - batch 350: 0.9056\n",
            "training loss at step 9 - batch 351: 0.29 (2019-08-04 13:38:14.586412)\n",
            "Accuracy at step 9 - batch 351: 0.9092\n",
            "training loss at step 9 - batch 352: 0.29 (2019-08-04 13:38:14.600406)\n",
            "Accuracy at step 9 - batch 352: 0.9088\n",
            "training loss at step 9 - batch 353: 0.31 (2019-08-04 13:38:14.614400)\n",
            "Accuracy at step 9 - batch 353: 0.9088\n",
            "training loss at step 9 - batch 354: 0.31 (2019-08-04 13:38:14.628743)\n",
            "Accuracy at step 9 - batch 354: 0.908\n",
            "training loss at step 9 - batch 355: 0.30 (2019-08-04 13:38:14.642222)\n",
            "Accuracy at step 9 - batch 355: 0.9104\n",
            "training loss at step 9 - batch 356: 0.32 (2019-08-04 13:38:14.761756)\n",
            "Accuracy at step 9 - batch 356: 0.9008\n",
            "training loss at step 9 - batch 357: 0.30 (2019-08-04 13:38:14.779379)\n",
            "Accuracy at step 9 - batch 357: 0.9104\n",
            "training loss at step 9 - batch 358: 0.27 (2019-08-04 13:38:14.792074)\n",
            "Accuracy at step 9 - batch 358: 0.9228\n",
            "training loss at step 9 - batch 359: 0.30 (2019-08-04 13:38:14.804975)\n",
            "Accuracy at step 9 - batch 359: 0.9124\n",
            "training loss at step 9 - batch 360: 0.32 (2019-08-04 13:38:14.817072)\n",
            "Accuracy at step 9 - batch 360: 0.9048\n",
            "training loss at step 9 - batch 361: 0.28 (2019-08-04 13:38:14.940341)\n",
            "Accuracy at step 9 - batch 361: 0.9152\n",
            "training loss at step 9 - batch 362: 0.32 (2019-08-04 13:38:14.952914)\n",
            "Accuracy at step 9 - batch 362: 0.9064\n",
            "training loss at step 9 - batch 363: 0.30 (2019-08-04 13:38:14.966259)\n",
            "Accuracy at step 9 - batch 363: 0.9084\n",
            "training loss at step 9 - batch 364: 0.31 (2019-08-04 13:38:14.979003)\n",
            "Accuracy at step 9 - batch 364: 0.9092\n",
            "training loss at step 9 - batch 365: 0.30 (2019-08-04 13:38:14.991127)\n",
            "Accuracy at step 9 - batch 365: 0.91\n",
            "training loss at step 9 - batch 366: 0.32 (2019-08-04 13:38:15.117924)\n",
            "Accuracy at step 9 - batch 366: 0.9036\n",
            "training loss at step 9 - batch 367: 0.32 (2019-08-04 13:38:15.130608)\n",
            "Accuracy at step 9 - batch 367: 0.9056\n",
            "training loss at step 9 - batch 368: 0.30 (2019-08-04 13:38:15.144756)\n",
            "Accuracy at step 9 - batch 368: 0.9124\n",
            "training loss at step 9 - batch 369: 0.31 (2019-08-04 13:38:15.161622)\n",
            "Accuracy at step 9 - batch 369: 0.9116\n",
            "training loss at step 9 - batch 370: 0.32 (2019-08-04 13:38:15.175264)\n",
            "Accuracy at step 9 - batch 370: 0.902\n",
            "training loss at step 9 - batch 371: 0.28 (2019-08-04 13:38:15.299462)\n",
            "Accuracy at step 9 - batch 371: 0.9108\n",
            "training loss at step 9 - batch 372: 0.30 (2019-08-04 13:38:15.313502)\n",
            "Accuracy at step 9 - batch 372: 0.904\n",
            "training loss at step 9 - batch 373: 0.30 (2019-08-04 13:38:15.326568)\n",
            "Accuracy at step 9 - batch 373: 0.9064\n",
            "training loss at step 9 - batch 374: 0.30 (2019-08-04 13:38:15.339543)\n",
            "Accuracy at step 9 - batch 374: 0.9116\n",
            "training loss at step 9 - batch 375: 0.31 (2019-08-04 13:38:15.354034)\n",
            "Accuracy at step 9 - batch 375: 0.906\n",
            "training loss at step 9 - batch 376: 0.33 (2019-08-04 13:38:15.472881)\n",
            "Accuracy at step 9 - batch 376: 0.904\n",
            "training loss at step 9 - batch 377: 0.34 (2019-08-04 13:38:15.488911)\n",
            "Accuracy at step 9 - batch 377: 0.9016\n",
            "training loss at step 9 - batch 378: 0.31 (2019-08-04 13:38:15.501629)\n",
            "Accuracy at step 9 - batch 378: 0.9072\n",
            "training loss at step 9 - batch 379: 0.31 (2019-08-04 13:38:15.513350)\n",
            "Accuracy at step 9 - batch 379: 0.9008\n",
            "training loss at step 9 - batch 380: 0.30 (2019-08-04 13:38:15.526670)\n",
            "Accuracy at step 9 - batch 380: 0.9088\n",
            "training loss at step 9 - batch 381: 0.30 (2019-08-04 13:38:15.651071)\n",
            "Accuracy at step 9 - batch 381: 0.9112\n",
            "training loss at step 9 - batch 382: 0.30 (2019-08-04 13:38:15.667309)\n",
            "Accuracy at step 9 - batch 382: 0.9132\n",
            "training loss at step 9 - batch 383: 0.30 (2019-08-04 13:38:15.681626)\n",
            "Accuracy at step 9 - batch 383: 0.9104\n",
            "training loss at step 9 - batch 384: 0.30 (2019-08-04 13:38:15.693947)\n",
            "Accuracy at step 9 - batch 384: 0.91\n",
            "training loss at step 9 - batch 385: 0.28 (2019-08-04 13:38:15.705994)\n",
            "Accuracy at step 9 - batch 385: 0.9164\n",
            "training loss at step 9 - batch 386: 0.29 (2019-08-04 13:38:15.826173)\n",
            "Accuracy at step 9 - batch 386: 0.9124\n",
            "training loss at step 9 - batch 387: 0.31 (2019-08-04 13:38:15.839815)\n",
            "Accuracy at step 9 - batch 387: 0.9084\n",
            "training loss at step 9 - batch 388: 0.30 (2019-08-04 13:38:15.852389)\n",
            "Accuracy at step 9 - batch 388: 0.9092\n",
            "training loss at step 9 - batch 389: 0.30 (2019-08-04 13:38:15.867656)\n",
            "Accuracy at step 9 - batch 389: 0.9112\n",
            "training loss at step 9 - batch 390: 0.32 (2019-08-04 13:38:15.880101)\n",
            "Accuracy at step 9 - batch 390: 0.904\n",
            "training loss at step 9 - batch 391: 0.32 (2019-08-04 13:38:15.997539)\n",
            "Accuracy at step 9 - batch 391: 0.9056\n",
            "training loss at step 9 - batch 392: 0.31 (2019-08-04 13:38:16.013064)\n",
            "Accuracy at step 9 - batch 392: 0.9128\n",
            "training loss at step 9 - batch 393: 0.33 (2019-08-04 13:38:16.025448)\n",
            "Accuracy at step 9 - batch 393: 0.9\n",
            "training loss at step 9 - batch 394: 0.33 (2019-08-04 13:38:16.037527)\n",
            "Accuracy at step 9 - batch 394: 0.9012\n",
            "training loss at step 9 - batch 395: 0.30 (2019-08-04 13:38:16.050664)\n",
            "Accuracy at step 9 - batch 395: 0.91\n",
            "training loss at step 9 - batch 396: 0.31 (2019-08-04 13:38:16.182889)\n",
            "Accuracy at step 9 - batch 396: 0.904\n",
            "training loss at step 9 - batch 397: 0.30 (2019-08-04 13:38:16.198415)\n",
            "Accuracy at step 9 - batch 397: 0.9136\n",
            "training loss at step 9 - batch 398: 0.30 (2019-08-04 13:38:16.210829)\n",
            "Accuracy at step 9 - batch 398: 0.9148\n",
            "training loss at step 9 - batch 399: 0.33 (2019-08-04 13:38:16.224134)\n",
            "Accuracy at step 9 - batch 399: 0.8976\n",
            "training loss at step 9 - batch 400: 0.33 (2019-08-04 13:38:16.238297)\n",
            "Accuracy at step 9 - batch 400: 0.9024\n",
            "training loss at step 9 - batch 401: 0.29 (2019-08-04 13:38:16.358610)\n",
            "Accuracy at step 9 - batch 401: 0.9112\n",
            "training loss at step 9 - batch 402: 0.31 (2019-08-04 13:38:16.371989)\n",
            "Accuracy at step 9 - batch 402: 0.9024\n",
            "training loss at step 9 - batch 403: 0.32 (2019-08-04 13:38:16.388001)\n",
            "Accuracy at step 9 - batch 403: 0.9048\n",
            "training loss at step 9 - batch 404: 0.31 (2019-08-04 13:38:16.403159)\n",
            "Accuracy at step 9 - batch 404: 0.906\n",
            "training loss at step 9 - batch 405: 0.32 (2019-08-04 13:38:16.415397)\n",
            "Accuracy at step 9 - batch 405: 0.904\n",
            "training loss at step 9 - batch 406: 0.32 (2019-08-04 13:38:16.531789)\n",
            "Accuracy at step 9 - batch 406: 0.9048\n",
            "training loss at step 9 - batch 407: 0.30 (2019-08-04 13:38:16.545159)\n",
            "Accuracy at step 9 - batch 407: 0.906\n",
            "training loss at step 9 - batch 408: 0.29 (2019-08-04 13:38:16.557217)\n",
            "Accuracy at step 9 - batch 408: 0.9092\n",
            "training loss at step 9 - batch 409: 0.28 (2019-08-04 13:38:16.569261)\n",
            "Accuracy at step 9 - batch 409: 0.9124\n",
            "training loss at step 9 - batch 410: 0.31 (2019-08-04 13:38:16.582192)\n",
            "Accuracy at step 9 - batch 410: 0.9088\n",
            "training loss at step 9 - batch 411: 0.32 (2019-08-04 13:38:16.715680)\n",
            "Accuracy at step 9 - batch 411: 0.9056\n",
            "training loss at step 9 - batch 412: 0.27 (2019-08-04 13:38:16.730227)\n",
            "Accuracy at step 9 - batch 412: 0.9144\n",
            "training loss at step 9 - batch 413: 0.31 (2019-08-04 13:38:16.743499)\n",
            "Accuracy at step 9 - batch 413: 0.906\n",
            "training loss at step 9 - batch 414: 0.32 (2019-08-04 13:38:16.755494)\n",
            "Accuracy at step 9 - batch 414: 0.9068\n",
            "training loss at step 9 - batch 415: 0.32 (2019-08-04 13:38:16.768457)\n",
            "Accuracy at step 9 - batch 415: 0.9016\n",
            "training loss at step 9 - batch 416: 0.31 (2019-08-04 13:38:16.906330)\n",
            "Accuracy at step 9 - batch 416: 0.9072\n",
            "training loss at step 9 - batch 417: 0.30 (2019-08-04 13:38:16.926983)\n",
            "Accuracy at step 9 - batch 417: 0.9076\n",
            "training loss at step 9 - batch 418: 0.32 (2019-08-04 13:38:16.942534)\n",
            "Accuracy at step 9 - batch 418: 0.9048\n",
            "training loss at step 9 - batch 419: 0.32 (2019-08-04 13:38:16.956125)\n",
            "Accuracy at step 9 - batch 419: 0.9048\n",
            "training loss at step 9 - batch 420: 0.30 (2019-08-04 13:38:16.970080)\n",
            "Accuracy at step 9 - batch 420: 0.9068\n",
            "training loss at step 9 - batch 421: 0.32 (2019-08-04 13:38:17.125938)\n",
            "Accuracy at step 9 - batch 421: 0.9056\n",
            "training loss at step 9 - batch 422: 0.30 (2019-08-04 13:38:17.148855)\n",
            "Accuracy at step 9 - batch 422: 0.9084\n",
            "training loss at step 9 - batch 423: 0.30 (2019-08-04 13:38:17.164246)\n",
            "Accuracy at step 9 - batch 423: 0.9072\n",
            "training loss at step 9 - batch 424: 0.31 (2019-08-04 13:38:17.177483)\n",
            "Accuracy at step 9 - batch 424: 0.9036\n",
            "training loss at step 9 - batch 425: 0.31 (2019-08-04 13:38:17.189944)\n",
            "Accuracy at step 9 - batch 425: 0.902\n",
            "training loss at step 9 - batch 426: 0.29 (2019-08-04 13:38:17.315043)\n",
            "Accuracy at step 9 - batch 426: 0.9088\n",
            "training loss at step 9 - batch 427: 0.30 (2019-08-04 13:38:17.329599)\n",
            "Accuracy at step 9 - batch 427: 0.9088\n",
            "training loss at step 9 - batch 428: 0.30 (2019-08-04 13:38:17.342034)\n",
            "Accuracy at step 9 - batch 428: 0.9068\n",
            "training loss at step 9 - batch 429: 0.31 (2019-08-04 13:38:17.356781)\n",
            "Accuracy at step 9 - batch 429: 0.9044\n",
            "training loss at step 9 - batch 430: 0.34 (2019-08-04 13:38:17.369557)\n",
            "Accuracy at step 9 - batch 430: 0.8948\n",
            "training loss at step 9 - batch 431: 0.29 (2019-08-04 13:38:17.493329)\n",
            "Accuracy at step 9 - batch 431: 0.908\n",
            "training loss at step 9 - batch 432: 0.29 (2019-08-04 13:38:17.510456)\n",
            "Accuracy at step 9 - batch 432: 0.912\n",
            "training loss at step 9 - batch 433: 0.32 (2019-08-04 13:38:17.526395)\n",
            "Accuracy at step 9 - batch 433: 0.9048\n",
            "training loss at step 9 - batch 434: 0.30 (2019-08-04 13:38:17.538927)\n",
            "Accuracy at step 9 - batch 434: 0.9124\n",
            "training loss at step 9 - batch 435: 0.29 (2019-08-04 13:38:17.551538)\n",
            "Accuracy at step 9 - batch 435: 0.9116\n",
            "training loss at step 9 - batch 436: 0.28 (2019-08-04 13:38:17.674767)\n",
            "Accuracy at step 9 - batch 436: 0.9212\n",
            "training loss at step 9 - batch 437: 0.32 (2019-08-04 13:38:17.690326)\n",
            "Accuracy at step 9 - batch 437: 0.9008\n",
            "training loss at step 9 - batch 438: 0.32 (2019-08-04 13:38:17.704521)\n",
            "Accuracy at step 9 - batch 438: 0.902\n",
            "training loss at step 9 - batch 439: 0.30 (2019-08-04 13:38:17.718127)\n",
            "Accuracy at step 9 - batch 439: 0.9072\n",
            "training loss at step 9 - batch 440: 0.30 (2019-08-04 13:38:17.730583)\n",
            "Accuracy at step 9 - batch 440: 0.9084\n",
            "training loss at step 9 - batch 441: 0.29 (2019-08-04 13:38:17.850689)\n",
            "Accuracy at step 9 - batch 441: 0.9104\n",
            "training loss at step 9 - batch 442: 0.30 (2019-08-04 13:38:17.862710)\n",
            "Accuracy at step 9 - batch 442: 0.9084\n",
            "training loss at step 9 - batch 443: 0.31 (2019-08-04 13:38:17.879483)\n",
            "Accuracy at step 9 - batch 443: 0.9076\n",
            "training loss at step 9 - batch 444: 0.33 (2019-08-04 13:38:17.892535)\n",
            "Accuracy at step 9 - batch 444: 0.8956\n",
            "training loss at step 9 - batch 445: 0.31 (2019-08-04 13:38:17.905209)\n",
            "Accuracy at step 9 - batch 445: 0.9028\n",
            "training loss at step 9 - batch 446: 0.31 (2019-08-04 13:38:18.029101)\n",
            "Accuracy at step 9 - batch 446: 0.9044\n",
            "training loss at step 9 - batch 447: 0.33 (2019-08-04 13:38:18.042050)\n",
            "Accuracy at step 9 - batch 447: 0.902\n",
            "training loss at step 9 - batch 448: 0.30 (2019-08-04 13:38:18.053790)\n",
            "Accuracy at step 9 - batch 448: 0.9136\n",
            "training loss at step 9 - batch 449: 0.29 (2019-08-04 13:38:18.066534)\n",
            "Accuracy at step 9 - batch 449: 0.9192\n",
            "training loss at step 9 - batch 450: 0.28 (2019-08-04 13:38:18.080072)\n",
            "Accuracy at step 9 - batch 450: 0.9136\n",
            "training loss at step 9 - batch 451: 0.32 (2019-08-04 13:38:18.215751)\n",
            "Accuracy at step 9 - batch 451: 0.9064\n",
            "training loss at step 9 - batch 452: 0.31 (2019-08-04 13:38:18.228969)\n",
            "Accuracy at step 9 - batch 452: 0.9048\n",
            "training loss at step 9 - batch 453: 0.31 (2019-08-04 13:38:18.241704)\n",
            "Accuracy at step 9 - batch 453: 0.9036\n",
            "training loss at step 9 - batch 454: 0.29 (2019-08-04 13:38:18.254339)\n",
            "Accuracy at step 9 - batch 454: 0.9116\n",
            "training loss at step 9 - batch 455: 0.31 (2019-08-04 13:38:18.267156)\n",
            "Accuracy at step 9 - batch 455: 0.9064\n",
            "training loss at step 9 - batch 456: 0.30 (2019-08-04 13:38:18.393028)\n",
            "Accuracy at step 9 - batch 456: 0.912\n",
            "training loss at step 9 - batch 457: 0.30 (2019-08-04 13:38:18.410392)\n",
            "Accuracy at step 9 - batch 457: 0.9116\n",
            "training loss at step 9 - batch 458: 0.30 (2019-08-04 13:38:18.423928)\n",
            "Accuracy at step 9 - batch 458: 0.91\n",
            "training loss at step 9 - batch 459: 0.31 (2019-08-04 13:38:18.436752)\n",
            "Accuracy at step 9 - batch 459: 0.9064\n",
            "training loss at step 9 - batch 460: 0.30 (2019-08-04 13:38:18.450368)\n",
            "Accuracy at step 9 - batch 460: 0.9144\n",
            "training loss at step 9 - batch 461: 0.30 (2019-08-04 13:38:18.572184)\n",
            "Accuracy at step 9 - batch 461: 0.9116\n",
            "training loss at step 9 - batch 462: 0.30 (2019-08-04 13:38:18.585052)\n",
            "Accuracy at step 9 - batch 462: 0.9076\n",
            "training loss at step 9 - batch 463: 0.30 (2019-08-04 13:38:18.599677)\n",
            "Accuracy at step 9 - batch 463: 0.9064\n",
            "training loss at step 9 - batch 464: 0.33 (2019-08-04 13:38:18.612477)\n",
            "Accuracy at step 9 - batch 464: 0.9012\n",
            "training loss at step 9 - batch 465: 0.29 (2019-08-04 13:38:18.625700)\n",
            "Accuracy at step 9 - batch 465: 0.9108\n",
            "training loss at step 9 - batch 466: 0.30 (2019-08-04 13:38:18.758006)\n",
            "Accuracy at step 9 - batch 466: 0.908\n",
            "training loss at step 9 - batch 467: 0.32 (2019-08-04 13:38:18.770048)\n",
            "Accuracy at step 9 - batch 467: 0.906\n",
            "training loss at step 9 - batch 468: 0.31 (2019-08-04 13:38:18.783357)\n",
            "Accuracy at step 9 - batch 468: 0.9056\n",
            "training loss at step 9 - batch 469: 0.32 (2019-08-04 13:38:18.796520)\n",
            "Accuracy at step 9 - batch 469: 0.9024\n",
            "training loss at step 9 - batch 470: 0.32 (2019-08-04 13:38:18.816097)\n",
            "Accuracy at step 9 - batch 470: 0.9068\n",
            "training loss at step 9 - batch 471: 0.32 (2019-08-04 13:38:18.937372)\n",
            "Accuracy at step 9 - batch 471: 0.8996\n",
            "training loss at step 9 - batch 472: 0.32 (2019-08-04 13:38:18.950312)\n",
            "Accuracy at step 9 - batch 472: 0.9016\n",
            "training loss at step 9 - batch 473: 0.29 (2019-08-04 13:38:18.963270)\n",
            "Accuracy at step 9 - batch 473: 0.9112\n",
            "training loss at step 9 - batch 474: 0.33 (2019-08-04 13:38:18.977219)\n",
            "Accuracy at step 9 - batch 474: 0.8956\n",
            "training loss at step 9 - batch 475: 0.31 (2019-08-04 13:38:18.990982)\n",
            "Accuracy at step 9 - batch 475: 0.9096\n",
            "training loss at step 9 - batch 476: 0.33 (2019-08-04 13:38:19.120247)\n",
            "Accuracy at step 9 - batch 476: 0.906\n",
            "training loss at step 9 - batch 477: 0.30 (2019-08-04 13:38:19.137701)\n",
            "Accuracy at step 9 - batch 477: 0.9128\n",
            "training loss at step 9 - batch 478: 0.30 (2019-08-04 13:38:19.150302)\n",
            "Accuracy at step 9 - batch 478: 0.9136\n",
            "training loss at step 9 - batch 479: 0.31 (2019-08-04 13:38:19.164030)\n",
            "Accuracy at step 9 - batch 479: 0.906\n",
            "training loss at step 9 - batch 480: 0.28 (2019-08-04 13:38:19.176516)\n",
            "Accuracy at step 9 - batch 480: 0.9164\n",
            "training loss at step 9 - batch 481: 0.31 (2019-08-04 13:38:19.304035)\n",
            "Accuracy at step 9 - batch 481: 0.9088\n",
            "training loss at step 9 - batch 482: 0.30 (2019-08-04 13:38:19.317655)\n",
            "Accuracy at step 9 - batch 482: 0.9068\n",
            "training loss at step 9 - batch 483: 0.30 (2019-08-04 13:38:19.335613)\n",
            "Accuracy at step 9 - batch 483: 0.906\n",
            "training loss at step 9 - batch 484: 0.30 (2019-08-04 13:38:19.348578)\n",
            "Accuracy at step 9 - batch 484: 0.9124\n",
            "training loss at step 9 - batch 485: 0.31 (2019-08-04 13:38:19.361277)\n",
            "Accuracy at step 9 - batch 485: 0.9104\n",
            "training loss at step 9 - batch 486: 0.28 (2019-08-04 13:38:19.484724)\n",
            "Accuracy at step 9 - batch 486: 0.912\n",
            "training loss at step 9 - batch 487: 0.29 (2019-08-04 13:38:19.501604)\n",
            "Accuracy at step 9 - batch 487: 0.91\n",
            "training loss at step 9 - batch 488: 0.32 (2019-08-04 13:38:19.514115)\n",
            "Accuracy at step 9 - batch 488: 0.9056\n",
            "training loss at step 9 - batch 489: 0.30 (2019-08-04 13:38:19.526306)\n",
            "Accuracy at step 9 - batch 489: 0.902\n",
            "training loss at step 9 - batch 490: 0.32 (2019-08-04 13:38:19.540414)\n",
            "Accuracy at step 9 - batch 490: 0.9064\n",
            "training loss at step 9 - batch 491: 0.30 (2019-08-04 13:38:19.658568)\n",
            "Accuracy at step 9 - batch 491: 0.9108\n",
            "training loss at step 9 - batch 492: 0.29 (2019-08-04 13:38:19.670915)\n",
            "Accuracy at step 9 - batch 492: 0.91\n",
            "training loss at step 9 - batch 493: 0.29 (2019-08-04 13:38:19.682679)\n",
            "Accuracy at step 9 - batch 493: 0.9124\n",
            "training loss at step 9 - batch 494: 0.31 (2019-08-04 13:38:19.695422)\n",
            "Accuracy at step 9 - batch 494: 0.912\n",
            "training loss at step 9 - batch 495: 0.31 (2019-08-04 13:38:19.708087)\n",
            "Accuracy at step 9 - batch 495: 0.9068\n",
            "training loss at step 9 - batch 496: 0.32 (2019-08-04 13:38:19.835599)\n",
            "Accuracy at step 9 - batch 496: 0.9036\n",
            "training loss at step 9 - batch 497: 0.31 (2019-08-04 13:38:19.848396)\n",
            "Accuracy at step 9 - batch 497: 0.9092\n",
            "training loss at step 9 - batch 498: 0.31 (2019-08-04 13:38:19.862096)\n",
            "Accuracy at step 9 - batch 498: 0.9076\n",
            "training loss at step 9 - batch 499: 0.33 (2019-08-04 13:38:19.875736)\n",
            "Accuracy at step 9 - batch 499: 0.904\n",
            "training loss at step 9 - batch 500: 0.29 (2019-08-04 13:38:19.890383)\n",
            "Accuracy at step 9 - batch 500: 0.9108\n",
            "training loss at step 9 - batch 501: 0.30 (2019-08-04 13:38:20.018383)\n",
            "Accuracy at step 9 - batch 501: 0.9124\n",
            "training loss at step 9 - batch 502: 0.31 (2019-08-04 13:38:20.034938)\n",
            "Accuracy at step 9 - batch 502: 0.9028\n",
            "training loss at step 9 - batch 503: 0.29 (2019-08-04 13:38:20.051003)\n",
            "Accuracy at step 9 - batch 503: 0.9104\n",
            "training loss at step 9 - batch 504: 0.32 (2019-08-04 13:38:20.064298)\n",
            "Accuracy at step 9 - batch 504: 0.9048\n",
            "training loss at step 9 - batch 505: 0.31 (2019-08-04 13:38:20.076792)\n",
            "Accuracy at step 9 - batch 505: 0.908\n",
            "training loss at step 9 - batch 506: 0.30 (2019-08-04 13:38:20.206536)\n",
            "Accuracy at step 9 - batch 506: 0.9084\n",
            "training loss at step 9 - batch 507: 0.30 (2019-08-04 13:38:20.224633)\n",
            "Accuracy at step 9 - batch 507: 0.9084\n",
            "training loss at step 9 - batch 508: 0.32 (2019-08-04 13:38:20.239859)\n",
            "Accuracy at step 9 - batch 508: 0.8988\n",
            "training loss at step 9 - batch 509: 0.30 (2019-08-04 13:38:20.254408)\n",
            "Accuracy at step 9 - batch 509: 0.9152\n",
            "training loss at step 9 - batch 510: 0.28 (2019-08-04 13:38:20.269196)\n",
            "Accuracy at step 9 - batch 510: 0.9184\n",
            "training loss at step 9 - batch 511: 0.29 (2019-08-04 13:38:20.396119)\n",
            "Accuracy at step 9 - batch 511: 0.9108\n",
            "training loss at step 9 - batch 512: 0.32 (2019-08-04 13:38:20.413097)\n",
            "Accuracy at step 9 - batch 512: 0.9092\n",
            "training loss at step 9 - batch 513: 0.30 (2019-08-04 13:38:20.426080)\n",
            "Accuracy at step 9 - batch 513: 0.9088\n",
            "training loss at step 9 - batch 514: 0.31 (2019-08-04 13:38:20.439594)\n",
            "Accuracy at step 9 - batch 514: 0.9068\n",
            "training loss at step 9 - batch 515: 0.33 (2019-08-04 13:38:20.451734)\n",
            "Accuracy at step 9 - batch 515: 0.9004\n",
            "training loss at step 9 - batch 516: 0.31 (2019-08-04 13:38:20.589016)\n",
            "Accuracy at step 9 - batch 516: 0.9008\n",
            "training loss at step 9 - batch 517: 0.32 (2019-08-04 13:38:20.603087)\n",
            "Accuracy at step 9 - batch 517: 0.9056\n",
            "training loss at step 9 - batch 518: 0.31 (2019-08-04 13:38:20.616498)\n",
            "Accuracy at step 9 - batch 518: 0.9056\n",
            "training loss at step 9 - batch 519: 0.31 (2019-08-04 13:38:20.628940)\n",
            "Accuracy at step 9 - batch 519: 0.902\n",
            "training loss at step 9 - batch 520: 0.30 (2019-08-04 13:38:20.642179)\n",
            "Accuracy at step 9 - batch 520: 0.9036\n",
            "training loss at step 9 - batch 521: 0.30 (2019-08-04 13:38:20.760915)\n",
            "Accuracy at step 9 - batch 521: 0.9028\n",
            "training loss at step 9 - batch 522: 0.29 (2019-08-04 13:38:20.774746)\n",
            "Accuracy at step 9 - batch 522: 0.9148\n",
            "training loss at step 9 - batch 523: 0.28 (2019-08-04 13:38:20.788376)\n",
            "Accuracy at step 9 - batch 523: 0.9108\n",
            "training loss at step 9 - batch 524: 0.31 (2019-08-04 13:38:20.802986)\n",
            "Accuracy at step 9 - batch 524: 0.9064\n",
            "training loss at step 9 - batch 525: 0.29 (2019-08-04 13:38:20.816477)\n",
            "Accuracy at step 9 - batch 525: 0.908\n",
            "training loss at step 9 - batch 526: 0.31 (2019-08-04 13:38:20.937347)\n",
            "Accuracy at step 9 - batch 526: 0.9084\n",
            "training loss at step 9 - batch 527: 0.30 (2019-08-04 13:38:20.949837)\n",
            "Accuracy at step 9 - batch 527: 0.904\n",
            "training loss at step 9 - batch 528: 0.32 (2019-08-04 13:38:20.964437)\n",
            "Accuracy at step 9 - batch 528: 0.9088\n",
            "training loss at step 9 - batch 529: 0.29 (2019-08-04 13:38:20.976595)\n",
            "Accuracy at step 9 - batch 529: 0.9124\n",
            "training loss at step 9 - batch 530: 0.31 (2019-08-04 13:38:20.989490)\n",
            "Accuracy at step 9 - batch 530: 0.9068\n",
            "training loss at step 9 - batch 531: 0.32 (2019-08-04 13:38:21.121166)\n",
            "Accuracy at step 9 - batch 531: 0.9032\n",
            "training loss at step 9 - batch 532: 0.31 (2019-08-04 13:38:21.138179)\n",
            "Accuracy at step 9 - batch 532: 0.9076\n",
            "training loss at step 9 - batch 533: 0.30 (2019-08-04 13:38:21.151033)\n",
            "Accuracy at step 9 - batch 533: 0.9072\n",
            "training loss at step 9 - batch 534: 0.28 (2019-08-04 13:38:21.164118)\n",
            "Accuracy at step 9 - batch 534: 0.9152\n",
            "training loss at step 9 - batch 535: 0.30 (2019-08-04 13:38:21.177088)\n",
            "Accuracy at step 9 - batch 535: 0.9092\n",
            "training loss at step 9 - batch 536: 0.30 (2019-08-04 13:38:21.313373)\n",
            "Accuracy at step 9 - batch 536: 0.9012\n",
            "training loss at step 9 - batch 537: 0.30 (2019-08-04 13:38:21.331201)\n",
            "Accuracy at step 9 - batch 537: 0.9088\n",
            "training loss at step 9 - batch 538: 0.30 (2019-08-04 13:38:21.343575)\n",
            "Accuracy at step 9 - batch 538: 0.9068\n",
            "training loss at step 9 - batch 539: 0.29 (2019-08-04 13:38:21.355524)\n",
            "Accuracy at step 9 - batch 539: 0.9124\n",
            "training loss at step 9 - batch 540: 0.30 (2019-08-04 13:38:21.367618)\n",
            "Accuracy at step 9 - batch 540: 0.9024\n",
            "training loss at step 9 - batch 541: 0.29 (2019-08-04 13:38:21.490659)\n",
            "Accuracy at step 9 - batch 541: 0.9144\n",
            "training loss at step 9 - batch 542: 0.31 (2019-08-04 13:38:21.507104)\n",
            "Accuracy at step 9 - batch 542: 0.9092\n",
            "training loss at step 9 - batch 543: 0.29 (2019-08-04 13:38:21.520643)\n",
            "Accuracy at step 9 - batch 543: 0.9092\n",
            "training loss at step 9 - batch 544: 0.30 (2019-08-04 13:38:21.535875)\n",
            "Accuracy at step 9 - batch 544: 0.9096\n",
            "training loss at step 9 - batch 545: 0.31 (2019-08-04 13:38:21.549102)\n",
            "Accuracy at step 9 - batch 545: 0.9084\n",
            "training loss at step 9 - batch 546: 0.31 (2019-08-04 13:38:21.667164)\n",
            "Accuracy at step 9 - batch 546: 0.9068\n",
            "training loss at step 9 - batch 547: 0.33 (2019-08-04 13:38:21.682215)\n",
            "Accuracy at step 9 - batch 547: 0.9004\n",
            "training loss at step 9 - batch 548: 0.30 (2019-08-04 13:38:21.695286)\n",
            "Accuracy at step 9 - batch 548: 0.9108\n",
            "training loss at step 9 - batch 549: 0.30 (2019-08-04 13:38:21.708473)\n",
            "Accuracy at step 9 - batch 549: 0.9068\n",
            "training loss at step 9 - batch 550: 0.30 (2019-08-04 13:38:21.720943)\n",
            "Accuracy at step 9 - batch 550: 0.9096\n",
            "training loss at step 9 - batch 551: 0.29 (2019-08-04 13:38:21.853197)\n",
            "Accuracy at step 9 - batch 551: 0.912\n",
            "training loss at step 9 - batch 552: 0.30 (2019-08-04 13:38:21.865687)\n",
            "Accuracy at step 9 - batch 552: 0.9072\n",
            "training loss at step 9 - batch 553: 0.30 (2019-08-04 13:38:21.878675)\n",
            "Accuracy at step 9 - batch 553: 0.9096\n",
            "training loss at step 9 - batch 554: 0.29 (2019-08-04 13:38:21.890471)\n",
            "Accuracy at step 9 - batch 554: 0.9108\n",
            "training loss at step 9 - batch 555: 0.31 (2019-08-04 13:38:21.902517)\n",
            "Accuracy at step 9 - batch 555: 0.9084\n",
            "training loss at step 9 - batch 556: 0.31 (2019-08-04 13:38:22.034446)\n",
            "Accuracy at step 9 - batch 556: 0.9036\n",
            "training loss at step 9 - batch 557: 0.31 (2019-08-04 13:38:22.051624)\n",
            "Accuracy at step 9 - batch 557: 0.906\n",
            "training loss at step 9 - batch 558: 0.32 (2019-08-04 13:38:22.068649)\n",
            "Accuracy at step 9 - batch 558: 0.9056\n",
            "training loss at step 9 - batch 559: 0.29 (2019-08-04 13:38:22.082675)\n",
            "Accuracy at step 9 - batch 559: 0.9072\n",
            "training loss at step 9 - batch 560: 0.33 (2019-08-04 13:38:22.096709)\n",
            "Accuracy at step 9 - batch 560: 0.9004\n",
            "training loss at step 9 - batch 561: 0.29 (2019-08-04 13:38:22.224432)\n",
            "Accuracy at step 9 - batch 561: 0.91\n",
            "training loss at step 9 - batch 562: 0.29 (2019-08-04 13:38:22.236661)\n",
            "Accuracy at step 9 - batch 562: 0.9152\n",
            "training loss at step 9 - batch 563: 0.30 (2019-08-04 13:38:22.248975)\n",
            "Accuracy at step 9 - batch 563: 0.906\n",
            "training loss at step 9 - batch 564: 0.30 (2019-08-04 13:38:22.263719)\n",
            "Accuracy at step 9 - batch 564: 0.9104\n",
            "training loss at step 9 - batch 565: 0.30 (2019-08-04 13:38:22.284876)\n",
            "Accuracy at step 9 - batch 565: 0.902\n",
            "training loss at step 9 - batch 566: 0.29 (2019-08-04 13:38:22.408663)\n",
            "Accuracy at step 9 - batch 566: 0.9152\n",
            "training loss at step 9 - batch 567: 0.34 (2019-08-04 13:38:22.423224)\n",
            "Accuracy at step 9 - batch 567: 0.9004\n",
            "training loss at step 9 - batch 568: 0.31 (2019-08-04 13:38:22.436450)\n",
            "Accuracy at step 9 - batch 568: 0.9036\n",
            "training loss at step 9 - batch 569: 0.33 (2019-08-04 13:38:22.448570)\n",
            "Accuracy at step 9 - batch 569: 0.9004\n",
            "training loss at step 9 - batch 570: 0.29 (2019-08-04 13:38:22.461133)\n",
            "Accuracy at step 9 - batch 570: 0.9132\n",
            "training loss at step 9 - batch 571: 0.29 (2019-08-04 13:38:22.585970)\n",
            "Accuracy at step 9 - batch 571: 0.9152\n",
            "training loss at step 9 - batch 572: 0.28 (2019-08-04 13:38:22.604233)\n",
            "Accuracy at step 9 - batch 572: 0.9152\n",
            "training loss at step 9 - batch 573: 0.31 (2019-08-04 13:38:22.617394)\n",
            "Accuracy at step 9 - batch 573: 0.9116\n",
            "training loss at step 9 - batch 574: 0.30 (2019-08-04 13:38:22.630305)\n",
            "Accuracy at step 9 - batch 574: 0.9084\n",
            "training loss at step 9 - batch 575: 0.31 (2019-08-04 13:38:22.643018)\n",
            "Accuracy at step 9 - batch 575: 0.9072\n",
            "training loss at step 9 - batch 576: 0.28 (2019-08-04 13:38:22.761425)\n",
            "Accuracy at step 9 - batch 576: 0.91\n",
            "training loss at step 9 - batch 577: 0.31 (2019-08-04 13:38:22.773601)\n",
            "Accuracy at step 9 - batch 577: 0.9064\n",
            "training loss at step 9 - batch 578: 0.31 (2019-08-04 13:38:22.790082)\n",
            "Accuracy at step 9 - batch 578: 0.91\n",
            "training loss at step 9 - batch 579: 0.30 (2019-08-04 13:38:22.804075)\n",
            "Accuracy at step 9 - batch 579: 0.914\n",
            "training loss at step 9 - batch 580: 0.31 (2019-08-04 13:38:22.816096)\n",
            "Accuracy at step 9 - batch 580: 0.904\n",
            "training loss at step 9 - batch 581: 0.31 (2019-08-04 13:38:22.934265)\n",
            "Accuracy at step 9 - batch 581: 0.9116\n",
            "training loss at step 9 - batch 582: 0.30 (2019-08-04 13:38:22.950657)\n",
            "Accuracy at step 9 - batch 582: 0.9092\n",
            "training loss at step 9 - batch 583: 0.30 (2019-08-04 13:38:22.963049)\n",
            "Accuracy at step 9 - batch 583: 0.9132\n",
            "training loss at step 9 - batch 584: 0.31 (2019-08-04 13:38:22.975359)\n",
            "Accuracy at step 9 - batch 584: 0.9056\n",
            "training loss at step 9 - batch 585: 0.31 (2019-08-04 13:38:22.989288)\n",
            "Accuracy at step 9 - batch 585: 0.9092\n",
            "training loss at step 9 - batch 586: 0.31 (2019-08-04 13:38:23.111452)\n",
            "Accuracy at step 9 - batch 586: 0.9068\n",
            "training loss at step 9 - batch 587: 0.30 (2019-08-04 13:38:23.125473)\n",
            "Accuracy at step 9 - batch 587: 0.9132\n",
            "training loss at step 9 - batch 588: 0.30 (2019-08-04 13:38:23.137489)\n",
            "Accuracy at step 9 - batch 588: 0.9044\n",
            "training loss at step 9 - batch 589: 0.32 (2019-08-04 13:38:23.151191)\n",
            "Accuracy at step 9 - batch 589: 0.9072\n",
            "training loss at step 9 - batch 590: 0.32 (2019-08-04 13:38:23.163684)\n",
            "Accuracy at step 9 - batch 590: 0.908\n",
            "training loss at step 9 - batch 591: 0.33 (2019-08-04 13:38:23.287210)\n",
            "Accuracy at step 9 - batch 591: 0.904\n",
            "training loss at step 9 - batch 592: 0.31 (2019-08-04 13:38:23.305051)\n",
            "Accuracy at step 9 - batch 592: 0.9092\n",
            "training loss at step 9 - batch 593: 0.34 (2019-08-04 13:38:23.322545)\n",
            "Accuracy at step 9 - batch 593: 0.9032\n",
            "training loss at step 9 - batch 594: 0.33 (2019-08-04 13:38:23.336195)\n",
            "Accuracy at step 9 - batch 594: 0.9012\n",
            "training loss at step 9 - batch 595: 0.30 (2019-08-04 13:38:23.351508)\n",
            "Accuracy at step 9 - batch 595: 0.9112\n",
            "training loss at step 9 - batch 596: 0.31 (2019-08-04 13:38:23.472568)\n",
            "Accuracy at step 9 - batch 596: 0.91\n",
            "training loss at step 9 - batch 597: 0.29 (2019-08-04 13:38:23.484206)\n",
            "Accuracy at step 9 - batch 597: 0.9128\n",
            "training loss at step 9 - batch 598: 0.29 (2019-08-04 13:38:23.495907)\n",
            "Accuracy at step 9 - batch 598: 0.9116\n",
            "training loss at step 9 - batch 599: 0.30 (2019-08-04 13:38:23.508518)\n",
            "Accuracy at step 9 - batch 599: 0.906\n",
            "training loss at step 9 - batch 600: 0.31 (2019-08-04 13:38:23.520329)\n",
            "Accuracy at step 9 - batch 600: 0.9068\n",
            "training loss at step 9 - batch 601: 0.31 (2019-08-04 13:38:23.642283)\n",
            "Accuracy at step 9 - batch 601: 0.906\n",
            "training loss at step 9 - batch 602: 0.30 (2019-08-04 13:38:23.658697)\n",
            "Accuracy at step 9 - batch 602: 0.9156\n",
            "training loss at step 9 - batch 603: 0.32 (2019-08-04 13:38:23.671197)\n",
            "Accuracy at step 9 - batch 603: 0.9016\n",
            "training loss at step 9 - batch 604: 0.32 (2019-08-04 13:38:23.684014)\n",
            "Accuracy at step 9 - batch 604: 0.8984\n",
            "training loss at step 9 - batch 605: 0.32 (2019-08-04 13:38:23.697314)\n",
            "Accuracy at step 9 - batch 605: 0.9056\n",
            "training loss at step 9 - batch 606: 0.29 (2019-08-04 13:38:23.822505)\n",
            "Accuracy at step 9 - batch 606: 0.9088\n",
            "training loss at step 9 - batch 607: 0.30 (2019-08-04 13:38:23.840451)\n",
            "Accuracy at step 9 - batch 607: 0.9084\n",
            "training loss at step 9 - batch 608: 0.28 (2019-08-04 13:38:23.856915)\n",
            "Accuracy at step 9 - batch 608: 0.9112\n",
            "training loss at step 9 - batch 609: 0.31 (2019-08-04 13:38:23.870386)\n",
            "Accuracy at step 9 - batch 609: 0.9056\n",
            "training loss at step 9 - batch 610: 0.30 (2019-08-04 13:38:23.882498)\n",
            "Accuracy at step 9 - batch 610: 0.91\n",
            "training loss at step 9 - batch 611: 0.31 (2019-08-04 13:38:24.001462)\n",
            "Accuracy at step 9 - batch 611: 0.9036\n",
            "training loss at step 9 - batch 612: 0.32 (2019-08-04 13:38:24.013355)\n",
            "Accuracy at step 9 - batch 612: 0.9036\n",
            "training loss at step 9 - batch 613: 0.31 (2019-08-04 13:38:24.025423)\n",
            "Accuracy at step 9 - batch 613: 0.9048\n",
            "training loss at step 9 - batch 614: 0.31 (2019-08-04 13:38:24.039106)\n",
            "Accuracy at step 9 - batch 614: 0.9024\n",
            "training loss at step 9 - batch 615: 0.31 (2019-08-04 13:38:24.052457)\n",
            "Accuracy at step 9 - batch 615: 0.904\n",
            "training loss at step 9 - batch 616: 0.29 (2019-08-04 13:38:24.177853)\n",
            "Accuracy at step 9 - batch 616: 0.9128\n",
            "training loss at step 9 - batch 617: 0.31 (2019-08-04 13:38:24.190343)\n",
            "Accuracy at step 9 - batch 617: 0.9024\n",
            "training loss at step 9 - batch 618: 0.29 (2019-08-04 13:38:24.203326)\n",
            "Accuracy at step 9 - batch 618: 0.9148\n",
            "training loss at step 9 - batch 619: 0.31 (2019-08-04 13:38:24.215403)\n",
            "Accuracy at step 9 - batch 619: 0.9044\n",
            "training loss at step 9 - batch 620: 0.31 (2019-08-04 13:38:24.232179)\n",
            "Accuracy at step 9 - batch 620: 0.91\n",
            "training loss at step 9 - batch 621: 0.31 (2019-08-04 13:38:24.360127)\n",
            "Accuracy at step 9 - batch 621: 0.9076\n",
            "training loss at step 9 - batch 622: 0.31 (2019-08-04 13:38:24.372402)\n",
            "Accuracy at step 9 - batch 622: 0.9084\n",
            "training loss at step 9 - batch 623: 0.33 (2019-08-04 13:38:24.386305)\n",
            "Accuracy at step 9 - batch 623: 0.9032\n",
            "training loss at step 9 - batch 624: 0.29 (2019-08-04 13:38:24.399455)\n",
            "Accuracy at step 9 - batch 624: 0.9076\n",
            "training loss at step 9 - batch 625: 0.30 (2019-08-04 13:38:24.412556)\n",
            "Accuracy at step 9 - batch 625: 0.904\n",
            "training loss at step 9 - batch 626: 0.32 (2019-08-04 13:38:24.532741)\n",
            "Accuracy at step 9 - batch 626: 0.9024\n",
            "training loss at step 9 - batch 627: 0.29 (2019-08-04 13:38:24.549176)\n",
            "Accuracy at step 9 - batch 627: 0.9108\n",
            "training loss at step 9 - batch 628: 0.31 (2019-08-04 13:38:24.562405)\n",
            "Accuracy at step 9 - batch 628: 0.9096\n",
            "training loss at step 9 - batch 629: 0.32 (2019-08-04 13:38:24.576438)\n",
            "Accuracy at step 9 - batch 629: 0.9048\n",
            "training loss at step 9 - batch 630: 0.29 (2019-08-04 13:38:24.589997)\n",
            "Accuracy at step 9 - batch 630: 0.9112\n",
            "training loss at step 9 - batch 631: 0.32 (2019-08-04 13:38:24.712499)\n",
            "Accuracy at step 9 - batch 631: 0.9052\n",
            "training loss at step 9 - batch 632: 0.31 (2019-08-04 13:38:24.728686)\n",
            "Accuracy at step 9 - batch 632: 0.9048\n",
            "training loss at step 9 - batch 633: 0.32 (2019-08-04 13:38:24.741637)\n",
            "Accuracy at step 9 - batch 633: 0.902\n",
            "training loss at step 9 - batch 634: 0.31 (2019-08-04 13:38:24.754255)\n",
            "Accuracy at step 9 - batch 634: 0.9076\n",
            "training loss at step 9 - batch 635: 0.32 (2019-08-04 13:38:24.766589)\n",
            "Accuracy at step 9 - batch 635: 0.9076\n",
            "training loss at step 9 - batch 636: 0.31 (2019-08-04 13:38:24.891362)\n",
            "Accuracy at step 9 - batch 636: 0.91\n",
            "training loss at step 9 - batch 637: 0.32 (2019-08-04 13:38:24.905344)\n",
            "Accuracy at step 9 - batch 637: 0.9064\n",
            "training loss at step 9 - batch 638: 0.33 (2019-08-04 13:38:24.918313)\n",
            "Accuracy at step 9 - batch 638: 0.8992\n",
            "training loss at step 9 - batch 639: 0.30 (2019-08-04 13:38:24.931183)\n",
            "Accuracy at step 9 - batch 639: 0.9096\n",
            "training loss at step 9 - batch 640: 0.31 (2019-08-04 13:38:24.944025)\n",
            "Accuracy at step 9 - batch 640: 0.91\n",
            "training loss at step 9 - batch 641: 0.30 (2019-08-04 13:38:25.073226)\n",
            "Accuracy at step 9 - batch 641: 0.9092\n",
            "training loss at step 9 - batch 642: 0.32 (2019-08-04 13:38:25.086324)\n",
            "Accuracy at step 9 - batch 642: 0.9044\n",
            "training loss at step 9 - batch 643: 0.31 (2019-08-04 13:38:25.101431)\n",
            "Accuracy at step 9 - batch 643: 0.908\n",
            "training loss at step 9 - batch 644: 0.31 (2019-08-04 13:38:25.114240)\n",
            "Accuracy at step 9 - batch 644: 0.9088\n",
            "training loss at step 9 - batch 645: 0.30 (2019-08-04 13:38:25.127092)\n",
            "Accuracy at step 9 - batch 645: 0.91\n",
            "training loss at step 9 - batch 646: 0.29 (2019-08-04 13:38:25.244423)\n",
            "Accuracy at step 9 - batch 646: 0.91\n",
            "training loss at step 9 - batch 647: 0.29 (2019-08-04 13:38:25.256893)\n",
            "Accuracy at step 9 - batch 647: 0.91\n",
            "training loss at step 9 - batch 648: 0.31 (2019-08-04 13:38:25.268912)\n",
            "Accuracy at step 9 - batch 648: 0.9096\n",
            "training loss at step 9 - batch 649: 0.32 (2019-08-04 13:38:25.284876)\n",
            "Accuracy at step 9 - batch 649: 0.9028\n",
            "training loss at step 9 - batch 650: 0.30 (2019-08-04 13:38:25.297358)\n",
            "Accuracy at step 9 - batch 650: 0.9124\n",
            "training loss at step 9 - batch 651: 0.31 (2019-08-04 13:38:25.432097)\n",
            "Accuracy at step 9 - batch 651: 0.9036\n",
            "training loss at step 9 - batch 652: 0.32 (2019-08-04 13:38:25.446247)\n",
            "Accuracy at step 9 - batch 652: 0.9044\n",
            "training loss at step 9 - batch 653: 0.31 (2019-08-04 13:38:25.458645)\n",
            "Accuracy at step 9 - batch 653: 0.9056\n",
            "training loss at step 9 - batch 654: 0.30 (2019-08-04 13:38:25.471156)\n",
            "Accuracy at step 9 - batch 654: 0.9132\n",
            "training loss at step 9 - batch 655: 0.31 (2019-08-04 13:38:25.483868)\n",
            "Accuracy at step 9 - batch 655: 0.9064\n",
            "training loss at step 9 - batch 656: 0.30 (2019-08-04 13:38:25.601472)\n",
            "Accuracy at step 9 - batch 656: 0.9116\n",
            "training loss at step 9 - batch 657: 0.30 (2019-08-04 13:38:25.617872)\n",
            "Accuracy at step 9 - batch 657: 0.9108\n",
            "training loss at step 9 - batch 658: 0.33 (2019-08-04 13:38:25.631213)\n",
            "Accuracy at step 9 - batch 658: 0.904\n",
            "training loss at step 9 - batch 659: 0.31 (2019-08-04 13:38:25.645973)\n",
            "Accuracy at step 9 - batch 659: 0.9088\n",
            "training loss at step 9 - batch 660: 0.31 (2019-08-04 13:38:25.659423)\n",
            "Accuracy at step 9 - batch 660: 0.9024\n",
            "training loss at step 9 - batch 661: 0.30 (2019-08-04 13:38:25.774847)\n",
            "Accuracy at step 9 - batch 661: 0.9116\n",
            "training loss at step 9 - batch 662: 0.31 (2019-08-04 13:38:25.787891)\n",
            "Accuracy at step 9 - batch 662: 0.8992\n",
            "training loss at step 9 - batch 663: 0.29 (2019-08-04 13:38:25.800366)\n",
            "Accuracy at step 9 - batch 663: 0.9108\n",
            "training loss at step 9 - batch 664: 0.29 (2019-08-04 13:38:25.814637)\n",
            "Accuracy at step 9 - batch 664: 0.9156\n",
            "training loss at step 9 - batch 665: 0.30 (2019-08-04 13:38:25.829363)\n",
            "Accuracy at step 9 - batch 665: 0.9068\n",
            "training loss at step 9 - batch 666: 0.30 (2019-08-04 13:38:25.956479)\n",
            "Accuracy at step 9 - batch 666: 0.9068\n",
            "training loss at step 9 - batch 667: 0.30 (2019-08-04 13:38:25.970372)\n",
            "Accuracy at step 9 - batch 667: 0.9136\n",
            "training loss at step 9 - batch 668: 0.31 (2019-08-04 13:38:25.983564)\n",
            "Accuracy at step 9 - batch 668: 0.906\n",
            "training loss at step 9 - batch 669: 0.30 (2019-08-04 13:38:25.995784)\n",
            "Accuracy at step 9 - batch 669: 0.91\n",
            "training loss at step 9 - batch 670: 0.29 (2019-08-04 13:38:26.007561)\n",
            "Accuracy at step 9 - batch 670: 0.9096\n",
            "training loss at step 9 - batch 671: 0.31 (2019-08-04 13:38:26.124453)\n",
            "Accuracy at step 9 - batch 671: 0.9076\n",
            "training loss at step 9 - batch 672: 0.31 (2019-08-04 13:38:26.138638)\n",
            "Accuracy at step 9 - batch 672: 0.9052\n",
            "training loss at step 9 - batch 673: 0.29 (2019-08-04 13:38:26.151262)\n",
            "Accuracy at step 9 - batch 673: 0.914\n",
            "training loss at step 9 - batch 674: 0.30 (2019-08-04 13:38:26.165665)\n",
            "Accuracy at step 9 - batch 674: 0.9088\n",
            "training loss at step 9 - batch 675: 0.32 (2019-08-04 13:38:26.179676)\n",
            "Accuracy at step 9 - batch 675: 0.9028\n",
            "training loss at step 9 - batch 676: 0.30 (2019-08-04 13:38:26.311836)\n",
            "Accuracy at step 9 - batch 676: 0.9084\n",
            "training loss at step 9 - batch 677: 0.31 (2019-08-04 13:38:26.329060)\n",
            "Accuracy at step 9 - batch 677: 0.9088\n",
            "training loss at step 9 - batch 678: 0.29 (2019-08-04 13:38:26.342627)\n",
            "Accuracy at step 9 - batch 678: 0.9116\n",
            "training loss at step 9 - batch 679: 0.32 (2019-08-04 13:38:26.357655)\n",
            "Accuracy at step 9 - batch 679: 0.9052\n",
            "training loss at step 9 - batch 680: 0.30 (2019-08-04 13:38:26.375228)\n",
            "Accuracy at step 9 - batch 680: 0.91\n",
            "training loss at step 9 - batch 681: 0.29 (2019-08-04 13:38:26.507034)\n",
            "Accuracy at step 9 - batch 681: 0.9104\n",
            "training loss at step 9 - batch 682: 0.31 (2019-08-04 13:38:26.522140)\n",
            "Accuracy at step 9 - batch 682: 0.912\n",
            "training loss at step 9 - batch 683: 0.29 (2019-08-04 13:38:26.534913)\n",
            "Accuracy at step 9 - batch 683: 0.9108\n",
            "training loss at step 9 - batch 684: 0.32 (2019-08-04 13:38:26.548518)\n",
            "Accuracy at step 9 - batch 684: 0.9024\n",
            "training loss at step 9 - batch 685: 0.31 (2019-08-04 13:38:26.561601)\n",
            "Accuracy at step 9 - batch 685: 0.9144\n",
            "training loss at step 9 - batch 686: 0.31 (2019-08-04 13:38:26.690361)\n",
            "Accuracy at step 9 - batch 686: 0.9064\n",
            "training loss at step 9 - batch 687: 0.30 (2019-08-04 13:38:26.704098)\n",
            "Accuracy at step 9 - batch 687: 0.914\n",
            "training loss at step 9 - batch 688: 0.29 (2019-08-04 13:38:26.717370)\n",
            "Accuracy at step 9 - batch 688: 0.9132\n",
            "training loss at step 9 - batch 689: 0.31 (2019-08-04 13:38:26.729603)\n",
            "Accuracy at step 9 - batch 689: 0.9096\n",
            "training loss at step 9 - batch 690: 0.33 (2019-08-04 13:38:26.741835)\n",
            "Accuracy at step 9 - batch 690: 0.9024\n",
            "training loss at step 9 - batch 691: 0.33 (2019-08-04 13:38:26.861249)\n",
            "Accuracy at step 9 - batch 691: 0.9028\n",
            "training loss at step 9 - batch 692: 0.31 (2019-08-04 13:38:26.878554)\n",
            "Accuracy at step 9 - batch 692: 0.9116\n",
            "training loss at step 9 - batch 693: 0.31 (2019-08-04 13:38:26.891146)\n",
            "Accuracy at step 9 - batch 693: 0.9056\n",
            "training loss at step 9 - batch 694: 0.33 (2019-08-04 13:38:26.906734)\n",
            "Accuracy at step 9 - batch 694: 0.9032\n",
            "training loss at step 9 - batch 695: 0.32 (2019-08-04 13:38:26.919934)\n",
            "Accuracy at step 9 - batch 695: 0.9016\n",
            "training loss at step 9 - batch 696: 0.31 (2019-08-04 13:38:27.058535)\n",
            "Accuracy at step 9 - batch 696: 0.9096\n",
            "training loss at step 9 - batch 697: 0.30 (2019-08-04 13:38:27.073438)\n",
            "Accuracy at step 9 - batch 697: 0.9108\n",
            "training loss at step 9 - batch 698: 0.29 (2019-08-04 13:38:27.086566)\n",
            "Accuracy at step 9 - batch 698: 0.9084\n",
            "training loss at step 9 - batch 699: 0.30 (2019-08-04 13:38:27.100185)\n",
            "Accuracy at step 9 - batch 699: 0.9032\n",
            "training loss at step 9 - batch 700: 0.32 (2019-08-04 13:38:27.115034)\n",
            "Accuracy at step 9 - batch 700: 0.9052\n",
            "training loss at step 9 - batch 701: 0.29 (2019-08-04 13:38:27.241883)\n",
            "Accuracy at step 9 - batch 701: 0.9128\n",
            "training loss at step 9 - batch 702: 0.30 (2019-08-04 13:38:27.257623)\n",
            "Accuracy at step 9 - batch 702: 0.9148\n",
            "training loss at step 9 - batch 703: 0.30 (2019-08-04 13:38:27.269407)\n",
            "Accuracy at step 9 - batch 703: 0.904\n",
            "training loss at step 9 - batch 704: 0.30 (2019-08-04 13:38:27.282092)\n",
            "Accuracy at step 9 - batch 704: 0.9088\n",
            "training loss at step 9 - batch 705: 0.31 (2019-08-04 13:38:27.294942)\n",
            "Accuracy at step 9 - batch 705: 0.9028\n",
            "training loss at step 9 - batch 706: 0.29 (2019-08-04 13:38:27.427624)\n",
            "Accuracy at step 9 - batch 706: 0.9132\n",
            "training loss at step 9 - batch 707: 0.31 (2019-08-04 13:38:27.439963)\n",
            "Accuracy at step 9 - batch 707: 0.908\n",
            "training loss at step 9 - batch 708: 0.31 (2019-08-04 13:38:27.453225)\n",
            "Accuracy at step 9 - batch 708: 0.9048\n",
            "training loss at step 9 - batch 709: 0.30 (2019-08-04 13:38:27.465210)\n",
            "Accuracy at step 9 - batch 709: 0.9124\n",
            "training loss at step 9 - batch 710: 0.31 (2019-08-04 13:38:27.477379)\n",
            "Accuracy at step 9 - batch 710: 0.904\n",
            "training loss at step 9 - batch 711: 0.30 (2019-08-04 13:38:27.596207)\n",
            "Accuracy at step 9 - batch 711: 0.9084\n",
            "training loss at step 9 - batch 712: 0.30 (2019-08-04 13:38:27.609539)\n",
            "Accuracy at step 9 - batch 712: 0.9084\n",
            "training loss at step 9 - batch 713: 0.31 (2019-08-04 13:38:27.621864)\n",
            "Accuracy at step 9 - batch 713: 0.906\n",
            "training loss at step 9 - batch 714: 0.30 (2019-08-04 13:38:27.637486)\n",
            "Accuracy at step 9 - batch 714: 0.9092\n",
            "training loss at step 9 - batch 715: 0.32 (2019-08-04 13:38:27.650045)\n",
            "Accuracy at step 9 - batch 715: 0.9048\n",
            "training loss at step 9 - batch 716: 0.32 (2019-08-04 13:38:27.776986)\n",
            "Accuracy at step 9 - batch 716: 0.9024\n",
            "training loss at step 9 - batch 717: 0.32 (2019-08-04 13:38:27.790465)\n",
            "Accuracy at step 9 - batch 717: 0.902\n",
            "training loss at step 9 - batch 718: 0.32 (2019-08-04 13:38:27.803482)\n",
            "Accuracy at step 9 - batch 718: 0.904\n",
            "training loss at step 9 - batch 719: 0.30 (2019-08-04 13:38:27.815751)\n",
            "Accuracy at step 9 - batch 719: 0.9136\n",
            "training loss at step 9 - batch 720: 0.29 (2019-08-04 13:38:27.828082)\n",
            "Accuracy at step 9 - batch 720: 0.9092\n",
            "training loss at step 9 - batch 721: 0.31 (2019-08-04 13:38:27.951857)\n",
            "Accuracy at step 9 - batch 721: 0.906\n",
            "training loss at step 9 - batch 722: 0.31 (2019-08-04 13:38:27.967691)\n",
            "Accuracy at step 9 - batch 722: 0.9088\n",
            "training loss at step 9 - batch 723: 0.30 (2019-08-04 13:38:27.979990)\n",
            "Accuracy at step 9 - batch 723: 0.9108\n",
            "training loss at step 9 - batch 724: 0.29 (2019-08-04 13:38:27.993228)\n",
            "Accuracy at step 9 - batch 724: 0.9056\n",
            "training loss at step 9 - batch 725: 0.31 (2019-08-04 13:38:28.005232)\n",
            "Accuracy at step 9 - batch 725: 0.9072\n",
            "training loss at step 9 - batch 726: 0.28 (2019-08-04 13:38:28.138702)\n",
            "Accuracy at step 9 - batch 726: 0.9128\n",
            "training loss at step 9 - batch 727: 0.30 (2019-08-04 13:38:28.154214)\n",
            "Accuracy at step 9 - batch 727: 0.9052\n",
            "training loss at step 9 - batch 728: 0.31 (2019-08-04 13:38:28.169486)\n",
            "Accuracy at step 9 - batch 728: 0.9036\n",
            "training loss at step 9 - batch 729: 0.28 (2019-08-04 13:38:28.181603)\n",
            "Accuracy at step 9 - batch 729: 0.9164\n",
            "training loss at step 9 - batch 730: 0.33 (2019-08-04 13:38:28.194640)\n",
            "Accuracy at step 9 - batch 730: 0.9004\n",
            "training loss at step 9 - batch 731: 0.31 (2019-08-04 13:38:28.319309)\n",
            "Accuracy at step 9 - batch 731: 0.9036\n",
            "training loss at step 9 - batch 732: 0.30 (2019-08-04 13:38:28.336705)\n",
            "Accuracy at step 9 - batch 732: 0.9088\n",
            "training loss at step 9 - batch 733: 0.32 (2019-08-04 13:38:28.349609)\n",
            "Accuracy at step 9 - batch 733: 0.9036\n",
            "training loss at step 9 - batch 734: 0.30 (2019-08-04 13:38:28.366396)\n",
            "Accuracy at step 9 - batch 734: 0.9088\n",
            "training loss at step 9 - batch 735: 0.32 (2019-08-04 13:38:28.379639)\n",
            "Accuracy at step 9 - batch 735: 0.9056\n",
            "training loss at step 9 - batch 736: 0.32 (2019-08-04 13:38:28.503884)\n",
            "Accuracy at step 9 - batch 736: 0.9032\n",
            "training loss at step 9 - batch 737: 0.34 (2019-08-04 13:38:28.516642)\n",
            "Accuracy at step 9 - batch 737: 0.8976\n",
            "training loss at step 9 - batch 738: 0.31 (2019-08-04 13:38:28.530048)\n",
            "Accuracy at step 9 - batch 738: 0.9104\n",
            "training loss at step 9 - batch 739: 0.31 (2019-08-04 13:38:28.543226)\n",
            "Accuracy at step 9 - batch 739: 0.9064\n",
            "training loss at step 9 - batch 740: 0.31 (2019-08-04 13:38:28.556227)\n",
            "Accuracy at step 9 - batch 740: 0.9024\n",
            "training loss at step 9 - batch 741: 0.30 (2019-08-04 13:38:28.674692)\n",
            "Accuracy at step 9 - batch 741: 0.9096\n",
            "training loss at step 9 - batch 742: 0.30 (2019-08-04 13:38:28.688960)\n",
            "Accuracy at step 9 - batch 742: 0.9064\n",
            "training loss at step 9 - batch 743: 0.32 (2019-08-04 13:38:28.701998)\n",
            "Accuracy at step 9 - batch 743: 0.8996\n",
            "training loss at step 9 - batch 744: 0.29 (2019-08-04 13:38:28.713554)\n",
            "Accuracy at step 9 - batch 744: 0.9084\n",
            "training loss at step 9 - batch 745: 0.33 (2019-08-04 13:38:28.726328)\n",
            "Accuracy at step 9 - batch 745: 0.9024\n",
            "training loss at step 9 - batch 746: 0.29 (2019-08-04 13:38:28.847315)\n",
            "Accuracy at step 9 - batch 746: 0.9124\n",
            "training loss at step 9 - batch 747: 0.30 (2019-08-04 13:38:28.865100)\n",
            "Accuracy at step 9 - batch 747: 0.9104\n",
            "training loss at step 9 - batch 748: 0.30 (2019-08-04 13:38:28.880354)\n",
            "Accuracy at step 9 - batch 748: 0.9076\n",
            "training loss at step 9 - batch 749: 0.31 (2019-08-04 13:38:28.895457)\n",
            "Accuracy at step 9 - batch 749: 0.9088\n",
            "training loss at step 9 - batch 750: 0.32 (2019-08-04 13:38:28.908061)\n",
            "Accuracy at step 9 - batch 750: 0.9064\n",
            "training loss at step 9 - batch 751: 0.31 (2019-08-04 13:38:29.029291)\n",
            "Accuracy at step 9 - batch 751: 0.908\n",
            "training loss at step 9 - batch 752: 0.30 (2019-08-04 13:38:29.042738)\n",
            "Accuracy at step 9 - batch 752: 0.9068\n",
            "training loss at step 9 - batch 753: 0.31 (2019-08-04 13:38:29.055383)\n",
            "Accuracy at step 9 - batch 753: 0.9096\n",
            "training loss at step 9 - batch 754: 0.32 (2019-08-04 13:38:29.067760)\n",
            "Accuracy at step 9 - batch 754: 0.908\n",
            "training loss at step 9 - batch 755: 0.31 (2019-08-04 13:38:29.080322)\n",
            "Accuracy at step 9 - batch 755: 0.9096\n",
            "training loss at step 9 - batch 756: 0.28 (2019-08-04 13:38:29.213308)\n",
            "Accuracy at step 9 - batch 756: 0.9188\n",
            "training loss at step 9 - batch 757: 0.32 (2019-08-04 13:38:29.227411)\n",
            "Accuracy at step 9 - batch 757: 0.9052\n",
            "training loss at step 9 - batch 758: 0.32 (2019-08-04 13:38:29.239561)\n",
            "Accuracy at step 9 - batch 758: 0.9088\n",
            "training loss at step 9 - batch 759: 0.30 (2019-08-04 13:38:29.255158)\n",
            "Accuracy at step 9 - batch 759: 0.9132\n",
            "training loss at step 9 - batch 760: 0.30 (2019-08-04 13:38:29.267575)\n",
            "Accuracy at step 9 - batch 760: 0.9088\n",
            "training loss at step 9 - batch 761: 0.31 (2019-08-04 13:38:29.390545)\n",
            "Accuracy at step 9 - batch 761: 0.9088\n",
            "training loss at step 9 - batch 762: 0.30 (2019-08-04 13:38:29.403108)\n",
            "Accuracy at step 9 - batch 762: 0.91\n",
            "training loss at step 9 - batch 763: 0.31 (2019-08-04 13:38:29.422337)\n",
            "Accuracy at step 9 - batch 763: 0.9056\n",
            "training loss at step 9 - batch 764: 0.30 (2019-08-04 13:38:29.440309)\n",
            "Accuracy at step 9 - batch 764: 0.9044\n",
            "training loss at step 9 - batch 765: 0.31 (2019-08-04 13:38:29.456920)\n",
            "Accuracy at step 9 - batch 765: 0.9112\n",
            "training loss at step 9 - batch 766: 0.30 (2019-08-04 13:38:29.574202)\n",
            "Accuracy at step 9 - batch 766: 0.9032\n",
            "training loss at step 9 - batch 767: 0.29 (2019-08-04 13:38:29.590268)\n",
            "Accuracy at step 9 - batch 767: 0.912\n",
            "training loss at step 9 - batch 768: 0.30 (2019-08-04 13:38:29.602353)\n",
            "Accuracy at step 9 - batch 768: 0.908\n",
            "training loss at step 9 - batch 769: 0.32 (2019-08-04 13:38:29.614452)\n",
            "Accuracy at step 9 - batch 769: 0.9028\n",
            "training loss at step 9 - batch 770: 0.30 (2019-08-04 13:38:29.628255)\n",
            "Accuracy at step 9 - batch 770: 0.908\n",
            "training loss at step 9 - batch 771: 0.28 (2019-08-04 13:38:29.748750)\n",
            "Accuracy at step 9 - batch 771: 0.9164\n",
            "training loss at step 9 - batch 772: 0.29 (2019-08-04 13:38:29.766423)\n",
            "Accuracy at step 9 - batch 772: 0.9124\n",
            "training loss at step 9 - batch 773: 0.30 (2019-08-04 13:38:29.778604)\n",
            "Accuracy at step 9 - batch 773: 0.9116\n",
            "training loss at step 9 - batch 774: 0.30 (2019-08-04 13:38:29.790868)\n",
            "Accuracy at step 9 - batch 774: 0.9068\n",
            "training loss at step 9 - batch 775: 0.31 (2019-08-04 13:38:29.806740)\n",
            "Accuracy at step 9 - batch 775: 0.9096\n",
            "training loss at step 9 - batch 776: 0.32 (2019-08-04 13:38:29.934560)\n",
            "Accuracy at step 9 - batch 776: 0.9032\n",
            "training loss at step 9 - batch 777: 0.31 (2019-08-04 13:38:29.951792)\n",
            "Accuracy at step 9 - batch 777: 0.9048\n",
            "training loss at step 9 - batch 778: 0.29 (2019-08-04 13:38:29.966291)\n",
            "Accuracy at step 9 - batch 778: 0.9156\n",
            "training loss at step 9 - batch 779: 0.30 (2019-08-04 13:38:29.979132)\n",
            "Accuracy at step 9 - batch 779: 0.9096\n",
            "Accuracy at step 9 - batch 0: [0.91]\n",
            "Accuracy at step 9 - batch 1: [0.912]\n",
            "Accuracy at step 9 - batch 2: [0.9088]\n",
            "Accuracy at step 9 - batch 3: [0.9092]\n",
            "Accuracy at step 9 - batch 4: [0.9]\n",
            "Accuracy at step 9 - batch 5: [0.8988]\n",
            "Accuracy at step 9 - batch 6: [0.9012]\n",
            "Accuracy at step 9 - batch 7: [0.9012]\n",
            "Accuracy at step 9 - batch 8: [0.9024]\n",
            "Accuracy at step 9 - batch 9: [0.9052]\n",
            "Accuracy at step 9 - batch 10: [0.9032]\n",
            "Accuracy at step 9 - batch 11: [0.9096]\n",
            "Accuracy at step 9 - batch 12: [0.906]\n",
            "Accuracy at step 9 - batch 13: [0.9032]\n",
            "Accuracy at step 9 - batch 14: [0.9104]\n",
            "Accuracy at step 9 - batch 15: [0.9064]\n",
            "Accuracy at step 9 - batch 16: [0.8964]\n",
            "Accuracy at step 9 - batch 17: [0.9068]\n",
            "Accuracy at step 9 - batch 18: [0.8956]\n",
            "Accuracy at step 9 - batch 19: [0.8996]\n",
            "Accuracy at step 9 - batch 20: [0.9044]\n",
            "Accuracy at step 9 - batch 21: [0.8988]\n",
            "Accuracy at step 9 - batch 22: [0.9032]\n",
            "Accuracy at step 9 - batch 23: [0.9124]\n",
            "Accuracy at step 9 - batch 24: [0.9056]\n",
            "Accuracy at step 9 - batch 25: [0.9016]\n",
            "Accuracy at step 9 - batch 26: [0.9024]\n",
            "Accuracy at step 9 - batch 27: [0.9076]\n",
            "Accuracy at step 9 - batch 28: [0.9072]\n",
            "Accuracy at step 9 - batch 29: [0.8988]\n",
            "Accuracy at step 9 - batch 30: [0.9008]\n",
            "Accuracy at step 9 - batch 31: [0.9136]\n",
            "Accuracy at step 9 - batch 32: [0.9088]\n",
            "Accuracy at step 9 - batch 33: [0.9004]\n",
            "Accuracy at step 9 - batch 34: [0.9072]\n",
            "Accuracy at step 9 - batch 35: [0.9056]\n",
            "Accuracy at step 9 - batch 36: [0.906]\n",
            "Accuracy at step 9 - batch 37: [0.9068]\n",
            "Accuracy at step 9 - batch 38: [0.9032]\n",
            "Accuracy at step 9 - batch 39: [0.9076]\n",
            "Accuracy at step 9 - batch 40: [0.9096]\n",
            "Accuracy at step 9 - batch 41: [0.9032]\n",
            "Accuracy at step 9 - batch 42: [0.902]\n",
            "Accuracy at step 9 - batch 43: [0.906]\n",
            "Accuracy at step 9 - batch 44: [0.9048]\n",
            "Accuracy at step 9 - batch 45: [0.904]\n",
            "Accuracy at step 9 - batch 46: [0.9016]\n",
            "Accuracy at step 9 - batch 47: [0.9016]\n",
            "Accuracy at step 9 - batch 48: [0.9064]\n",
            "Accuracy at step 9 - batch 49: [0.8996]\n",
            "Accuracy at step 9 - batch 50: [0.9032]\n",
            "Accuracy at step 9 - batch 51: [0.9076]\n",
            "Accuracy at step 9 - batch 52: [0.9008]\n",
            "Accuracy at step 9 - batch 53: [0.9128]\n",
            "Accuracy at step 9 - batch 54: [0.9076]\n",
            "Accuracy at step 9 - batch 55: [0.9044]\n",
            "Accuracy at step 9 - batch 56: [0.9004]\n",
            "Accuracy at step 9 - batch 57: [0.9016]\n",
            "Accuracy at step 9 - batch 58: [0.9052]\n",
            "Accuracy at step 9 - batch 59: [0.9024]\n",
            "Accuracy at step 9 - batch 60: [0.904]\n",
            "Accuracy at step 9 - batch 61: [0.9068]\n",
            "Accuracy at step 9 - batch 62: [0.91]\n",
            "Accuracy at step 9 - batch 63: [0.906]\n",
            "Accuracy at step 9 - batch 64: [0.9024]\n",
            "Accuracy at step 9 - batch 65: [0.9124]\n",
            "Accuracy at step 9 - batch 66: [0.9064]\n",
            "Accuracy at step 9 - batch 67: [0.9072]\n",
            "Accuracy at step 9 - batch 68: [0.9008]\n",
            "Accuracy at step 9 - batch 69: [0.9036]\n",
            "Accuracy at step 9 - batch 70: [0.9148]\n",
            "Accuracy at step 9 - batch 71: [0.9008]\n",
            "Accuracy at step 9 - batch 72: [0.9108]\n",
            "Accuracy at step 9 - batch 73: [0.9044]\n",
            "Accuracy at step 9 - batch 74: [0.9056]\n",
            "Accuracy at step 9 - batch 75: [0.904]\n",
            "Accuracy at step 9 - batch 76: [0.8992]\n",
            "Accuracy at step 9 - batch 77: [0.904]\n",
            "Accuracy at step 9 - batch 78: [0.9036]\n",
            "Accuracy at step 9 - batch 79: [0.9068]\n",
            "Accuracy at step 9 - batch 80: [0.9052]\n",
            "Accuracy at step 9 - batch 81: [0.898]\n",
            "Accuracy at step 9 - batch 82: [0.9064]\n",
            "Accuracy at step 9 - batch 83: [0.9068]\n",
            "Accuracy at step 9 - batch 84: [0.9004]\n",
            "Accuracy at step 9 - batch 85: [0.9008]\n",
            "Accuracy at step 9 - batch 86: [0.9088]\n",
            "Accuracy at step 9 - batch 87: [0.8996]\n",
            "Accuracy at step 9 - batch 88: [0.916]\n",
            "Accuracy at step 9 - batch 89: [0.9084]\n",
            "Accuracy at step 9 - batch 90: [0.9024]\n",
            "Accuracy at step 9 - batch 91: [0.906]\n",
            "Accuracy at step 9 - batch 92: [0.9032]\n",
            "Accuracy at step 9 - batch 93: [0.902]\n",
            "Accuracy at step 9 - batch 94: [0.9048]\n",
            "Accuracy at step 9 - batch 95: [0.9084]\n",
            "Accuracy at step 9 - batch 96: [0.904]\n",
            "Accuracy at step 9 - batch 97: [0.9004]\n",
            "Accuracy at step 9 - batch 98: [0.9004]\n",
            "Accuracy at step 9 - batch 99: [0.9068]\n",
            "Accuracy at step 9 - batch 100: [0.9012]\n",
            "Accuracy at step 9 - batch 101: [0.906]\n",
            "Accuracy at step 9 - batch 102: [0.9044]\n",
            "Accuracy at step 9 - batch 103: [0.9044]\n",
            "Accuracy at step 9 - batch 104: [0.898]\n",
            "Accuracy at step 9 - batch 105: [0.8984]\n",
            "Accuracy at step 9 - batch 106: [0.9056]\n",
            "Accuracy at step 9 - batch 107: [0.9032]\n",
            "Accuracy at step 9 - batch 108: [0.9]\n",
            "Accuracy at step 9 - batch 109: [0.9136]\n",
            "Accuracy at step 9 - batch 110: [0.906]\n",
            "Accuracy at step 9 - batch 111: [0.9056]\n",
            "Accuracy at step 9 - batch 112: [0.9076]\n",
            "Accuracy at step 9 - batch 113: [0.9036]\n",
            "Accuracy at step 9 - batch 114: [0.9076]\n",
            "Accuracy at step 9 - batch 115: [0.8956]\n",
            "Accuracy at step 9 - batch 116: [0.8984]\n",
            "Accuracy at step 9 - batch 117: [0.9068]\n",
            "Accuracy at step 9 - batch 118: [0.9048]\n",
            "Accuracy at step 9 - batch 119: [0.9012]\n",
            "Accuracy at step 9 - batch 120: [0.9044]\n",
            "Accuracy at step 9 - batch 121: [0.908]\n",
            "Accuracy at step 9 - batch 122: [0.8924]\n",
            "Accuracy at step 9 - batch 123: [0.9028]\n",
            "Accuracy at step 9 - batch 124: [0.9076]\n",
            "Accuracy at step 9 - batch 125: [0.9008]\n",
            "Accuracy at step 9 - batch 126: [0.9012]\n",
            "Accuracy at step 9 - batch 127: [0.9064]\n",
            "Accuracy at step 9 - batch 128: [0.9012]\n",
            "Accuracy at step 9 - batch 129: [0.9056]\n",
            "Accuracy at step 9 - batch 130: [0.8956]\n",
            "Accuracy at step 9 - batch 131: [0.906]\n",
            "Accuracy at step 9 - batch 132: [0.9016]\n",
            "Accuracy at step 9 - batch 133: [0.9024]\n",
            "Accuracy at step 9 - batch 134: [0.908]\n",
            "Accuracy at step 9 - batch 135: [0.9016]\n",
            "Accuracy at step 9 - batch 136: [0.9016]\n",
            "Accuracy at step 9 - batch 137: [0.9]\n",
            "Accuracy at step 9 - batch 138: [0.9028]\n",
            "Accuracy at step 9 - batch 139: [0.9124]\n",
            "Accuracy at step 9 - batch 140: [0.8964]\n",
            "Accuracy at step 9 - batch 141: [0.9072]\n",
            "Accuracy at step 9 - batch 142: [0.9064]\n",
            "Accuracy at step 9 - batch 143: [0.902]\n",
            "Accuracy at step 9 - batch 144: [0.9004]\n",
            "Accuracy at step 9 - batch 145: [0.9052]\n",
            "Accuracy at step 9 - batch 146: [0.9016]\n",
            "Accuracy at step 9 - batch 147: [0.9028]\n",
            "Accuracy at step 9 - batch 148: [0.904]\n",
            "Accuracy at step 9 - batch 149: [0.9048]\n",
            "Accuracy at step 9 - batch 150: [0.9076]\n",
            "Accuracy at step 9 - batch 151: [0.9068]\n",
            "Accuracy at step 9 - batch 152: [0.9048]\n",
            "Accuracy at step 9 - batch 153: [0.9036]\n",
            "Accuracy at step 9 - batch 154: [0.9016]\n",
            "Accuracy at step 9 - batch 155: [0.904]\n",
            "Accuracy at step 9 - batch 156: [0.9052]\n",
            "Accuracy at step 9 - batch 157: [0.91]\n",
            "Accuracy at step 9 - batch 158: [0.9104]\n",
            "Accuracy at step 9 - batch 159: [0.906]\n",
            "Accuracy at step 9 - batch 160: [0.9052]\n",
            "Accuracy at step 9 - batch 161: [0.9092]\n",
            "Accuracy at step 9 - batch 162: [0.9024]\n",
            "Accuracy at step 9 - batch 163: [0.902]\n",
            "Accuracy at step 9 - batch 164: [0.9016]\n",
            "Accuracy at step 9 - batch 165: [0.906]\n",
            "Accuracy at step 9 - batch 166: [0.9064]\n",
            "Accuracy at step 9 - batch 167: [0.9068]\n",
            "Accuracy at step 9 - batch 168: [0.9048]\n",
            "Accuracy at step 9 - batch 169: [0.9044]\n",
            "Accuracy at step 9 - batch 170: [0.9088]\n",
            "Accuracy at step 9 - batch 171: [0.9028]\n",
            "Accuracy at step 9 - batch 172: [0.9068]\n",
            "Accuracy at step 9 - batch 173: [0.908]\n",
            "Accuracy at step 9 - batch 174: [0.9076]\n",
            "Accuracy at step 9 - batch 175: [0.9072]\n",
            "Accuracy at step 9 - batch 176: [0.9016]\n",
            "Accuracy at step 9 - batch 177: [0.9124]\n",
            "Accuracy at step 9 - batch 178: [0.906]\n",
            "Accuracy at step 9 - batch 179: [0.9024]\n",
            "Accuracy at step 9 - batch 180: [0.9052]\n",
            "Accuracy at step 9 - batch 181: [0.904]\n",
            "Accuracy at step 9 - batch 182: [0.9]\n",
            "Accuracy at step 9 - batch 183: [0.9024]\n",
            "Accuracy at step 9 - batch 184: [0.904]\n",
            "Accuracy at step 9 - batch 185: [0.9032]\n",
            "Accuracy at step 9 - batch 186: [0.9072]\n",
            "Accuracy at step 9 - batch 187: [0.9048]\n",
            "Accuracy at step 9 - batch 188: [0.9036]\n",
            "Accuracy at step 9 - batch 189: [0.9024]\n",
            "Accuracy at step 9 - batch 190: [0.9016]\n",
            "Accuracy at step 9 - batch 191: [0.9032]\n",
            "Accuracy at step 9 - batch 192: [0.8948]\n",
            "Accuracy at step 9 - batch 193: [0.9072]\n",
            "Accuracy at step 9 - batch 194: [0.9084]\n",
            "Accuracy at step 9 - batch 195: [0.9064]\n",
            "Accuracy at step 9 - batch 196: [0.9012]\n",
            "Accuracy at step 9 - batch 197: [0.8996]\n",
            "Accuracy at step 9 - batch 198: [0.9056]\n",
            "Accuracy at step 9 - batch 199: [0.9068]\n",
            "Accuracy at step 9 - batch 200: [0.902]\n",
            "Accuracy at step 9 - batch 201: [0.8948]\n",
            "Accuracy at step 9 - batch 202: [0.9004]\n",
            "Accuracy at step 9 - batch 203: [0.9084]\n",
            "Accuracy at step 9 - batch 204: [0.9056]\n",
            "Accuracy at step 9 - batch 205: [0.9008]\n",
            "Accuracy at step 9 - batch 206: [0.902]\n",
            "Accuracy at step 9 - batch 207: [0.9064]\n",
            "Accuracy at step 9 - batch 208: [0.9052]\n",
            "Accuracy at step 9 - batch 209: [0.9108]\n",
            "Accuracy at step 9 - batch 210: [0.9048]\n",
            "Accuracy at step 9 - batch 211: [0.9]\n",
            "Accuracy at step 9 - batch 212: [0.9072]\n",
            "Accuracy at step 9 - batch 213: [0.9028]\n",
            "Accuracy at step 9 - batch 214: [0.9112]\n",
            "Accuracy at step 9 - batch 215: [0.9012]\n",
            "Accuracy at step 9 - batch 216: [0.9068]\n",
            "Accuracy at step 9 - batch 217: [0.8992]\n",
            "Accuracy at step 9 - batch 218: [0.9056]\n",
            "Accuracy at step 9 - batch 219: [0.902]\n",
            "Accuracy at step 9 - batch 220: [0.9056]\n",
            "Accuracy at step 9 - batch 221: [0.9004]\n",
            "Accuracy at step 9 - batch 222: [0.91]\n",
            "Accuracy at step 9 - batch 223: [0.904]\n",
            "Accuracy at step 9 - batch 224: [0.904]\n",
            "Accuracy at step 9 - batch 225: [0.906]\n",
            "Accuracy at step 9 - batch 226: [0.902]\n",
            "Accuracy at step 9 - batch 227: [0.9004]\n",
            "Accuracy at step 9 - batch 228: [0.9056]\n",
            "Accuracy at step 9 - batch 229: [0.9056]\n",
            "Accuracy at step 9 - batch 230: [0.9012]\n",
            "Accuracy at step 9 - batch 231: [0.9092]\n",
            "Accuracy at step 9 - batch 232: [0.902]\n",
            "Accuracy at step 9 - batch 233: [0.9112]\n",
            "Accuracy at step 9 - batch 234: [0.9036]\n",
            "Accuracy at step 9 - batch 235: [0.9076]\n",
            "Accuracy at step 9 - batch 236: [0.9044]\n",
            "Accuracy at step 9 - batch 237: [0.908]\n",
            "Accuracy at step 9 - batch 238: [0.904]\n",
            "Accuracy at step 9 - batch 239: [0.9052]\n",
            "Accuracy at step 9 - batch 240: [0.9104]\n",
            "Accuracy at step 9 - batch 241: [0.9052]\n",
            "Accuracy at step 9 - batch 242: [0.9004]\n",
            "Accuracy at step 9 - batch 243: [0.9112]\n",
            "Accuracy at step 9 - batch 244: [0.9032]\n",
            "Accuracy at step 9 - batch 245: [0.9076]\n",
            "Accuracy at step 9 - batch 246: [0.9016]\n",
            "Accuracy at step 9 - batch 247: [0.9044]\n",
            "Accuracy at step 9 - batch 248: [0.9028]\n",
            "Accuracy at step 9 - batch 249: [0.9004]\n",
            "Accuracy at step 9 - batch 250: [0.9056]\n",
            "Accuracy at step 9 - batch 251: [0.9128]\n",
            "Accuracy at step 9 - batch 252: [0.9072]\n",
            "Accuracy at step 9 - batch 253: [0.9104]\n",
            "Accuracy at step 9 - batch 254: [0.9012]\n",
            "Accuracy at step 9 - batch 255: [0.9048]\n",
            "Accuracy at step 9 - batch 256: [0.9052]\n",
            "Accuracy at step 9 - batch 257: [0.9068]\n",
            "Accuracy at step 9 - batch 258: [0.8984]\n",
            "Accuracy at step 9 - batch 259: [0.9024]\n",
            "Accuracy at step 9 - batch 260: [0.9008]\n",
            "Accuracy at step 9 - batch 261: [0.9052]\n",
            "Accuracy at step 9 - batch 262: [0.9068]\n",
            "Accuracy at step 9 - batch 263: [0.9024]\n",
            "Accuracy at step 9 - batch 264: [0.902]\n",
            "Accuracy at step 9 - batch 265: [0.9032]\n",
            "Accuracy at step 9 - batch 266: [0.8968]\n",
            "Accuracy at step 9 - batch 267: [0.9168]\n",
            "Accuracy at step 9 - batch 268: [0.908]\n",
            "Accuracy at step 9 - batch 269: [0.9092]\n",
            "Accuracy at step 9 - batch 270: [0.904]\n",
            "Accuracy at step 9 - batch 271: [0.8976]\n",
            "Accuracy at step 9 - batch 272: [0.9004]\n",
            "Accuracy at step 9 - batch 273: [0.9032]\n",
            "Accuracy at step 9 - batch 274: [0.902]\n",
            "Accuracy at step 9 - batch 275: [0.9064]\n",
            "Accuracy at step 9 - batch 276: [0.906]\n",
            "Accuracy at step 9 - batch 277: [0.9092]\n",
            "Accuracy at step 9 - batch 278: [0.89]\n",
            "Accuracy at step 9 - batch 279: [0.8976]\n",
            "Accuracy at step 9 - batch 280: [0.9024]\n",
            "Accuracy at step 9 - batch 281: [0.9028]\n",
            "Accuracy at step 9 - batch 282: [0.91]\n",
            "Accuracy at step 9 - batch 283: [0.9096]\n",
            "Accuracy at step 9 - batch 284: [0.9064]\n",
            "Accuracy at step 9 - batch 285: [0.9024]\n",
            "Accuracy at step 9 - batch 286: [0.904]\n",
            "Accuracy at step 9 - batch 287: [0.9092]\n",
            "Accuracy at step 9 - batch 288: [0.9052]\n",
            "Accuracy at step 9 - batch 289: [0.9096]\n",
            "Accuracy at step 9 - batch 290: [0.904]\n",
            "Accuracy at step 9 - batch 291: [0.904]\n",
            "Accuracy at step 9 - batch 292: [0.9036]\n",
            "Accuracy at step 9 - batch 293: [0.9116]\n",
            "Accuracy at step 9 - batch 294: [0.9032]\n",
            "Accuracy at step 9 - batch 295: [0.912]\n",
            "Accuracy at step 9 - batch 296: [0.9084]\n",
            "Accuracy at step 9 - batch 297: [0.9076]\n",
            "Accuracy at step 9 - batch 298: [0.9096]\n",
            "Accuracy at step 9 - batch 299: [0.9088]\n",
            "Accuracy at step 9 - batch 300: [0.9008]\n",
            "Accuracy at step 9 - batch 301: [0.9092]\n",
            "Accuracy at step 9 - batch 302: [0.9008]\n",
            "Accuracy at step 9 - batch 303: [0.9036]\n",
            "Accuracy at step 9 - batch 304: [0.9044]\n",
            "Accuracy at step 9 - batch 305: [0.8956]\n",
            "Accuracy at step 9 - batch 306: [0.9028]\n",
            "Accuracy at step 9 - batch 307: [0.9044]\n",
            "Accuracy at step 9 - batch 308: [0.9056]\n",
            "Accuracy at step 9 - batch 309: [0.8996]\n",
            "Accuracy at step 9 - batch 310: [0.9144]\n",
            "Accuracy at step 9 - batch 311: [0.9044]\n",
            "Accuracy at step 9 - batch 312: [0.9048]\n",
            "Accuracy at step 9 - batch 313: [0.9112]\n",
            "Accuracy at step 9 - batch 314: [0.9004]\n",
            "Accuracy at step 9 - batch 315: [0.902]\n",
            "Accuracy at step 9 - batch 316: [0.902]\n",
            "Accuracy at step 9 - batch 317: [0.9076]\n",
            "Accuracy at step 9 - batch 318: [0.9008]\n",
            "Accuracy at step 9 - batch 319: [0.9028]\n",
            "Accuracy at step 9 - batch 320: [0.9116]\n",
            "Accuracy at step 9 - batch 321: [0.9024]\n",
            "Accuracy at step 9 - batch 322: [0.9052]\n",
            "Accuracy at step 9 - batch 323: [0.9116]\n",
            "Accuracy at step 9 - batch 324: [0.9012]\n",
            "Accuracy at step 9 - batch 325: [0.902]\n",
            "Accuracy at step 9 - batch 326: [0.888]\n",
            "Accuracy at step 9 - batch 327: [0.9024]\n",
            "Accuracy at step 9 - batch 328: [0.9088]\n",
            "Accuracy at step 9 - batch 329: [0.9068]\n",
            "Accuracy at step 9 - batch 330: [0.904]\n",
            "Accuracy at step 9 - batch 331: [0.9076]\n",
            "Accuracy at step 9 - batch 332: [0.9072]\n",
            "Accuracy at step 9 - batch 333: [0.9032]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MUO2tnc3sHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "101 % 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYmGs__LkqS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}